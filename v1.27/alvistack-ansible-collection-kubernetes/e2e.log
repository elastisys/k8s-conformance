  I0824 11:40:38.901252      13 e2e.go:117] Starting e2e run "da90e73d-ba27-48cb-8688-e9f6ef028ca1" on Ginkgo node 1
  Aug 24 11:40:38.969: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692877238 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Aug 24 11:40:39.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:40:39.313: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Aug 24 11:40:39.423: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Aug 24 11:40:39.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
  Aug 24 11:40:39.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
  Aug 24 11:40:39.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Aug 24 11:40:39.443: INFO: e2e test version: v1.27.5
  Aug 24 11:40:39.447: INFO: kube-apiserver version: v1.27.5
  Aug 24 11:40:39.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:40:39.455: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.149 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 08/24/23 11:40:39.97
  Aug 24 11:40:39.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:40:39.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:40.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:40.015
  STEP: Creating configMap with name configmap-projected-all-test-volume-914913ca-132b-4631-8072-46ee1b981cf3 @ 08/24/23 11:40:40.02
  STEP: Creating secret with name secret-projected-all-test-volume-19223b0b-5331-4940-b286-0cd3525051ac @ 08/24/23 11:40:40.029
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 08/24/23 11:40:40.039
  STEP: Saw pod success @ 08/24/23 11:40:44.086
  Aug 24 11:40:44.095: INFO: Trying to get logs from node pohje9aimahx-3 pod projected-volume-63467721-bd60-476e-bbd2-75fff46f6392 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:40:44.131
  Aug 24 11:40:44.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9253" for this suite. @ 08/24/23 11:40:44.174
• [4.224 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 08/24/23 11:40:44.195
  Aug 24 11:40:44.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 11:40:44.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:44.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:44.251
  STEP: Creating a pod to test service account token:  @ 08/24/23 11:40:44.256
  STEP: Saw pod success @ 08/24/23 11:40:56.333
  Aug 24 11:40:56.343: INFO: Trying to get logs from node pohje9aimahx-3 pod test-pod-0707c55a-cf7e-48d0-962b-212b6008315d container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 11:40:56.355
  Aug 24 11:40:56.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7935" for this suite. @ 08/24/23 11:40:56.385
• [12.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 08/24/23 11:40:56.402
  Aug 24 11:40:56.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 11:40:56.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:56.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:56.446
  STEP: Creating service test in namespace statefulset-2741 @ 08/24/23 11:40:56.452
  STEP: Creating a new StatefulSet @ 08/24/23 11:40:56.465
  Aug 24 11:40:56.490: INFO: Found 0 stateful pods, waiting for 3
  Aug 24 11:41:06.501: INFO: Found 2 stateful pods, waiting for 3
  Aug 24 11:41:16.502: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 11:41:16.503: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 11:41:16.503: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  Aug 24 11:41:26.504: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 11:41:26.504: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 11:41:26.505: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 11:41:26.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-2741 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 11:41:26.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 11:41:26.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 11:41:26.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/24/23 11:41:37.019
  Aug 24 11:41:37.048: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/24/23 11:41:37.049
  STEP: Updating Pods in reverse ordinal order @ 08/24/23 11:41:47.086
  Aug 24 11:41:47.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-2741 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 11:41:47.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 11:41:47.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 11:41:47.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 11:41:57.523: INFO: Waiting for StatefulSet statefulset-2741/ss2 to complete update
  Aug 24 11:41:57.524: INFO: Waiting for Pod statefulset-2741/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 24 11:41:57.524: INFO: Waiting for Pod statefulset-2741/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 24 11:42:07.539: INFO: Waiting for StatefulSet statefulset-2741/ss2 to complete update
  Aug 24 11:42:07.540: INFO: Waiting for Pod statefulset-2741/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 24 11:42:17.569: INFO: Waiting for StatefulSet statefulset-2741/ss2 to complete update
  Aug 24 11:42:17.570: INFO: Waiting for Pod statefulset-2741/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Rolling back to a previous revision @ 08/24/23 11:42:27.54
  Aug 24 11:42:27.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-2741 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 11:42:27.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 11:42:27.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 11:42:27.827: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 11:42:37.892: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 08/24/23 11:42:47.932
  Aug 24 11:42:47.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-2741 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 11:42:48.261: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 11:42:48.261: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 11:42:48.261: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 11:42:58.305: INFO: Deleting all statefulset in ns statefulset-2741
  Aug 24 11:42:58.310: INFO: Scaling statefulset ss2 to 0
  Aug 24 11:43:08.349: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 11:43:08.354: INFO: Deleting statefulset ss2
  Aug 24 11:43:08.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2741" for this suite. @ 08/24/23 11:43:08.386
• [132.001 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 08/24/23 11:43:08.407
  Aug 24 11:43:08.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:43:08.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:43:08.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:43:08.443
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-d5753c06-8bbb-4beb-997a-3aa091532494 @ 08/24/23 11:43:08.458
  STEP: Creating the pod @ 08/24/23 11:43:08.467
  STEP: Updating configmap projected-configmap-test-upd-d5753c06-8bbb-4beb-997a-3aa091532494 @ 08/24/23 11:43:10.534
  STEP: waiting to observe update in volume @ 08/24/23 11:43:10.544
  Aug 24 11:44:19.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2167" for this suite. @ 08/24/23 11:44:19.151
• [70.758 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 08/24/23 11:44:19.168
  Aug 24 11:44:19.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:44:19.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:44:19.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:44:19.205
  STEP: Creating the pod @ 08/24/23 11:44:19.21
  Aug 24 11:44:21.791: INFO: Successfully updated pod "annotationupdateadbc1eb9-9535-4458-8fde-4e1864c9b784"
  Aug 24 11:44:23.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8828" for this suite. @ 08/24/23 11:44:23.833
• [4.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 08/24/23 11:44:23.857
  Aug 24 11:44:23.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/24/23 11:44:23.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:44:23.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:44:23.895
  STEP: creating @ 08/24/23 11:44:23.901
  STEP: getting @ 08/24/23 11:44:23.976
  STEP: listing in namespace @ 08/24/23 11:44:23.982
  STEP: patching @ 08/24/23 11:44:24.003
  STEP: deleting @ 08/24/23 11:44:24.037
  Aug 24 11:44:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6186" for this suite. @ 08/24/23 11:44:24.068
• [0.225 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 08/24/23 11:44:24.083
  Aug 24 11:44:24.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename job @ 08/24/23 11:44:24.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:44:24.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:44:24.119
  STEP: Creating a job @ 08/24/23 11:44:24.123
  STEP: Ensuring active pods == parallelism @ 08/24/23 11:44:24.139
  STEP: delete a job @ 08/24/23 11:44:28.149
  STEP: deleting Job.batch foo in namespace job-245, will wait for the garbage collector to delete the pods @ 08/24/23 11:44:28.149
  Aug 24 11:44:28.217: INFO: Deleting Job.batch foo took: 12.585505ms
  Aug 24 11:44:28.318: INFO: Terminating Job.batch foo pods took: 100.396936ms
  STEP: Ensuring job was deleted @ 08/24/23 11:45:00.219
  Aug 24 11:45:00.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-245" for this suite. @ 08/24/23 11:45:00.233
• [36.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 08/24/23 11:45:00.261
  Aug 24 11:45:00.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 11:45:00.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:00.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:00.309
  STEP: Creating configMap with name configmap-test-volume-map-db354b5b-e460-49a9-8803-15e0b995ae99 @ 08/24/23 11:45:00.315
  STEP: Creating a pod to test consume configMaps @ 08/24/23 11:45:00.322
  STEP: Saw pod success @ 08/24/23 11:45:04.353
  Aug 24 11:45:04.361: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-61f64416-a2b4-48cb-8ca9-d446392cb5ca container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 11:45:04.374
  Aug 24 11:45:04.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3737" for this suite. @ 08/24/23 11:45:04.403
• [4.152 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 08/24/23 11:45:04.415
  Aug 24 11:45:04.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 11:45:04.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:04.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:04.451
  STEP: Read namespace status @ 08/24/23 11:45:04.455
  Aug 24 11:45:04.465: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 08/24/23 11:45:04.465
  Aug 24 11:45:04.476: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 08/24/23 11:45:04.476
  Aug 24 11:45:04.497: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Aug 24 11:45:04.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-527" for this suite. @ 08/24/23 11:45:04.506
• [0.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 08/24/23 11:45:04.55
  Aug 24 11:45:04.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 11:45:04.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:04.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:04.586
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 11:45:04.6
  STEP: create the pod with lifecycle hook @ 08/24/23 11:45:18.712
  STEP: delete the pod with lifecycle hook @ 08/24/23 11:45:20.776
  STEP: check prestop hook @ 08/24/23 11:45:24.837
  Aug 24 11:45:24.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1573" for this suite. @ 08/24/23 11:45:24.902
• [20.366 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 08/24/23 11:45:24.917
  Aug 24 11:45:24.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 11:45:24.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:24.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:24.961
  Aug 24 11:45:24.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: creating the pod @ 08/24/23 11:45:24.976
  STEP: submitting the pod to kubernetes @ 08/24/23 11:45:24.978
  Aug 24 11:45:29.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2467" for this suite. @ 08/24/23 11:45:29.188
• [4.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 08/24/23 11:45:29.204
  Aug 24 11:45:29.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl-logs @ 08/24/23 11:45:29.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:29.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:29.247
  STEP: creating an pod @ 08/24/23 11:45:29.252
  Aug 24 11:45:29.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Aug 24 11:45:29.450: INFO: stderr: ""
  Aug 24 11:45:29.450: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 08/24/23 11:45:29.45
  Aug 24 11:45:29.450: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Aug 24 11:45:31.472: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 08/24/23 11:45:31.472
  Aug 24 11:45:31.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator'
  Aug 24 11:45:31.645: INFO: stderr: ""
  Aug 24 11:45:31.645: INFO: stdout: "I0824 11:45:30.548272       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vdt 445\nI0824 11:45:30.748311       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/qtq 579\nI0824 11:45:30.948674       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/62l 333\nI0824 11:45:31.147932       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/h6tp 594\nI0824 11:45:31.348384       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ffw 567\nI0824 11:45:31.548371       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/9w4j 222\n"
  Aug 24 11:45:33.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator'
  Aug 24 11:45:33.816: INFO: stderr: ""
  Aug 24 11:45:33.816: INFO: stdout: "I0824 11:45:30.548272       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vdt 445\nI0824 11:45:30.748311       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/qtq 579\nI0824 11:45:30.948674       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/62l 333\nI0824 11:45:31.147932       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/h6tp 594\nI0824 11:45:31.348384       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ffw 567\nI0824 11:45:31.548371       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/9w4j 222\nI0824 11:45:31.748878       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/tf5n 568\nI0824 11:45:31.948429       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/n5d 257\nI0824 11:45:32.147894       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jrf 488\nI0824 11:45:32.348569       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/v7gj 511\nI0824 11:45:32.548950       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/v2xm 469\nI0824 11:45:32.748449       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/2w4l 410\nI0824 11:45:32.947904       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/fzfl 577\nI0824 11:45:33.148289       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/xfz 365\nI0824 11:45:33.348733       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/78c 371\nI0824 11:45:33.547947       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/v9d9 522\nI0824 11:45:33.748966       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/tq2 341\n"
  STEP: limiting log lines @ 08/24/23 11:45:33.816
  Aug 24 11:45:33.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator --tail=1'
  Aug 24 11:45:33.961: INFO: stderr: ""
  Aug 24 11:45:33.961: INFO: stdout: "I0824 11:45:33.948233       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/nrdb 221\n"
  Aug 24 11:45:33.961: INFO: got output "I0824 11:45:33.948233       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/nrdb 221\n"
  STEP: limiting log bytes @ 08/24/23 11:45:33.961
  Aug 24 11:45:33.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator --limit-bytes=1'
  Aug 24 11:45:34.104: INFO: stderr: ""
  Aug 24 11:45:34.104: INFO: stdout: "I"
  Aug 24 11:45:34.104: INFO: got output "I"
  STEP: exposing timestamps @ 08/24/23 11:45:34.104
  Aug 24 11:45:34.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator --tail=1 --timestamps'
  Aug 24 11:45:34.269: INFO: stderr: ""
  Aug 24 11:45:34.269: INFO: stdout: "2023-08-24T11:45:34.148719268Z I0824 11:45:34.148650       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/k9j 294\n"
  Aug 24 11:45:34.269: INFO: got output "2023-08-24T11:45:34.148719268Z I0824 11:45:34.148650       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/k9j 294\n"
  STEP: restricting to a time range @ 08/24/23 11:45:34.269
  Aug 24 11:45:36.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator --since=1s'
  Aug 24 11:45:36.956: INFO: stderr: ""
  Aug 24 11:45:36.956: INFO: stdout: "I0824 11:45:36.148796       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/wt5 247\nI0824 11:45:36.348243       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/znw 511\nI0824 11:45:36.549065       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/bnq 574\nI0824 11:45:36.748738       1 logs_generator.go:76] 31 POST /api/v1/namespaces/ns/pods/tfl 208\nI0824 11:45:36.948277       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/dzh4 283\n"
  Aug 24 11:45:36.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 logs logs-generator logs-generator --since=24h'
  Aug 24 11:45:37.125: INFO: stderr: ""
  Aug 24 11:45:37.125: INFO: stdout: "I0824 11:45:30.548272       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/vdt 445\nI0824 11:45:30.748311       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/qtq 579\nI0824 11:45:30.948674       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/62l 333\nI0824 11:45:31.147932       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/h6tp 594\nI0824 11:45:31.348384       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/ffw 567\nI0824 11:45:31.548371       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/9w4j 222\nI0824 11:45:31.748878       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/tf5n 568\nI0824 11:45:31.948429       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/n5d 257\nI0824 11:45:32.147894       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jrf 488\nI0824 11:45:32.348569       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/v7gj 511\nI0824 11:45:32.548950       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/v2xm 469\nI0824 11:45:32.748449       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/2w4l 410\nI0824 11:45:32.947904       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/fzfl 577\nI0824 11:45:33.148289       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/xfz 365\nI0824 11:45:33.348733       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/78c 371\nI0824 11:45:33.547947       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/v9d9 522\nI0824 11:45:33.748966       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/tq2 341\nI0824 11:45:33.948233       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/nrdb 221\nI0824 11:45:34.148650       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/k9j 294\nI0824 11:45:34.348220       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/p4p 344\nI0824 11:45:34.548682       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/jwn 489\nI0824 11:45:34.747967       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/qnj 430\nI0824 11:45:34.948431       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/mfw 450\nI0824 11:45:35.148912       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/v7l 568\nI0824 11:45:35.348353       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/6fh 418\nI0824 11:45:35.548678       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/mg2 531\nI0824 11:45:35.747925       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/c4hl 491\nI0824 11:45:35.948361       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/cgdr 218\nI0824 11:45:36.148796       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/wt5 247\nI0824 11:45:36.348243       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/znw 511\nI0824 11:45:36.549065       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/bnq 574\nI0824 11:45:36.748738       1 logs_generator.go:76] 31 POST /api/v1/namespaces/ns/pods/tfl 208\nI0824 11:45:36.948277       1 logs_generator.go:76] 32 PUT /api/v1/namespaces/ns/pods/dzh4 283\n"
  Aug 24 11:45:37.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-logs-3573 delete pod logs-generator'
  Aug 24 11:45:38.106: INFO: stderr: ""
  Aug 24 11:45:38.106: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Aug 24 11:45:38.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-3573" for this suite. @ 08/24/23 11:45:38.113
• [8.921 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 08/24/23 11:45:38.128
  Aug 24 11:45:38.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 11:45:38.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:38.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:38.179
  STEP: creating the pod @ 08/24/23 11:45:38.186
  Aug 24 11:45:38.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 create -f -'
  Aug 24 11:45:40.059: INFO: stderr: ""
  Aug 24 11:45:40.059: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 08/24/23 11:45:44.092
  Aug 24 11:45:44.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 label pods pause testing-label=testing-label-value'
  Aug 24 11:45:44.289: INFO: stderr: ""
  Aug 24 11:45:44.290: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 08/24/23 11:45:44.29
  Aug 24 11:45:44.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 get pod pause -L testing-label'
  Aug 24 11:45:44.434: INFO: stderr: ""
  Aug 24 11:45:44.434: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 08/24/23 11:45:44.434
  Aug 24 11:45:44.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 label pods pause testing-label-'
  Aug 24 11:45:44.590: INFO: stderr: ""
  Aug 24 11:45:44.590: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 08/24/23 11:45:44.59
  Aug 24 11:45:44.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 get pod pause -L testing-label'
  Aug 24 11:45:44.727: INFO: stderr: ""
  Aug 24 11:45:44.727: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
  STEP: using delete to clean up resources @ 08/24/23 11:45:44.728
  Aug 24 11:45:44.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 delete --grace-period=0 --force -f -'
  Aug 24 11:45:44.902: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 11:45:44.902: INFO: stdout: "pod \"pause\" force deleted\n"
  Aug 24 11:45:44.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 get rc,svc -l name=pause --no-headers'
  Aug 24 11:45:45.102: INFO: stderr: "No resources found in kubectl-2589 namespace.\n"
  Aug 24 11:45:45.102: INFO: stdout: ""
  Aug 24 11:45:45.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2589 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 24 11:45:45.240: INFO: stderr: ""
  Aug 24 11:45:45.240: INFO: stdout: ""
  Aug 24 11:45:45.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2589" for this suite. @ 08/24/23 11:45:45.25
• [7.135 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 08/24/23 11:45:45.264
  Aug 24 11:45:45.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 11:45:45.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:45.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:45.297
  Aug 24 11:45:45.329: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 24 11:46:45.408: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/24/23 11:46:45.415
  Aug 24 11:46:45.487: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 24 11:46:45.500: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 24 11:46:45.600: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 24 11:46:45.621: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 24 11:46:45.703: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 24 11:46:45.725: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/24/23 11:46:45.725
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 08/24/23 11:46:49.791
  Aug 24 11:46:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7914" for this suite. @ 08/24/23 11:46:54.048
• [68.832 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 08/24/23 11:46:54.097
  Aug 24 11:46:54.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 11:46:54.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:46:54.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:46:54.173
  Aug 24 11:46:56.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-162" for this suite. @ 08/24/23 11:46:56.275
• [2.190 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 08/24/23 11:46:56.291
  Aug 24 11:46:56.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 11:46:56.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:46:56.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:46:56.321
  Aug 24 11:46:56.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 create -f -'
  Aug 24 11:46:57.013: INFO: stderr: ""
  Aug 24 11:46:57.013: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Aug 24 11:46:57.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 create -f -'
  Aug 24 11:46:57.685: INFO: stderr: ""
  Aug 24 11:46:57.685: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/24/23 11:46:57.687
  Aug 24 11:46:58.695: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:46:58.695: INFO: Found 1 / 1
  Aug 24 11:46:58.695: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 24 11:46:58.701: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:46:58.701: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 11:46:58.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 describe pod agnhost-primary-k7278'
  Aug 24 11:46:58.867: INFO: stderr: ""
  Aug 24 11:46:58.867: INFO: stdout: "Name:             agnhost-primary-k7278\nNamespace:        kubectl-6513\nPriority:         0\nService Account:  default\nNode:             pohje9aimahx-1/192.168.121.37\nStart Time:       Thu, 24 Aug 2023 11:46:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.64.240\nIPs:\n  IP:           10.233.64.240\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://81964032d609d81a3d572460f42152df66e1857acdb82f71d8dfd921cfe1909b\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Aug 2023 11:46:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sk9w8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-sk9w8:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-6513/agnhost-primary-k7278 to pohje9aimahx-1\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
  Aug 24 11:46:58.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 describe rc agnhost-primary'
  Aug 24 11:46:59.052: INFO: stderr: ""
  Aug 24 11:46:59.052: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6513\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-k7278\n"
  Aug 24 11:46:59.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 describe service agnhost-primary'
  Aug 24 11:46:59.246: INFO: stderr: ""
  Aug 24 11:46:59.246: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6513\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.61.93\nIPs:               10.233.61.93\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.64.240:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Aug 24 11:46:59.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 describe node pohje9aimahx-1'
  Aug 24 11:46:59.565: INFO: stderr: ""
  Aug 24 11:46:59.566: INFO: stdout: "Name:               pohje9aimahx-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pohje9aimahx-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Aug 2023 11:27:18 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pohje9aimahx-1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Aug 2023 11:46:51 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Aug 2023 11:31:16 +0000   Thu, 24 Aug 2023 11:31:16 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Thu, 24 Aug 2023 11:45:40 +0000   Thu, 24 Aug 2023 11:27:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Aug 2023 11:45:40 +0000   Thu, 24 Aug 2023 11:27:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Aug 2023 11:45:40 +0000   Thu, 24 Aug 2023 11:27:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Aug 2023 11:45:40 +0000   Thu, 24 Aug 2023 11:28:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.37\n  Hostname:    pohje9aimahx-1\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      115008636Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8123904Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1600m\n  ephemeral-storage:      111880401014\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3274240Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 628c0cb332c946eb903131052cdabcaa\n  System UUID:                628c0cb3-32c9-46eb-9031-31052cdabcaa\n  Boot ID:                    a8de9aed-70cd-4adb-83e3-58957898e159\n  Kernel Version:             6.2.0-26-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.1\n  Kubelet Version:            v1.27.5\n  Kube-Proxy Version:         v1.27.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-5bz85                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         16m\n  kube-system                 cilium-node-init-js6v2                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         16m\n  kube-system                 kube-addon-manager-pohje9aimahx-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         16m\n  kube-system                 kube-apiserver-pohje9aimahx-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-controller-manager-pohje9aimahx-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-proxy-l6rtn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                 kube-scheduler-pohje9aimahx-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         19m\n  kubectl-6513                agnhost-primary-k7278                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sched-preemption-7914       pod0-1-sched-preemption-medium-priority                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         14s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-z7825    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    755m (47%)  0 (0%)\n  memory                 250Mi (7%)  0 (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  2           2\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 19m                kube-proxy       \n  Normal  Starting                 19m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  19m (x8 over 19m)  kubelet          Node pohje9aimahx-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    19m (x8 over 19m)  kubelet          Node pohje9aimahx-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     19m (x7 over 19m)  kubelet          Node pohje9aimahx-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  19m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeNotReady             19m                kubelet          Node pohje9aimahx-1 status is now: NodeNotReady\n  Normal  Starting                 19m                kubelet          Starting kubelet.\n  Normal  NodeHasNoDiskPressure    19m                kubelet          Node pohje9aimahx-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     19m                kubelet          Node pohje9aimahx-1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  19m                kubelet          Node pohje9aimahx-1 status is now: NodeHasSufficientMemory\n  Normal  NodeAllocatableEnforced  19m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                19m                kubelet          Node pohje9aimahx-1 status is now: NodeReady\n  Normal  RegisteredNode           19m                node-controller  Node pohje9aimahx-1 event: Registered Node pohje9aimahx-1 in Controller\n  Normal  Starting                 18m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  18m                kubelet          Node pohje9aimahx-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    18m                kubelet          Node pohje9aimahx-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     18m                kubelet          Node pohje9aimahx-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             18m                kubelet          Node pohje9aimahx-1 status is now: NodeNotReady\n  Normal  NodeReady                18m                kubelet          Node pohje9aimahx-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  18m                kubelet          Updated Node Allocatable limit across pods\n"
  Aug 24 11:46:59.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6513 describe namespace kubectl-6513'
  Aug 24 11:46:59.803: INFO: stderr: ""
  Aug 24 11:46:59.803: INFO: stdout: "Name:         kubectl-6513\nLabels:       e2e-framework=kubectl\n              e2e-run=da90e73d-ba27-48cb-8688-e9f6ef028ca1\n              kubernetes.io/metadata.name=kubectl-6513\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Aug 24 11:46:59.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6513" for this suite. @ 08/24/23 11:46:59.828
• [3.600 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 08/24/23 11:46:59.897
  Aug 24 11:46:59.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:46:59.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:46:59.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:46:59.999
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 11:47:00.006
  STEP: Saw pod success @ 08/24/23 11:47:04.077
  Aug 24 11:47:04.083: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-0a30a49b-eb74-4cb1-bad9-1379fb1cdd67 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 11:47:04.1
  Aug 24 11:47:04.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3066" for this suite. @ 08/24/23 11:47:04.144
• [4.261 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 08/24/23 11:47:04.166
  Aug 24 11:47:04.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:47:04.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:04.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:04.2
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 11:47:04.205
  STEP: Saw pod success @ 08/24/23 11:47:08.255
  Aug 24 11:47:08.260: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-4f0fe71c-7b34-417a-8e72-601ace7310a5 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 11:47:08.273
  Aug 24 11:47:08.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4436" for this suite. @ 08/24/23 11:47:08.317
• [4.175 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 08/24/23 11:47:08.343
  Aug 24 11:47:08.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 11:47:08.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:08.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:08.377
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/24/23 11:47:08.381
  STEP: Saw pod success @ 08/24/23 11:47:12.427
  Aug 24 11:47:12.431: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-9932c87b-7665-47d5-a846-0f8960186c45 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 11:47:12.443
  Aug 24 11:47:12.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3443" for this suite. @ 08/24/23 11:47:12.48
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 08/24/23 11:47:12.503
  Aug 24 11:47:12.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 11:47:12.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:12.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:12.542
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 08/24/23 11:47:12.547
  Aug 24 11:47:12.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:47:14.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:47:23.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6239" for this suite. @ 08/24/23 11:47:23.37
• [10.886 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 08/24/23 11:47:23.394
  Aug 24 11:47:23.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 11:47:23.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:23.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:23.456
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 08/24/23 11:47:23.462
  Aug 24 11:47:23.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:47:25.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:47:32.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2809" for this suite. @ 08/24/23 11:47:32.461
• [9.083 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 08/24/23 11:47:32.478
  Aug 24 11:47:32.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 11:47:32.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:32.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:32.512
  STEP: Setting up server cert @ 08/24/23 11:47:32.554
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 11:47:33.627
  STEP: Deploying the webhook pod @ 08/24/23 11:47:33.637
  STEP: Wait for the deployment to be ready @ 08/24/23 11:47:33.66
  Aug 24 11:47:33.673: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Aug 24 11:47:35.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 47, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 47, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 47, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 47, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 08/24/23 11:47:37.727
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 11:47:37.765
  Aug 24 11:47:38.766: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 11:47:38.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 08/24/23 11:47:39.296
  STEP: Creating a custom resource that should be denied by the webhook @ 08/24/23 11:47:39.331
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 08/24/23 11:47:41.466
  STEP: Updating the custom resource with disallowed data should be denied @ 08/24/23 11:47:41.484
  STEP: Deleting the custom resource should be denied @ 08/24/23 11:47:41.503
  STEP: Remove the offending key and value from the custom resource data @ 08/24/23 11:47:41.515
  STEP: Deleting the updated custom resource should be successful @ 08/24/23 11:47:41.539
  Aug 24 11:47:41.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8013" for this suite. @ 08/24/23 11:47:42.247
  STEP: Destroying namespace "webhook-markers-1317" for this suite. @ 08/24/23 11:47:42.264
• [9.800 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 08/24/23 11:47:42.294
  Aug 24 11:47:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename podtemplate @ 08/24/23 11:47:42.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:42.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:42.328
  STEP: Create a pod template @ 08/24/23 11:47:42.334
  STEP: Replace a pod template @ 08/24/23 11:47:42.344
  Aug 24 11:47:42.356: INFO: Found updated podtemplate annotation: "true"

  Aug 24 11:47:42.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9187" for this suite. @ 08/24/23 11:47:42.365
• [0.081 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 08/24/23 11:47:42.376
  Aug 24 11:47:42.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 11:47:42.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:42.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:42.411
  STEP: Creating ServiceAccount "e2e-sa-5t48c"  @ 08/24/23 11:47:42.415
  Aug 24 11:47:42.425: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-5t48c"  @ 08/24/23 11:47:42.426
  Aug 24 11:47:42.441: INFO: AutomountServiceAccountToken: true
  Aug 24 11:47:42.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-975" for this suite. @ 08/24/23 11:47:42.45
• [0.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 08/24/23 11:47:42.464
  Aug 24 11:47:42.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:47:42.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:42.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:42.497
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 11:47:42.501
  STEP: Saw pod success @ 08/24/23 11:47:46.538
  Aug 24 11:47:46.543: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-007b3d44-5d97-4a86-b81b-265439439f3b container client-container: <nil>
  STEP: delete the pod @ 08/24/23 11:47:46.554
  Aug 24 11:47:46.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5048" for this suite. @ 08/24/23 11:47:46.586
• [4.131 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 08/24/23 11:47:46.598
  Aug 24 11:47:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename containers @ 08/24/23 11:47:46.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:46.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:46.634
  Aug 24 11:47:50.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5597" for this suite. @ 08/24/23 11:47:50.701
• [4.116 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 08/24/23 11:47:50.715
  Aug 24 11:47:50.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 11:47:50.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:50.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:50.753
  STEP: create the rc1 @ 08/24/23 11:47:50.766
  STEP: create the rc2 @ 08/24/23 11:47:50.776
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 08/24/23 11:47:57.284
  STEP: delete the rc simpletest-rc-to-be-deleted @ 08/24/23 11:48:06.374
  STEP: wait for the rc to be deleted @ 08/24/23 11:48:06.396
  Aug 24 11:48:11.536: INFO: 97 pods remaining
  Aug 24 11:48:11.536: INFO: 70 pods has nil DeletionTimestamp
  Aug 24 11:48:11.537: INFO: 
  Aug 24 11:48:16.581: INFO: 86 pods remaining
  Aug 24 11:48:16.585: INFO: 50 pods has nil DeletionTimestamp
  Aug 24 11:48:16.585: INFO: 
  Aug 24 11:48:21.438: INFO: 53 pods remaining
  Aug 24 11:48:21.438: INFO: 50 pods has nil DeletionTimestamp
  Aug 24 11:48:21.438: INFO: 
  STEP: Gathering metrics @ 08/24/23 11:48:26.43
  Aug 24 11:48:26.642: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 11:48:26.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-22pwf" in namespace "gc-6975"
  Aug 24 11:48:26.723: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lldb" in namespace "gc-6975"
  Aug 24 11:48:26.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bbzh" in namespace "gc-6975"
  Aug 24 11:48:26.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ddlg" in namespace "gc-6975"
  Aug 24 11:48:27.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hwsf" in namespace "gc-6975"
  Aug 24 11:48:27.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pqms" in namespace "gc-6975"
  Aug 24 11:48:27.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pst9" in namespace "gc-6975"
  Aug 24 11:48:27.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qszd" in namespace "gc-6975"
  Aug 24 11:48:27.603: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sd8g" in namespace "gc-6975"
  Aug 24 11:48:27.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vwtd" in namespace "gc-6975"
  Aug 24 11:48:27.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-74hwm" in namespace "gc-6975"
  Aug 24 11:48:27.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cbpf" in namespace "gc-6975"
  Aug 24 11:48:28.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q89q" in namespace "gc-6975"
  Aug 24 11:48:28.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-84gl8" in namespace "gc-6975"
  Aug 24 11:48:28.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-86bdb" in namespace "gc-6975"
  Aug 24 11:48:28.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zm2p" in namespace "gc-6975"
  Aug 24 11:48:28.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dd5k" in namespace "gc-6975"
  Aug 24 11:48:28.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dwcr" in namespace "gc-6975"
  Aug 24 11:48:28.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rc72" in namespace "gc-6975"
  Aug 24 11:48:28.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vl9c" in namespace "gc-6975"
  Aug 24 11:48:28.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-b65jt" in namespace "gc-6975"
  Aug 24 11:48:29.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdcxm" in namespace "gc-6975"
  Aug 24 11:48:29.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgnt2" in namespace "gc-6975"
  Aug 24 11:48:29.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-c97rw" in namespace "gc-6975"
  Aug 24 11:48:29.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9z7z" in namespace "gc-6975"
  Aug 24 11:48:29.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnpn7" in namespace "gc-6975"
  Aug 24 11:48:29.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqztq" in namespace "gc-6975"
  Aug 24 11:48:29.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv4s4" in namespace "gc-6975"
  Aug 24 11:48:30.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-czm5l" in namespace "gc-6975"
  Aug 24 11:48:30.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-d28hd" in namespace "gc-6975"
  Aug 24 11:48:30.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpnzm" in namespace "gc-6975"
  Aug 24 11:48:30.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtqk9" in namespace "gc-6975"
  Aug 24 11:48:30.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffxzg" in namespace "gc-6975"
  Aug 24 11:48:30.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-frjnd" in namespace "gc-6975"
  Aug 24 11:48:30.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwgqt" in namespace "gc-6975"
  Aug 24 11:48:30.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfnnk" in namespace "gc-6975"
  Aug 24 11:48:30.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs2rh" in namespace "gc-6975"
  Aug 24 11:48:30.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxzzr" in namespace "gc-6975"
  Aug 24 11:48:30.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2br9" in namespace "gc-6975"
  Aug 24 11:48:30.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2z9v" in namespace "gc-6975"
  Aug 24 11:48:30.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8tfh" in namespace "gc-6975"
  Aug 24 11:48:31.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9hqz" in namespace "gc-6975"
  Aug 24 11:48:31.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-hb2dg" in namespace "gc-6975"
  Aug 24 11:48:31.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpgzd" in namespace "gc-6975"
  Aug 24 11:48:31.447: INFO: Deleting pod "simpletest-rc-to-be-deleted-j48xn" in namespace "gc-6975"
  Aug 24 11:48:31.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjmv4" in namespace "gc-6975"
  Aug 24 11:48:31.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-jm4m8" in namespace "gc-6975"
  Aug 24 11:48:31.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtkq8" in namespace "gc-6975"
  Aug 24 11:48:31.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-k22px" in namespace "gc-6975"
  Aug 24 11:48:31.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-kg7bh" in namespace "gc-6975"
  Aug 24 11:48:31.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6975" for this suite. @ 08/24/23 11:48:32.044
• [41.360 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 08/24/23 11:48:32.08
  Aug 24 11:48:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 11:48:32.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:32.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:32.144
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 11:48:32.186
  STEP: create the pod with lifecycle hook @ 08/24/23 11:48:36.364
  STEP: check poststart hook @ 08/24/23 11:48:40.425
  STEP: delete the pod with lifecycle hook @ 08/24/23 11:48:40.457
  Aug 24 11:48:42.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5321" for this suite. @ 08/24/23 11:48:42.506
• [10.436 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 08/24/23 11:48:42.523
  Aug 24 11:48:42.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 11:48:42.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:42.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:42.567
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6812 @ 08/24/23 11:48:42.574
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/24/23 11:48:42.61
  STEP: creating service externalsvc in namespace services-6812 @ 08/24/23 11:48:42.611
  STEP: creating replication controller externalsvc in namespace services-6812 @ 08/24/23 11:48:42.643
  I0824 11:48:42.654226      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-6812, replica count: 2
  I0824 11:48:45.706341      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:48.708020      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:51.709656      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:54.710426      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 08/24/23 11:48:54.72
  Aug 24 11:48:54.755: INFO: Creating new exec pod
  Aug 24 11:48:56.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-6812 exec execpodwjnjw -- /bin/sh -x -c nslookup clusterip-service.services-6812.svc.cluster.local'
  Aug 24 11:48:57.238: INFO: stderr: "+ nslookup clusterip-service.services-6812.svc.cluster.local\n"
  Aug 24 11:48:57.238: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-6812.svc.cluster.local\tcanonical name = externalsvc.services-6812.svc.cluster.local.\nName:\texternalsvc.services-6812.svc.cluster.local\nAddress: 10.233.29.98\n\n"
  Aug 24 11:48:57.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-6812, will wait for the garbage collector to delete the pods @ 08/24/23 11:48:57.255
  Aug 24 11:48:57.332: INFO: Deleting ReplicationController externalsvc took: 16.13712ms
  Aug 24 11:48:57.433: INFO: Terminating ReplicationController externalsvc pods took: 101.537497ms
  Aug 24 11:48:59.682: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-6812" for this suite. @ 08/24/23 11:48:59.713
• [17.209 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 08/24/23 11:48:59.733
  Aug 24 11:48:59.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename job @ 08/24/23 11:48:59.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:59.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:59.794
  STEP: Creating a job @ 08/24/23 11:48:59.8
  STEP: Ensuring active pods == parallelism @ 08/24/23 11:48:59.811
  STEP: Orphaning one of the Job's Pods @ 08/24/23 11:49:03.826
  Aug 24 11:49:04.363: INFO: Successfully updated pod "adopt-release-s9dbn"
  STEP: Checking that the Job readopts the Pod @ 08/24/23 11:49:04.363
  STEP: Removing the labels from the Job's Pod @ 08/24/23 11:49:06.381
  Aug 24 11:49:06.897: INFO: Successfully updated pod "adopt-release-s9dbn"
  STEP: Checking that the Job releases the Pod @ 08/24/23 11:49:06.897
  Aug 24 11:49:08.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1312" for this suite. @ 08/24/23 11:49:08.927
• [9.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 08/24/23 11:49:08.942
  Aug 24 11:49:08.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 11:49:08.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:08.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:08.981
  STEP: creating pod @ 08/24/23 11:49:08.994
  Aug 24 11:49:11.042: INFO: Pod pod-hostip-57d973b0-f7f9-47b5-baee-a0eba1c214a5 has hostIP: 192.168.121.5
  Aug 24 11:49:11.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4048" for this suite. @ 08/24/23 11:49:11.05
• [2.124 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 08/24/23 11:49:11.067
  Aug 24 11:49:11.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 11:49:11.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:11.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:11.101
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7637 @ 08/24/23 11:49:11.105
  STEP: changing the ExternalName service to type=NodePort @ 08/24/23 11:49:11.115
  STEP: creating replication controller externalname-service in namespace services-7637 @ 08/24/23 11:49:11.153
  I0824 11:49:11.173294      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7637, replica count: 2
  I0824 11:49:14.227304      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 11:49:14.227: INFO: Creating new exec pod
  Aug 24 11:49:17.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7637 exec execpodr6cbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 24 11:49:17.564: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 24 11:49:17.564: INFO: stdout: ""
  Aug 24 11:49:18.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7637 exec execpodr6cbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 24 11:49:18.831: INFO: stderr: "+ echo+ nc -v -t -w 2 externalname-service 80\n hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 24 11:49:18.831: INFO: stdout: "externalname-service-7kzgw"
  Aug 24 11:49:18.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7637 exec execpodr6cbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.243 80'
  Aug 24 11:49:19.127: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.243 80\nConnection to 10.233.23.243 80 port [tcp/http] succeeded!\n"
  Aug 24 11:49:19.127: INFO: stdout: ""
  Aug 24 11:49:20.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7637 exec execpodr6cbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.243 80'
  Aug 24 11:49:20.407: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.243 80\nConnection to 10.233.23.243 80 port [tcp/http] succeeded!\n"
  Aug 24 11:49:20.407: INFO: stdout: "externalname-service-7kzgw"
  Aug 24 11:49:20.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7637 exec execpodr6cbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.5 32492'
  Aug 24 11:49:20.686: INFO: stderr: "+ + nc -v -t -w 2 192.168.121.5 32492\necho hostName\nConnection to 192.168.121.5 32492 port [tcp/*] succeeded!\n"
  Aug 24 11:49:20.686: INFO: stdout: "externalname-service-7kzgw"
  Aug 24 11:49:20.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7637 exec execpodr6cbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 32492'
  Aug 24 11:49:21.062: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 32492\nConnection to 192.168.121.248 32492 port [tcp/*] succeeded!\n"
  Aug 24 11:49:21.062: INFO: stdout: "externalname-service-7kzgw"
  Aug 24 11:49:21.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 11:49:21.076: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-7637" for this suite. @ 08/24/23 11:49:21.126
• [10.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 08/24/23 11:49:21.171
  Aug 24 11:49:21.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename podtemplate @ 08/24/23 11:49:21.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:21.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:21.22
  Aug 24 11:49:21.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4104" for this suite. @ 08/24/23 11:49:21.303
• [0.143 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 08/24/23 11:49:21.316
  Aug 24 11:49:21.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 11:49:21.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:21.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:21.354
  STEP: Creating a suspended cronjob @ 08/24/23 11:49:21.361
  STEP: Ensuring no jobs are scheduled @ 08/24/23 11:49:21.377
  STEP: Ensuring no job exists by listing jobs explicitly @ 08/24/23 11:54:21.398
  STEP: Removing cronjob @ 08/24/23 11:54:21.409
  Aug 24 11:54:21.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2268" for this suite. @ 08/24/23 11:54:21.461
• [300.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 08/24/23 11:54:21.493
  Aug 24 11:54:21.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 11:54:21.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:21.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:21.568
  Aug 24 11:54:21.632: INFO: created pod pod-service-account-defaultsa
  Aug 24 11:54:21.632: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Aug 24 11:54:21.657: INFO: created pod pod-service-account-mountsa
  Aug 24 11:54:21.657: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Aug 24 11:54:21.683: INFO: created pod pod-service-account-nomountsa
  Aug 24 11:54:21.683: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Aug 24 11:54:21.699: INFO: created pod pod-service-account-defaultsa-mountspec
  Aug 24 11:54:21.699: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Aug 24 11:54:21.752: INFO: created pod pod-service-account-mountsa-mountspec
  Aug 24 11:54:21.752: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Aug 24 11:54:21.774: INFO: created pod pod-service-account-nomountsa-mountspec
  Aug 24 11:54:21.774: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Aug 24 11:54:21.807: INFO: created pod pod-service-account-defaultsa-nomountspec
  Aug 24 11:54:21.807: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Aug 24 11:54:21.860: INFO: created pod pod-service-account-mountsa-nomountspec
  Aug 24 11:54:21.860: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Aug 24 11:54:21.913: INFO: created pod pod-service-account-nomountsa-nomountspec
  Aug 24 11:54:21.913: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Aug 24 11:54:21.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8995" for this suite. @ 08/24/23 11:54:21.955
• [0.523 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 08/24/23 11:54:22.022
  Aug 24 11:54:22.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 11:54:22.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:22.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:22.087
  STEP: Creating ReplicationController "e2e-rc-7rjpt" @ 08/24/23 11:54:22.094
  Aug 24 11:54:22.116: INFO: Get Replication Controller "e2e-rc-7rjpt" to confirm replicas
  Aug 24 11:54:23.152: INFO: Get Replication Controller "e2e-rc-7rjpt" to confirm replicas
  Aug 24 11:54:23.185: INFO: Found 1 replicas for "e2e-rc-7rjpt" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-7rjpt" @ 08/24/23 11:54:23.185
  STEP: Updating a scale subresource @ 08/24/23 11:54:23.336
  STEP: Verifying replicas where modified for replication controller "e2e-rc-7rjpt" @ 08/24/23 11:54:23.558
  Aug 24 11:54:23.558: INFO: Get Replication Controller "e2e-rc-7rjpt" to confirm replicas
  Aug 24 11:54:24.621: INFO: Get Replication Controller "e2e-rc-7rjpt" to confirm replicas
  Aug 24 11:54:24.630: INFO: Found 2 replicas for "e2e-rc-7rjpt" replication controller
  Aug 24 11:54:24.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6067" for this suite. @ 08/24/23 11:54:24.642
• [2.667 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 08/24/23 11:54:24.689
  Aug 24 11:54:24.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 11:54:24.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:24.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:24.746
  STEP: Creating simple DaemonSet "daemon-set" @ 08/24/23 11:54:24.911
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 11:54:24.939
  Aug 24 11:54:25.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:54:25.077: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 11:54:26.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:54:26.135: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 11:54:27.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 11:54:27.129: INFO: Node pohje9aimahx-2 is running 0 daemon pod, expected 1
  Aug 24 11:54:28.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 11:54:28.099: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 08/24/23 11:54:28.106
  Aug 24 11:54:28.128: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 08/24/23 11:54:28.128
  Aug 24 11:54:28.151: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 08/24/23 11:54:28.152
  Aug 24 11:54:28.156: INFO: Observed &DaemonSet event: ADDED
  Aug 24 11:54:28.157: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.157: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.157: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.157: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.162: INFO: Found daemon set daemon-set in namespace daemonsets-1353 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 11:54:28.162: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 08/24/23 11:54:28.162
  STEP: watching for the daemon set status to be patched @ 08/24/23 11:54:28.18
  Aug 24 11:54:28.186: INFO: Observed &DaemonSet event: ADDED
  Aug 24 11:54:28.187: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.187: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.187: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.188: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.188: INFO: Observed daemon set daemon-set in namespace daemonsets-1353 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 11:54:28.188: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:54:28.188: INFO: Found daemon set daemon-set in namespace daemonsets-1353 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Aug 24 11:54:28.194: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 11:54:28.217
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1353, will wait for the garbage collector to delete the pods @ 08/24/23 11:54:28.217
  Aug 24 11:54:28.300: INFO: Deleting DaemonSet.extensions daemon-set took: 25.46229ms
  Aug 24 11:54:28.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.026596ms
  Aug 24 11:54:30.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:54:30.633: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 11:54:30.645: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8690"},"items":null}

  Aug 24 11:54:30.674: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8691"},"items":null}

  Aug 24 11:54:30.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1353" for this suite. @ 08/24/23 11:54:30.788
• [6.130 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 08/24/23 11:54:30.822
  Aug 24 11:54:30.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename subpath @ 08/24/23 11:54:30.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:30.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:30.871
  STEP: Setting up data @ 08/24/23 11:54:30.877
  STEP: Creating pod pod-subpath-test-downwardapi-vwzq @ 08/24/23 11:54:30.893
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 11:54:30.893
  STEP: Saw pod success @ 08/24/23 11:54:55.072
  Aug 24 11:54:55.081: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-subpath-test-downwardapi-vwzq container test-container-subpath-downwardapi-vwzq: <nil>
  STEP: delete the pod @ 08/24/23 11:54:55.119
  STEP: Deleting pod pod-subpath-test-downwardapi-vwzq @ 08/24/23 11:54:55.159
  Aug 24 11:54:55.159: INFO: Deleting pod "pod-subpath-test-downwardapi-vwzq" in namespace "subpath-7150"
  Aug 24 11:54:55.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7150" for this suite. @ 08/24/23 11:54:55.182
• [24.379 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 08/24/23 11:54:55.206
  Aug 24 11:54:55.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 11:54:55.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:55.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:55.259
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-4134 @ 08/24/23 11:54:55.267
  STEP: changing the ExternalName service to type=ClusterIP @ 08/24/23 11:54:55.279
  STEP: creating replication controller externalname-service in namespace services-4134 @ 08/24/23 11:54:55.313
  I0824 11:54:55.335523      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-4134, replica count: 2
  I0824 11:54:58.387830      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:55:01.390319      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 11:55:01.390: INFO: Creating new exec pod
  Aug 24 11:55:06.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-4134 exec execpodggpmm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 24 11:55:06.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 24 11:55:06.785: INFO: stdout: "externalname-service-vlwts"
  Aug 24 11:55:06.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-4134 exec execpodggpmm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.15 80'
  Aug 24 11:55:07.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.15 80\nConnection to 10.233.61.15 80 port [tcp/http] succeeded!\n"
  Aug 24 11:55:07.080: INFO: stdout: "externalname-service-wr7lk"
  Aug 24 11:55:07.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 11:55:07.089: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-4134" for this suite. @ 08/24/23 11:55:07.152
• [11.963 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 08/24/23 11:55:07.169
  Aug 24 11:55:07.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 11:55:07.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:07.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:07.215
  Aug 24 11:55:07.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 11:55:10.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2556" for this suite. @ 08/24/23 11:55:10.705
• [3.548 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 08/24/23 11:55:10.72
  Aug 24 11:55:10.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 11:55:10.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:10.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:10.749
  STEP: Creating a ReplaceConcurrent cronjob @ 08/24/23 11:55:10.753
  STEP: Ensuring a job is scheduled @ 08/24/23 11:55:10.766
  STEP: Ensuring exactly one is scheduled @ 08/24/23 11:56:00.871
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/24/23 11:56:00.88
  STEP: Ensuring the job is replaced with a new one @ 08/24/23 11:56:00.887
  STEP: Removing cronjob @ 08/24/23 11:57:00.896
  Aug 24 11:57:00.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-227" for this suite. @ 08/24/23 11:57:00.929
• [110.246 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 08/24/23 11:57:00.972
  Aug 24 11:57:00.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 11:57:00.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:01.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:01.03
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/24/23 11:57:01.035
  STEP: Saw pod success @ 08/24/23 11:57:05.091
  Aug 24 11:57:05.095: INFO: Trying to get logs from node pohje9aimahx-1 pod pod-dd993aa9-f82e-4b00-bbfd-9b2171543aa5 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 11:57:05.121
  Aug 24 11:57:05.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1194" for this suite. @ 08/24/23 11:57:05.154
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 08/24/23 11:57:05.173
  Aug 24 11:57:05.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:57:05.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:05.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:05.209
  STEP: Creating configMap with name cm-test-opt-del-d155f682-74e4-490a-8b76-409b1d321b2f @ 08/24/23 11:57:05.221
  STEP: Creating configMap with name cm-test-opt-upd-2aaaae8a-5895-4e88-b7ef-3e40d67b905a @ 08/24/23 11:57:05.23
  STEP: Creating the pod @ 08/24/23 11:57:05.237
  STEP: Deleting configmap cm-test-opt-del-d155f682-74e4-490a-8b76-409b1d321b2f @ 08/24/23 11:57:07.317
  STEP: Updating configmap cm-test-opt-upd-2aaaae8a-5895-4e88-b7ef-3e40d67b905a @ 08/24/23 11:57:07.326
  STEP: Creating configMap with name cm-test-opt-create-39291835-cbdf-4b1b-8c69-e50b3faf186e @ 08/24/23 11:57:07.333
  STEP: waiting to observe update in volume @ 08/24/23 11:57:07.338
  Aug 24 11:58:10.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8905" for this suite. @ 08/24/23 11:58:10.025
• [64.865 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 08/24/23 11:58:10.047
  Aug 24 11:58:10.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename subpath @ 08/24/23 11:58:10.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:10.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:10.088
  STEP: Setting up data @ 08/24/23 11:58:10.096
  STEP: Creating pod pod-subpath-test-configmap-dtq4 @ 08/24/23 11:58:10.12
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 11:58:10.121
  STEP: Saw pod success @ 08/24/23 11:58:34.269
  Aug 24 11:58:34.274: INFO: Trying to get logs from node pohje9aimahx-1 pod pod-subpath-test-configmap-dtq4 container test-container-subpath-configmap-dtq4: <nil>
  STEP: delete the pod @ 08/24/23 11:58:34.288
  STEP: Deleting pod pod-subpath-test-configmap-dtq4 @ 08/24/23 11:58:34.319
  Aug 24 11:58:34.319: INFO: Deleting pod "pod-subpath-test-configmap-dtq4" in namespace "subpath-4617"
  Aug 24 11:58:34.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4617" for this suite. @ 08/24/23 11:58:34.333
• [24.296 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 08/24/23 11:58:34.344
  Aug 24 11:58:34.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename subjectreview @ 08/24/23 11:58:34.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:34.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:34.383
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-7791" @ 08/24/23 11:58:34.389
  Aug 24 11:58:34.399: INFO: saUsername: "system:serviceaccount:subjectreview-7791:e2e"
  Aug 24 11:58:34.400: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-7791"}
  Aug 24 11:58:34.400: INFO: saUID: "847e9c5a-78a5-47f6-b8e9-27194f7a0654"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-7791:e2e" @ 08/24/23 11:58:34.4
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-7791:e2e" @ 08/24/23 11:58:34.401
  Aug 24 11:58:34.404: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-7791:e2e" api 'list' configmaps in "subjectreview-7791" namespace @ 08/24/23 11:58:34.405
  Aug 24 11:58:34.408: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-7791:e2e" @ 08/24/23 11:58:34.408
  Aug 24 11:58:34.412: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Aug 24 11:58:34.413: INFO: LocalSubjectAccessReview has been verified
  Aug 24 11:58:34.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-7791" for this suite. @ 08/24/23 11:58:34.425
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 08/24/23 11:58:34.448
  Aug 24 11:58:34.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 11:58:34.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:34.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:34.485
  STEP: set up a multi version CRD @ 08/24/23 11:58:34.489
  Aug 24 11:58:34.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: mark a version not serverd @ 08/24/23 11:58:39.329
  STEP: check the unserved version gets removed @ 08/24/23 11:58:39.367
  STEP: check the other version is not changed @ 08/24/23 11:58:41.383
  Aug 24 11:58:45.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7895" for this suite. @ 08/24/23 11:58:45.635
• [11.208 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 08/24/23 11:58:45.657
  Aug 24 11:58:45.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 11:58:45.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:45.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:45.726
  STEP: Creating a ResourceQuota with best effort scope @ 08/24/23 11:58:45.73
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 11:58:45.744
  STEP: Creating a ResourceQuota with not best effort scope @ 08/24/23 11:58:47.753
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 11:58:47.76
  STEP: Creating a best-effort pod @ 08/24/23 11:58:49.771
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 08/24/23 11:58:49.795
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 08/24/23 11:58:51.806
  STEP: Deleting the pod @ 08/24/23 11:58:53.817
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 11:58:53.852
  STEP: Creating a not best-effort pod @ 08/24/23 11:58:55.86
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 08/24/23 11:58:55.89
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 08/24/23 11:58:57.902
  STEP: Deleting the pod @ 08/24/23 11:58:59.916
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 11:58:59.951
  Aug 24 11:59:01.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7459" for this suite. @ 08/24/23 11:59:01.977
• [16.337 seconds]
------------------------------
SS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 08/24/23 11:59:01.997
  Aug 24 11:59:01.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename prestop @ 08/24/23 11:59:02.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:59:02.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:59:02.044
  STEP: Creating server pod server in namespace prestop-9106 @ 08/24/23 11:59:02.048
  STEP: Waiting for pods to come up. @ 08/24/23 11:59:02.067
  STEP: Creating tester pod tester in namespace prestop-9106 @ 08/24/23 11:59:04.099
  STEP: Deleting pre-stop pod @ 08/24/23 11:59:06.123
  Aug 24 11:59:11.154: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Aug 24 11:59:11.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 08/24/23 11:59:11.163
  STEP: Destroying namespace "prestop-9106" for this suite. @ 08/24/23 11:59:11.199
• [9.219 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 08/24/23 11:59:11.219
  Aug 24 11:59:11.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename taint-single-pod @ 08/24/23 11:59:11.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:59:11.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:59:11.252
  Aug 24 11:59:11.257: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 24 12:00:11.305: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:00:11.313: INFO: Starting informer...
  STEP: Starting pod... @ 08/24/23 12:00:11.313
  Aug 24 12:00:11.546: INFO: Pod is running on pohje9aimahx-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/24/23 12:00:11.546
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 12:00:11.57
  STEP: Waiting short time to make sure Pod is queued for deletion @ 08/24/23 12:00:11.585
  Aug 24 12:00:11.585: INFO: Pod wasn't evicted. Proceeding
  Aug 24 12:00:11.586: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 12:00:11.649
  STEP: Waiting some time to make sure that toleration time passed. @ 08/24/23 12:00:11.668
  Aug 24 12:01:26.669: INFO: Pod wasn't evicted. Test successful
  Aug 24 12:01:26.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-2186" for this suite. @ 08/24/23 12:01:26.688
• [135.487 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 08/24/23 12:01:26.71
  Aug 24 12:01:26.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 08/24/23 12:01:26.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:26.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:26.753
  STEP: creating a target pod @ 08/24/23 12:01:26.758
  STEP: adding an ephemeral container @ 08/24/23 12:01:28.805
  STEP: checking pod container endpoints @ 08/24/23 12:01:30.858
  Aug 24 12:01:30.858: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-1047 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:01:30.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:01:30.859: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:01:30.860: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-1047/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Aug 24 12:01:30.994: INFO: Exec stderr: ""
  Aug 24 12:01:31.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-1047" for this suite. @ 08/24/23 12:01:31.051
• [4.352 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 08/24/23 12:01:31.074
  Aug 24 12:01:31.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 12:01:31.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:31.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:31.114
  Aug 24 12:01:31.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:01:37.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6513" for this suite. @ 08/24/23 12:01:37.736
• [6.677 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 08/24/23 12:01:37.752
  Aug 24 12:01:37.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:01:37.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:37.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:37.792
  STEP: Creating a test headless service @ 08/24/23 12:01:37.796
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 198.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.198_udp@PTR;check="$$(dig +tcp +noall +answer +search 198.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.198_tcp@PTR;sleep 1; done
   @ 08/24/23 12:01:37.827
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 198.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.198_udp@PTR;check="$$(dig +tcp +noall +answer +search 198.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.198_tcp@PTR;sleep 1; done
   @ 08/24/23 12:01:37.827
  STEP: creating a pod to probe DNS @ 08/24/23 12:01:37.827
  STEP: submitting the pod to kubernetes @ 08/24/23 12:01:37.827
  STEP: retrieving the pod @ 08/24/23 12:02:08.046
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:02:08.057
  Aug 24 12:02:08.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.088: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.100: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.141: INFO: Unable to read jessie_udp@dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.147: INFO: Unable to read jessie_tcp@dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.155: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.163: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-919.svc.cluster.local from pod dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8: the server could not find the requested resource (get pods dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8)
  Aug 24 12:02:08.195: INFO: Lookups using dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8 failed for: [wheezy_udp@dns-test-service.dns-919.svc.cluster.local wheezy_tcp@dns-test-service.dns-919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-919.svc.cluster.local jessie_udp@dns-test-service.dns-919.svc.cluster.local jessie_tcp@dns-test-service.dns-919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-919.svc.cluster.local]

  Aug 24 12:02:13.321: INFO: DNS probes using dns-919/dns-test-af80fba1-1bbf-4365-8143-ca554b85a7f8 succeeded

  Aug 24 12:02:13.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:02:13.335
  STEP: deleting the test service @ 08/24/23 12:02:13.42
  STEP: deleting the test headless service @ 08/24/23 12:02:13.488
  STEP: Destroying namespace "dns-919" for this suite. @ 08/24/23 12:02:13.537
• [35.852 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 08/24/23 12:02:13.615
  Aug 24 12:02:13.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename aggregator @ 08/24/23 12:02:13.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:13.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:13.66
  Aug 24 12:02:13.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Registering the sample API server. @ 08/24/23 12:02:13.667
  Aug 24 12:02:14.999: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Aug 24 12:02:15.076: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Aug 24 12:02:17.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:19.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:21.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:23.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:25.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:27.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:29.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:31.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:33.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:35.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:37.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:39.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:41.334: INFO: Waited 139.653758ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 08/24/23 12:02:41.451
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 08/24/23 12:02:41.462
  STEP: List APIServices @ 08/24/23 12:02:41.484
  Aug 24 12:02:41.507: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 08/24/23 12:02:41.507
  Aug 24 12:02:41.528: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 08/24/23 12:02:41.529
  Aug 24 12:02:41.563: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 41, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 08/24/23 12:02:41.564
  Aug 24 12:02:41.572: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-08-24 12:02:41 +0000 UTC Passed all checks passed}
  Aug 24 12:02:41.572: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:02:41.573: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 08/24/23 12:02:41.573
  Aug 24 12:02:41.597: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-934442256" @ 08/24/23 12:02:41.597
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 08/24/23 12:02:41.625
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 08/24/23 12:02:41.639
  STEP: Patch APIService Status @ 08/24/23 12:02:41.646
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 08/24/23 12:02:41.664
  Aug 24 12:02:41.672: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-08-24 12:02:41 +0000 UTC Passed all checks passed}
  Aug 24 12:02:41.673: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:02:41.673: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Aug 24 12:02:41.674: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 08/24/23 12:02:41.674
  STEP: Confirm that the generated APIService has been deleted @ 08/24/23 12:02:41.685
  Aug 24 12:02:41.685: INFO: Requesting list of APIServices to confirm quantity
  Aug 24 12:02:41.695: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Aug 24 12:02:41.696: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Aug 24 12:02:41.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-6336" for this suite. @ 08/24/23 12:02:41.975
• [28.374 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 08/24/23 12:02:41.993
  Aug 24 12:02:41.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:02:41.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:42.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:42.038
  STEP: creating service in namespace services-7844 @ 08/24/23 12:02:42.045
  STEP: creating service affinity-nodeport in namespace services-7844 @ 08/24/23 12:02:42.045
  STEP: creating replication controller affinity-nodeport in namespace services-7844 @ 08/24/23 12:02:42.08
  I0824 12:02:42.098594      13 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-7844, replica count: 3
  I0824 12:02:45.154045      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:02:45.183: INFO: Creating new exec pod
  Aug 24 12:02:48.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7844 exec execpod-affinityk5xbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Aug 24 12:02:48.616: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Aug 24 12:02:48.617: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:02:48.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7844 exec execpod-affinityk5xbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.110 80'
  Aug 24 12:02:48.909: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.110 80\nConnection to 10.233.40.110 80 port [tcp/http] succeeded!\n"
  Aug 24 12:02:48.909: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:02:48.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7844 exec execpod-affinityk5xbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 32233'
  Aug 24 12:02:49.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 32233\nConnection to 192.168.121.248 32233 port [tcp/*] succeeded!\n"
  Aug 24 12:02:49.198: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:02:49.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7844 exec execpod-affinityk5xbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.37 32233'
  Aug 24 12:02:49.439: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.37 32233\nConnection to 192.168.121.37 32233 port [tcp/*] succeeded!\n"
  Aug 24 12:02:49.439: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:02:49.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7844 exec execpod-affinityk5xbq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.37:32233/ ; done'
  Aug 24 12:02:50.010: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:32233/\n"
  Aug 24 12:02:50.010: INFO: stdout: "\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4\naffinity-nodeport-qddp4"
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.010: INFO: Received response from host: affinity-nodeport-qddp4
  Aug 24 12:02:50.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:02:50.018: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-7844, will wait for the garbage collector to delete the pods @ 08/24/23 12:02:50.044
  Aug 24 12:02:50.112: INFO: Deleting ReplicationController affinity-nodeport took: 10.771091ms
  Aug 24 12:02:50.213: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.024848ms
  STEP: Destroying namespace "services-7844" for this suite. @ 08/24/23 12:02:52.37
• [10.388 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 08/24/23 12:02:52.383
  Aug 24 12:02:52.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:02:52.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:52.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:52.426
  STEP: creating service in namespace services-9382 @ 08/24/23 12:02:52.432
  STEP: creating service affinity-nodeport-transition in namespace services-9382 @ 08/24/23 12:02:52.432
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9382 @ 08/24/23 12:02:52.456
  I0824 12:02:52.478017      13 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-9382, replica count: 3
  I0824 12:02:55.530484      13 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:02:55.552: INFO: Creating new exec pod
  Aug 24 12:03:00.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9382 exec execpod-affinityd652g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Aug 24 12:03:00.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Aug 24 12:03:00.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:03:00.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9382 exec execpod-affinityd652g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.255 80'
  Aug 24 12:03:01.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.255 80\nConnection to 10.233.52.255 80 port [tcp/http] succeeded!\n"
  Aug 24 12:03:01.183: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:03:01.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9382 exec execpod-affinityd652g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 30796'
  Aug 24 12:03:01.430: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 30796\nConnection to 192.168.121.248 30796 port [tcp/*] succeeded!\n"
  Aug 24 12:03:01.430: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:03:01.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9382 exec execpod-affinityd652g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.37 30796'
  Aug 24 12:03:01.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.37 30796\nConnection to 192.168.121.37 30796 port [tcp/*] succeeded!\n"
  Aug 24 12:03:01.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:03:01.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9382 exec execpod-affinityd652g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.37:30796/ ; done'
  Aug 24 12:03:02.230: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n"
  Aug 24 12:03:02.230: INFO: stdout: "\naffinity-nodeport-transition-x7nbc\naffinity-nodeport-transition-stzbv\naffinity-nodeport-transition-x7nbc\naffinity-nodeport-transition-x7nbc\naffinity-nodeport-transition-stzbv\naffinity-nodeport-transition-x7nbc\naffinity-nodeport-transition-x7nbc\naffinity-nodeport-transition-stzbv\naffinity-nodeport-transition-stzbv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-x7nbc\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv"
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-x7nbc
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-stzbv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-x7nbc
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-x7nbc
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-stzbv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-x7nbc
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-x7nbc
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-stzbv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-stzbv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-x7nbc
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.230: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9382 exec execpod-affinityd652g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.37:30796/ ; done'
  Aug 24 12:03:02.722: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.37:30796/\n"
  Aug 24 12:03:02.722: INFO: stdout: "\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv\naffinity-nodeport-transition-r5spv"
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Received response from host: affinity-nodeport-transition-r5spv
  Aug 24 12:03:02.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:03:02.731: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9382, will wait for the garbage collector to delete the pods @ 08/24/23 12:03:02.753
  Aug 24 12:03:02.844: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.701516ms
  Aug 24 12:03:02.946: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.197639ms
  STEP: Destroying namespace "services-9382" for this suite. @ 08/24/23 12:03:05.383
• [13.009 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 08/24/23 12:03:05.395
  Aug 24 12:03:05.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:03:05.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:05.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:05.43
  STEP: Creating a test externalName service @ 08/24/23 12:03:05.434
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7603.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local; sleep 1; done
   @ 08/24/23 12:03:05.444
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7603.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local; sleep 1; done
   @ 08/24/23 12:03:05.444
  STEP: creating a pod to probe DNS @ 08/24/23 12:03:05.444
  STEP: submitting the pod to kubernetes @ 08/24/23 12:03:05.444
  STEP: retrieving the pod @ 08/24/23 12:03:09.485
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:03:09.493
  Aug 24 12:03:09.524: INFO: DNS probes using dns-test-7126f6c0-f399-42d2-8faa-79e617c96d02 succeeded

  STEP: changing the externalName to bar.example.com @ 08/24/23 12:03:09.524
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7603.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local; sleep 1; done
   @ 08/24/23 12:03:09.564
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7603.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local; sleep 1; done
   @ 08/24/23 12:03:09.564
  STEP: creating a second pod to probe DNS @ 08/24/23 12:03:09.564
  STEP: submitting the pod to kubernetes @ 08/24/23 12:03:09.564
  STEP: retrieving the pod @ 08/24/23 12:03:13.607
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:03:13.618
  Aug 24 12:03:13.631: INFO: File wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:13.639: INFO: File jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:13.639: INFO: Lookups using dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea failed for: [wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local]

  Aug 24 12:03:18.656: INFO: File wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:18.663: INFO: File jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:18.663: INFO: Lookups using dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea failed for: [wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local]

  Aug 24 12:03:23.658: INFO: File wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:23.666: INFO: File jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:23.667: INFO: Lookups using dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea failed for: [wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local]

  Aug 24 12:03:28.654: INFO: File wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains '' instead of 'bar.example.com.'
  Aug 24 12:03:28.662: INFO: File jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains '' instead of 'bar.example.com.'
  Aug 24 12:03:28.662: INFO: Lookups using dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea failed for: [wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local]

  Aug 24 12:03:33.654: INFO: File wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:33.665: INFO: File jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:33.665: INFO: Lookups using dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea failed for: [wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local]

  Aug 24 12:03:38.660: INFO: File jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local from pod  dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 12:03:38.660: INFO: Lookups using dns-7603/dns-test-2fa29650-3b3b-4818-875c-88a38a026cea failed for: [jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local]

  Aug 24 12:03:43.663: INFO: DNS probes using dns-test-2fa29650-3b3b-4818-875c-88a38a026cea succeeded

  STEP: changing the service to type=ClusterIP @ 08/24/23 12:03:43.664
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7603.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7603.svc.cluster.local; sleep 1; done
   @ 08/24/23 12:03:43.71
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7603.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7603.svc.cluster.local; sleep 1; done
   @ 08/24/23 12:03:43.713
  STEP: creating a third pod to probe DNS @ 08/24/23 12:03:43.713
  STEP: submitting the pod to kubernetes @ 08/24/23 12:03:43.723
  STEP: retrieving the pod @ 08/24/23 12:03:47.781
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:03:47.793
  Aug 24 12:03:47.816: INFO: DNS probes using dns-test-e7013505-6973-4bae-9953-0da1f4f5fb1b succeeded

  Aug 24 12:03:47.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:03:47.825
  STEP: deleting the pod @ 08/24/23 12:03:47.858
  STEP: deleting the pod @ 08/24/23 12:03:47.893
  STEP: deleting the test externalName service @ 08/24/23 12:03:47.977
  STEP: Destroying namespace "dns-7603" for this suite. @ 08/24/23 12:03:48.017
• [42.641 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 08/24/23 12:03:48.039
  Aug 24 12:03:48.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename security-context @ 08/24/23 12:03:48.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:48.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:48.101
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/24/23 12:03:48.107
  STEP: Saw pod success @ 08/24/23 12:03:52.175
  Aug 24 12:03:52.180: INFO: Trying to get logs from node pohje9aimahx-3 pod security-context-d5002697-6813-43be-b13a-00c0b49fc6b1 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:03:52.206
  Aug 24 12:03:52.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-1638" for this suite. @ 08/24/23 12:03:52.244
• [4.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 08/24/23 12:03:52.258
  Aug 24 12:03:52.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:03:52.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:52.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:52.293
  STEP: Creating configMap with name configmap-test-upd-f586bfb4-05ea-42fc-b7f7-ceee64bb5ac7 @ 08/24/23 12:03:52.304
  STEP: Creating the pod @ 08/24/23 12:03:52.312
  STEP: Updating configmap configmap-test-upd-f586bfb4-05ea-42fc-b7f7-ceee64bb5ac7 @ 08/24/23 12:03:54.361
  STEP: waiting to observe update in volume @ 08/24/23 12:03:54.371
  Aug 24 12:05:19.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6073" for this suite. @ 08/24/23 12:05:19.171
• [86.928 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 08/24/23 12:05:19.198
  Aug 24 12:05:19.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:05:19.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:19.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:19.238
  STEP: Creating secret with name secret-test-a24a52e7-7bae-433c-be39-fe0981d73313 @ 08/24/23 12:05:19.282
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:05:19.29
  STEP: Saw pod success @ 08/24/23 12:05:23.333
  Aug 24 12:05:23.341: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-e53880f6-3330-413b-a0ef-50bcf10a046e container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:05:23.353
  Aug 24 12:05:23.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6768" for this suite. @ 08/24/23 12:05:23.385
  STEP: Destroying namespace "secret-namespace-6271" for this suite. @ 08/24/23 12:05:23.401
• [4.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 08/24/23 12:05:23.426
  Aug 24 12:05:23.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:05:23.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:23.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:23.476
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-1645 @ 08/24/23 12:05:23.48
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/24/23 12:05:23.506
  STEP: creating service externalsvc in namespace services-1645 @ 08/24/23 12:05:23.507
  STEP: creating replication controller externalsvc in namespace services-1645 @ 08/24/23 12:05:23.527
  I0824 12:05:23.543413      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-1645, replica count: 2
  I0824 12:05:26.597444      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 08/24/23 12:05:26.613
  Aug 24 12:05:26.644: INFO: Creating new exec pod
  Aug 24 12:05:28.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-1645 exec execpodgqvjl -- /bin/sh -x -c nslookup nodeport-service.services-1645.svc.cluster.local'
  Aug 24 12:05:29.188: INFO: stderr: "+ nslookup nodeport-service.services-1645.svc.cluster.local\n"
  Aug 24 12:05:29.189: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-1645.svc.cluster.local\tcanonical name = externalsvc.services-1645.svc.cluster.local.\nName:\texternalsvc.services-1645.svc.cluster.local\nAddress: 10.233.34.25\n\n"
  Aug 24 12:05:29.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-1645, will wait for the garbage collector to delete the pods @ 08/24/23 12:05:29.2
  Aug 24 12:05:29.273: INFO: Deleting ReplicationController externalsvc took: 14.948475ms
  Aug 24 12:05:29.375: INFO: Terminating ReplicationController externalsvc pods took: 101.715871ms
  Aug 24 12:05:31.522: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-1645" for this suite. @ 08/24/23 12:05:31.575
• [8.167 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 08/24/23 12:05:31.595
  Aug 24 12:05:31.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:05:31.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:31.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:31.633
  STEP: Creating configMap with name projected-configmap-test-volume-map-ad36f3f8-4c2d-49e8-a27e-cf5140323e5c @ 08/24/23 12:05:31.638
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:05:31.646
  STEP: Saw pod success @ 08/24/23 12:05:35.687
  Aug 24 12:05:35.695: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-9501a4a9-1dfe-4b5a-9255-3d17200828d8 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:05:35.707
  Aug 24 12:05:35.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4889" for this suite. @ 08/24/23 12:05:35.749
• [4.172 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 08/24/23 12:05:35.768
  Aug 24 12:05:35.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:05:35.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:35.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:35.816
  Aug 24 12:05:35.822: INFO: Creating deployment "test-recreate-deployment"
  Aug 24 12:05:35.833: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Aug 24 12:05:35.847: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Aug 24 12:05:37.865: INFO: Waiting deployment "test-recreate-deployment" to complete
  Aug 24 12:05:37.872: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Aug 24 12:05:37.891: INFO: Updating deployment test-recreate-deployment
  Aug 24 12:05:37.892: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Aug 24 12:05:38.105: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1254  5b43f71d-1ebb-406c-a5e0-8f9bf06773cf 11599 2 2023-08-24 12:05:35 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cb89d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 12:05:38 +0000 UTC,LastTransitionTime:2023-08-24 12:05:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-08-24 12:05:38 +0000 UTC,LastTransitionTime:2023-08-24 12:05:35 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 24 12:05:38.114: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1254  4c1845d1-5c6c-453e-a3e9-b81c9f0b4f65 11598 1 2023-08-24 12:05:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5b43f71d-1ebb-406c-a5e0-8f9bf06773cf 0xc005cb8d97 0xc005cb8d98}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b43f71d-1ebb-406c-a5e0-8f9bf06773cf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cb8e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:05:38.114: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Aug 24 12:05:38.114: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1254  06208ffb-d659-41d5-9aaa-013fad88e709 11586 2 2023-08-24 12:05:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5b43f71d-1ebb-406c-a5e0-8f9bf06773cf 0xc005cb8ea7 0xc005cb8ea8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b43f71d-1ebb-406c-a5e0-8f9bf06773cf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cb8f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:05:38.124: INFO: Pod "test-recreate-deployment-54757ffd6c-2vn6m" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-2vn6m test-recreate-deployment-54757ffd6c- deployment-1254  042fbc67-e8c5-4fca-b9f5-f03cf9913dfe 11597 0 2023-08-24 12:05:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 4c1845d1-5c6c-453e-a3e9-b81c9f0b4f65 0xc005cb93e7 0xc005cb93e8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c1845d1-5c6c-453e-a3e9-b81c9f0b4f65\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpkm9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpkm9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2023-08-24 12:05:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:05:38.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1254" for this suite. @ 08/24/23 12:05:38.137
• [2.382 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 08/24/23 12:05:38.154
  Aug 24 12:05:38.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:05:38.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:38.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:38.19
  STEP: Creating configMap with name configmap-test-volume-c9e13f26-e143-44ef-8cfc-029f125cb65e @ 08/24/23 12:05:38.196
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:05:38.205
  STEP: Saw pod success @ 08/24/23 12:05:42.241
  Aug 24 12:05:42.247: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-d96a5225-17a7-4d11-bd9f-b2fa1964bf0a container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:05:42.258
  Aug 24 12:05:42.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-843" for this suite. @ 08/24/23 12:05:42.298
• [4.157 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 08/24/23 12:05:42.313
  Aug 24 12:05:42.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:05:42.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:42.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:42.352
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 08/24/23 12:05:42.356
  STEP: Saw pod success @ 08/24/23 12:05:46.394
  Aug 24 12:05:46.400: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-ad83b2af-6bf7-4f9c-b34f-1332ca355546 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:05:46.414
  Aug 24 12:05:46.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-913" for this suite. @ 08/24/23 12:05:46.46
• [4.162 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 08/24/23 12:05:46.476
  Aug 24 12:05:46.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:05:46.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:46.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:46.523
  Aug 24 12:05:46.551: INFO: Pod name sample-pod: Found 0 pods out of 1
  Aug 24 12:05:51.559: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:05:51.559
  STEP: Scaling up "test-rs" replicaset  @ 08/24/23 12:05:51.559
  Aug 24 12:05:51.583: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 08/24/23 12:05:51.584
  W0824 12:05:51.599025      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 24 12:05:51.604: INFO: observed ReplicaSet test-rs in namespace replicaset-8891 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 12:05:51.637: INFO: observed ReplicaSet test-rs in namespace replicaset-8891 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 12:05:51.672: INFO: observed ReplicaSet test-rs in namespace replicaset-8891 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 12:05:51.705: INFO: observed ReplicaSet test-rs in namespace replicaset-8891 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 12:05:52.878: INFO: observed ReplicaSet test-rs in namespace replicaset-8891 with ReadyReplicas 2, AvailableReplicas 2
  Aug 24 12:05:53.323: INFO: observed Replicaset test-rs in namespace replicaset-8891 with ReadyReplicas 3 found true
  Aug 24 12:05:53.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8891" for this suite. @ 08/24/23 12:05:53.333
• [6.867 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 08/24/23 12:05:53.348
  Aug 24 12:05:53.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 12:05:53.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:05:53.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:05:53.391
  STEP: Performing setup for networking test in namespace pod-network-test-40 @ 08/24/23 12:05:53.395
  STEP: creating a selector @ 08/24/23 12:05:53.396
  STEP: Creating the service pods in kubernetes @ 08/24/23 12:05:53.396
  Aug 24 12:05:53.396: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/24/23 12:06:15.672
  Aug 24 12:06:17.708: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 12:06:17.709: INFO: Breadth first check of 10.233.64.94 on host 192.168.121.37...
  Aug 24 12:06:17.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.42:9080/dial?request=hostname&protocol=udp&host=10.233.64.94&port=8081&tries=1'] Namespace:pod-network-test-40 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:06:17.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:06:17.721: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:06:17.723: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-40/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.94%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 12:06:17.896: INFO: Waiting for responses: map[]
  Aug 24 12:06:17.897: INFO: reached 10.233.64.94 after 0/1 tries
  Aug 24 12:06:17.897: INFO: Breadth first check of 10.233.65.138 on host 192.168.121.248...
  Aug 24 12:06:17.905: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.42:9080/dial?request=hostname&protocol=udp&host=10.233.65.138&port=8081&tries=1'] Namespace:pod-network-test-40 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:06:17.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:06:17.906: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:06:17.907: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-40/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.138%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 12:06:18.037: INFO: Waiting for responses: map[]
  Aug 24 12:06:18.038: INFO: reached 10.233.65.138 after 0/1 tries
  Aug 24 12:06:18.039: INFO: Breadth first check of 10.233.66.128 on host 192.168.121.5...
  Aug 24 12:06:18.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.42:9080/dial?request=hostname&protocol=udp&host=10.233.66.128&port=8081&tries=1'] Namespace:pod-network-test-40 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:06:18.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:06:18.049: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:06:18.049: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-40/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.42%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.128%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 12:06:18.173: INFO: Waiting for responses: map[]
  Aug 24 12:06:18.173: INFO: reached 10.233.66.128 after 0/1 tries
  Aug 24 12:06:18.173: INFO: Going to retry 0 out of 3 pods....
  Aug 24 12:06:18.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-40" for this suite. @ 08/24/23 12:06:18.181
• [24.845 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 08/24/23 12:06:18.194
  Aug 24 12:06:18.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/24/23 12:06:18.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:06:18.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:06:18.225
  STEP: Creating 50 configmaps @ 08/24/23 12:06:18.229
  STEP: Creating RC which spawns configmap-volume pods @ 08/24/23 12:06:18.596
  Aug 24 12:06:18.626: INFO: Pod name wrapped-volume-race-4655f423-d215-4ab2-9b69-8ded38e2dc1b: Found 0 pods out of 5
  Aug 24 12:06:23.679: INFO: Pod name wrapped-volume-race-4655f423-d215-4ab2-9b69-8ded38e2dc1b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/24/23 12:06:23.679
  STEP: Creating RC which spawns configmap-volume pods @ 08/24/23 12:06:23.793
  Aug 24 12:06:23.832: INFO: Pod name wrapped-volume-race-de780cb6-74b4-40dd-8eee-18734d2c45da: Found 0 pods out of 5
  Aug 24 12:06:28.854: INFO: Pod name wrapped-volume-race-de780cb6-74b4-40dd-8eee-18734d2c45da: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/24/23 12:06:28.854
  STEP: Creating RC which spawns configmap-volume pods @ 08/24/23 12:06:28.919
  Aug 24 12:06:28.959: INFO: Pod name wrapped-volume-race-b65a2261-655b-4644-9eb7-ee08bd7ea351: Found 0 pods out of 5
  Aug 24 12:06:33.986: INFO: Pod name wrapped-volume-race-b65a2261-655b-4644-9eb7-ee08bd7ea351: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/24/23 12:06:33.986
  Aug 24 12:06:34.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-b65a2261-655b-4644-9eb7-ee08bd7ea351 in namespace emptydir-wrapper-3389, will wait for the garbage collector to delete the pods @ 08/24/23 12:06:34.032
  Aug 24 12:06:34.109: INFO: Deleting ReplicationController wrapped-volume-race-b65a2261-655b-4644-9eb7-ee08bd7ea351 took: 17.36867ms
  Aug 24 12:06:34.310: INFO: Terminating ReplicationController wrapped-volume-race-b65a2261-655b-4644-9eb7-ee08bd7ea351 pods took: 201.210735ms
  STEP: deleting ReplicationController wrapped-volume-race-de780cb6-74b4-40dd-8eee-18734d2c45da in namespace emptydir-wrapper-3389, will wait for the garbage collector to delete the pods @ 08/24/23 12:06:35.811
  Aug 24 12:06:35.888: INFO: Deleting ReplicationController wrapped-volume-race-de780cb6-74b4-40dd-8eee-18734d2c45da took: 13.122844ms
  Aug 24 12:06:35.989: INFO: Terminating ReplicationController wrapped-volume-race-de780cb6-74b4-40dd-8eee-18734d2c45da pods took: 101.194677ms
  STEP: deleting ReplicationController wrapped-volume-race-4655f423-d215-4ab2-9b69-8ded38e2dc1b in namespace emptydir-wrapper-3389, will wait for the garbage collector to delete the pods @ 08/24/23 12:06:37.494
  Aug 24 12:06:37.567: INFO: Deleting ReplicationController wrapped-volume-race-4655f423-d215-4ab2-9b69-8ded38e2dc1b took: 15.083328ms
  Aug 24 12:06:37.768: INFO: Terminating ReplicationController wrapped-volume-race-4655f423-d215-4ab2-9b69-8ded38e2dc1b pods took: 201.050513ms
  STEP: Cleaning up the configMaps @ 08/24/23 12:06:39.771
  STEP: Destroying namespace "emptydir-wrapper-3389" for this suite. @ 08/24/23 12:06:40.291
• [22.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 08/24/23 12:06:40.306
  Aug 24 12:06:40.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 12:06:40.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:06:40.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:06:40.347
  Aug 24 12:06:42.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5437" for this suite. @ 08/24/23 12:06:42.414
• [2.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 08/24/23 12:06:42.433
  Aug 24 12:06:42.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:06:42.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:06:42.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:06:42.471
  STEP: Creating configMap configmap-3377/configmap-test-9e099f24-0fcf-4b7c-83c2-850c739d210e @ 08/24/23 12:06:42.477
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:06:42.486
  STEP: Saw pod success @ 08/24/23 12:06:46.537
  Aug 24 12:06:46.543: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-68b80663-2ddc-4d28-b864-478334289661 container env-test: <nil>
  STEP: delete the pod @ 08/24/23 12:06:46.558
  Aug 24 12:06:46.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3377" for this suite. @ 08/24/23 12:06:46.624
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 08/24/23 12:06:46.635
  Aug 24 12:06:46.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:06:46.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:06:46.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:06:46.662
  STEP: Creating secret with name secret-test-2ddc8ac6-ca39-44fa-a84a-e424194abbad @ 08/24/23 12:06:46.668
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:06:46.678
  STEP: Saw pod success @ 08/24/23 12:06:50.715
  Aug 24 12:06:50.720: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-939372f4-971c-4410-bc47-2ec5a563c03d container secret-env-test: <nil>
  STEP: delete the pod @ 08/24/23 12:06:50.736
  Aug 24 12:06:50.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8229" for this suite. @ 08/24/23 12:06:50.771
• [4.147 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 08/24/23 12:06:50.787
  Aug 24 12:06:50.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:06:50.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:06:50.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:06:50.833
  STEP: Creating a ResourceQuota @ 08/24/23 12:06:50.842
  STEP: Getting a ResourceQuota @ 08/24/23 12:06:50.852
  STEP: Updating a ResourceQuota @ 08/24/23 12:06:50.861
  STEP: Verifying a ResourceQuota was modified @ 08/24/23 12:06:50.874
  STEP: Deleting a ResourceQuota @ 08/24/23 12:06:50.883
  STEP: Verifying the deleted ResourceQuota @ 08/24/23 12:06:50.896
  Aug 24 12:06:50.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1574" for this suite. @ 08/24/23 12:06:50.911
• [0.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 08/24/23 12:06:50.929
  Aug 24 12:06:50.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename watch @ 08/24/23 12:06:50.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:06:50.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:06:50.98
  STEP: creating a watch on configmaps with label A @ 08/24/23 12:06:50.984
  STEP: creating a watch on configmaps with label B @ 08/24/23 12:06:50.987
  STEP: creating a watch on configmaps with label A or B @ 08/24/23 12:06:50.99
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 08/24/23 12:06:50.996
  Aug 24 12:06:51.005: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12577 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:06:51.030: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12577 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 08/24/23 12:06:51.031
  Aug 24 12:06:51.046: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12578 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:06:51.047: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12578 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 08/24/23 12:06:51.047
  Aug 24 12:06:51.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12580 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:06:51.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12580 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 08/24/23 12:06:51.059
  Aug 24 12:06:51.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12581 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:06:51.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9792  53f2e4f9-45f7-4068-94fa-e5039d01dda1 12581 0 2023-08-24 12:06:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 08/24/23 12:06:51.069
  Aug 24 12:06:51.075: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9792  2ffefa6d-55c7-4f98-a6ea-603c7a4a468e 12582 0 2023-08-24 12:06:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:06:51.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9792  2ffefa6d-55c7-4f98-a6ea-603c7a4a468e 12582 0 2023-08-24 12:06:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 08/24/23 12:07:01.078
  Aug 24 12:07:01.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9792  2ffefa6d-55c7-4f98-a6ea-603c7a4a468e 12627 0 2023-08-24 12:06:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:07:01.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9792  2ffefa6d-55c7-4f98-a6ea-603c7a4a468e 12627 0 2023-08-24 12:06:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 12:06:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:07:11.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9792" for this suite. @ 08/24/23 12:07:11.102
• [20.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 08/24/23 12:07:11.119
  Aug 24 12:07:11.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 12:07:11.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:07:11.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:07:11.154
  STEP: Creating a ForbidConcurrent cronjob @ 08/24/23 12:07:11.162
  STEP: Ensuring a job is scheduled @ 08/24/23 12:07:11.174
  STEP: Ensuring exactly one is scheduled @ 08/24/23 12:08:01.183
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/24/23 12:08:01.188
  STEP: Ensuring no more jobs are scheduled @ 08/24/23 12:08:01.194
  STEP: Removing cronjob @ 08/24/23 12:13:01.208
  Aug 24 12:13:01.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2254" for this suite. @ 08/24/23 12:13:01.239
• [350.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 08/24/23 12:13:01.268
  Aug 24 12:13:01.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename proxy @ 08/24/23 12:13:01.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:01.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:01.336
  Aug 24 12:13:01.343: INFO: Creating pod...
  Aug 24 12:13:03.379: INFO: Creating service...
  Aug 24 12:13:03.402: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=DELETE
  Aug 24 12:13:03.417: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 12:13:03.418: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=OPTIONS
  Aug 24 12:13:03.435: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 12:13:03.435: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=PATCH
  Aug 24 12:13:03.445: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 12:13:03.445: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=POST
  Aug 24 12:13:03.453: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 12:13:03.453: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=PUT
  Aug 24 12:13:03.460: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 12:13:03.460: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=DELETE
  Aug 24 12:13:03.471: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 12:13:03.472: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Aug 24 12:13:03.483: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 12:13:03.484: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=PATCH
  Aug 24 12:13:03.495: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 12:13:03.495: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=POST
  Aug 24 12:13:03.505: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 12:13:03.506: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=PUT
  Aug 24 12:13:03.514: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 12:13:03.514: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=GET
  Aug 24 12:13:03.521: INFO: http.Client request:GET StatusCode:301
  Aug 24 12:13:03.521: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=GET
  Aug 24 12:13:03.530: INFO: http.Client request:GET StatusCode:301
  Aug 24 12:13:03.531: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/pods/agnhost/proxy?method=HEAD
  Aug 24 12:13:03.535: INFO: http.Client request:HEAD StatusCode:301
  Aug 24 12:13:03.535: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8270/services/e2e-proxy-test-service/proxy?method=HEAD
  Aug 24 12:13:03.542: INFO: http.Client request:HEAD StatusCode:301
  Aug 24 12:13:03.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8270" for this suite. @ 08/24/23 12:13:03.552
• [2.294 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 08/24/23 12:13:03.563
  Aug 24 12:13:03.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:13:03.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:03.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:03.597
  Aug 24 12:13:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/24/23 12:13:05.599
  Aug 24 12:13:05.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1794 --namespace=crd-publish-openapi-1794 create -f -'
  Aug 24 12:13:07.600: INFO: stderr: ""
  Aug 24 12:13:07.600: INFO: stdout: "e2e-test-crd-publish-openapi-871-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 24 12:13:07.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1794 --namespace=crd-publish-openapi-1794 delete e2e-test-crd-publish-openapi-871-crds test-cr'
  Aug 24 12:13:07.779: INFO: stderr: ""
  Aug 24 12:13:07.779: INFO: stdout: "e2e-test-crd-publish-openapi-871-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Aug 24 12:13:07.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1794 --namespace=crd-publish-openapi-1794 apply -f -'
  Aug 24 12:13:09.339: INFO: stderr: ""
  Aug 24 12:13:09.339: INFO: stdout: "e2e-test-crd-publish-openapi-871-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 24 12:13:09.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1794 --namespace=crd-publish-openapi-1794 delete e2e-test-crd-publish-openapi-871-crds test-cr'
  Aug 24 12:13:09.637: INFO: stderr: ""
  Aug 24 12:13:09.637: INFO: stdout: "e2e-test-crd-publish-openapi-871-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/24/23 12:13:09.637
  Aug 24 12:13:09.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1794 explain e2e-test-crd-publish-openapi-871-crds'
  Aug 24 12:13:10.109: INFO: stderr: ""
  Aug 24 12:13:10.109: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-871-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Aug 24 12:13:11.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1794" for this suite. @ 08/24/23 12:13:11.958
• [8.411 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 08/24/23 12:13:11.975
  Aug 24 12:13:11.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:13:11.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:12.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:12.034
  STEP: Create set of pods @ 08/24/23 12:13:12.038
  Aug 24 12:13:12.060: INFO: created test-pod-1
  Aug 24 12:13:12.084: INFO: created test-pod-2
  Aug 24 12:13:12.106: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 08/24/23 12:13:12.106
  STEP: waiting for all pods to be deleted @ 08/24/23 12:13:16.245
  Aug 24 12:13:16.262: INFO: Pod quantity 3 is different from expected quantity 0
  Aug 24 12:13:17.282: INFO: Pod quantity 2 is different from expected quantity 0
  Aug 24 12:13:18.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5998" for this suite. @ 08/24/23 12:13:18.28
• [6.318 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 08/24/23 12:13:18.295
  Aug 24 12:13:18.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:13:18.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:18.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:18.329
  Aug 24 12:13:18.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-9895 version'
  Aug 24 12:13:18.468: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Aug 24 12:13:18.468: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:48:26Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:42:11Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Aug 24 12:13:18.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9895" for this suite. @ 08/24/23 12:13:18.479
• [0.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 08/24/23 12:13:18.496
  Aug 24 12:13:18.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename watch @ 08/24/23 12:13:18.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:18.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:18.528
  STEP: getting a starting resourceVersion @ 08/24/23 12:13:18.534
  STEP: starting a background goroutine to produce watch events @ 08/24/23 12:13:18.539
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 08/24/23 12:13:18.54
  Aug 24 12:13:21.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-539" for this suite. @ 08/24/23 12:13:21.361
• [2.926 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 08/24/23 12:13:21.426
  Aug 24 12:13:21.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename ingressclass @ 08/24/23 12:13:21.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:21.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:21.475
  STEP: getting /apis @ 08/24/23 12:13:21.48
  STEP: getting /apis/networking.k8s.io @ 08/24/23 12:13:21.488
  STEP: getting /apis/networking.k8s.iov1 @ 08/24/23 12:13:21.493
  STEP: creating @ 08/24/23 12:13:21.495
  STEP: getting @ 08/24/23 12:13:21.575
  STEP: listing @ 08/24/23 12:13:21.58
  STEP: watching @ 08/24/23 12:13:21.591
  Aug 24 12:13:21.591: INFO: starting watch
  STEP: patching @ 08/24/23 12:13:21.593
  STEP: updating @ 08/24/23 12:13:21.601
  Aug 24 12:13:21.611: INFO: waiting for watch events with expected annotations
  Aug 24 12:13:21.611: INFO: saw patched and updated annotations
  STEP: deleting @ 08/24/23 12:13:21.611
  STEP: deleting a collection @ 08/24/23 12:13:21.636
  Aug 24 12:13:21.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-106" for this suite. @ 08/24/23 12:13:21.682
• [0.268 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 08/24/23 12:13:21.7
  Aug 24 12:13:21.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 12:13:21.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:21.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:21.736
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 12:13:21.753
  STEP: create the pod with lifecycle hook @ 08/24/23 12:13:23.812
  STEP: check poststart hook @ 08/24/23 12:13:25.859
  STEP: delete the pod with lifecycle hook @ 08/24/23 12:13:25.896
  Aug 24 12:13:27.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8217" for this suite. @ 08/24/23 12:13:27.946
• [6.260 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 08/24/23 12:13:27.964
  Aug 24 12:13:27.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:13:27.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:27.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:28.005
  STEP: Creating configMap with name projected-configmap-test-volume-b52867fd-e1db-4299-8441-b5d428640a6d @ 08/24/23 12:13:28.014
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:13:28.025
  STEP: Saw pod success @ 08/24/23 12:13:32.077
  Aug 24 12:13:32.083: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-5e4f5379-bedb-4bc0-a7c4-6370df32bbab container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:13:32.103
  Aug 24 12:13:32.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4724" for this suite. @ 08/24/23 12:13:32.141
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 08/24/23 12:13:32.157
  Aug 24 12:13:32.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:13:32.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:32.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:32.193
  Aug 24 12:13:32.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: creating the pod @ 08/24/23 12:13:32.202
  STEP: submitting the pod to kubernetes @ 08/24/23 12:13:32.202
  Aug 24 12:13:34.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-876" for this suite. @ 08/24/23 12:13:34.303
• [2.166 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 08/24/23 12:13:34.326
  Aug 24 12:13:34.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename init-container @ 08/24/23 12:13:34.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:34.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:34.374
  STEP: creating the pod @ 08/24/23 12:13:34.382
  Aug 24 12:13:34.382: INFO: PodSpec: initContainers in spec.initContainers
  Aug 24 12:13:38.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7818" for this suite. @ 08/24/23 12:13:38.938
• [4.634 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 08/24/23 12:13:38.963
  Aug 24 12:13:38.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:13:38.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:39.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:39.015
  STEP: Setting up server cert @ 08/24/23 12:13:39.073
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:13:39.987
  STEP: Deploying the webhook pod @ 08/24/23 12:13:40.001
  STEP: Wait for the deployment to be ready @ 08/24/23 12:13:40.035
  Aug 24 12:13:40.046: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 08/24/23 12:13:42.076
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:13:42.124
  Aug 24 12:13:43.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/24/23 12:13:43.137
  STEP: create a pod that should be denied by the webhook @ 08/24/23 12:13:43.182
  STEP: create a pod that causes the webhook to hang @ 08/24/23 12:13:43.213
  STEP: create a configmap that should be denied by the webhook @ 08/24/23 12:13:53.23
  STEP: create a configmap that should be admitted by the webhook @ 08/24/23 12:13:53.404
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/24/23 12:13:53.424
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/24/23 12:13:53.444
  STEP: create a namespace that bypass the webhook @ 08/24/23 12:13:53.458
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 08/24/23 12:13:53.489
  Aug 24 12:13:53.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-26" for this suite. @ 08/24/23 12:13:53.71
  STEP: Destroying namespace "webhook-markers-1904" for this suite. @ 08/24/23 12:13:53.73
  STEP: Destroying namespace "exempted-namespace-9497" for this suite. @ 08/24/23 12:13:53.752
• [14.807 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 08/24/23 12:13:53.773
  Aug 24 12:13:53.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:13:53.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:53.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:53.811
  STEP: creating service endpoint-test2 in namespace services-2671 @ 08/24/23 12:13:53.816
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2671 to expose endpoints map[] @ 08/24/23 12:13:53.836
  Aug 24 12:13:53.859: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Aug 24 12:13:54.877: INFO: successfully validated that service endpoint-test2 in namespace services-2671 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2671 @ 08/24/23 12:13:54.877
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2671 to expose endpoints map[pod1:[80]] @ 08/24/23 12:13:56.927
  Aug 24 12:13:56.952: INFO: successfully validated that service endpoint-test2 in namespace services-2671 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 08/24/23 12:13:56.952
  Aug 24 12:13:56.953: INFO: Creating new exec pod
  Aug 24 12:14:00.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-2671 exec execpod66jrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 24 12:14:00.312: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 24 12:14:00.312: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:14:00.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-2671 exec execpod66jrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.15.35 80'
  Aug 24 12:14:00.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.15.35 80\nConnection to 10.233.15.35 80 port [tcp/http] succeeded!\n"
  Aug 24 12:14:00.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2671 @ 08/24/23 12:14:00.605
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2671 to expose endpoints map[pod1:[80] pod2:[80]] @ 08/24/23 12:14:02.655
  Aug 24 12:14:02.683: INFO: successfully validated that service endpoint-test2 in namespace services-2671 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 08/24/23 12:14:02.684
  Aug 24 12:14:03.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-2671 exec execpod66jrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 24 12:14:03.988: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 24 12:14:03.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:14:03.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-2671 exec execpod66jrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.15.35 80'
  Aug 24 12:14:04.263: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.15.35 80\nConnection to 10.233.15.35 80 port [tcp/http] succeeded!\n"
  Aug 24 12:14:04.263: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2671 @ 08/24/23 12:14:04.263
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2671 to expose endpoints map[pod2:[80]] @ 08/24/23 12:14:04.29
  Aug 24 12:14:04.335: INFO: successfully validated that service endpoint-test2 in namespace services-2671 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 08/24/23 12:14:04.335
  Aug 24 12:14:05.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-2671 exec execpod66jrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 24 12:14:05.633: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 24 12:14:05.633: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:14:05.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-2671 exec execpod66jrc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.15.35 80'
  Aug 24 12:14:05.872: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.15.35 80\nConnection to 10.233.15.35 80 port [tcp/http] succeeded!\n"
  Aug 24 12:14:05.872: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2671 @ 08/24/23 12:14:05.872
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2671 to expose endpoints map[] @ 08/24/23 12:14:05.942
  Aug 24 12:14:06.014: INFO: successfully validated that service endpoint-test2 in namespace services-2671 exposes endpoints map[]
  Aug 24 12:14:06.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2671" for this suite. @ 08/24/23 12:14:06.111
• [12.351 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 08/24/23 12:14:06.128
  Aug 24 12:14:06.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svc-latency @ 08/24/23 12:14:06.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:06.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:06.177
  Aug 24 12:14:06.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-9772 @ 08/24/23 12:14:06.184
  I0824 12:14:06.200063      13 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9772, replica count: 1
  I0824 12:14:07.252304      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 12:14:08.255054      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:14:08.388: INFO: Created: latency-svc-25gnh
  Aug 24 12:14:08.399: INFO: Got endpoints: latency-svc-25gnh [42.212924ms]
  Aug 24 12:14:08.432: INFO: Created: latency-svc-b5zdr
  Aug 24 12:14:08.449: INFO: Created: latency-svc-tg75q
  Aug 24 12:14:08.454: INFO: Got endpoints: latency-svc-b5zdr [54.670717ms]
  Aug 24 12:14:08.473: INFO: Created: latency-svc-pz2tq
  Aug 24 12:14:08.482: INFO: Got endpoints: latency-svc-tg75q [82.595098ms]
  Aug 24 12:14:08.490: INFO: Got endpoints: latency-svc-pz2tq [89.596406ms]
  Aug 24 12:14:08.508: INFO: Created: latency-svc-fzpxv
  Aug 24 12:14:08.519: INFO: Got endpoints: latency-svc-fzpxv [118.820031ms]
  Aug 24 12:14:08.519: INFO: Created: latency-svc-p857r
  Aug 24 12:14:08.535: INFO: Got endpoints: latency-svc-p857r [134.86981ms]
  Aug 24 12:14:08.549: INFO: Created: latency-svc-q78mb
  Aug 24 12:14:08.581: INFO: Created: latency-svc-9vgss
  Aug 24 12:14:08.612: INFO: Created: latency-svc-87sm7
  Aug 24 12:14:08.616: INFO: Got endpoints: latency-svc-q78mb [215.780028ms]
  Aug 24 12:14:08.617: INFO: Got endpoints: latency-svc-9vgss [216.529219ms]
  Aug 24 12:14:08.633: INFO: Got endpoints: latency-svc-87sm7 [232.349741ms]
  Aug 24 12:14:08.634: INFO: Created: latency-svc-ds9rg
  Aug 24 12:14:08.655: INFO: Got endpoints: latency-svc-ds9rg [254.041286ms]
  Aug 24 12:14:08.663: INFO: Created: latency-svc-c6wd9
  Aug 24 12:14:08.675: INFO: Got endpoints: latency-svc-c6wd9 [274.655557ms]
  Aug 24 12:14:08.885: INFO: Created: latency-svc-fzbt9
  Aug 24 12:14:08.886: INFO: Created: latency-svc-75tfw
  Aug 24 12:14:08.886: INFO: Created: latency-svc-r95ht
  Aug 24 12:14:08.886: INFO: Created: latency-svc-nfmwn
  Aug 24 12:14:08.886: INFO: Created: latency-svc-gcffh
  Aug 24 12:14:08.889: INFO: Created: latency-svc-sxjpg
  Aug 24 12:14:08.889: INFO: Created: latency-svc-hjq72
  Aug 24 12:14:08.889: INFO: Created: latency-svc-kptwp
  Aug 24 12:14:08.893: INFO: Created: latency-svc-ghwrz
  Aug 24 12:14:08.896: INFO: Created: latency-svc-n2l8x
  Aug 24 12:14:08.910: INFO: Created: latency-svc-2c444
  Aug 24 12:14:08.912: INFO: Created: latency-svc-vpv8k
  Aug 24 12:14:08.913: INFO: Created: latency-svc-zkq9s
  Aug 24 12:14:08.914: INFO: Created: latency-svc-9fjqt
  Aug 24 12:14:08.915: INFO: Created: latency-svc-nms7n
  Aug 24 12:14:08.935: INFO: Got endpoints: latency-svc-n2l8x [279.970643ms]
  Aug 24 12:14:08.964: INFO: Got endpoints: latency-svc-sxjpg [347.290524ms]
  Aug 24 12:14:08.964: INFO: Got endpoints: latency-svc-kptwp [562.998235ms]
  Aug 24 12:14:08.964: INFO: Got endpoints: latency-svc-hjq72 [562.734993ms]
  Aug 24 12:14:08.964: INFO: Got endpoints: latency-svc-ghwrz [474.155127ms]
  Aug 24 12:14:08.980: INFO: Got endpoints: latency-svc-nfmwn [304.97801ms]
  Aug 24 12:14:09.029: INFO: Got endpoints: latency-svc-gcffh [627.656707ms]
  Aug 24 12:14:09.029: INFO: Created: latency-svc-f2llz
  Aug 24 12:14:09.046: INFO: Got endpoints: latency-svc-zkq9s [645.124636ms]
  Aug 24 12:14:09.046: INFO: Got endpoints: latency-svc-fzbt9 [591.405299ms]
  Aug 24 12:14:09.047: INFO: Got endpoints: latency-svc-r95ht [429.835358ms]
  Aug 24 12:14:09.054: INFO: Got endpoints: latency-svc-75tfw [535.325935ms]
  Aug 24 12:14:09.120: INFO: Got endpoints: latency-svc-9fjqt [486.942134ms]
  Aug 24 12:14:09.121: INFO: Got endpoints: latency-svc-nms7n [719.166064ms]
  Aug 24 12:14:09.152: INFO: Got endpoints: latency-svc-2c444 [668.434332ms]
  Aug 24 12:14:09.153: INFO: Created: latency-svc-s98fg
  Aug 24 12:14:09.156: INFO: Got endpoints: latency-svc-f2llz [220.382639ms]
  Aug 24 12:14:09.156: INFO: Got endpoints: latency-svc-vpv8k [621.220674ms]
  Aug 24 12:14:09.181: INFO: Got endpoints: latency-svc-s98fg [216.565665ms]
  Aug 24 12:14:09.308: INFO: Created: latency-svc-hnz2q
  Aug 24 12:14:09.332: INFO: Created: latency-svc-44n8n
  Aug 24 12:14:09.333: INFO: Created: latency-svc-7mhtc
  Aug 24 12:14:09.333: INFO: Created: latency-svc-xpgds
  Aug 24 12:14:09.334: INFO: Created: latency-svc-nrzfl
  Aug 24 12:14:09.334: INFO: Created: latency-svc-8zvkr
  Aug 24 12:14:09.341: INFO: Created: latency-svc-2qp9x
  Aug 24 12:14:09.348: INFO: Created: latency-svc-t6mjw
  Aug 24 12:14:09.350: INFO: Created: latency-svc-cnp7l
  Aug 24 12:14:09.350: INFO: Created: latency-svc-l64m7
  Aug 24 12:14:09.351: INFO: Created: latency-svc-b4kh8
  Aug 24 12:14:09.353: INFO: Created: latency-svc-58mwg
  Aug 24 12:14:09.354: INFO: Created: latency-svc-x2wgn
  Aug 24 12:14:09.355: INFO: Created: latency-svc-jglsq
  Aug 24 12:14:09.355: INFO: Created: latency-svc-cs6bh
  Aug 24 12:14:09.374: INFO: Got endpoints: latency-svc-hnz2q [193.375111ms]
  Aug 24 12:14:09.401: INFO: Got endpoints: latency-svc-jglsq [280.624775ms]
  Aug 24 12:14:09.402: INFO: Got endpoints: latency-svc-cs6bh [347.403465ms]
  Aug 24 12:14:09.416: INFO: Got endpoints: latency-svc-x2wgn [259.669314ms]
  Aug 24 12:14:09.416: INFO: Got endpoints: latency-svc-44n8n [450.891792ms]
  Aug 24 12:14:09.417: INFO: Got endpoints: latency-svc-58mwg [436.232173ms]
  Aug 24 12:14:09.434: INFO: Got endpoints: latency-svc-xpgds [469.008916ms]
  Aug 24 12:14:09.449: INFO: Created: latency-svc-xh6mw
  Aug 24 12:14:09.477: INFO: Created: latency-svc-rcmdl
  Aug 24 12:14:09.484: INFO: Got endpoints: latency-svc-b4kh8 [519.619544ms]
  Aug 24 12:14:09.485: INFO: Got endpoints: latency-svc-nrzfl [438.261101ms]
  Aug 24 12:14:09.486: INFO: Got endpoints: latency-svc-7mhtc [333.75623ms]
  Aug 24 12:14:09.487: INFO: Got endpoints: latency-svc-8zvkr [366.779963ms]
  Aug 24 12:14:09.498: INFO: Got endpoints: latency-svc-l64m7 [451.618138ms]
  Aug 24 12:14:09.542: INFO: Got endpoints: latency-svc-2qp9x [386.00222ms]
  Aug 24 12:14:09.543: INFO: Got endpoints: latency-svc-xh6mw [169.001572ms]
  Aug 24 12:14:09.542: INFO: Got endpoints: latency-svc-t6mjw [513.217139ms]
  Aug 24 12:14:09.545: INFO: Created: latency-svc-xftn5
  Aug 24 12:14:09.542: INFO: Got endpoints: latency-svc-cnp7l [495.739364ms]
  Aug 24 12:14:09.558: INFO: Got endpoints: latency-svc-rcmdl [155.530329ms]
  Aug 24 12:14:09.567: INFO: Created: latency-svc-5n5gw
  Aug 24 12:14:09.578: INFO: Got endpoints: latency-svc-xftn5 [175.148316ms]
  Aug 24 12:14:09.639: INFO: Got endpoints: latency-svc-5n5gw [223.225242ms]
  Aug 24 12:14:09.640: INFO: Created: latency-svc-j7n6s
  Aug 24 12:14:09.654: INFO: Got endpoints: latency-svc-j7n6s [235.908137ms]
  Aug 24 12:14:09.842: INFO: Created: latency-svc-qqlrg
  Aug 24 12:14:09.843: INFO: Created: latency-svc-hf95b
  Aug 24 12:14:09.846: INFO: Created: latency-svc-s49hl
  Aug 24 12:14:09.846: INFO: Created: latency-svc-4pfwz
  Aug 24 12:14:09.847: INFO: Created: latency-svc-7tvtr
  Aug 24 12:14:09.847: INFO: Created: latency-svc-bz76z
  Aug 24 12:14:09.848: INFO: Created: latency-svc-z8ncl
  Aug 24 12:14:09.848: INFO: Created: latency-svc-hwj7x
  Aug 24 12:14:09.850: INFO: Created: latency-svc-k7z2f
  Aug 24 12:14:09.851: INFO: Created: latency-svc-qh8q4
  Aug 24 12:14:09.871: INFO: Created: latency-svc-q2qp5
  Aug 24 12:14:09.872: INFO: Created: latency-svc-t2sf5
  Aug 24 12:14:09.890: INFO: Created: latency-svc-t7wk6
  Aug 24 12:14:09.891: INFO: Created: latency-svc-qh2fr
  Aug 24 12:14:09.891: INFO: Got endpoints: latency-svc-t7wk6 [313.174881ms]
  Aug 24 12:14:09.891: INFO: Created: latency-svc-w27f2
  Aug 24 12:14:09.944: INFO: Got endpoints: latency-svc-hwj7x [400.225284ms]
  Aug 24 12:14:09.944: INFO: Got endpoints: latency-svc-qh2fr [401.496596ms]
  Aug 24 12:14:09.952: INFO: Got endpoints: latency-svc-qqlrg [465.671274ms]
  Aug 24 12:14:09.953: INFO: Got endpoints: latency-svc-hf95b [298.608453ms]
  Aug 24 12:14:09.973: INFO: Got endpoints: latency-svc-qh8q4 [487.808769ms]
  Aug 24 12:14:09.988: INFO: Got endpoints: latency-svc-w27f2 [348.743137ms]
  Aug 24 12:14:09.989: INFO: Got endpoints: latency-svc-t2sf5 [554.081876ms]
  Aug 24 12:14:09.989: INFO: Got endpoints: latency-svc-k7z2f [572.255504ms]
  Aug 24 12:14:10.031: INFO: Got endpoints: latency-svc-bz76z [485.32527ms]
  Aug 24 12:14:10.032: INFO: Got endpoints: latency-svc-q2qp5 [487.19316ms]
  Aug 24 12:14:10.032: INFO: Got endpoints: latency-svc-4pfwz [473.167856ms]
  Aug 24 12:14:10.048: INFO: Got endpoints: latency-svc-z8ncl [550.457157ms]
  Aug 24 12:14:10.053: INFO: Created: latency-svc-sz2dd
  Aug 24 12:14:10.058: INFO: Got endpoints: latency-svc-s49hl [571.254286ms]
  Aug 24 12:14:10.069: INFO: Got endpoints: latency-svc-7tvtr [584.054384ms]
  Aug 24 12:14:10.078: INFO: Created: latency-svc-9v9fz
  Aug 24 12:14:10.105: INFO: Got endpoints: latency-svc-sz2dd [213.9366ms]
  Aug 24 12:14:10.167: INFO: Got endpoints: latency-svc-9v9fz [223.006305ms]
  Aug 24 12:14:10.355: INFO: Created: latency-svc-w6scb
  Aug 24 12:14:10.356: INFO: Created: latency-svc-vbxdt
  Aug 24 12:14:10.358: INFO: Created: latency-svc-2tkvw
  Aug 24 12:14:10.358: INFO: Created: latency-svc-cvx8c
  Aug 24 12:14:10.365: INFO: Created: latency-svc-jrrgb
  Aug 24 12:14:10.365: INFO: Created: latency-svc-d5c9k
  Aug 24 12:14:10.365: INFO: Created: latency-svc-4ns7q
  Aug 24 12:14:10.366: INFO: Created: latency-svc-9pvnr
  Aug 24 12:14:10.385: INFO: Created: latency-svc-sggl6
  Aug 24 12:14:10.385: INFO: Created: latency-svc-m6nj9
  Aug 24 12:14:10.386: INFO: Created: latency-svc-7fqrl
  Aug 24 12:14:10.386: INFO: Created: latency-svc-p9sgd
  Aug 24 12:14:10.386: INFO: Created: latency-svc-77bwg
  Aug 24 12:14:10.386: INFO: Created: latency-svc-m74w9
  Aug 24 12:14:10.408: INFO: Created: latency-svc-qpxn9
  Aug 24 12:14:10.479: INFO: Got endpoints: latency-svc-vbxdt [490.171624ms]
  Aug 24 12:14:10.514: INFO: Got endpoints: latency-svc-w6scb [481.877064ms]
  Aug 24 12:14:10.515: INFO: Got endpoints: latency-svc-cvx8c [445.92823ms]
  Aug 24 12:14:10.583: INFO: Got endpoints: latency-svc-2tkvw [524.01244ms]
  Aug 24 12:14:10.583: INFO: Got endpoints: latency-svc-m6nj9 [550.770424ms]
  Aug 24 12:14:10.650: INFO: Created: latency-svc-mqlbd
  Aug 24 12:14:10.651: INFO: Got endpoints: latency-svc-4ns7q [602.129589ms]
  Aug 24 12:14:10.668: INFO: Got endpoints: latency-svc-9pvnr [563.072461ms]
  Aug 24 12:14:10.719: INFO: Created: latency-svc-5tbbc
  Aug 24 12:14:10.750: INFO: Got endpoints: latency-svc-m74w9 [805.015573ms]
  Aug 24 12:14:10.751: INFO: Got endpoints: latency-svc-d5c9k [797.662082ms]
  Aug 24 12:14:10.779: INFO: Got endpoints: latency-svc-sggl6 [611.464134ms]
  Aug 24 12:14:10.779: INFO: Got endpoints: latency-svc-77bwg [748.360588ms]
  Aug 24 12:14:10.784: INFO: Got endpoints: latency-svc-jrrgb [810.797667ms]
  Aug 24 12:14:10.786: INFO: Created: latency-svc-4dp8c
  Aug 24 12:14:10.820: INFO: Got endpoints: latency-svc-7fqrl [831.413442ms]
  Aug 24 12:14:10.835: INFO: Created: latency-svc-zk7sz
  Aug 24 12:14:10.839: INFO: Created: latency-svc-qpbqd
  Aug 24 12:14:10.863: INFO: Created: latency-svc-6v6nq
  Aug 24 12:14:10.869: INFO: Got endpoints: latency-svc-p9sgd [916.957757ms]
  Aug 24 12:14:10.873: INFO: Created: latency-svc-l2sv9
  Aug 24 12:14:10.900: INFO: Created: latency-svc-nqdwj
  Aug 24 12:14:10.908: INFO: Got endpoints: latency-svc-qpxn9 [919.078292ms]
  Aug 24 12:14:10.924: INFO: Created: latency-svc-n7q9n
  Aug 24 12:14:10.937: INFO: Created: latency-svc-9p2ps
  Aug 24 12:14:10.947: INFO: Got endpoints: latency-svc-mqlbd [467.611387ms]
  Aug 24 12:14:10.955: INFO: Created: latency-svc-vv7l8
  Aug 24 12:14:10.988: INFO: Created: latency-svc-4cgl5
  Aug 24 12:14:11.012: INFO: Created: latency-svc-blq9l
  Aug 24 12:14:11.021: INFO: Got endpoints: latency-svc-5tbbc [507.166089ms]
  Aug 24 12:14:11.050: INFO: Created: latency-svc-kdg5c
  Aug 24 12:14:11.070: INFO: Got endpoints: latency-svc-4dp8c [554.757268ms]
  Aug 24 12:14:11.113: INFO: Created: latency-svc-g4vkk
  Aug 24 12:14:11.116: INFO: Got endpoints: latency-svc-zk7sz [533.490751ms]
  Aug 24 12:14:11.179: INFO: Created: latency-svc-hdd6h
  Aug 24 12:14:11.188: INFO: Got endpoints: latency-svc-qpbqd [604.60969ms]
  Aug 24 12:14:11.208: INFO: Got endpoints: latency-svc-6v6nq [556.662661ms]
  Aug 24 12:14:11.215: INFO: Created: latency-svc-4n2wt
  Aug 24 12:14:11.256: INFO: Got endpoints: latency-svc-l2sv9 [587.866646ms]
  Aug 24 12:14:11.309: INFO: Got endpoints: latency-svc-nqdwj [558.545547ms]
  Aug 24 12:14:11.343: INFO: Created: latency-svc-sb2hf
  Aug 24 12:14:11.343: INFO: Created: latency-svc-6mbdp
  Aug 24 12:14:11.345: INFO: Created: latency-svc-tvbrk
  Aug 24 12:14:11.352: INFO: Created: latency-svc-mxdsj
  Aug 24 12:14:11.353: INFO: Created: latency-svc-glfnl
  Aug 24 12:14:11.372: INFO: Created: latency-svc-hcvwz
  Aug 24 12:14:11.373: INFO: Got endpoints: latency-svc-n7q9n [622.403608ms]
  Aug 24 12:14:11.396: INFO: Got endpoints: latency-svc-9p2ps [617.249529ms]
  Aug 24 12:14:11.401: INFO: Created: latency-svc-n9qvt
  Aug 24 12:14:11.435: INFO: Created: latency-svc-tt84b
  Aug 24 12:14:11.459: INFO: Got endpoints: latency-svc-vv7l8 [675.167856ms]
  Aug 24 12:14:11.513: INFO: Got endpoints: latency-svc-4cgl5 [733.939066ms]
  Aug 24 12:14:11.528: INFO: Created: latency-svc-m465q
  Aug 24 12:14:11.547: INFO: Got endpoints: latency-svc-blq9l [727.29933ms]
  Aug 24 12:14:11.555: INFO: Created: latency-svc-qrq4s
  Aug 24 12:14:11.573: INFO: Created: latency-svc-4mffg
  Aug 24 12:14:11.597: INFO: Got endpoints: latency-svc-kdg5c [727.282175ms]
  Aug 24 12:14:11.624: INFO: Created: latency-svc-wgfgc
  Aug 24 12:14:11.659: INFO: Got endpoints: latency-svc-g4vkk [751.224354ms]
  Aug 24 12:14:11.693: INFO: Created: latency-svc-gpfrs
  Aug 24 12:14:11.704: INFO: Got endpoints: latency-svc-hdd6h [756.402715ms]
  Aug 24 12:14:11.737: INFO: Created: latency-svc-6cptp
  Aug 24 12:14:11.747: INFO: Got endpoints: latency-svc-4n2wt [726.453821ms]
  Aug 24 12:14:11.776: INFO: Created: latency-svc-vvxrp
  Aug 24 12:14:11.801: INFO: Got endpoints: latency-svc-6mbdp [612.538824ms]
  Aug 24 12:14:11.829: INFO: Created: latency-svc-kgjhq
  Aug 24 12:14:11.847: INFO: Got endpoints: latency-svc-tvbrk [590.429815ms]
  Aug 24 12:14:11.875: INFO: Created: latency-svc-vr9hs
  Aug 24 12:14:11.901: INFO: Got endpoints: latency-svc-sb2hf [830.603107ms]
  Aug 24 12:14:11.933: INFO: Created: latency-svc-kcmgd
  Aug 24 12:14:11.965: INFO: Got endpoints: latency-svc-mxdsj [848.990808ms]
  Aug 24 12:14:11.992: INFO: Created: latency-svc-8zfg5
  Aug 24 12:14:12.013: INFO: Got endpoints: latency-svc-glfnl [804.642152ms]
  Aug 24 12:14:12.039: INFO: Created: latency-svc-zcpkb
  Aug 24 12:14:12.051: INFO: Got endpoints: latency-svc-hcvwz [742.554377ms]
  Aug 24 12:14:12.073: INFO: Created: latency-svc-bnp79
  Aug 24 12:14:12.101: INFO: Got endpoints: latency-svc-n9qvt [728.076522ms]
  Aug 24 12:14:12.123: INFO: Created: latency-svc-jxstj
  Aug 24 12:14:12.151: INFO: Got endpoints: latency-svc-tt84b [755.013704ms]
  Aug 24 12:14:12.173: INFO: Created: latency-svc-jwxmh
  Aug 24 12:14:12.199: INFO: Got endpoints: latency-svc-m465q [739.92178ms]
  Aug 24 12:14:12.222: INFO: Created: latency-svc-gcgqt
  Aug 24 12:14:12.251: INFO: Got endpoints: latency-svc-qrq4s [736.794144ms]
  Aug 24 12:14:12.275: INFO: Created: latency-svc-rhkt2
  Aug 24 12:14:12.298: INFO: Got endpoints: latency-svc-4mffg [750.712053ms]
  Aug 24 12:14:12.329: INFO: Created: latency-svc-p5b5g
  Aug 24 12:14:12.360: INFO: Got endpoints: latency-svc-wgfgc [762.169074ms]
  Aug 24 12:14:12.386: INFO: Created: latency-svc-pqt7g
  Aug 24 12:14:12.401: INFO: Got endpoints: latency-svc-gpfrs [741.25353ms]
  Aug 24 12:14:12.427: INFO: Created: latency-svc-rjkcb
  Aug 24 12:14:12.453: INFO: Got endpoints: latency-svc-6cptp [748.974673ms]
  Aug 24 12:14:12.479: INFO: Created: latency-svc-x44kx
  Aug 24 12:14:12.498: INFO: Got endpoints: latency-svc-vvxrp [750.172546ms]
  Aug 24 12:14:12.530: INFO: Created: latency-svc-qnsfq
  Aug 24 12:14:12.550: INFO: Got endpoints: latency-svc-kgjhq [749.06238ms]
  Aug 24 12:14:12.606: INFO: Created: latency-svc-996xq
  Aug 24 12:14:12.607: INFO: Got endpoints: latency-svc-vr9hs [759.816861ms]
  Aug 24 12:14:12.632: INFO: Created: latency-svc-s4jkc
  Aug 24 12:14:12.664: INFO: Got endpoints: latency-svc-kcmgd [762.624224ms]
  Aug 24 12:14:12.695: INFO: Created: latency-svc-kpfx6
  Aug 24 12:14:12.704: INFO: Got endpoints: latency-svc-8zfg5 [737.86396ms]
  Aug 24 12:14:12.730: INFO: Created: latency-svc-v2tsf
  Aug 24 12:14:12.749: INFO: Got endpoints: latency-svc-zcpkb [735.55294ms]
  Aug 24 12:14:12.779: INFO: Created: latency-svc-c9lbs
  Aug 24 12:14:12.803: INFO: Got endpoints: latency-svc-bnp79 [751.127318ms]
  Aug 24 12:14:12.848: INFO: Got endpoints: latency-svc-jxstj [746.769157ms]
  Aug 24 12:14:12.853: INFO: Created: latency-svc-v9xtv
  Aug 24 12:14:12.877: INFO: Created: latency-svc-7fgb6
  Aug 24 12:14:12.907: INFO: Got endpoints: latency-svc-jwxmh [755.108112ms]
  Aug 24 12:14:12.928: INFO: Created: latency-svc-2mktr
  Aug 24 12:14:12.950: INFO: Got endpoints: latency-svc-gcgqt [750.924115ms]
  Aug 24 12:14:12.979: INFO: Created: latency-svc-szkc4
  Aug 24 12:14:13.001: INFO: Got endpoints: latency-svc-rhkt2 [750.492067ms]
  Aug 24 12:14:13.029: INFO: Created: latency-svc-h24mx
  Aug 24 12:14:13.052: INFO: Got endpoints: latency-svc-p5b5g [753.737315ms]
  Aug 24 12:14:13.073: INFO: Created: latency-svc-wd2dj
  Aug 24 12:14:13.099: INFO: Got endpoints: latency-svc-pqt7g [739.055556ms]
  Aug 24 12:14:13.131: INFO: Created: latency-svc-7d9z2
  Aug 24 12:14:13.151: INFO: Got endpoints: latency-svc-rjkcb [749.200238ms]
  Aug 24 12:14:13.173: INFO: Created: latency-svc-xz4s9
  Aug 24 12:14:13.206: INFO: Got endpoints: latency-svc-x44kx [753.147297ms]
  Aug 24 12:14:13.227: INFO: Created: latency-svc-v6zj9
  Aug 24 12:14:13.250: INFO: Got endpoints: latency-svc-qnsfq [752.269233ms]
  Aug 24 12:14:13.275: INFO: Created: latency-svc-rsr8s
  Aug 24 12:14:13.300: INFO: Got endpoints: latency-svc-996xq [748.807411ms]
  Aug 24 12:14:13.321: INFO: Created: latency-svc-fjl2c
  Aug 24 12:14:13.350: INFO: Got endpoints: latency-svc-s4jkc [742.834618ms]
  Aug 24 12:14:13.374: INFO: Created: latency-svc-twcbt
  Aug 24 12:14:13.399: INFO: Got endpoints: latency-svc-kpfx6 [734.442895ms]
  Aug 24 12:14:13.431: INFO: Created: latency-svc-dzl62
  Aug 24 12:14:13.450: INFO: Got endpoints: latency-svc-v2tsf [745.74887ms]
  Aug 24 12:14:13.481: INFO: Created: latency-svc-9xwt6
  Aug 24 12:14:13.501: INFO: Got endpoints: latency-svc-c9lbs [751.795418ms]
  Aug 24 12:14:13.533: INFO: Created: latency-svc-ndddw
  Aug 24 12:14:13.605: INFO: Got endpoints: latency-svc-v9xtv [801.839532ms]
  Aug 24 12:14:13.636: INFO: Got endpoints: latency-svc-7fgb6 [787.115609ms]
  Aug 24 12:14:13.660: INFO: Got endpoints: latency-svc-2mktr [753.379935ms]
  Aug 24 12:14:13.710: INFO: Created: latency-svc-7l8sn
  Aug 24 12:14:13.725: INFO: Got endpoints: latency-svc-szkc4 [773.00256ms]
  Aug 24 12:14:13.739: INFO: Created: latency-svc-kl5fk
  Aug 24 12:14:13.752: INFO: Got endpoints: latency-svc-h24mx [750.417063ms]
  Aug 24 12:14:13.774: INFO: Created: latency-svc-fp8f5
  Aug 24 12:14:13.784: INFO: Created: latency-svc-sv7jk
  Aug 24 12:14:13.797: INFO: Created: latency-svc-vkp44
  Aug 24 12:14:13.801: INFO: Got endpoints: latency-svc-wd2dj [748.606085ms]
  Aug 24 12:14:13.827: INFO: Created: latency-svc-p9xhw
  Aug 24 12:14:13.850: INFO: Got endpoints: latency-svc-7d9z2 [750.218228ms]
  Aug 24 12:14:13.879: INFO: Created: latency-svc-t4rqv
  Aug 24 12:14:13.903: INFO: Got endpoints: latency-svc-xz4s9 [750.748355ms]
  Aug 24 12:14:13.928: INFO: Created: latency-svc-t2sv8
  Aug 24 12:14:13.949: INFO: Got endpoints: latency-svc-v6zj9 [741.857354ms]
  Aug 24 12:14:13.970: INFO: Created: latency-svc-hsf2g
  Aug 24 12:14:13.999: INFO: Got endpoints: latency-svc-rsr8s [748.915016ms]
  Aug 24 12:14:14.032: INFO: Created: latency-svc-jmp7j
  Aug 24 12:14:14.059: INFO: Got endpoints: latency-svc-fjl2c [759.2355ms]
  Aug 24 12:14:14.083: INFO: Created: latency-svc-hwcpr
  Aug 24 12:14:14.107: INFO: Got endpoints: latency-svc-twcbt [756.533482ms]
  Aug 24 12:14:14.130: INFO: Created: latency-svc-qbwhc
  Aug 24 12:14:14.147: INFO: Got endpoints: latency-svc-dzl62 [747.204993ms]
  Aug 24 12:14:14.181: INFO: Created: latency-svc-wqn85
  Aug 24 12:14:14.205: INFO: Got endpoints: latency-svc-9xwt6 [754.903726ms]
  Aug 24 12:14:14.226: INFO: Created: latency-svc-kph2s
  Aug 24 12:14:14.253: INFO: Got endpoints: latency-svc-ndddw [751.484952ms]
  Aug 24 12:14:14.288: INFO: Created: latency-svc-gz8cr
  Aug 24 12:14:14.309: INFO: Got endpoints: latency-svc-7l8sn [703.533444ms]
  Aug 24 12:14:14.340: INFO: Created: latency-svc-4v6sg
  Aug 24 12:14:14.349: INFO: Got endpoints: latency-svc-kl5fk [712.673241ms]
  Aug 24 12:14:14.377: INFO: Created: latency-svc-qhvlb
  Aug 24 12:14:14.399: INFO: Got endpoints: latency-svc-fp8f5 [737.906837ms]
  Aug 24 12:14:14.418: INFO: Created: latency-svc-24b9x
  Aug 24 12:14:14.447: INFO: Got endpoints: latency-svc-sv7jk [722.528937ms]
  Aug 24 12:14:14.477: INFO: Created: latency-svc-hzssn
  Aug 24 12:14:14.499: INFO: Got endpoints: latency-svc-vkp44 [747.554783ms]
  Aug 24 12:14:14.536: INFO: Created: latency-svc-4d6v6
  Aug 24 12:14:14.549: INFO: Got endpoints: latency-svc-p9xhw [747.386718ms]
  Aug 24 12:14:14.570: INFO: Created: latency-svc-vv8r2
  Aug 24 12:14:14.603: INFO: Got endpoints: latency-svc-t4rqv [752.7621ms]
  Aug 24 12:14:14.623: INFO: Created: latency-svc-q667l
  Aug 24 12:14:14.652: INFO: Got endpoints: latency-svc-t2sv8 [748.926618ms]
  Aug 24 12:14:14.675: INFO: Created: latency-svc-zrkvw
  Aug 24 12:14:14.698: INFO: Got endpoints: latency-svc-hsf2g [749.234208ms]
  Aug 24 12:14:14.723: INFO: Created: latency-svc-vdmq4
  Aug 24 12:14:14.763: INFO: Got endpoints: latency-svc-jmp7j [763.206484ms]
  Aug 24 12:14:14.783: INFO: Created: latency-svc-rdf8v
  Aug 24 12:14:14.797: INFO: Got endpoints: latency-svc-hwcpr [737.419621ms]
  Aug 24 12:14:14.819: INFO: Created: latency-svc-6jjhp
  Aug 24 12:14:14.856: INFO: Got endpoints: latency-svc-qbwhc [748.302294ms]
  Aug 24 12:14:14.886: INFO: Created: latency-svc-n7qc8
  Aug 24 12:14:14.898: INFO: Got endpoints: latency-svc-wqn85 [750.606659ms]
  Aug 24 12:14:14.925: INFO: Created: latency-svc-spglf
  Aug 24 12:14:14.948: INFO: Got endpoints: latency-svc-kph2s [742.582434ms]
  Aug 24 12:14:14.977: INFO: Created: latency-svc-x2852
  Aug 24 12:14:15.008: INFO: Got endpoints: latency-svc-gz8cr [754.722446ms]
  Aug 24 12:14:15.027: INFO: Created: latency-svc-j5fns
  Aug 24 12:14:15.057: INFO: Got endpoints: latency-svc-4v6sg [747.887566ms]
  Aug 24 12:14:15.083: INFO: Created: latency-svc-7skwj
  Aug 24 12:14:15.101: INFO: Got endpoints: latency-svc-qhvlb [751.434911ms]
  Aug 24 12:14:15.121: INFO: Created: latency-svc-86tr4
  Aug 24 12:14:15.150: INFO: Got endpoints: latency-svc-24b9x [751.456224ms]
  Aug 24 12:14:15.171: INFO: Created: latency-svc-fwhn4
  Aug 24 12:14:15.201: INFO: Got endpoints: latency-svc-hzssn [753.165784ms]
  Aug 24 12:14:15.222: INFO: Created: latency-svc-t67zm
  Aug 24 12:14:15.259: INFO: Got endpoints: latency-svc-4d6v6 [759.6495ms]
  Aug 24 12:14:15.284: INFO: Created: latency-svc-9zrcf
  Aug 24 12:14:15.297: INFO: Got endpoints: latency-svc-vv8r2 [747.102772ms]
  Aug 24 12:14:15.318: INFO: Created: latency-svc-bs998
  Aug 24 12:14:15.347: INFO: Got endpoints: latency-svc-q667l [743.934072ms]
  Aug 24 12:14:15.373: INFO: Created: latency-svc-8tsrr
  Aug 24 12:14:15.406: INFO: Got endpoints: latency-svc-zrkvw [754.343158ms]
  Aug 24 12:14:15.436: INFO: Created: latency-svc-6tlsp
  Aug 24 12:14:15.465: INFO: Got endpoints: latency-svc-vdmq4 [767.047495ms]
  Aug 24 12:14:15.488: INFO: Created: latency-svc-4ttqj
  Aug 24 12:14:15.511: INFO: Got endpoints: latency-svc-rdf8v [747.639581ms]
  Aug 24 12:14:15.543: INFO: Created: latency-svc-sm9wc
  Aug 24 12:14:15.568: INFO: Got endpoints: latency-svc-6jjhp [771.13266ms]
  Aug 24 12:14:15.615: INFO: Got endpoints: latency-svc-n7qc8 [759.628842ms]
  Aug 24 12:14:15.623: INFO: Created: latency-svc-kn2sr
  Aug 24 12:14:15.672: INFO: Got endpoints: latency-svc-spglf [773.644063ms]
  Aug 24 12:14:15.672: INFO: Created: latency-svc-qrx7r
  Aug 24 12:14:15.709: INFO: Created: latency-svc-rmqvk
  Aug 24 12:14:15.710: INFO: Got endpoints: latency-svc-x2852 [761.320379ms]
  Aug 24 12:14:15.734: INFO: Created: latency-svc-x2dtm
  Aug 24 12:14:15.747: INFO: Got endpoints: latency-svc-j5fns [738.853034ms]
  Aug 24 12:14:15.769: INFO: Created: latency-svc-kx9tb
  Aug 24 12:14:15.799: INFO: Got endpoints: latency-svc-7skwj [741.836621ms]
  Aug 24 12:14:15.831: INFO: Created: latency-svc-kfhbh
  Aug 24 12:14:15.849: INFO: Got endpoints: latency-svc-86tr4 [747.900384ms]
  Aug 24 12:14:15.875: INFO: Created: latency-svc-ms8xf
  Aug 24 12:14:15.900: INFO: Got endpoints: latency-svc-fwhn4 [749.452225ms]
  Aug 24 12:14:15.922: INFO: Created: latency-svc-sh7hs
  Aug 24 12:14:15.948: INFO: Got endpoints: latency-svc-t67zm [747.452045ms]
  Aug 24 12:14:15.985: INFO: Created: latency-svc-cftgh
  Aug 24 12:14:16.010: INFO: Got endpoints: latency-svc-9zrcf [750.474969ms]
  Aug 24 12:14:16.048: INFO: Created: latency-svc-wgmhz
  Aug 24 12:14:16.057: INFO: Got endpoints: latency-svc-bs998 [759.601217ms]
  Aug 24 12:14:16.078: INFO: Created: latency-svc-98swz
  Aug 24 12:14:16.100: INFO: Got endpoints: latency-svc-8tsrr [752.950345ms]
  Aug 24 12:14:16.121: INFO: Created: latency-svc-8q6mc
  Aug 24 12:14:16.149: INFO: Got endpoints: latency-svc-6tlsp [742.214192ms]
  Aug 24 12:14:16.173: INFO: Created: latency-svc-g75ns
  Aug 24 12:14:16.199: INFO: Got endpoints: latency-svc-4ttqj [733.629097ms]
  Aug 24 12:14:16.223: INFO: Created: latency-svc-mf7ht
  Aug 24 12:14:16.252: INFO: Got endpoints: latency-svc-sm9wc [741.554044ms]
  Aug 24 12:14:16.299: INFO: Got endpoints: latency-svc-kn2sr [730.811953ms]
  Aug 24 12:14:16.353: INFO: Got endpoints: latency-svc-qrx7r [737.402428ms]
  Aug 24 12:14:16.400: INFO: Got endpoints: latency-svc-rmqvk [727.718673ms]
  Aug 24 12:14:16.449: INFO: Got endpoints: latency-svc-x2dtm [739.412862ms]
  Aug 24 12:14:16.499: INFO: Got endpoints: latency-svc-kx9tb [751.516381ms]
  Aug 24 12:14:16.549: INFO: Got endpoints: latency-svc-kfhbh [749.86372ms]
  Aug 24 12:14:16.598: INFO: Got endpoints: latency-svc-ms8xf [749.052989ms]
  Aug 24 12:14:16.650: INFO: Got endpoints: latency-svc-sh7hs [749.849616ms]
  Aug 24 12:14:16.700: INFO: Got endpoints: latency-svc-cftgh [751.692163ms]
  Aug 24 12:14:16.753: INFO: Got endpoints: latency-svc-wgmhz [741.928715ms]
  Aug 24 12:14:16.800: INFO: Got endpoints: latency-svc-98swz [742.910582ms]
  Aug 24 12:14:16.853: INFO: Got endpoints: latency-svc-8q6mc [753.199072ms]
  Aug 24 12:14:16.898: INFO: Got endpoints: latency-svc-g75ns [748.692294ms]
  Aug 24 12:14:16.951: INFO: Got endpoints: latency-svc-mf7ht [751.574291ms]
  Aug 24 12:14:16.951: INFO: Latencies: [54.670717ms 82.595098ms 89.596406ms 118.820031ms 134.86981ms 155.530329ms 169.001572ms 175.148316ms 193.375111ms 213.9366ms 215.780028ms 216.529219ms 216.565665ms 220.382639ms 223.006305ms 223.225242ms 232.349741ms 235.908137ms 254.041286ms 259.669314ms 274.655557ms 279.970643ms 280.624775ms 298.608453ms 304.97801ms 313.174881ms 333.75623ms 347.290524ms 347.403465ms 348.743137ms 366.779963ms 386.00222ms 400.225284ms 401.496596ms 429.835358ms 436.232173ms 438.261101ms 445.92823ms 450.891792ms 451.618138ms 465.671274ms 467.611387ms 469.008916ms 473.167856ms 474.155127ms 481.877064ms 485.32527ms 486.942134ms 487.19316ms 487.808769ms 490.171624ms 495.739364ms 507.166089ms 513.217139ms 519.619544ms 524.01244ms 533.490751ms 535.325935ms 550.457157ms 550.770424ms 554.081876ms 554.757268ms 556.662661ms 558.545547ms 562.734993ms 562.998235ms 563.072461ms 571.254286ms 572.255504ms 584.054384ms 587.866646ms 590.429815ms 591.405299ms 602.129589ms 604.60969ms 611.464134ms 612.538824ms 617.249529ms 621.220674ms 622.403608ms 627.656707ms 645.124636ms 668.434332ms 675.167856ms 703.533444ms 712.673241ms 719.166064ms 722.528937ms 726.453821ms 727.282175ms 727.29933ms 727.718673ms 728.076522ms 730.811953ms 733.629097ms 733.939066ms 734.442895ms 735.55294ms 736.794144ms 737.402428ms 737.419621ms 737.86396ms 737.906837ms 738.853034ms 739.055556ms 739.412862ms 739.92178ms 741.25353ms 741.554044ms 741.836621ms 741.857354ms 741.928715ms 742.214192ms 742.554377ms 742.582434ms 742.834618ms 742.910582ms 743.934072ms 745.74887ms 746.769157ms 747.102772ms 747.204993ms 747.386718ms 747.452045ms 747.554783ms 747.639581ms 747.887566ms 747.900384ms 748.302294ms 748.360588ms 748.606085ms 748.692294ms 748.807411ms 748.915016ms 748.926618ms 748.974673ms 749.052989ms 749.06238ms 749.200238ms 749.234208ms 749.452225ms 749.849616ms 749.86372ms 750.172546ms 750.218228ms 750.417063ms 750.474969ms 750.492067ms 750.606659ms 750.712053ms 750.748355ms 750.924115ms 751.127318ms 751.224354ms 751.434911ms 751.456224ms 751.484952ms 751.516381ms 751.574291ms 751.692163ms 751.795418ms 752.269233ms 752.7621ms 752.950345ms 753.147297ms 753.165784ms 753.199072ms 753.379935ms 753.737315ms 754.343158ms 754.722446ms 754.903726ms 755.013704ms 755.108112ms 756.402715ms 756.533482ms 759.2355ms 759.601217ms 759.628842ms 759.6495ms 759.816861ms 761.320379ms 762.169074ms 762.624224ms 763.206484ms 767.047495ms 771.13266ms 773.00256ms 773.644063ms 787.115609ms 797.662082ms 801.839532ms 804.642152ms 805.015573ms 810.797667ms 830.603107ms 831.413442ms 848.990808ms 916.957757ms 919.078292ms]
  Aug 24 12:14:16.952: INFO: 50 %ile: 737.419621ms
  Aug 24 12:14:16.952: INFO: 90 %ile: 759.816861ms
  Aug 24 12:14:16.952: INFO: 99 %ile: 916.957757ms
  Aug 24 12:14:16.952: INFO: Total sample count: 200
  Aug 24 12:14:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-9772" for this suite. @ 08/24/23 12:14:16.969
• [10.860 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 08/24/23 12:14:16.995
  Aug 24 12:14:16.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:14:16.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:17.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:17.033
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:14:17.052
  STEP: Waiting for all pods to be running @ 08/24/23 12:14:19.109
  Aug 24 12:14:19.144: INFO: running pods: 0 < 3
  Aug 24 12:14:21.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5457" for this suite. @ 08/24/23 12:14:21.173
• [4.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 08/24/23 12:14:21.21
  Aug 24 12:14:21.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:14:21.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:21.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:21.246
  STEP: Setting up server cert @ 08/24/23 12:14:21.289
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:14:22.405
  STEP: Deploying the webhook pod @ 08/24/23 12:14:22.478
  STEP: Wait for the deployment to be ready @ 08/24/23 12:14:22.598
  Aug 24 12:14:22.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 12:14:24.683
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:14:24.719
  Aug 24 12:14:25.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:26.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:27.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:28.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:29.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:30.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:31.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:32.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:33.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:14:34.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 08/24/23 12:14:34.728
  Aug 24 12:14:34.770: INFO: Waiting for webhook configuration to be ready...
  STEP: create a configmap that should be updated by the webhook @ 08/24/23 12:14:34.89
  Aug 24 12:14:34.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7940" for this suite. @ 08/24/23 12:14:35.09
  STEP: Destroying namespace "webhook-markers-1306" for this suite. @ 08/24/23 12:14:35.106
• [13.921 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 08/24/23 12:14:35.136
  Aug 24 12:14:35.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:14:35.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:35.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:35.183
  STEP: Creating simple DaemonSet "daemon-set" @ 08/24/23 12:14:35.227
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 12:14:35.246
  Aug 24 12:14:35.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:14:35.265: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 12:14:36.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:14:36.292: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 12:14:37.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:14:37.285: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 08/24/23 12:14:37.294
  Aug 24 12:14:37.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:14:37.338: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 12:14:38.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:14:38.363: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 12:14:39.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:14:39.365: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  Aug 24 12:14:40.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:14:40.357: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:14:40.368
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3941, will wait for the garbage collector to delete the pods @ 08/24/23 12:14:40.369
  Aug 24 12:14:40.441: INFO: Deleting DaemonSet.extensions daemon-set took: 11.612917ms
  Aug 24 12:14:40.543: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.801514ms
  Aug 24 12:14:42.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:14:42.252: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:14:42.257: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16349"},"items":null}

  Aug 24 12:14:42.261: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16349"},"items":null}

  Aug 24 12:14:42.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3941" for this suite. @ 08/24/23 12:14:42.29
• [7.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 08/24/23 12:14:42.312
  Aug 24 12:14:42.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:14:42.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:42.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:42.347
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:14:42.352
  STEP: Saw pod success @ 08/24/23 12:14:46.396
  Aug 24 12:14:46.404: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-36eaf98f-705c-452a-ad58-4c6215b8e08c container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:14:46.419
  Aug 24 12:14:46.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1209" for this suite. @ 08/24/23 12:14:46.452
• [4.151 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 08/24/23 12:14:46.463
  Aug 24 12:14:46.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:14:46.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:46.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:46.496
  STEP: Creating a pod to test substitution in container's command @ 08/24/23 12:14:46.5
  STEP: Saw pod success @ 08/24/23 12:14:50.545
  Aug 24 12:14:50.552: INFO: Trying to get logs from node pohje9aimahx-3 pod var-expansion-56008d6c-83ab-47a5-84f0-c08b384c9eb2 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:14:50.568
  Aug 24 12:14:50.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2811" for this suite. @ 08/24/23 12:14:50.611
• [4.159 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 08/24/23 12:14:50.625
  Aug 24 12:14:50.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 12:14:50.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:50.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:50.659
  STEP: creating a ReplicationController @ 08/24/23 12:14:50.669
  STEP: waiting for RC to be added @ 08/24/23 12:14:50.68
  STEP: waiting for available Replicas @ 08/24/23 12:14:50.683
  STEP: patching ReplicationController @ 08/24/23 12:14:52.367
  STEP: waiting for RC to be modified @ 08/24/23 12:14:52.38
  STEP: patching ReplicationController status @ 08/24/23 12:14:52.381
  STEP: waiting for RC to be modified @ 08/24/23 12:14:52.39
  STEP: waiting for available Replicas @ 08/24/23 12:14:52.391
  STEP: fetching ReplicationController status @ 08/24/23 12:14:52.402
  STEP: patching ReplicationController scale @ 08/24/23 12:14:52.409
  STEP: waiting for RC to be modified @ 08/24/23 12:14:52.418
  STEP: waiting for ReplicationController's scale to be the max amount @ 08/24/23 12:14:52.419
  STEP: fetching ReplicationController; ensuring that it's patched @ 08/24/23 12:14:54.163
  STEP: updating ReplicationController status @ 08/24/23 12:14:54.171
  STEP: waiting for RC to be modified @ 08/24/23 12:14:54.186
  STEP: listing all ReplicationControllers @ 08/24/23 12:14:54.188
  STEP: checking that ReplicationController has expected values @ 08/24/23 12:14:54.193
  STEP: deleting ReplicationControllers by collection @ 08/24/23 12:14:54.193
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 08/24/23 12:14:54.211
  Aug 24 12:14:54.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 12:14:54.294995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-2666" for this suite. @ 08/24/23 12:14:54.309
• [3.696 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 08/24/23 12:14:54.329
  Aug 24 12:14:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:14:54.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:54.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:54.434
  Aug 24 12:14:54.439: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:14:54.464: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:14:54.471: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-1 before test
  Aug 24 12:14:54.486: INFO: cilium-5bz85 from kube-system started at 2023-08-24 11:30:24 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.487: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:14:54.487: INFO: cilium-node-init-js6v2 from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.487: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:14:54.487: INFO: coredns-5d78c9869d-4zkjt from kube-system started at 2023-08-24 12:00:11 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.487: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:14:54.487: INFO: kube-addon-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.487: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:14:54.487: INFO: kube-apiserver-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.488: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:14:54.488: INFO: kube-controller-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.488: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:14:54.488: INFO: kube-proxy-l6rtn from kube-system started at 2023-08-24 11:27:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.488: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:14:54.488: INFO: kube-scheduler-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.488: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:14:54.488: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-z7825 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:14:54.489: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:14:54.489: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:14:54.489: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-2 before test
  Aug 24 12:14:54.523: INFO: cilium-node-init-xrb2l from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: cilium-zx72t from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: coredns-5d78c9869d-znmdb from kube-system started at 2023-08-24 11:31:21 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: kube-addon-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: kube-apiserver-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: kube-controller-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: kube-proxy-nz65t from kube-system started at 2023-08-24 11:28:08 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: kube-scheduler-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: rc-test-l9cwk from replication-controller-2666 started at 2023-08-24 12:14:52 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container rc-test ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-8jtw4 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:14:54.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:14:54.523: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-3 before test
  Aug 24 12:14:54.552: INFO: cilium-node-init-42bmw from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.553: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:14:54.554: INFO: cilium-operator-b8f479cd9-gv7jv from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.554: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:14:54.555: INFO: cilium-xptxb from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.555: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:14:54.555: INFO: kube-proxy-vtcsn from kube-system started at 2023-08-24 11:28:50 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.556: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:14:54.557: INFO: rc-test-9qsnr from replication-controller-2666 started at 2023-08-24 12:14:50 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.557: INFO: 	Container rc-test ready: true, restart count 0
  Aug 24 12:14:54.557: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:39:28 +0000 UTC (1 container statuses recorded)
  Aug 24 12:14:54.558: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:14:54.558: INFO: sonobuoy-e2e-job-ee97c55b29594c3a from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:14:54.558: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:14:54.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:14:54.559: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-4l69j from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:14:54.559: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:14:54.560: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/24/23 12:14:54.56
  E0824 12:14:55.295282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:14:56.295588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/24/23 12:14:56.603
  STEP: Trying to apply a random label on the found node. @ 08/24/23 12:14:56.634
  STEP: verifying the node has the label kubernetes.io/e2e-e7b91704-53be-42f6-9ddc-24abc65177ff 95 @ 08/24/23 12:14:56.651
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 08/24/23 12:14:56.666
  E0824 12:14:57.296856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:14:58.296144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.5 on the node which pod4 resides and expect not scheduled @ 08/24/23 12:14:58.708
  E0824 12:14:59.297092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:00.297524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:01.297521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:02.297759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:03.298835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:04.299696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:05.300415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:06.300667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:07.301513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:08.301820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:09.302675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:10.302687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:11.303397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:12.303737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:13.304920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:14.305312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:15.306102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:16.306847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:17.307045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:18.307810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:19.308761      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:20.309272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:21.311703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:22.311362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:23.311589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:24.321912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:25.314521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:26.314584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:27.314809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:28.315384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:29.316056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:30.315422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:31.315746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:32.315985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:33.316939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:34.317622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:35.317869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:36.318007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:37.319124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:38.319245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:39.326876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:40.319855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:41.320129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:42.320268      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:43.321360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:44.322395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:45.322824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:46.325420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:47.324299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:48.324436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:49.325443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:50.325753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:51.325925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:52.326252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:53.327033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:54.327764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:55.328968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:56.329138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:57.329283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:58.329494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:15:59.329462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:00.329733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:01.329874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:02.330124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:03.330779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:04.330968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:05.331755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:06.331700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:07.332874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:08.333878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:09.334691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:10.335094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:11.335186      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:12.335553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:13.335945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:14.336231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:15.337219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:16.337977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:17.338482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:18.339509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:19.340631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:20.340844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:21.341666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:22.342359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:23.343026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:24.344118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:25.345557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:26.346623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:27.347620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:28.347859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:29.348276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:30.348887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:31.349338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:32.350272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:33.350627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:34.351257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:35.352204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:36.352439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:37.353205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:38.353438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:39.354464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:40.354875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:41.355976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:42.356214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:43.356700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:44.362339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:45.358941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:46.359100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:47.359414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:48.360021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:49.360686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:50.361545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:51.362468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:52.362667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:53.363658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:54.364265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:55.369773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:56.366700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:57.367467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:58.367586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:16:59.368465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:00.369342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:01.370510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:02.369801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:03.370098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:04.371297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:05.372124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:06.372821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:07.373587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:08.373879      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:09.374487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:10.375187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:11.375251      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:12.375567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:13.376230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:14.377052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:15.377625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:16.377982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:17.378859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:18.378971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:19.379252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:20.379400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:21.379897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:22.380389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:23.380132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:24.380210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:25.381725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:26.381311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:27.381419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:28.381950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:29.382327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:30.382687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:31.382769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:32.382936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:33.383617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:34.384464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:35.384815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:36.385250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:37.385295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:38.385605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:39.385816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:40.386145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:41.386738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:42.386845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:43.387013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:44.387165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:45.388097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:46.387638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:47.387871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:48.388152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:49.389141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:50.389936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:51.390502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:52.391340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:53.392037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:54.392096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:55.392569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:56.392584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:57.392920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:58.393072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:17:59.393591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:00.394675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:01.395732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:02.396160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:03.396451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:04.396864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:05.397093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:06.397030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:07.397300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:08.398283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:09.399050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:10.399252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:11.399359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:12.399910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:13.408695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:14.402998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:15.402978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:16.403498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:17.406642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:18.404898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:19.405836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:20.406139      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:21.406205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:22.407270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:23.406646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:24.407720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:25.408298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:26.408559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:27.408919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:28.409098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:29.409831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:30.410112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:31.411555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:32.411253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:33.411767      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:34.412808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:35.413821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:36.413994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:37.414563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:38.415008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:39.415291      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:40.415986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:41.416590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:42.416792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:43.417602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:44.418619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:45.417858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:46.418140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:47.418381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:48.418438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:49.419637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:50.419872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:51.420779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:52.421724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:53.421918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:54.422597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:55.423479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:56.424572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:57.424833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:58.425961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:18:59.426913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:00.427655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:01.428572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:02.428704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:03.428847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:04.429178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:05.429349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:06.429446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:07.429759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:08.430453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:09.431352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:10.431474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:11.431855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:12.432679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:13.433403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:14.434324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:15.435090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:16.435512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:17.436338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:18.437228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:19.437548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:20.437578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:21.439709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:22.440117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:23.440013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:24.441043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:25.440874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:26.441017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:27.441627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:28.442367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:29.442587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:30.442996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:31.443185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:32.443256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:33.443551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:34.444582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:35.445300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:36.446456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:37.447265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:38.448078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:39.448082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:40.449150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:41.449636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:42.450693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:43.451380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:44.452285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:45.452693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:46.452867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:47.453793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:48.454000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:49.454973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:50.455110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:51.455595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:52.456151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:53.456302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:54.457119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:55.457862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:56.458552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:57.458786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:19:58.459337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-e7b91704-53be-42f6-9ddc-24abc65177ff off the node pohje9aimahx-3 @ 08/24/23 12:19:58.72
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7b91704-53be-42f6-9ddc-24abc65177ff @ 08/24/23 12:19:58.746
  Aug 24 12:19:58.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3847" for this suite. @ 08/24/23 12:19:58.777
• [304.459 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 08/24/23 12:19:58.79
  Aug 24 12:19:58.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 12:19:58.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:19:58.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:19:58.865
  STEP: Performing setup for networking test in namespace pod-network-test-4591 @ 08/24/23 12:19:58.869
  STEP: creating a selector @ 08/24/23 12:19:58.871
  STEP: Creating the service pods in kubernetes @ 08/24/23 12:19:58.871
  Aug 24 12:19:58.872: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0824 12:19:59.459579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:00.460327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:01.460717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:02.460884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:03.460962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:04.461846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:05.463126      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:06.463087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:07.463531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:08.463877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:09.464619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:10.465184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:11.465861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:12.466827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:13.466898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:14.467125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:15.468087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:16.469048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:17.469108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:18.469304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:19.469426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:20.469600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/24/23 12:20:21.094
  E0824 12:20:21.470261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:22.470734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:23.165: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 12:20:23.165: INFO: Going to poll 10.233.64.167 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 12:20:23.171: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.167 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:20:23.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:20:23.174: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:20:23.174: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.167+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0824 12:20:23.471278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:24.374: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 24 12:20:24.374: INFO: Going to poll 10.233.65.103 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 12:20:24.383: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.103 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:20:24.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:20:24.389: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:20:24.389: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.103+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0824 12:20:24.471675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:25.472355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:25.522: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 24 12:20:25.522: INFO: Going to poll 10.233.66.213 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 12:20:25.533: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.213 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4591 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:20:25.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:20:25.534: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:20:25.534: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4591/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.213+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0824 12:20:26.472716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:26.668: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 24 12:20:26.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4591" for this suite. @ 08/24/23 12:20:26.678
• [27.906 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 08/24/23 12:20:26.696
  Aug 24 12:20:26.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 12:20:26.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:26.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:26.746
  STEP: create the container @ 08/24/23 12:20:26.753
  W0824 12:20:26.771308      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/24/23 12:20:26.772
  E0824 12:20:27.474483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:28.474701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:29.474309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/24/23 12:20:29.839
  STEP: the container should be terminated @ 08/24/23 12:20:29.848
  STEP: the termination message should be set @ 08/24/23 12:20:29.848
  Aug 24 12:20:29.849: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/24/23 12:20:29.849
  Aug 24 12:20:29.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6315" for this suite. @ 08/24/23 12:20:29.89
• [3.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 08/24/23 12:20:29.925
  Aug 24 12:20:29.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:20:29.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:29.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:29.976
  STEP: Starting the proxy @ 08/24/23 12:20:29.98
  Aug 24 12:20:29.985: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-6618 proxy --unix-socket=/tmp/kubectl-proxy-unix1782270684/test'
  STEP: retrieving proxy /api/ output @ 08/24/23 12:20:30.124
  Aug 24 12:20:30.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6618" for this suite. @ 08/24/23 12:20:30.135
• [0.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 08/24/23 12:20:30.149
  Aug 24 12:20:30.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename endpointslicemirroring @ 08/24/23 12:20:30.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:30.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:30.181
  STEP: mirroring a new custom Endpoint @ 08/24/23 12:20:30.213
  Aug 24 12:20:30.233: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0824 12:20:30.475482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:31.476730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 08/24/23 12:20:32.301
  Aug 24 12:20:32.350: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0824 12:20:32.477184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:33.477275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 08/24/23 12:20:34.361
  Aug 24 12:20:34.383: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0824 12:20:34.478069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:35.478305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:36.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-1599" for this suite. @ 08/24/23 12:20:36.401
• [6.264 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 08/24/23 12:20:36.416
  Aug 24 12:20:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:20:36.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:36.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:36.444
  STEP: create deployment with httpd image @ 08/24/23 12:20:36.45
  Aug 24 12:20:36.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-9972 create -f -'
  E0824 12:20:36.480914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:37.482380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:37.942: INFO: stderr: ""
  Aug 24 12:20:37.942: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 08/24/23 12:20:37.942
  Aug 24 12:20:37.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-9972 diff -f -'
  E0824 12:20:38.483379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:38.538: INFO: rc: 1
  Aug 24 12:20:38.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-9972 delete -f -'
  Aug 24 12:20:38.792: INFO: stderr: ""
  Aug 24 12:20:38.792: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Aug 24 12:20:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9972" for this suite. @ 08/24/23 12:20:38.803
• [2.404 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 08/24/23 12:20:38.821
  Aug 24 12:20:38.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:20:38.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:38.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:38.862
  STEP: Creating secret with name secret-test-271eb6ee-2ee3-47ca-bc2d-4d2c259bc469 @ 08/24/23 12:20:38.868
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:20:38.894
  E0824 12:20:39.518231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:40.517850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:41.519094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:42.519705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:20:42.957
  Aug 24 12:20:42.965: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-fc27da61-24dc-4f55-a4a1-6fe6b1ed6cef container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:20:43
  Aug 24 12:20:43.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9949" for this suite. @ 08/24/23 12:20:43.066
• [4.259 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 08/24/23 12:20:43.085
  Aug 24 12:20:43.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sysctl @ 08/24/23 12:20:43.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:43.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:43.136
  STEP: Creating a pod with one valid and two invalid sysctls @ 08/24/23 12:20:43.142
  Aug 24 12:20:43.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7562" for this suite. @ 08/24/23 12:20:43.186
• [0.113 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 08/24/23 12:20:43.199
  Aug 24 12:20:43.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename ingress @ 08/24/23 12:20:43.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:43.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:43.238
  STEP: getting /apis @ 08/24/23 12:20:43.242
  STEP: getting /apis/networking.k8s.io @ 08/24/23 12:20:43.253
  STEP: getting /apis/networking.k8s.iov1 @ 08/24/23 12:20:43.255
  STEP: creating @ 08/24/23 12:20:43.256
  STEP: getting @ 08/24/23 12:20:43.285
  STEP: listing @ 08/24/23 12:20:43.292
  STEP: watching @ 08/24/23 12:20:43.299
  Aug 24 12:20:43.299: INFO: starting watch
  STEP: cluster-wide listing @ 08/24/23 12:20:43.301
  STEP: cluster-wide watching @ 08/24/23 12:20:43.307
  Aug 24 12:20:43.308: INFO: starting watch
  STEP: patching @ 08/24/23 12:20:43.311
  STEP: updating @ 08/24/23 12:20:43.325
  Aug 24 12:20:43.355: INFO: waiting for watch events with expected annotations
  Aug 24 12:20:43.355: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/24/23 12:20:43.355
  STEP: updating /status @ 08/24/23 12:20:43.375
  STEP: get /status @ 08/24/23 12:20:43.389
  STEP: deleting @ 08/24/23 12:20:43.394
  STEP: deleting a collection @ 08/24/23 12:20:43.421
  Aug 24 12:20:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9833" for this suite. @ 08/24/23 12:20:43.451
• [0.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 08/24/23 12:20:43.467
  Aug 24 12:20:43.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename limitrange @ 08/24/23 12:20:43.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:43.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:43.502
  STEP: Creating LimitRange "e2e-limitrange-k5p4m" in namespace "limitrange-2215" @ 08/24/23 12:20:43.506
  STEP: Creating another limitRange in another namespace @ 08/24/23 12:20:43.519
  E0824 12:20:43.519870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:20:43.543: INFO: Namespace "e2e-limitrange-k5p4m-6057" created
  Aug 24 12:20:43.543: INFO: Creating LimitRange "e2e-limitrange-k5p4m" in namespace "e2e-limitrange-k5p4m-6057"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-k5p4m" @ 08/24/23 12:20:43.553
  Aug 24 12:20:43.559: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-k5p4m" in "limitrange-2215" namespace @ 08/24/23 12:20:43.559
  Aug 24 12:20:43.574: INFO: LimitRange "e2e-limitrange-k5p4m" has been patched
  STEP: Delete LimitRange "e2e-limitrange-k5p4m" by Collection with labelSelector: "e2e-limitrange-k5p4m=patched" @ 08/24/23 12:20:43.574
  STEP: Confirm that the limitRange "e2e-limitrange-k5p4m" has been deleted @ 08/24/23 12:20:43.59
  Aug 24 12:20:43.590: INFO: Requesting list of LimitRange to confirm quantity
  Aug 24 12:20:43.597: INFO: Found 0 LimitRange with label "e2e-limitrange-k5p4m=patched"
  Aug 24 12:20:43.597: INFO: LimitRange "e2e-limitrange-k5p4m" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-k5p4m" @ 08/24/23 12:20:43.597
  Aug 24 12:20:43.602: INFO: Found 1 limitRange
  Aug 24 12:20:43.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2215" for this suite. @ 08/24/23 12:20:43.612
  STEP: Destroying namespace "e2e-limitrange-k5p4m-6057" for this suite. @ 08/24/23 12:20:43.621
• [0.167 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 08/24/23 12:20:43.636
  Aug 24 12:20:43.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:20:43.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:20:43.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:20:43.671
  STEP: Creating configMap with name cm-test-opt-del-1c0f5b59-2031-4bb9-8cb5-fc42df24c462 @ 08/24/23 12:20:43.684
  STEP: Creating configMap with name cm-test-opt-upd-f8cb91b7-0db1-4538-b049-601ea2c39591 @ 08/24/23 12:20:43.693
  STEP: Creating the pod @ 08/24/23 12:20:43.701
  E0824 12:20:44.520780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:45.520786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:46.521087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:47.521414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-1c0f5b59-2031-4bb9-8cb5-fc42df24c462 @ 08/24/23 12:20:47.783
  STEP: Updating configmap cm-test-opt-upd-f8cb91b7-0db1-4538-b049-601ea2c39591 @ 08/24/23 12:20:47.794
  STEP: Creating configMap with name cm-test-opt-create-d4f58bd7-1e67-4f87-bde5-c3b395243c1f @ 08/24/23 12:20:47.805
  STEP: waiting to observe update in volume @ 08/24/23 12:20:47.812
  E0824 12:20:48.521568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:49.521913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:50.522216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:51.522878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:52.523155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:53.523658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:54.524536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:55.524233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:56.524955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:57.525763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:58.526411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:20:59.526899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:00.527172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:01.527702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:02.528428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:03.528450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:04.528819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:05.529050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:06.530075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:07.529746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:08.529758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:09.530884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:10.532242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:11.532111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:12.532639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:13.533611      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:14.535300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:15.534123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:16.534783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:17.535306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:18.537779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:19.538153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:20.539014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:21.539407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:22.539618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:23.539824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:24.540874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:25.541024      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:26.541912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:27.542187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:28.542471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:29.542656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:30.543638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:31.543829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:32.544586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:33.545173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:34.546375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:35.547068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:36.547639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:37.547982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:38.548624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:39.558535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:40.555995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:41.556747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:42.557527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:43.557717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:44.558081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:45.558293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:46.558697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:47.559244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:48.559434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:49.560359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:50.560396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:51.560891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:52.561521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:53.562278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:54.562106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:55.562479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:21:56.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3804" for this suite. @ 08/24/23 12:21:56.503
• [72.890 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 08/24/23 12:21:56.531
  Aug 24 12:21:56.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:21:56.534
  E0824 12:21:56.563579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:21:56.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:21:56.579
  E0824 12:21:57.564002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:21:58.564616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:21:58.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:21:58.643: INFO: Deleting pod "var-expansion-ee68710c-47ff-4c78-bf89-0a6547204884" in namespace "var-expansion-3650"
  Aug 24 12:21:58.658: INFO: Wait up to 5m0s for pod "var-expansion-ee68710c-47ff-4c78-bf89-0a6547204884" to be fully deleted
  E0824 12:21:59.564960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:00.565169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3650" for this suite. @ 08/24/23 12:22:00.674
• [4.155 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 08/24/23 12:22:00.687
  Aug 24 12:22:00.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:22:00.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:00.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:00.724
  STEP: Creating a pod to test substitution in container's args @ 08/24/23 12:22:00.729
  E0824 12:22:01.565390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:02.565539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:03.566037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:04.566150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:22:04.773
  Aug 24 12:22:04.781: INFO: Trying to get logs from node pohje9aimahx-3 pod var-expansion-77714273-c31f-4f94-a054-c7e9efcfded1 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:22:04.804
  Aug 24 12:22:04.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6321" for this suite. @ 08/24/23 12:22:04.859
• [4.185 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 08/24/23 12:22:04.874
  Aug 24 12:22:04.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:22:04.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:04.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:04.971
  STEP: Creating configMap with name configmap-test-upd-56561006-9543-4723-acbe-fce759e01094 @ 08/24/23 12:22:04.985
  STEP: Creating the pod @ 08/24/23 12:22:04.995
  E0824 12:22:05.566578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:06.568116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:07.567610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:08.567972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 08/24/23 12:22:09.053
  STEP: Waiting for pod with binary data @ 08/24/23 12:22:09.069
  Aug 24 12:22:09.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-894" for this suite. @ 08/24/23 12:22:09.098
• [4.238 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 08/24/23 12:22:09.115
  Aug 24 12:22:09.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:22:09.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:09.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:09.157
  STEP: Creating a pod to test substitution in volume subpath @ 08/24/23 12:22:09.162
  E0824 12:22:09.568157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:10.574511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:11.571095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:12.572334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:22:13.215
  Aug 24 12:22:13.220: INFO: Trying to get logs from node pohje9aimahx-3 pod var-expansion-48dc0e1e-7255-4a87-880b-14d60a56dc98 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:22:13.231
  Aug 24 12:22:13.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-461" for this suite. @ 08/24/23 12:22:13.273
• [4.167 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 08/24/23 12:22:13.286
  Aug 24 12:22:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 12:22:13.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:13.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:13.323
  STEP: Creating a cronjob @ 08/24/23 12:22:13.329
  STEP: creating @ 08/24/23 12:22:13.329
  STEP: getting @ 08/24/23 12:22:13.344
  STEP: listing @ 08/24/23 12:22:13.349
  STEP: watching @ 08/24/23 12:22:13.355
  Aug 24 12:22:13.355: INFO: starting watch
  STEP: cluster-wide listing @ 08/24/23 12:22:13.358
  STEP: cluster-wide watching @ 08/24/23 12:22:13.364
  Aug 24 12:22:13.364: INFO: starting watch
  STEP: patching @ 08/24/23 12:22:13.366
  STEP: updating @ 08/24/23 12:22:13.377
  Aug 24 12:22:13.396: INFO: waiting for watch events with expected annotations
  Aug 24 12:22:13.396: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/24/23 12:22:13.397
  STEP: updating /status @ 08/24/23 12:22:13.408
  STEP: get /status @ 08/24/23 12:22:13.427
  STEP: deleting @ 08/24/23 12:22:13.431
  STEP: deleting a collection @ 08/24/23 12:22:13.454
  Aug 24 12:22:13.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9722" for this suite. @ 08/24/23 12:22:13.481
• [0.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 08/24/23 12:22:13.5
  Aug 24 12:22:13.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename proxy @ 08/24/23 12:22:13.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:13.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:13.532
  E0824 12:22:13.571775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: starting an echo server on multiple ports @ 08/24/23 12:22:13.583
  STEP: creating replication controller proxy-service-jftlm in namespace proxy-4376 @ 08/24/23 12:22:13.583
  I0824 12:22:13.600990      13 runners.go:194] Created replication controller with name: proxy-service-jftlm, namespace: proxy-4376, replica count: 1
  E0824 12:22:14.573779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:22:14.652417      13 runners.go:194] proxy-service-jftlm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:22:15.575979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:22:15.653319      13 runners.go:194] proxy-service-jftlm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:22:15.666: INFO: setup took 2.130183269s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 08/24/23 12:22:15.666
  Aug 24 12:22:15.675: INFO: (0) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 8.783106ms)
  Aug 24 12:22:15.682: INFO: (0) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 15.412389ms)
  Aug 24 12:22:15.682: INFO: (0) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 15.641679ms)
  Aug 24 12:22:15.700: INFO: (0) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 32.684557ms)
  Aug 24 12:22:15.705: INFO: (0) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 37.098504ms)
  Aug 24 12:22:15.714: INFO: (0) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 46.479468ms)
  Aug 24 12:22:15.714: INFO: (0) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 47.070315ms)
  Aug 24 12:22:15.723: INFO: (0) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 53.477862ms)
  Aug 24 12:22:15.723: INFO: (0) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 55.437844ms)
  Aug 24 12:22:15.737: INFO: (0) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 70.065072ms)
  Aug 24 12:22:15.737: INFO: (0) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 70.016154ms)
  Aug 24 12:22:15.737: INFO: (0) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 68.929913ms)
  Aug 24 12:22:15.738: INFO: (0) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 70.262081ms)
  Aug 24 12:22:15.739: INFO: (0) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 71.496431ms)
  Aug 24 12:22:15.740: INFO: (0) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 72.739752ms)
  Aug 24 12:22:15.740: INFO: (0) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 71.468009ms)
  Aug 24 12:22:15.760: INFO: (1) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 19.001651ms)
  Aug 24 12:22:15.766: INFO: (1) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 25.173742ms)
  Aug 24 12:22:15.766: INFO: (1) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 23.539335ms)
  Aug 24 12:22:15.767: INFO: (1) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 25.298582ms)
  Aug 24 12:22:15.775: INFO: (1) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 32.570451ms)
  Aug 24 12:22:15.776: INFO: (1) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 34.233613ms)
  Aug 24 12:22:15.779: INFO: (1) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 36.935548ms)
  Aug 24 12:22:15.779: INFO: (1) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 38.755359ms)
  Aug 24 12:22:15.783: INFO: (1) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 42.311345ms)
  Aug 24 12:22:15.783: INFO: (1) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 40.359002ms)
  Aug 24 12:22:15.784: INFO: (1) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 41.47178ms)
  Aug 24 12:22:15.784: INFO: (1) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 40.959847ms)
  Aug 24 12:22:15.788: INFO: (1) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 45.380256ms)
  Aug 24 12:22:15.789: INFO: (1) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 45.518916ms)
  Aug 24 12:22:15.789: INFO: (1) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 46.302798ms)
  Aug 24 12:22:15.791: INFO: (1) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 47.500858ms)
  Aug 24 12:22:15.798: INFO: (2) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 6.728454ms)
  Aug 24 12:22:15.807: INFO: (2) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 15.323023ms)
  Aug 24 12:22:15.810: INFO: (2) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 17.989678ms)
  Aug 24 12:22:15.810: INFO: (2) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 18.123398ms)
  Aug 24 12:22:15.810: INFO: (2) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 18.878075ms)
  Aug 24 12:22:15.812: INFO: (2) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 19.352187ms)
  Aug 24 12:22:15.812: INFO: (2) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 19.801787ms)
  Aug 24 12:22:15.814: INFO: (2) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 21.492855ms)
  Aug 24 12:22:15.814: INFO: (2) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 21.298976ms)
  Aug 24 12:22:15.815: INFO: (2) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 23.278012ms)
  Aug 24 12:22:15.815: INFO: (2) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 22.752026ms)
  Aug 24 12:22:15.816: INFO: (2) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 23.884251ms)
  Aug 24 12:22:15.816: INFO: (2) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 24.085744ms)
  Aug 24 12:22:15.818: INFO: (2) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 25.317391ms)
  Aug 24 12:22:15.818: INFO: (2) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 25.841959ms)
  Aug 24 12:22:15.818: INFO: (2) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 25.811974ms)
  Aug 24 12:22:15.833: INFO: (3) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 14.626071ms)
  Aug 24 12:22:15.834: INFO: (3) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 14.783555ms)
  Aug 24 12:22:15.835: INFO: (3) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 16.523958ms)
  Aug 24 12:22:15.837: INFO: (3) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 17.544977ms)
  Aug 24 12:22:15.838: INFO: (3) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 18.67597ms)
  Aug 24 12:22:15.839: INFO: (3) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 18.892591ms)
  Aug 24 12:22:15.839: INFO: (3) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 19.521306ms)
  Aug 24 12:22:15.840: INFO: (3) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 20.622613ms)
  Aug 24 12:22:15.843: INFO: (3) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 22.620017ms)
  Aug 24 12:22:15.843: INFO: (3) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 23.396043ms)
  Aug 24 12:22:15.844: INFO: (3) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 25.335855ms)
  Aug 24 12:22:15.860: INFO: (3) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 41.376899ms)
  Aug 24 12:22:15.860: INFO: (3) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 40.824637ms)
  Aug 24 12:22:15.860: INFO: (3) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 40.604317ms)
  Aug 24 12:22:15.863: INFO: (3) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 43.802507ms)
  Aug 24 12:22:15.863: INFO: (3) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 42.576909ms)
  Aug 24 12:22:15.921: INFO: (4) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 56.84043ms)
  Aug 24 12:22:15.921: INFO: (4) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 57.046064ms)
  Aug 24 12:22:15.934: INFO: (4) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 70.218019ms)
  Aug 24 12:22:15.934: INFO: (4) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 70.227764ms)
  Aug 24 12:22:15.939: INFO: (4) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 74.303701ms)
  Aug 24 12:22:15.955: INFO: (4) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 89.540024ms)
  Aug 24 12:22:15.955: INFO: (4) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 90.101298ms)
  Aug 24 12:22:15.955: INFO: (4) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 90.225793ms)
  Aug 24 12:22:15.955: INFO: (4) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 90.559366ms)
  Aug 24 12:22:15.955: INFO: (4) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 90.030434ms)
  Aug 24 12:22:15.957: INFO: (4) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 91.892003ms)
  Aug 24 12:22:15.967: INFO: (4) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 102.411283ms)
  Aug 24 12:22:15.968: INFO: (4) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 102.768241ms)
  Aug 24 12:22:15.968: INFO: (4) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 103.139541ms)
  Aug 24 12:22:15.971: INFO: (4) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 106.141868ms)
  Aug 24 12:22:15.972: INFO: (4) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 106.728985ms)
  Aug 24 12:22:16.050: INFO: (5) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 76.914519ms)
  Aug 24 12:22:16.058: INFO: (5) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 84.383046ms)
  Aug 24 12:22:16.059: INFO: (5) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 86.596464ms)
  Aug 24 12:22:16.078: INFO: (5) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 104.740055ms)
  Aug 24 12:22:16.078: INFO: (5) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 105.422411ms)
  Aug 24 12:22:16.078: INFO: (5) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 104.784353ms)
  Aug 24 12:22:16.083: INFO: (5) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 110.341938ms)
  Aug 24 12:22:16.083: INFO: (5) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 108.677523ms)
  Aug 24 12:22:16.083: INFO: (5) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 109.283694ms)
  Aug 24 12:22:16.083: INFO: (5) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 109.894971ms)
  Aug 24 12:22:16.083: INFO: (5) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 110.881935ms)
  Aug 24 12:22:16.084: INFO: (5) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 109.252436ms)
  Aug 24 12:22:16.084: INFO: (5) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 109.882102ms)
  Aug 24 12:22:16.084: INFO: (5) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 110.855389ms)
  Aug 24 12:22:16.084: INFO: (5) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 110.275385ms)
  Aug 24 12:22:16.089: INFO: (5) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 116.42456ms)
  Aug 24 12:22:16.116: INFO: (6) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 27.48196ms)
  Aug 24 12:22:16.123: INFO: (6) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 33.766927ms)
  Aug 24 12:22:16.125: INFO: (6) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 35.286541ms)
  Aug 24 12:22:16.125: INFO: (6) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 36.006319ms)
  Aug 24 12:22:16.126: INFO: (6) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 37.520648ms)
  Aug 24 12:22:16.127: INFO: (6) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 37.868309ms)
  Aug 24 12:22:16.129: INFO: (6) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 39.156922ms)
  Aug 24 12:22:16.129: INFO: (6) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 39.884917ms)
  Aug 24 12:22:16.129: INFO: (6) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 39.823996ms)
  Aug 24 12:22:16.129: INFO: (6) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 40.229727ms)
  Aug 24 12:22:16.129: INFO: (6) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 40.129077ms)
  Aug 24 12:22:16.130: INFO: (6) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 40.050811ms)
  Aug 24 12:22:16.130: INFO: (6) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 40.205369ms)
  Aug 24 12:22:16.130: INFO: (6) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 40.655548ms)
  Aug 24 12:22:16.137: INFO: (6) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 48.093912ms)
  Aug 24 12:22:16.138: INFO: (6) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 48.68923ms)
  Aug 24 12:22:16.150: INFO: (7) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 11.176151ms)
  Aug 24 12:22:16.151: INFO: (7) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 12.422503ms)
  Aug 24 12:22:16.154: INFO: (7) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 14.546085ms)
  Aug 24 12:22:16.156: INFO: (7) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 16.298791ms)
  Aug 24 12:22:16.158: INFO: (7) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 18.675779ms)
  Aug 24 12:22:16.158: INFO: (7) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 18.639512ms)
  Aug 24 12:22:16.158: INFO: (7) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 19.155118ms)
  Aug 24 12:22:16.158: INFO: (7) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 18.998595ms)
  Aug 24 12:22:16.158: INFO: (7) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 18.946905ms)
  Aug 24 12:22:16.158: INFO: (7) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 19.408536ms)
  Aug 24 12:22:16.159: INFO: (7) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 19.539172ms)
  Aug 24 12:22:16.159: INFO: (7) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 19.681291ms)
  Aug 24 12:22:16.159: INFO: (7) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 20.321513ms)
  Aug 24 12:22:16.164: INFO: (7) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 24.581211ms)
  Aug 24 12:22:16.164: INFO: (7) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 24.50669ms)
  Aug 24 12:22:16.166: INFO: (7) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 26.577673ms)
  Aug 24 12:22:16.183: INFO: (8) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 16.086943ms)
  Aug 24 12:22:16.185: INFO: (8) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 17.609596ms)
  Aug 24 12:22:16.185: INFO: (8) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 16.739077ms)
  Aug 24 12:22:16.185: INFO: (8) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 17.876142ms)
  Aug 24 12:22:16.185: INFO: (8) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 18.053412ms)
  Aug 24 12:22:16.186: INFO: (8) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 18.073305ms)
  Aug 24 12:22:16.189: INFO: (8) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 23.181689ms)
  Aug 24 12:22:16.189: INFO: (8) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 22.028498ms)
  Aug 24 12:22:16.192: INFO: (8) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 24.660547ms)
  Aug 24 12:22:16.192: INFO: (8) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 24.807768ms)
  Aug 24 12:22:16.193: INFO: (8) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 24.838807ms)
  Aug 24 12:22:16.193: INFO: (8) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 26.910492ms)
  Aug 24 12:22:16.193: INFO: (8) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 25.430496ms)
  Aug 24 12:22:16.193: INFO: (8) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 25.848678ms)
  Aug 24 12:22:16.194: INFO: (8) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 25.847858ms)
  Aug 24 12:22:16.195: INFO: (8) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 26.572416ms)
  Aug 24 12:22:16.209: INFO: (9) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 13.443881ms)
  Aug 24 12:22:16.209: INFO: (9) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 13.1992ms)
  Aug 24 12:22:16.210: INFO: (9) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 14.691398ms)
  Aug 24 12:22:16.211: INFO: (9) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 14.999719ms)
  Aug 24 12:22:16.211: INFO: (9) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 15.771633ms)
  Aug 24 12:22:16.214: INFO: (9) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 19.141137ms)
  Aug 24 12:22:16.215: INFO: (9) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 18.788406ms)
  Aug 24 12:22:16.215: INFO: (9) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 19.81962ms)
  Aug 24 12:22:16.215: INFO: (9) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 20.057657ms)
  Aug 24 12:22:16.215: INFO: (9) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 19.882711ms)
  Aug 24 12:22:16.216: INFO: (9) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 21.053421ms)
  Aug 24 12:22:16.219: INFO: (9) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 24.133396ms)
  Aug 24 12:22:16.220: INFO: (9) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 24.717684ms)
  Aug 24 12:22:16.221: INFO: (9) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 25.710709ms)
  Aug 24 12:22:16.221: INFO: (9) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 25.463944ms)
  Aug 24 12:22:16.222: INFO: (9) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 26.63488ms)
  Aug 24 12:22:16.231: INFO: (10) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 8.7462ms)
  Aug 24 12:22:16.238: INFO: (10) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 15.170411ms)
  Aug 24 12:22:16.239: INFO: (10) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 16.517571ms)
  Aug 24 12:22:16.242: INFO: (10) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 18.78568ms)
  Aug 24 12:22:16.245: INFO: (10) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 21.568678ms)
  Aug 24 12:22:16.245: INFO: (10) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 21.93619ms)
  Aug 24 12:22:16.245: INFO: (10) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 22.212706ms)
  Aug 24 12:22:16.245: INFO: (10) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 22.223574ms)
  Aug 24 12:22:16.247: INFO: (10) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 23.676381ms)
  Aug 24 12:22:16.250: INFO: (10) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 26.255063ms)
  Aug 24 12:22:16.250: INFO: (10) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 26.871506ms)
  Aug 24 12:22:16.251: INFO: (10) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 27.227774ms)
  Aug 24 12:22:16.252: INFO: (10) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 28.333144ms)
  Aug 24 12:22:16.252: INFO: (10) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 29.3845ms)
  Aug 24 12:22:16.253: INFO: (10) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 30.038398ms)
  Aug 24 12:22:16.254: INFO: (10) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 30.28615ms)
  Aug 24 12:22:16.269: INFO: (11) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 12.611424ms)
  Aug 24 12:22:16.271: INFO: (11) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 16.231567ms)
  Aug 24 12:22:16.271: INFO: (11) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 16.006319ms)
  Aug 24 12:22:16.272: INFO: (11) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 15.82191ms)
  Aug 24 12:22:16.272: INFO: (11) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 16.55633ms)
  Aug 24 12:22:16.272: INFO: (11) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 15.577328ms)
  Aug 24 12:22:16.272: INFO: (11) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 17.092282ms)
  Aug 24 12:22:16.272: INFO: (11) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 17.477103ms)
  Aug 24 12:22:16.276: INFO: (11) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 20.44367ms)
  Aug 24 12:22:16.276: INFO: (11) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 20.020245ms)
  Aug 24 12:22:16.277: INFO: (11) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 21.706245ms)
  Aug 24 12:22:16.277: INFO: (11) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 21.340169ms)
  Aug 24 12:22:16.279: INFO: (11) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 22.564645ms)
  Aug 24 12:22:16.279: INFO: (11) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 22.407561ms)
  Aug 24 12:22:16.280: INFO: (11) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 24.101983ms)
  Aug 24 12:22:16.281: INFO: (11) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 25.003573ms)
  Aug 24 12:22:16.291: INFO: (12) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 9.352279ms)
  Aug 24 12:22:16.296: INFO: (12) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 13.836812ms)
  Aug 24 12:22:16.296: INFO: (12) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 13.499387ms)
  Aug 24 12:22:16.310: INFO: (12) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 28.210274ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 26.525938ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 26.480492ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 26.736981ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 26.883041ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 27.141615ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 27.702264ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 28.455492ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 28.706503ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 28.130133ms)
  Aug 24 12:22:16.311: INFO: (12) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 28.692307ms)
  Aug 24 12:22:16.312: INFO: (12) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 27.807581ms)
  Aug 24 12:22:16.313: INFO: (12) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 29.459948ms)
  Aug 24 12:22:16.328: INFO: (13) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 14.217679ms)
  Aug 24 12:22:16.329: INFO: (13) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 15.530627ms)
  Aug 24 12:22:16.329: INFO: (13) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 14.635753ms)
  Aug 24 12:22:16.329: INFO: (13) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 15.360082ms)
  Aug 24 12:22:16.330: INFO: (13) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 15.834548ms)
  Aug 24 12:22:16.333: INFO: (13) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 19.548696ms)
  Aug 24 12:22:16.335: INFO: (13) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 21.036205ms)
  Aug 24 12:22:16.335: INFO: (13) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 20.664877ms)
  Aug 24 12:22:16.337: INFO: (13) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 23.39979ms)
  Aug 24 12:22:16.339: INFO: (13) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 24.503986ms)
  Aug 24 12:22:16.339: INFO: (13) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 24.559124ms)
  Aug 24 12:22:16.340: INFO: (13) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 25.051836ms)
  Aug 24 12:22:16.340: INFO: (13) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 25.323663ms)
  Aug 24 12:22:16.340: INFO: (13) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 25.408995ms)
  Aug 24 12:22:16.340: INFO: (13) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 26.161442ms)
  Aug 24 12:22:16.343: INFO: (13) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 28.406121ms)
  Aug 24 12:22:16.355: INFO: (14) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 11.786553ms)
  Aug 24 12:22:16.364: INFO: (14) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 20.116528ms)
  Aug 24 12:22:16.370: INFO: (14) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 24.981677ms)
  Aug 24 12:22:16.375: INFO: (14) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 32.455961ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 30.347573ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 30.531152ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 30.677959ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 32.197187ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 30.985833ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 32.491706ms)
  Aug 24 12:22:16.376: INFO: (14) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 32.100424ms)
  Aug 24 12:22:16.377: INFO: (14) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 33.600326ms)
  Aug 24 12:22:16.378: INFO: (14) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 33.007335ms)
  Aug 24 12:22:16.378: INFO: (14) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 33.394143ms)
  Aug 24 12:22:16.379: INFO: (14) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 33.611887ms)
  Aug 24 12:22:16.379: INFO: (14) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 35.868983ms)
  Aug 24 12:22:16.397: INFO: (15) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 17.811496ms)
  Aug 24 12:22:16.397: INFO: (15) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 17.853114ms)
  Aug 24 12:22:16.398: INFO: (15) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 18.028012ms)
  Aug 24 12:22:16.399: INFO: (15) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 19.828695ms)
  Aug 24 12:22:16.400: INFO: (15) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 19.534774ms)
  Aug 24 12:22:16.402: INFO: (15) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 21.633121ms)
  Aug 24 12:22:16.404: INFO: (15) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 23.460107ms)
  Aug 24 12:22:16.406: INFO: (15) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 25.349086ms)
  Aug 24 12:22:16.406: INFO: (15) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 25.842755ms)
  Aug 24 12:22:16.406: INFO: (15) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 25.764472ms)
  Aug 24 12:22:16.406: INFO: (15) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 26.87043ms)
  Aug 24 12:22:16.406: INFO: (15) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 26.852054ms)
  Aug 24 12:22:16.406: INFO: (15) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 25.887734ms)
  Aug 24 12:22:16.407: INFO: (15) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 26.53163ms)
  Aug 24 12:22:16.407: INFO: (15) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 26.684074ms)
  Aug 24 12:22:16.409: INFO: (15) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 29.661064ms)
  Aug 24 12:22:16.420: INFO: (16) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 10.117237ms)
  Aug 24 12:22:16.421: INFO: (16) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 10.513197ms)
  Aug 24 12:22:16.421: INFO: (16) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 11.306509ms)
  Aug 24 12:22:16.421: INFO: (16) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 10.911136ms)
  Aug 24 12:22:16.422: INFO: (16) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 10.943778ms)
  Aug 24 12:22:16.423: INFO: (16) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 12.912361ms)
  Aug 24 12:22:16.424: INFO: (16) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 14.132875ms)
  Aug 24 12:22:16.424: INFO: (16) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 13.756346ms)
  Aug 24 12:22:16.426: INFO: (16) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 14.354735ms)
  Aug 24 12:22:16.429: INFO: (16) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 18.0493ms)
  Aug 24 12:22:16.430: INFO: (16) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 18.249552ms)
  Aug 24 12:22:16.431: INFO: (16) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 19.803425ms)
  Aug 24 12:22:16.431: INFO: (16) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 19.956495ms)
  Aug 24 12:22:16.432: INFO: (16) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 20.987564ms)
  Aug 24 12:22:16.433: INFO: (16) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 23.585442ms)
  Aug 24 12:22:16.436: INFO: (16) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 24.546099ms)
  Aug 24 12:22:16.450: INFO: (17) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 13.583174ms)
  Aug 24 12:22:16.451: INFO: (17) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 14.107122ms)
  Aug 24 12:22:16.451: INFO: (17) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 15.25501ms)
  Aug 24 12:22:16.451: INFO: (17) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 15.346982ms)
  Aug 24 12:22:16.452: INFO: (17) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 14.875355ms)
  Aug 24 12:22:16.455: INFO: (17) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 17.962553ms)
  Aug 24 12:22:16.455: INFO: (17) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 18.689461ms)
  Aug 24 12:22:16.457: INFO: (17) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 20.916383ms)
  Aug 24 12:22:16.460: INFO: (17) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 24.06928ms)
  Aug 24 12:22:16.461: INFO: (17) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 24.265131ms)
  Aug 24 12:22:16.461: INFO: (17) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 23.722666ms)
  Aug 24 12:22:16.463: INFO: (17) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 26.091711ms)
  Aug 24 12:22:16.463: INFO: (17) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 25.863341ms)
  Aug 24 12:22:16.463: INFO: (17) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 26.784843ms)
  Aug 24 12:22:16.463: INFO: (17) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 26.612539ms)
  Aug 24 12:22:16.464: INFO: (17) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 26.90442ms)
  Aug 24 12:22:16.474: INFO: (18) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 9.586517ms)
  Aug 24 12:22:16.476: INFO: (18) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 11.311136ms)
  Aug 24 12:22:16.476: INFO: (18) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 10.817302ms)
  Aug 24 12:22:16.477: INFO: (18) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 12.319737ms)
  Aug 24 12:22:16.480: INFO: (18) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 15.841396ms)
  Aug 24 12:22:16.481: INFO: (18) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 15.503382ms)
  Aug 24 12:22:16.481: INFO: (18) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 16.18017ms)
  Aug 24 12:22:16.482: INFO: (18) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 17.746017ms)
  Aug 24 12:22:16.482: INFO: (18) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 18.304468ms)
  Aug 24 12:22:16.484: INFO: (18) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 19.149704ms)
  Aug 24 12:22:16.485: INFO: (18) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 20.546537ms)
  Aug 24 12:22:16.486: INFO: (18) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 21.092721ms)
  Aug 24 12:22:16.486: INFO: (18) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 20.676729ms)
  Aug 24 12:22:16.487: INFO: (18) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 22.098884ms)
  Aug 24 12:22:16.487: INFO: (18) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 21.983466ms)
  Aug 24 12:22:16.487: INFO: (18) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 23.111151ms)
  Aug 24 12:22:16.496: INFO: (19) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 8.07985ms)
  Aug 24 12:22:16.497: INFO: (19) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:462/proxy/: tls qux (200; 9.679249ms)
  Aug 24 12:22:16.500: INFO: (19) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:443/proxy/tlsrewritem... (200; 11.484152ms)
  Aug 24 12:22:16.500: INFO: (19) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 10.704699ms)
  Aug 24 12:22:16.500: INFO: (19) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:160/proxy/: foo (200; 10.880372ms)
  Aug 24 12:22:16.503: INFO: (19) /api/v1/namespaces/proxy-4376/pods/https:proxy-service-jftlm-8xl8x:460/proxy/: tls baz (200; 14.329952ms)
  Aug 24 12:22:16.505: INFO: (19) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname2/proxy/: bar (200; 17.812988ms)
  Aug 24 12:22:16.506: INFO: (19) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">... (200; 16.929198ms)
  Aug 24 12:22:16.506: INFO: (19) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x/proxy/rewriteme">test</a> (200; 17.633575ms)
  Aug 24 12:22:16.509: INFO: (19) /api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/: <a href="/api/v1/namespaces/proxy-4376/pods/proxy-service-jftlm-8xl8x:1080/proxy/rewriteme">test<... (200; 19.530264ms)
  Aug 24 12:22:16.509: INFO: (19) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname2/proxy/: tls qux (200; 20.889494ms)
  Aug 24 12:22:16.509: INFO: (19) /api/v1/namespaces/proxy-4376/services/https:proxy-service-jftlm:tlsportname1/proxy/: tls baz (200; 21.447837ms)
  Aug 24 12:22:16.509: INFO: (19) /api/v1/namespaces/proxy-4376/pods/http:proxy-service-jftlm-8xl8x:162/proxy/: bar (200; 20.919229ms)
  Aug 24 12:22:16.510: INFO: (19) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname2/proxy/: bar (200; 21.169926ms)
  Aug 24 12:22:16.512: INFO: (19) /api/v1/namespaces/proxy-4376/services/http:proxy-service-jftlm:portname1/proxy/: foo (200; 24.357086ms)
  Aug 24 12:22:16.514: INFO: (19) /api/v1/namespaces/proxy-4376/services/proxy-service-jftlm:portname1/proxy/: foo (200; 25.252828ms)
  Aug 24 12:22:16.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-jftlm in namespace proxy-4376, will wait for the garbage collector to delete the pods @ 08/24/23 12:22:16.522
  E0824 12:22:16.575427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:16.591: INFO: Deleting ReplicationController proxy-service-jftlm took: 13.461581ms
  Aug 24 12:22:16.692: INFO: Terminating ReplicationController proxy-service-jftlm pods took: 101.092424ms
  E0824 12:22:17.575540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:18.575979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-4376" for this suite. @ 08/24/23 12:22:19.294
• [5.806 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 08/24/23 12:22:19.309
  Aug 24 12:22:19.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:22:19.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:19.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:19.35
  STEP: Creating configMap with name configmap-test-volume-2d68ad98-a542-44c6-a16d-bd4bd294b23e @ 08/24/23 12:22:19.353
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:22:19.362
  E0824 12:22:19.576551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:20.577144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:21.577531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:22.578437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:22:23.401
  Aug 24 12:22:23.411: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-515b88ce-e0eb-411c-9c5f-b926c980c975 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:22:23.423
  Aug 24 12:22:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1033" for this suite. @ 08/24/23 12:22:23.464
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 08/24/23 12:22:23.479
  Aug 24 12:22:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:22:23.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:23.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:23.511
  STEP: Updating Namespace "namespaces-7644" @ 08/24/23 12:22:23.517
  Aug 24 12:22:23.533: INFO: Namespace "namespaces-7644" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"da90e73d-ba27-48cb-8688-e9f6ef028ca1", "kubernetes.io/metadata.name":"namespaces-7644", "namespaces-7644":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Aug 24 12:22:23.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7644" for this suite. @ 08/24/23 12:22:23.541
• [0.077 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 08/24/23 12:22:23.559
  Aug 24 12:22:23.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:22:23.561
  E0824 12:22:23.578810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:23.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:23.599
  STEP: Setting up server cert @ 08/24/23 12:22:23.643
  E0824 12:22:24.579563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:22:24.597
  STEP: Deploying the webhook pod @ 08/24/23 12:22:24.615
  STEP: Wait for the deployment to be ready @ 08/24/23 12:22:24.639
  Aug 24 12:22:24.660: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:22:25.580228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:26.580967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:22:26.68
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:22:26.702
  E0824 12:22:27.581244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:27.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/24/23 12:22:27.841
  Aug 24 12:22:27.901: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:22:28.055
  STEP: Deleting the collection of validation webhooks @ 08/24/23 12:22:28.121
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:22:28.225
  Aug 24 12:22:28.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2613" for this suite. @ 08/24/23 12:22:28.348
  STEP: Destroying namespace "webhook-markers-4616" for this suite. @ 08/24/23 12:22:28.364
• [4.824 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 08/24/23 12:22:28.387
  Aug 24 12:22:28.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:22:28.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:28.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:28.423
  Aug 24 12:22:28.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:22:28.581924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:29.582835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 08/24/23 12:22:30.247
  Aug 24 12:22:30.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 create -f -'
  E0824 12:22:30.583081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:31.583236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:31.873: INFO: stderr: ""
  Aug 24 12:22:31.873: INFO: stdout: "e2e-test-crd-publish-openapi-3190-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 24 12:22:31.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 delete e2e-test-crd-publish-openapi-3190-crds test-foo'
  Aug 24 12:22:32.198: INFO: stderr: ""
  Aug 24 12:22:32.199: INFO: stdout: "e2e-test-crd-publish-openapi-3190-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Aug 24 12:22:32.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 apply -f -'
  E0824 12:22:32.584201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:33.439: INFO: stderr: ""
  Aug 24 12:22:33.439: INFO: stdout: "e2e-test-crd-publish-openapi-3190-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 24 12:22:33.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 delete e2e-test-crd-publish-openapi-3190-crds test-foo'
  E0824 12:22:33.584850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:33.616: INFO: stderr: ""
  Aug 24 12:22:33.616: INFO: stdout: "e2e-test-crd-publish-openapi-3190-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 08/24/23 12:22:33.616
  Aug 24 12:22:33.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 create -f -'
  Aug 24 12:22:34.194: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 08/24/23 12:22:34.195
  Aug 24 12:22:34.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 create -f -'
  E0824 12:22:34.585882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:34.724: INFO: rc: 1
  Aug 24 12:22:34.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 apply -f -'
  Aug 24 12:22:35.185: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 08/24/23 12:22:35.185
  Aug 24 12:22:35.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 create -f -'
  E0824 12:22:35.586505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:35.620: INFO: rc: 1
  Aug 24 12:22:35.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 --namespace=crd-publish-openapi-3003 apply -f -'
  Aug 24 12:22:36.037: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 08/24/23 12:22:36.037
  Aug 24 12:22:36.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 explain e2e-test-crd-publish-openapi-3190-crds'
  Aug 24 12:22:36.459: INFO: stderr: ""
  Aug 24 12:22:36.459: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3190-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 08/24/23 12:22:36.46
  Aug 24 12:22:36.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 explain e2e-test-crd-publish-openapi-3190-crds.metadata'
  E0824 12:22:36.587121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:36.892: INFO: stderr: ""
  Aug 24 12:22:36.892: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3190-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Aug 24 12:22:36.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 explain e2e-test-crd-publish-openapi-3190-crds.spec'
  Aug 24 12:22:37.301: INFO: stderr: ""
  Aug 24 12:22:37.301: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3190-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Aug 24 12:22:37.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 explain e2e-test-crd-publish-openapi-3190-crds.spec.bars'
  E0824 12:22:37.587985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:37.718: INFO: stderr: ""
  Aug 24 12:22:37.718: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3190-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 08/24/23 12:22:37.718
  Aug 24 12:22:37.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-3003 explain e2e-test-crd-publish-openapi-3190-crds.spec.bars2'
  Aug 24 12:22:38.147: INFO: rc: 1
  E0824 12:22:38.588927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:39.588998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:40.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3003" for this suite. @ 08/24/23 12:22:40.074
• [11.698 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 08/24/23 12:22:40.087
  Aug 24 12:22:40.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:22:40.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:40.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:40.121
  STEP: Creating a simple DaemonSet "daemon-set" @ 08/24/23 12:22:40.163
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 12:22:40.174
  Aug 24 12:22:40.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:22:40.188: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 12:22:40.589374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:41.217: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:22:41.217: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 12:22:41.589809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:42.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:22:42.225: INFO: Node pohje9aimahx-3 is running 0 daemon pod, expected 1
  E0824 12:22:42.589782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:43.209: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:22:43.209: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 08/24/23 12:22:43.214
  Aug 24 12:22:43.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:22:43.266: INFO: Node pohje9aimahx-2 is running 0 daemon pod, expected 1
  E0824 12:22:43.590038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:44.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:22:44.298: INFO: Node pohje9aimahx-2 is running 0 daemon pod, expected 1
  E0824 12:22:44.590954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:45.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:22:45.298: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 08/24/23 12:22:45.298
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:22:45.317
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6247, will wait for the garbage collector to delete the pods @ 08/24/23 12:22:45.317
  Aug 24 12:22:45.391: INFO: Deleting DaemonSet.extensions daemon-set took: 15.377565ms
  Aug 24 12:22:45.492: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.809623ms
  E0824 12:22:45.591452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:46.593318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:46.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:22:46.901: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:22:46.913: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18370"},"items":null}

  Aug 24 12:22:46.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18370"},"items":null}

  Aug 24 12:22:46.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6247" for this suite. @ 08/24/23 12:22:46.955
• [6.881 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 08/24/23 12:22:46.975
  Aug 24 12:22:46.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:22:46.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:22:47.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:22:47.052
  STEP: Creating service test in namespace statefulset-1370 @ 08/24/23 12:22:47.057
  STEP: Creating stateful set ss in namespace statefulset-1370 @ 08/24/23 12:22:47.07
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1370 @ 08/24/23 12:22:47.086
  Aug 24 12:22:47.098: INFO: Found 0 stateful pods, waiting for 1
  E0824 12:22:47.593965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:48.595119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:49.595863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:50.596066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:51.596171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:52.596822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:53.597549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:54.597748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:55.597886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:56.598521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:22:57.105: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 08/24/23 12:22:57.105
  Aug 24 12:22:57.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:22:57.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:22:57.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:22:57.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:22:57.391: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0824 12:22:57.599041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:58.599143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:22:59.599348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:00.599668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:01.600289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:02.600472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:03.600839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:04.601067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:05.602240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:06.602891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:07.403: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:23:07.403: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:23:07.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999716s
  E0824 12:23:07.603556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:08.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976891947s
  E0824 12:23:08.603940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:09.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.954080116s
  E0824 12:23:09.604772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:10.502: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.935124284s
  E0824 12:23:10.604750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:11.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.924529709s
  E0824 12:23:11.605226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:12.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.916045176s
  E0824 12:23:12.605767      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:13.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.906796107s
  E0824 12:23:13.606466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:14.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.862741523s
  E0824 12:23:14.607639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:15.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.852949672s
  E0824 12:23:15.607197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:16.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 843.362774ms
  E0824 12:23:16.607777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1370 @ 08/24/23 12:23:17.594
  Aug 24 12:23:17.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 12:23:17.608056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:17.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 12:23:17.881: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:23:17.881: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:23:17.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:23:18.154: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 24 12:23:18.154: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:23:18.154: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:23:18.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:23:18.429: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 24 12:23:18.429: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:23:18.429: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:23:18.437: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0824 12:23:18.608730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:19.609715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:20.609884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:21.610172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:22.610331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:23.610627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:24.610750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:25.610923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:26.611208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:27.611769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:28.450: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:23:28.450: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:23:28.450: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 08/24/23 12:23:28.45
  Aug 24 12:23:28.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0824 12:23:28.611973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:28.752: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:23:28.753: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:23:28.753: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:23:28.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:23:29.067: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:23:29.067: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:23:29.067: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:23:29.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-1370 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:23:29.360: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:23:29.360: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:23:29.360: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:23:29.360: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:23:29.367: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0824 12:23:29.612327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:30.613056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:31.613326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:32.613515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:33.613713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:34.614459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:35.615239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:36.615554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:37.616145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:38.616253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:39.385: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:23:39.385: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:23:39.385: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:23:39.425: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Aug 24 12:23:39.425: INFO: ss-0  pohje9aimahx-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC  }]
  Aug 24 12:23:39.426: INFO: ss-1  pohje9aimahx-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:07 +0000 UTC  }]
  Aug 24 12:23:39.426: INFO: ss-2  pohje9aimahx-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:07 +0000 UTC  }]
  Aug 24 12:23:39.427: INFO: 
  Aug 24 12:23:39.427: INFO: StatefulSet ss has not reached scale 0, at 3
  E0824 12:23:39.616523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:40.438: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Aug 24 12:23:40.438: INFO: ss-0  pohje9aimahx-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:29 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC  }]
  Aug 24 12:23:40.439: INFO: 
  Aug 24 12:23:40.439: INFO: StatefulSet ss has not reached scale 0, at 1
  E0824 12:23:40.616932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:41.449: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.966653321s
  E0824 12:23:41.617447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:42.456: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.95712091s
  E0824 12:23:42.617960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:43.463: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.949369383s
  E0824 12:23:43.618947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:44.472: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.942326852s
  E0824 12:23:44.620337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:45.478: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.934012448s
  E0824 12:23:45.620585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:46.486: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.927268769s
  E0824 12:23:46.621377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:47.496: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.919717338s
  E0824 12:23:47.622319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:48.502: INFO: Verifying statefulset ss doesn't scale past 0 for another 910.231936ms
  E0824 12:23:48.622757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1370 @ 08/24/23 12:23:49.503
  Aug 24 12:23:49.511: INFO: Scaling statefulset ss to 0
  Aug 24 12:23:49.557: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:23:49.564: INFO: Deleting all statefulset in ns statefulset-1370
  Aug 24 12:23:49.570: INFO: Scaling statefulset ss to 0
  Aug 24 12:23:49.591: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:23:49.600: INFO: Deleting statefulset ss
  E0824 12:23:49.624816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:49.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1370" for this suite. @ 08/24/23 12:23:49.637
• [62.679 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 08/24/23 12:23:49.659
  Aug 24 12:23:49.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename watch @ 08/24/23 12:23:49.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:23:49.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:23:49.702
  STEP: creating a watch on configmaps @ 08/24/23 12:23:49.708
  STEP: creating a new configmap @ 08/24/23 12:23:49.711
  STEP: modifying the configmap once @ 08/24/23 12:23:49.722
  STEP: closing the watch once it receives two notifications @ 08/24/23 12:23:49.736
  Aug 24 12:23:49.736: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-367  6a2704f1-f652-45a6-b32f-e6008ff935ba 18713 0 2023-08-24 12:23:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:23:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:23:49.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-367  6a2704f1-f652-45a6-b32f-e6008ff935ba 18714 0 2023-08-24 12:23:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:23:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 08/24/23 12:23:49.737
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 08/24/23 12:23:49.754
  STEP: deleting the configmap @ 08/24/23 12:23:49.756
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 08/24/23 12:23:49.774
  Aug 24 12:23:49.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-367  6a2704f1-f652-45a6-b32f-e6008ff935ba 18715 0 2023-08-24 12:23:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:23:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:23:49.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-367  6a2704f1-f652-45a6-b32f-e6008ff935ba 18716 0 2023-08-24 12:23:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:23:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:23:49.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-367" for this suite. @ 08/24/23 12:23:49.787
• [0.138 seconds]
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 08/24/23 12:23:49.798
  Aug 24 12:23:49.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:23:49.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:23:49.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:23:49.844
  E0824 12:23:50.624305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:51.625274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:23:51.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3712" for this suite. @ 08/24/23 12:23:51.933
• [2.147 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 08/24/23 12:23:51.947
  Aug 24 12:23:51.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:23:51.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:23:51.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:23:51.986
  STEP: creating the pod @ 08/24/23 12:23:51.991
  STEP: submitting the pod to kubernetes @ 08/24/23 12:23:51.991
  STEP: verifying QOS class is set on the pod @ 08/24/23 12:23:52.011
  Aug 24 12:23:52.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2912" for this suite. @ 08/24/23 12:23:52.028
• [0.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 08/24/23 12:23:52.06
  Aug 24 12:23:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:23:52.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:23:52.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:23:52.102
  STEP: Creating secret with name s-test-opt-del-e20a3020-e1dc-4a44-ae9e-f9c77641ab46 @ 08/24/23 12:23:52.12
  STEP: Creating secret with name s-test-opt-upd-dafa2f2f-8e76-43b5-bba3-49e32dfa3af2 @ 08/24/23 12:23:52.13
  STEP: Creating the pod @ 08/24/23 12:23:52.141
  E0824 12:23:52.626129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:53.627789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:54.627819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:55.629147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-e20a3020-e1dc-4a44-ae9e-f9c77641ab46 @ 08/24/23 12:23:56.254
  STEP: Updating secret s-test-opt-upd-dafa2f2f-8e76-43b5-bba3-49e32dfa3af2 @ 08/24/23 12:23:56.266
  STEP: Creating secret with name s-test-opt-create-540a29a1-d897-4fbc-8f0a-52a8ef898488 @ 08/24/23 12:23:56.277
  STEP: waiting to observe update in volume @ 08/24/23 12:23:56.287
  E0824 12:23:56.628543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:57.628832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:58.630000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:23:59.630228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:00.630290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:01.630511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:02.630900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:03.631487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:04.632659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:05.633238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:06.634056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:07.634029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:08.634880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:09.635133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:10.635785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:11.635802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:12.636011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:13.636695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:14.636966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:15.637091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:16.638024      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:17.639059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:18.650474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:19.640383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:20.640645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:21.640759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:22.641583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:23.642427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:24.643457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:25.644050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:26.644993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:27.645978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:28.646122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:29.647110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:30.647721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:31.648278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:32.648967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:33.649634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:34.650383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:35.651052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:36.651496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:37.651778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:38.652818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:39.652978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:40.656332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:41.656470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:42.657390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:43.657975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:44.658171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:45.658510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:46.660456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:47.659247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:48.659205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:49.659261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:50.659593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:51.659676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:52.660157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:53.670956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:54.667198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:55.667705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:56.668407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:57.669297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:58.669282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:24:59.669932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:00.670457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:01.672065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:02.671179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:03.671821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:04.672527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:05.672771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:06.672959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:07.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2284" for this suite. @ 08/24/23 12:25:07.077
• [75.037 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 08/24/23 12:25:07.098
  Aug 24 12:25:07.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:25:07.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:07.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:07.146
  STEP: create the deployment @ 08/24/23 12:25:07.152
  W0824 12:25:07.163128      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/24/23 12:25:07.163
  E0824 12:25:07.673072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 08/24/23 12:25:07.687
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 08/24/23 12:25:07.702
  STEP: Gathering metrics @ 08/24/23 12:25:08.275
  Aug 24 12:25:08.520: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 12:25:08.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3456" for this suite. @ 08/24/23 12:25:08.55
• [1.469 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 08/24/23 12:25:08.568
  Aug 24 12:25:08.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:25:08.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:08.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:08.599
  STEP: creating an Endpoint @ 08/24/23 12:25:08.619
  STEP: waiting for available Endpoint @ 08/24/23 12:25:08.634
  STEP: listing all Endpoints @ 08/24/23 12:25:08.637
  STEP: updating the Endpoint @ 08/24/23 12:25:08.641
  STEP: fetching the Endpoint @ 08/24/23 12:25:08.657
  STEP: patching the Endpoint @ 08/24/23 12:25:08.664
  E0824 12:25:08.673330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: fetching the Endpoint @ 08/24/23 12:25:08.679
  STEP: deleting the Endpoint by Collection @ 08/24/23 12:25:08.689
  STEP: waiting for Endpoint deletion @ 08/24/23 12:25:08.703
  STEP: fetching the Endpoint @ 08/24/23 12:25:08.706
  Aug 24 12:25:08.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5204" for this suite. @ 08/24/23 12:25:08.718
• [0.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 08/24/23 12:25:08.734
  Aug 24 12:25:08.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:25:08.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:08.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:08.765
  STEP: Creating configMap that has name configmap-test-emptyKey-fb61a301-1ef7-44af-ae9c-65b32630edda @ 08/24/23 12:25:08.775
  Aug 24 12:25:08.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3384" for this suite. @ 08/24/23 12:25:08.793
• [0.074 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 08/24/23 12:25:08.811
  Aug 24 12:25:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:25:08.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:08.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:08.9
  Aug 24 12:25:08.903: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Aug 24 12:25:08.923: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0824 12:25:09.673941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:10.674382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:11.674644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:12.674660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:13.675057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:13.950: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:25:13.951
  Aug 24 12:25:13.951: INFO: Creating deployment "test-rolling-update-deployment"
  Aug 24 12:25:13.976: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Aug 24 12:25:14.036: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0824 12:25:14.675303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:15.675480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:16.051: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Aug 24 12:25:16.057: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Aug 24 12:25:16.096: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6160  390abac4-04cb-400a-b5c4-22a4882bac43 19156 1 2023-08-24 12:25:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-24 12:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056b0a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:25:14 +0000 UTC,LastTransitionTime:2023-08-24 12:25:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-08-24 12:25:16 +0000 UTC,LastTransitionTime:2023-08-24 12:25:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 12:25:16.104: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-6160  5536ed2a-abf2-4dbb-8c67-ecbdb6d04227 19143 1 2023-08-24 12:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 390abac4-04cb-400a-b5c4-22a4882bac43 0xc002269737 0xc002269738}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"390abac4-04cb-400a-b5c4-22a4882bac43\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:25:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022697e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:25:16.105: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Aug 24 12:25:16.105: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6160  18d7a4ae-7b41-458e-90c6-d2c294bac718 19154 2 2023-08-24 12:25:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 390abac4-04cb-400a-b5c4-22a4882bac43 0xc002269437 0xc002269438}] [] [{e2e.test Update apps/v1 2023-08-24 12:25:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"390abac4-04cb-400a-b5c4-22a4882bac43\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:25:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0022696c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:25:16.119: INFO: Pod "test-rolling-update-deployment-656d657cd8-xkxmp" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-xkxmp test-rolling-update-deployment-656d657cd8- deployment-6160  e5bd3994-8090-4bdf-9ff9-efdd88d899dd 19142 0 2023-08-24 12:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 5536ed2a-abf2-4dbb-8c67-ecbdb6d04227 0xc002269c87 0xc002269c88}] [] [{kube-controller-manager Update v1 2023-08-24 12:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5536ed2a-abf2-4dbb-8c67-ecbdb6d04227\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5d94f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5d94f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:25:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.47,StartTime:2023-08-24 12:25:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:25:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://46ef83b39a902e0dfc7f1edf5355c1cf9ea8c9f87aa3d1fa38ca63a885ee31b7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.47,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:25:16.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6160" for this suite. @ 08/24/23 12:25:16.129
• [7.329 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 08/24/23 12:25:16.14
  Aug 24 12:25:16.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:25:16.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:16.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:16.18
  STEP: creating Agnhost RC @ 08/24/23 12:25:16.185
  Aug 24 12:25:16.185: INFO: namespace kubectl-217
  Aug 24 12:25:16.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-217 create -f -'
  E0824 12:25:16.676329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:17.029: INFO: stderr: ""
  Aug 24 12:25:17.029: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/24/23 12:25:17.029
  E0824 12:25:17.677267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:18.038: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:25:18.038: INFO: Found 0 / 1
  E0824 12:25:18.677425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:19.036: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:25:19.036: INFO: Found 1 / 1
  Aug 24 12:25:19.036: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 24 12:25:19.048: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:25:19.048: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 12:25:19.049: INFO: wait on agnhost-primary startup in kubectl-217 
  Aug 24 12:25:19.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-217 logs agnhost-primary-wj4bp agnhost-primary'
  Aug 24 12:25:19.232: INFO: stderr: ""
  Aug 24 12:25:19.232: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 08/24/23 12:25:19.232
  Aug 24 12:25:19.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-217 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Aug 24 12:25:19.399: INFO: stderr: ""
  Aug 24 12:25:19.399: INFO: stdout: "service/rm2 exposed\n"
  Aug 24 12:25:19.409: INFO: Service rm2 in namespace kubectl-217 found.
  E0824 12:25:19.678486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:20.679041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 08/24/23 12:25:21.425
  Aug 24 12:25:21.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-217 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  E0824 12:25:21.679192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:21.707: INFO: stderr: ""
  Aug 24 12:25:21.707: INFO: stdout: "service/rm3 exposed\n"
  Aug 24 12:25:21.724: INFO: Service rm3 in namespace kubectl-217 found.
  E0824 12:25:22.681900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:23.680288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:23.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-217" for this suite. @ 08/24/23 12:25:23.75
• [7.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 08/24/23 12:25:23.771
  Aug 24 12:25:23.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:25:23.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:23.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:23.816
  Aug 24 12:25:23.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:25:24.680699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:25.681121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/24/23 12:25:25.793
  Aug 24 12:25:25.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1761 --namespace=crd-publish-openapi-1761 create -f -'
  E0824 12:25:26.681372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:27.409: INFO: stderr: ""
  Aug 24 12:25:27.409: INFO: stdout: "e2e-test-crd-publish-openapi-4149-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 24 12:25:27.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1761 --namespace=crd-publish-openapi-1761 delete e2e-test-crd-publish-openapi-4149-crds test-cr'
  Aug 24 12:25:27.575: INFO: stderr: ""
  Aug 24 12:25:27.575: INFO: stdout: "e2e-test-crd-publish-openapi-4149-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Aug 24 12:25:27.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1761 --namespace=crd-publish-openapi-1761 apply -f -'
  E0824 12:25:27.681531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:28.682395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:29.040: INFO: stderr: ""
  Aug 24 12:25:29.040: INFO: stdout: "e2e-test-crd-publish-openapi-4149-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 24 12:25:29.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1761 --namespace=crd-publish-openapi-1761 delete e2e-test-crd-publish-openapi-4149-crds test-cr'
  Aug 24 12:25:29.209: INFO: stderr: ""
  Aug 24 12:25:29.210: INFO: stdout: "e2e-test-crd-publish-openapi-4149-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/24/23 12:25:29.21
  Aug 24 12:25:29.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-1761 explain e2e-test-crd-publish-openapi-4149-crds'
  E0824 12:25:29.683607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:29.865: INFO: stderr: ""
  Aug 24 12:25:29.865: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-4149-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0824 12:25:30.684798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:31.686501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:32.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1761" for this suite. @ 08/24/23 12:25:32.199
• [8.445 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 08/24/23 12:25:32.221
  Aug 24 12:25:32.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:25:32.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:32.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:32.263
  STEP: creating service in namespace services-218 @ 08/24/23 12:25:32.267
  STEP: creating service affinity-clusterip-transition in namespace services-218 @ 08/24/23 12:25:32.267
  STEP: creating replication controller affinity-clusterip-transition in namespace services-218 @ 08/24/23 12:25:32.307
  I0824 12:25:32.337311      13 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-218, replica count: 3
  E0824 12:25:32.685358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:33.685892      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:34.687022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:25:35.388817      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:25:35.404: INFO: Creating new exec pod
  E0824 12:25:35.687037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:36.687502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:37.687988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:38.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-218 exec execpod-affinityjqvqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  E0824 12:25:38.690141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:38.794: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Aug 24 12:25:38.794: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:25:38.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-218 exec execpod-affinityjqvqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.14.93 80'
  Aug 24 12:25:39.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.14.93 80\nConnection to 10.233.14.93 80 port [tcp/http] succeeded!\n"
  Aug 24 12:25:39.082: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:25:39.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-218 exec execpod-affinityjqvqp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.93:80/ ; done'
  Aug 24 12:25:39.630: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n"
  Aug 24 12:25:39.630: INFO: stdout: "\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-452hm\naffinity-clusterip-transition-452hm\naffinity-clusterip-transition-452hm\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-452hm\naffinity-clusterip-transition-452hm\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-9dksp\naffinity-clusterip-transition-452hm\naffinity-clusterip-transition-9dksp\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx"
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-452hm
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-452hm
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-452hm
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-452hm
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-452hm
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-9dksp
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-452hm
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-9dksp
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.630: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:39.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-218 exec execpod-affinityjqvqp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.14.93:80/ ; done'
  E0824 12:25:39.690800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:40.168: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.14.93:80/\n"
  Aug 24 12:25:40.168: INFO: stdout: "\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx\naffinity-clusterip-transition-r5smx"
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.168: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.169: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.169: INFO: Received response from host: affinity-clusterip-transition-r5smx
  Aug 24 12:25:40.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:25:40.185: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-218, will wait for the garbage collector to delete the pods @ 08/24/23 12:25:40.233
  Aug 24 12:25:40.310: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.200553ms
  Aug 24 12:25:40.420: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 110.340324ms
  E0824 12:25:40.691131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:41.691179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:42.692002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-218" for this suite. @ 08/24/23 12:25:42.768
• [10.557 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 08/24/23 12:25:42.778
  Aug 24 12:25:42.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:25:42.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:42.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:42.814
  STEP: Creating configMap with name configmap-test-volume-278b15f0-e53c-472f-8525-df7a87cc5363 @ 08/24/23 12:25:42.818
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:25:42.824
  E0824 12:25:43.692622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:44.693358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:45.694982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:46.694117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:25:46.884
  Aug 24 12:25:46.890: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-098dfddb-0b9e-4baa-a730-89718054d894 container configmap-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:25:46.906
  Aug 24 12:25:46.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9185" for this suite. @ 08/24/23 12:25:46.945
• [4.182 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 08/24/23 12:25:46.966
  Aug 24 12:25:46.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename security-context @ 08/24/23 12:25:46.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:47.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:47.028
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/24/23 12:25:47.044
  E0824 12:25:47.698892      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:48.696277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:49.696462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:50.696951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:25:51.124
  Aug 24 12:25:51.132: INFO: Trying to get logs from node pohje9aimahx-3 pod security-context-486c756a-646e-412e-9037-58d4a511908d container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:25:51.15
  Aug 24 12:25:51.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7623" for this suite. @ 08/24/23 12:25:51.19
• [4.243 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 08/24/23 12:25:51.212
  Aug 24 12:25:51.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:25:51.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:51.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:51.264
  STEP: Setting up server cert @ 08/24/23 12:25:51.335
  E0824 12:25:51.698218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:52.698541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:25:52.8
  STEP: Deploying the webhook pod @ 08/24/23 12:25:52.829
  STEP: Wait for the deployment to be ready @ 08/24/23 12:25:52.87
  Aug 24 12:25:52.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 25, 52, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-7497495989\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 25, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 25, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
  E0824 12:25:53.699141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:54.699305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:25:54.945
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:25:54.975
  E0824 12:25:55.700286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:25:55.976: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 08/24/23 12:25:55.986
  STEP: Creating a custom resource definition that should be denied by the webhook @ 08/24/23 12:25:56.035
  Aug 24 12:25:56.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:25:56.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5478" for this suite. @ 08/24/23 12:25:56.187
  STEP: Destroying namespace "webhook-markers-7872" for this suite. @ 08/24/23 12:25:56.214
• [5.028 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 08/24/23 12:25:56.241
  Aug 24 12:25:56.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:25:56.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:56.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:56.312
  STEP: Creating a ResourceQuota with terminating scope @ 08/24/23 12:25:56.326
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 12:25:56.336
  E0824 12:25:56.700890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:57.701105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 08/24/23 12:25:58.345
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 12:25:58.352
  E0824 12:25:58.701344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:25:59.701676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 08/24/23 12:26:00.36
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 08/24/23 12:26:00.393
  E0824 12:26:00.701692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:01.707966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 08/24/23 12:26:02.403
  E0824 12:26:02.705770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:03.707435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/24/23 12:26:04.413
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 12:26:04.443
  E0824 12:26:04.716764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:05.719633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 08/24/23 12:26:06.454
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 08/24/23 12:26:06.475
  E0824 12:26:06.714944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:07.714652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 08/24/23 12:26:08.486
  E0824 12:26:08.715501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:09.715599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/24/23 12:26:10.496
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 12:26:10.521
  E0824 12:26:10.716217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:11.717031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:26:12.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-660" for this suite. @ 08/24/23 12:26:12.546
• [16.315 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 08/24/23 12:26:12.559
  Aug 24 12:26:12.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:26:12.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:12.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:12.605
  STEP: getting /apis @ 08/24/23 12:26:12.609
  STEP: getting /apis/node.k8s.io @ 08/24/23 12:26:12.618
  STEP: getting /apis/node.k8s.io/v1 @ 08/24/23 12:26:12.621
  STEP: creating @ 08/24/23 12:26:12.623
  STEP: watching @ 08/24/23 12:26:12.664
  Aug 24 12:26:12.664: INFO: starting watch
  STEP: getting @ 08/24/23 12:26:12.678
  STEP: listing @ 08/24/23 12:26:12.687
  STEP: patching @ 08/24/23 12:26:12.694
  STEP: updating @ 08/24/23 12:26:12.704
  E0824 12:26:12.717273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:26:12.718: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 08/24/23 12:26:12.719
  STEP: deleting a collection @ 08/24/23 12:26:12.745
  Aug 24 12:26:12.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-887" for this suite. @ 08/24/23 12:26:12.794
• [0.271 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 08/24/23 12:26:12.832
  Aug 24 12:26:12.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:26:12.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:12.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:12.898
  STEP: Creating configMap with name projected-configmap-test-volume-map-6cd9350a-c68b-4787-a177-554f459ea6bd @ 08/24/23 12:26:12.904
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:26:12.915
  E0824 12:26:13.717911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:14.719233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:15.718974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:16.719255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:26:16.974
  Aug 24 12:26:16.991: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-d3a812e5-c8ef-43f2-894b-e2bde8bed23c container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:26:17.008
  Aug 24 12:26:17.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1606" for this suite. @ 08/24/23 12:26:17.066
• [4.255 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 08/24/23 12:26:17.089
  Aug 24 12:26:17.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename proxy @ 08/24/23 12:26:17.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:17.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:17.141
  Aug 24 12:26:17.151: INFO: Creating pod...
  E0824 12:26:17.720730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:18.720966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:19.721376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:20.721672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:26:21.205: INFO: Creating service...
  Aug 24 12:26:21.229: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/DELETE
  Aug 24 12:26:21.254: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 12:26:21.254: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/GET
  Aug 24 12:26:21.262: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 24 12:26:21.262: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/HEAD
  Aug 24 12:26:21.272: INFO: http.Client request:HEAD | StatusCode:200
  Aug 24 12:26:21.273: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/OPTIONS
  Aug 24 12:26:21.280: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 12:26:21.281: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/PATCH
  Aug 24 12:26:21.287: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 12:26:21.287: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/POST
  Aug 24 12:26:21.298: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 12:26:21.298: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/pods/agnhost/proxy/some/path/with/PUT
  Aug 24 12:26:21.310: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 12:26:21.310: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/DELETE
  Aug 24 12:26:21.320: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 12:26:21.320: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/GET
  Aug 24 12:26:21.334: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 24 12:26:21.334: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/HEAD
  Aug 24 12:26:21.348: INFO: http.Client request:HEAD | StatusCode:200
  Aug 24 12:26:21.348: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/OPTIONS
  Aug 24 12:26:21.363: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 12:26:21.363: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/PATCH
  Aug 24 12:26:21.379: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 12:26:21.379: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/POST
  Aug 24 12:26:21.391: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 12:26:21.391: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9650/services/test-service/proxy/some/path/with/PUT
  Aug 24 12:26:21.404: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 12:26:21.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9650" for this suite. @ 08/24/23 12:26:21.414
• [4.339 seconds]
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 08/24/23 12:26:21.43
  Aug 24 12:26:21.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename watch @ 08/24/23 12:26:21.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:21.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:21.472
  STEP: creating a watch on configmaps with a certain label @ 08/24/23 12:26:21.478
  STEP: creating a new configmap @ 08/24/23 12:26:21.48
  STEP: modifying the configmap once @ 08/24/23 12:26:21.494
  STEP: changing the label value of the configmap @ 08/24/23 12:26:21.509
  STEP: Expecting to observe a delete notification for the watched object @ 08/24/23 12:26:21.523
  Aug 24 12:26:21.524: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3298  fde58f72-ac26-4407-8569-72173ed844b6 19745 0 2023-08-24 12:26:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:26:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:26:21.525: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3298  fde58f72-ac26-4407-8569-72173ed844b6 19746 0 2023-08-24 12:26:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:26:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:26:21.526: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3298  fde58f72-ac26-4407-8569-72173ed844b6 19747 0 2023-08-24 12:26:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:26:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 08/24/23 12:26:21.526
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 08/24/23 12:26:21.556
  E0824 12:26:21.722720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:22.723371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:23.724049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:24.724738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:25.724837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:26.725069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:27.725162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:28.726092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:29.727142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:30.727489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 08/24/23 12:26:31.556
  STEP: modifying the configmap a third time @ 08/24/23 12:26:31.578
  STEP: deleting the configmap @ 08/24/23 12:26:31.595
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 08/24/23 12:26:31.605
  Aug 24 12:26:31.605: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3298  fde58f72-ac26-4407-8569-72173ed844b6 19799 0 2023-08-24 12:26:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:26:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:26:31.606: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3298  fde58f72-ac26-4407-8569-72173ed844b6 19800 0 2023-08-24 12:26:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:26:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:26:31.606: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3298  fde58f72-ac26-4407-8569-72173ed844b6 19801 0 2023-08-24 12:26:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:26:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:26:31.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3298" for this suite. @ 08/24/23 12:26:31.617
• [10.198 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 08/24/23 12:26:31.637
  Aug 24 12:26:31.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:26:31.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:31.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:31.681
  STEP: Setting up data @ 08/24/23 12:26:31.685
  STEP: Creating pod pod-subpath-test-configmap-xkw7 @ 08/24/23 12:26:31.705
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:26:31.705
  E0824 12:26:31.727946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:32.728351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:33.729056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:34.729474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:35.730267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:36.730654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:37.730790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:38.731623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:39.732441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:40.732930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:41.733610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:42.733971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:43.734114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:44.735034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:45.735736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:46.736061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:47.736441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:48.736771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:49.736985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:50.737364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:51.737356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:52.737651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:53.737967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:54.739045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:55.741195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:26:55.858
  Aug 24 12:26:55.866: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-subpath-test-configmap-xkw7 container test-container-subpath-configmap-xkw7: <nil>
  STEP: delete the pod @ 08/24/23 12:26:55.883
  STEP: Deleting pod pod-subpath-test-configmap-xkw7 @ 08/24/23 12:26:55.918
  Aug 24 12:26:55.918: INFO: Deleting pod "pod-subpath-test-configmap-xkw7" in namespace "subpath-5762"
  Aug 24 12:26:55.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5762" for this suite. @ 08/24/23 12:26:55.936
• [24.313 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 08/24/23 12:26:55.951
  Aug 24 12:26:55.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:26:55.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:55.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:55.995
  Aug 24 12:26:56.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1723" for this suite. @ 08/24/23 12:26:56.024
• [0.085 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 08/24/23 12:26:56.039
  Aug 24 12:26:56.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename certificates @ 08/24/23 12:26:56.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:56.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:56.082
  E0824 12:26:56.752086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 08/24/23 12:26:57.516
  STEP: getting /apis/certificates.k8s.io @ 08/24/23 12:26:57.533
  STEP: getting /apis/certificates.k8s.io/v1 @ 08/24/23 12:26:57.538
  STEP: creating @ 08/24/23 12:26:57.544
  STEP: getting @ 08/24/23 12:26:57.616
  STEP: listing @ 08/24/23 12:26:57.622
  STEP: watching @ 08/24/23 12:26:57.631
  Aug 24 12:26:57.631: INFO: starting watch
  STEP: patching @ 08/24/23 12:26:57.634
  STEP: updating @ 08/24/23 12:26:57.649
  Aug 24 12:26:57.659: INFO: waiting for watch events with expected annotations
  Aug 24 12:26:57.659: INFO: saw patched and updated annotations
  STEP: getting /approval @ 08/24/23 12:26:57.659
  STEP: patching /approval @ 08/24/23 12:26:57.668
  STEP: updating /approval @ 08/24/23 12:26:57.688
  STEP: getting /status @ 08/24/23 12:26:57.721
  STEP: patching /status @ 08/24/23 12:26:57.73
  E0824 12:26:57.750696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating /status @ 08/24/23 12:26:57.755
  STEP: deleting @ 08/24/23 12:26:57.769
  STEP: deleting a collection @ 08/24/23 12:26:57.799
  Aug 24 12:26:57.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-7864" for this suite. @ 08/24/23 12:26:57.846
• [1.822 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 08/24/23 12:26:57.894
  Aug 24 12:26:57.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:26:57.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:57.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:57.943
  STEP: Creating a ResourceQuota @ 08/24/23 12:26:57.95
  STEP: Getting a ResourceQuota @ 08/24/23 12:26:57.96
  STEP: Listing all ResourceQuotas with LabelSelector @ 08/24/23 12:26:57.967
  STEP: Patching the ResourceQuota @ 08/24/23 12:26:57.974
  STEP: Deleting a Collection of ResourceQuotas @ 08/24/23 12:26:57.987
  STEP: Verifying the deleted ResourceQuota @ 08/24/23 12:26:58.013
  Aug 24 12:26:58.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2372" for this suite. @ 08/24/23 12:26:58.027
• [0.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 08/24/23 12:26:58.047
  Aug 24 12:26:58.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:26:58.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:58.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:58.082
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/24/23 12:26:58.094
  E0824 12:26:58.750595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:26:59.750777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:00.751178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:01.751273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:27:02.141
  Aug 24 12:27:02.147: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-bda13416-b027-4dbb-822b-9fafc35534da container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:27:02.163
  Aug 24 12:27:02.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7207" for this suite. @ 08/24/23 12:27:02.2
• [4.167 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 08/24/23 12:27:02.217
  Aug 24 12:27:02.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:27:02.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:02.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:02.251
  STEP: Creating pod busybox-d7565d0a-28d7-44ab-86fb-6a09c1c2b92d in namespace container-probe-8891 @ 08/24/23 12:27:02.256
  E0824 12:27:02.751504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:03.751917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:27:04.294: INFO: Started pod busybox-d7565d0a-28d7-44ab-86fb-6a09c1c2b92d in namespace container-probe-8891
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:27:04.294
  Aug 24 12:27:04.301: INFO: Initial restart count of pod busybox-d7565d0a-28d7-44ab-86fb-6a09c1c2b92d is 0
  E0824 12:27:04.752036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:05.753037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:06.753436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:07.753649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:08.753832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:09.754173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:10.755671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:11.755893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:12.756638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:13.756951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:14.757637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:15.757941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:16.759039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:17.759450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:18.760390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:19.761669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:20.761730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:21.762152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:22.762376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:23.762719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:24.762912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:25.763229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:26.763219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:27.763920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:28.763971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:29.764269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:30.764782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:31.764966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:32.765048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:33.765249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:34.766005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:35.766169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:36.767110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:37.767476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:38.768113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:39.768280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:40.769412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:41.769709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:42.770081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:43.770402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:44.771347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:45.771964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:46.772857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:47.773752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:48.773243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:49.773999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:50.774246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:51.775918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:52.787804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:53.780172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:27:54.560: INFO: Restart count of pod container-probe-8891/busybox-d7565d0a-28d7-44ab-86fb-6a09c1c2b92d is now 1 (50.258758836s elapsed)
  Aug 24 12:27:54.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:27:54.574
  STEP: Destroying namespace "container-probe-8891" for this suite. @ 08/24/23 12:27:54.606
• [52.414 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 08/24/23 12:27:54.631
  Aug 24 12:27:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:27:54.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:54.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:54.685
  Aug 24 12:27:54.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  W0824 12:27:54.694369      13 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00637a0a0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0824 12:27:54.781148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:55.782130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:27:56.781387      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:27:57.677472      13 warnings.go:70] unknown field "alpha"
  W0824 12:27:57.677739      13 warnings.go:70] unknown field "beta"
  W0824 12:27:57.677783      13 warnings.go:70] unknown field "delta"
  W0824 12:27:57.677927      13 warnings.go:70] unknown field "epsilon"
  W0824 12:27:57.677950      13 warnings.go:70] unknown field "gamma"
  E0824 12:27:57.782516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:27:58.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6789" for this suite. @ 08/24/23 12:27:58.295
• [3.676 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 08/24/23 12:27:58.317
  Aug 24 12:27:58.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:27:58.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:58.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:58.36
  STEP: Setting up server cert @ 08/24/23 12:27:58.405
  E0824 12:27:58.784121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:27:59.638
  STEP: Deploying the webhook pod @ 08/24/23 12:27:59.657
  STEP: Wait for the deployment to be ready @ 08/24/23 12:27:59.692
  Aug 24 12:27:59.704: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 12:27:59.784108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:00.784183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:28:01.738
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:28:01.76
  E0824 12:28:01.785349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:02.761: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/24/23 12:28:02.773
  E0824 12:28:02.786962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a pod @ 08/24/23 12:28:02.812
  E0824 12:28:03.788214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:04.788377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 08/24/23 12:28:04.854
  Aug 24 12:28:04.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=webhook-4037 attach --namespace=webhook-4037 to-be-attached-pod -i -c=container1'
  Aug 24 12:28:05.056: INFO: rc: 1
  Aug 24 12:28:05.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4037" for this suite. @ 08/24/23 12:28:05.172
  STEP: Destroying namespace "webhook-markers-4594" for this suite. @ 08/24/23 12:28:05.201
• [6.917 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 08/24/23 12:28:05.236
  Aug 24 12:28:05.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:28:05.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:05.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:05.28
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:28:05.287
  E0824 12:28:05.789171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:06.789535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:07.790108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:08.790931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:28:09.349
  Aug 24 12:28:09.356: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-4fd26603-fdb9-44ab-a655-7d3233042f6c container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:28:09.374
  Aug 24 12:28:09.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6129" for this suite. @ 08/24/23 12:28:09.417
• [4.192 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 08/24/23 12:28:09.431
  Aug 24 12:28:09.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:28:09.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:09.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:09.468
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:28:09.474
  E0824 12:28:09.792752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:10.792925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:11.793759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:12.794755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:28:13.567
  Aug 24 12:28:13.575: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-9a1e5523-4100-42cb-9e25-c3a3b7ad36a6 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:28:13.587
  Aug 24 12:28:13.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8676" for this suite. @ 08/24/23 12:28:13.629
• [4.213 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 08/24/23 12:28:13.645
  Aug 24 12:28:13.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:28:13.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:13.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:13.683
  STEP: creating a Deployment @ 08/24/23 12:28:13.695
  STEP: waiting for Deployment to be created @ 08/24/23 12:28:13.707
  STEP: waiting for all Replicas to be Ready @ 08/24/23 12:28:13.711
  Aug 24 12:28:13.714: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 12:28:13.715: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 12:28:13.733: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 12:28:13.733: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 12:28:13.787: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 12:28:13.787: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0824 12:28:13.794989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:13.884: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 12:28:13.884: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0824 12:28:14.795339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:15.299: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 24 12:28:15.300: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0824 12:28:15.795746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:15.809: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 08/24/23 12:28:15.809
  W0824 12:28:15.824008      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 24 12:28:15.827: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 08/24/23 12:28:15.827
  Aug 24 12:28:15.835: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.835: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.835: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.835: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 0
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.836: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.857: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.857: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.897: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.897: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:15.955: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:15.955: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:16.007: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:16.008: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  E0824 12:28:16.796876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:17.797969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:17.957: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:17.957: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:18.036: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  STEP: listing Deployments @ 08/24/23 12:28:18.036
  Aug 24 12:28:18.063: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 08/24/23 12:28:18.063
  Aug 24 12:28:18.107: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 08/24/23 12:28:18.108
  Aug 24 12:28:18.136: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:18.153: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:18.319: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:18.374: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:18.420: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0824 12:28:18.805153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:19.808474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:19.862: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:19.896: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:19.960: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 12:28:20.002: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0824 12:28:20.810172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:21.810951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:28:22.406: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 08/24/23 12:28:22.444
  STEP: fetching the DeploymentStatus @ 08/24/23 12:28:22.463
  Aug 24 12:28:22.476: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:22.476: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:22.476: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:22.476: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:22.477: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 1
  Aug 24 12:28:22.477: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:22.477: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:22.477: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:22.477: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 2
  Aug 24 12:28:22.477: INFO: observed Deployment test-deployment in namespace deployment-8735 with ReadyReplicas 3
  STEP: deleting the Deployment @ 08/24/23 12:28:22.477
  Aug 24 12:28:22.498: INFO: observed event type MODIFIED
  Aug 24 12:28:22.498: INFO: observed event type MODIFIED
  Aug 24 12:28:22.499: INFO: observed event type MODIFIED
  Aug 24 12:28:22.499: INFO: observed event type MODIFIED
  Aug 24 12:28:22.500: INFO: observed event type MODIFIED
  Aug 24 12:28:22.500: INFO: observed event type MODIFIED
  Aug 24 12:28:22.501: INFO: observed event type MODIFIED
  Aug 24 12:28:22.501: INFO: observed event type MODIFIED
  Aug 24 12:28:22.501: INFO: observed event type MODIFIED
  Aug 24 12:28:22.501: INFO: observed event type MODIFIED
  Aug 24 12:28:22.502: INFO: observed event type MODIFIED
  Aug 24 12:28:22.511: INFO: Log out all the ReplicaSets if there is no deployment created
  Aug 24 12:28:22.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8735" for this suite. @ 08/24/23 12:28:22.552
• [8.931 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 08/24/23 12:28:22.579
  Aug 24 12:28:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:28:22.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:22.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:22.652
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 08/24/23 12:28:22.667
  E0824 12:28:22.811144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:23.812964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 08/24/23 12:28:24.719
  STEP: Then the orphan pod is adopted @ 08/24/23 12:28:24.731
  E0824 12:28:24.813514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 08/24/23 12:28:25.746
  Aug 24 12:28:25.761: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/24/23 12:28:25.785
  Aug 24 12:28:25.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 12:28:25.813948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replicaset-7056" for this suite. @ 08/24/23 12:28:25.823
• [3.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 08/24/23 12:28:25.877
  Aug 24 12:28:25.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:28:25.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:25.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:25.935
  STEP: creating a ConfigMap @ 08/24/23 12:28:25.94
  STEP: fetching the ConfigMap @ 08/24/23 12:28:25.955
  STEP: patching the ConfigMap @ 08/24/23 12:28:25.965
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 08/24/23 12:28:25.973
  STEP: deleting the ConfigMap by collection with a label selector @ 08/24/23 12:28:25.985
  STEP: listing all ConfigMaps in test namespace @ 08/24/23 12:28:26.004
  Aug 24 12:28:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6474" for this suite. @ 08/24/23 12:28:26.017
• [0.152 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 08/24/23 12:28:26.03
  Aug 24 12:28:26.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:28:26.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:26.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:26.065
  STEP: creating the pod with failed condition @ 08/24/23 12:28:26.07
  E0824 12:28:26.814376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:27.814547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:28.814968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:29.814969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:30.815216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:31.815622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:32.816167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:33.816100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:34.816162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:35.816346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:36.817658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:37.818450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:38.818852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:39.819228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:40.820100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:41.820814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:42.821077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:43.821797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:44.821925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:45.822757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:46.823525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:47.824636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:48.825346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:49.825646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:50.825763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:51.826287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:52.826409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:53.827109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:54.828294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:55.829026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:56.832786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:57.833109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:58.833348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:28:59.833498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:00.833806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:01.834688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:02.834943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:03.835496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:04.835906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:05.836641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:06.837574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:07.837745      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:08.837859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:09.838248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:10.838338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:11.839119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:12.841599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:13.842530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:14.841212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:15.841701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:16.841925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:17.847723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:18.843188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:19.843286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:20.843567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:21.844242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:22.844678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:23.845220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:24.845193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:25.846963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:26.845685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:27.845921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:28.846756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:29.847241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:30.847179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:31.847691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:32.848512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:33.848825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:34.849799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:35.849818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:36.850931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:37.851985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:38.852308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:39.852440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:40.852709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:41.852858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:42.852913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:43.853263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:44.853597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:45.853750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:46.854066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:47.854061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:48.854964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:49.855248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:50.855412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:51.856186      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:52.856447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:53.857050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:54.857634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:55.858403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:56.858195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:57.858930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:58.859234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:29:59.859337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:00.860351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:01.861089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:02.861499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:03.861779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:04.862764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:05.863494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:06.864149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:07.864472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:08.865120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:09.865181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:10.865310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:11.865513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:12.865679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:13.866416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:14.867371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:15.867484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:16.867404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:17.867769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:18.868004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:19.868292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:20.868619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:21.868789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:22.869438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:23.869403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:24.869932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:25.870781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 08/24/23 12:30:26.092
  Aug 24 12:30:26.622: INFO: Successfully updated pod "var-expansion-ea828c7b-9a27-4e75-b968-f23115d05852"
  STEP: waiting for pod running @ 08/24/23 12:30:26.622
  E0824 12:30:26.871632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:27.873094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/24/23 12:30:28.641
  Aug 24 12:30:28.641: INFO: Deleting pod "var-expansion-ea828c7b-9a27-4e75-b968-f23115d05852" in namespace "var-expansion-6607"
  Aug 24 12:30:28.657: INFO: Wait up to 5m0s for pod "var-expansion-ea828c7b-9a27-4e75-b968-f23115d05852" to be fully deleted
  E0824 12:30:28.873376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:29.874328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:30.875124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:31.875583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:32.882313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:33.877917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:34.878015      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:35.878103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:36.880883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:37.881038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:38.881616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:39.881745      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:40.882853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:41.883041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:42.883260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:43.883263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:44.883695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:45.883926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:46.884768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:47.885042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:48.885577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:49.886675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:50.886887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:51.887357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:52.888051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:53.888286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:54.889082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:55.890330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:56.890148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:57.890229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:58.890571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:30:59.890554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:00.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6607" for this suite. @ 08/24/23 12:31:00.85
• [154.838 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 08/24/23 12:31:00.869
  Aug 24 12:31:00.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 12:31:00.873
  E0824 12:31:00.890862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:00.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:00.913
  STEP: getting /apis @ 08/24/23 12:31:00.919
  STEP: getting /apis/discovery.k8s.io @ 08/24/23 12:31:00.927
  STEP: getting /apis/discovery.k8s.iov1 @ 08/24/23 12:31:00.929
  STEP: creating @ 08/24/23 12:31:00.931
  STEP: getting @ 08/24/23 12:31:00.963
  STEP: listing @ 08/24/23 12:31:00.968
  STEP: watching @ 08/24/23 12:31:00.975
  Aug 24 12:31:00.975: INFO: starting watch
  STEP: cluster-wide listing @ 08/24/23 12:31:00.978
  STEP: cluster-wide watching @ 08/24/23 12:31:00.985
  Aug 24 12:31:00.985: INFO: starting watch
  STEP: patching @ 08/24/23 12:31:00.987
  STEP: updating @ 08/24/23 12:31:00.998
  Aug 24 12:31:01.015: INFO: waiting for watch events with expected annotations
  Aug 24 12:31:01.015: INFO: saw patched and updated annotations
  STEP: deleting @ 08/24/23 12:31:01.015
  STEP: deleting a collection @ 08/24/23 12:31:01.051
  Aug 24 12:31:01.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4459" for this suite. @ 08/24/23 12:31:01.093
• [0.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 08/24/23 12:31:01.127
  Aug 24 12:31:01.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:31:01.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:01.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:01.183
  STEP: Creating a test headless service @ 08/24/23 12:31:01.188
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3825 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3825;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3825 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3825;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3825.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3825.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3825.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3825.svc;check="$$(dig +notcp +noall +answer +search 203.4.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.4.203_udp@PTR;check="$$(dig +tcp +noall +answer +search 203.4.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.4.203_tcp@PTR;sleep 1; done
   @ 08/24/23 12:31:01.23
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3825 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3825;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3825 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3825;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3825.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3825.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3825.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3825.svc;check="$$(dig +notcp +noall +answer +search 203.4.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.4.203_udp@PTR;check="$$(dig +tcp +noall +answer +search 203.4.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.4.203_tcp@PTR;sleep 1; done
   @ 08/24/23 12:31:01.23
  STEP: creating a pod to probe DNS @ 08/24/23 12:31:01.231
  STEP: submitting the pod to kubernetes @ 08/24/23 12:31:01.231
  E0824 12:31:01.891236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:02.892932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:03.892153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:04.892523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 12:31:05.288
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:31:05.294
  Aug 24 12:31:05.305: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.314: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.340: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.352: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.390: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.401: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.408: INFO: Unable to read jessie_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.418: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.425: INFO: Unable to read jessie_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.431: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.445: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:05.477: INFO: Lookups using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3825 wheezy_tcp@dns-test-service.dns-3825 wheezy_udp@dns-test-service.dns-3825.svc wheezy_tcp@dns-test-service.dns-3825.svc wheezy_udp@_http._tcp.dns-test-service.dns-3825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3825 jessie_tcp@dns-test-service.dns-3825 jessie_udp@dns-test-service.dns-3825.svc jessie_tcp@dns-test-service.dns-3825.svc jessie_tcp@_http._tcp.dns-test-service.dns-3825.svc]

  E0824 12:31:05.893029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:06.893396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:07.893915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:08.897231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:09.897018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:10.499: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.511: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.532: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.543: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.555: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.567: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.644: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.652: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.664: INFO: Unable to read jessie_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.675: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.684: INFO: Unable to read jessie_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.693: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:10.747: INFO: Lookups using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3825 wheezy_tcp@dns-test-service.dns-3825 wheezy_udp@dns-test-service.dns-3825.svc wheezy_tcp@dns-test-service.dns-3825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3825 jessie_tcp@dns-test-service.dns-3825 jessie_udp@dns-test-service.dns-3825.svc jessie_tcp@dns-test-service.dns-3825.svc]

  E0824 12:31:10.897974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:11.898089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:12.898130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:13.898476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:14.898831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:15.486: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.496: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.503: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.512: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.519: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.529: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.589: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.597: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.602: INFO: Unable to read jessie_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.610: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.617: INFO: Unable to read jessie_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.622: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:15.662: INFO: Lookups using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3825 wheezy_tcp@dns-test-service.dns-3825 wheezy_udp@dns-test-service.dns-3825.svc wheezy_tcp@dns-test-service.dns-3825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3825 jessie_tcp@dns-test-service.dns-3825 jessie_udp@dns-test-service.dns-3825.svc jessie_tcp@dns-test-service.dns-3825.svc]

  E0824 12:31:15.899682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:16.900240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:17.900372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:18.901454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:19.903298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:20.489: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.497: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.504: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.510: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.521: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.572: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.578: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.585: INFO: Unable to read jessie_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.592: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.601: INFO: Unable to read jessie_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.609: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:20.646: INFO: Lookups using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3825 wheezy_tcp@dns-test-service.dns-3825 wheezy_udp@dns-test-service.dns-3825.svc wheezy_tcp@dns-test-service.dns-3825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3825 jessie_tcp@dns-test-service.dns-3825 jessie_udp@dns-test-service.dns-3825.svc jessie_tcp@dns-test-service.dns-3825.svc]

  E0824 12:31:20.903120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:21.904739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:22.903837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:23.904584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:24.904528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:25.503: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.514: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.520: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.591: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.665: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.673: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.679: INFO: Unable to read jessie_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.686: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.697: INFO: Unable to read jessie_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.711: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:25.775: INFO: Lookups using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3825 wheezy_tcp@dns-test-service.dns-3825 wheezy_udp@dns-test-service.dns-3825.svc wheezy_tcp@dns-test-service.dns-3825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3825 jessie_tcp@dns-test-service.dns-3825 jessie_udp@dns-test-service.dns-3825.svc jessie_tcp@dns-test-service.dns-3825.svc]

  E0824 12:31:25.905953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:26.906145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:27.906998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:28.907155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:29.907346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:30.487: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.493: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.505: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.511: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.516: INFO: Unable to read wheezy_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.526: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.570: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.577: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.584: INFO: Unable to read jessie_udp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.590: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825 from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.596: INFO: Unable to read jessie_udp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.602: INFO: Unable to read jessie_tcp@dns-test-service.dns-3825.svc from pod dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf: the server could not find the requested resource (get pods dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf)
  Aug 24 12:31:30.659: INFO: Lookups using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3825 wheezy_tcp@dns-test-service.dns-3825 wheezy_udp@dns-test-service.dns-3825.svc wheezy_tcp@dns-test-service.dns-3825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3825 jessie_tcp@dns-test-service.dns-3825 jessie_udp@dns-test-service.dns-3825.svc jessie_tcp@dns-test-service.dns-3825.svc]

  E0824 12:31:30.908676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:31.908540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:32.908860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:33.909721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:34.909918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:35.658: INFO: DNS probes using dns-3825/dns-test-53385c66-eeab-4bc6-8d01-da5dfa4187bf succeeded

  Aug 24 12:31:35.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:31:35.666
  STEP: deleting the test service @ 08/24/23 12:31:35.702
  STEP: deleting the test headless service @ 08/24/23 12:31:35.787
  STEP: Destroying namespace "dns-3825" for this suite. @ 08/24/23 12:31:35.848
• [34.737 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 08/24/23 12:31:35.87
  Aug 24 12:31:35.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:31:35.881
  E0824 12:31:35.910211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:35.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:35.921
  Aug 24 12:31:35.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:31:36.911242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/24/23 12:31:37.793
  Aug 24 12:31:37.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-7481 --namespace=crd-publish-openapi-7481 create -f -'
  E0824 12:31:37.911450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:38.911802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:39.478: INFO: stderr: ""
  Aug 24 12:31:39.478: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 24 12:31:39.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-7481 --namespace=crd-publish-openapi-7481 delete e2e-test-crd-publish-openapi-5860-crds test-cr'
  Aug 24 12:31:39.639: INFO: stderr: ""
  Aug 24 12:31:39.639: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Aug 24 12:31:39.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-7481 --namespace=crd-publish-openapi-7481 apply -f -'
  E0824 12:31:39.912831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:40.913893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:41.222: INFO: stderr: ""
  Aug 24 12:31:41.222: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 24 12:31:41.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-7481 --namespace=crd-publish-openapi-7481 delete e2e-test-crd-publish-openapi-5860-crds test-cr'
  Aug 24 12:31:41.454: INFO: stderr: ""
  Aug 24 12:31:41.455: INFO: stdout: "e2e-test-crd-publish-openapi-5860-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 08/24/23 12:31:41.455
  Aug 24 12:31:41.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=crd-publish-openapi-7481 explain e2e-test-crd-publish-openapi-5860-crds'
  E0824 12:31:41.914256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:41.951: INFO: stderr: ""
  Aug 24 12:31:41.951: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5860-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0824 12:31:42.914726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:31:43.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7481" for this suite. @ 08/24/23 12:31:43.814
• [7.960 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 08/24/23 12:31:43.831
  Aug 24 12:31:43.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename watch @ 08/24/23 12:31:43.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:43.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:43.868
  STEP: creating a new configmap @ 08/24/23 12:31:43.873
  STEP: modifying the configmap once @ 08/24/23 12:31:43.882
  STEP: modifying the configmap a second time @ 08/24/23 12:31:43.898
  STEP: deleting the configmap @ 08/24/23 12:31:43.914
  E0824 12:31:43.915727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 08/24/23 12:31:43.924
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 08/24/23 12:31:43.926
  Aug 24 12:31:43.927: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1795  0b632d72-67bd-4326-b83b-64efd0757bca 21277 0 2023-08-24 12:31:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 12:31:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:31:43.927: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1795  0b632d72-67bd-4326-b83b-64efd0757bca 21278 0 2023-08-24 12:31:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 12:31:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:31:43.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1795" for this suite. @ 08/24/23 12:31:43.936
• [0.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 08/24/23 12:31:43.954
  Aug 24 12:31:43.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 12:31:43.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:43.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:43.987
  Aug 24 12:31:44.015: INFO: created pod
  E0824 12:31:44.916312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:45.916407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:46.916905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:47.917616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:31:48.051
  E0824 12:31:48.918062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:49.918229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:50.918337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:51.918513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:52.919229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:53.919751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:54.919940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:55.920048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:56.920798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:57.920957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:58.921329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:31:59.921639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:00.922045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:01.922375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:02.923059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:03.924140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:04.924312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:05.925339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:06.925916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:07.926491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:08.927569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:09.927944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:10.928407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:11.928810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:12.929197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:13.930200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:14.930776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:15.931279      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:16.932814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:17.933229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:32:18.052: INFO: polling logs
  Aug 24 12:32:18.092: INFO: Pod logs: 
  I0824 12:31:45.076890       1 log.go:198] OK: Got token
  I0824 12:31:45.077066       1 log.go:198] validating with in-cluster discovery
  I0824 12:31:45.077845       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0824 12:31:45.077924       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8683:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692880904, NotBefore:1692880304, IssuedAt:1692880304, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8683", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a6081ca2-230b-4d59-a8b4-7dcc81bf8d30"}}}
  I0824 12:31:45.099520       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0824 12:31:45.109416       1 log.go:198] OK: Validated signature on JWT
  I0824 12:31:45.109669       1 log.go:198] OK: Got valid claims from token!
  I0824 12:31:45.109819       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8683:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692880904, NotBefore:1692880304, IssuedAt:1692880304, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8683", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a6081ca2-230b-4d59-a8b4-7dcc81bf8d30"}}}

  Aug 24 12:32:18.093: INFO: completed pod
  Aug 24 12:32:18.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8683" for this suite. @ 08/24/23 12:32:18.112
• [34.171 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 08/24/23 12:32:18.131
  Aug 24 12:32:18.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 12:32:18.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:32:18.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:32:18.171
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 08/24/23 12:32:18.175
  E0824 12:32:18.935370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:19.934442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 08/24/23 12:32:20.206
  STEP: Then the orphan pod is adopted @ 08/24/23 12:32:20.215
  E0824 12:32:20.934862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:32:21.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7288" for this suite. @ 08/24/23 12:32:21.241
• [3.122 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 08/24/23 12:32:21.257
  Aug 24 12:32:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 12:32:21.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:32:21.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:32:21.293
  Aug 24 12:32:21.323: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 12:32:21.935216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:22.936026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:23.936873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:24.937274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:25.937552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:26.940224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:27.938734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:28.939047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:29.940019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:30.940253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:31.941131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:32.941626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:33.942417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:34.942788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:35.942980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:36.943260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:37.943376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:38.944351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:39.946335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:40.945621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:41.946648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:42.946821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:43.947595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:44.948050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:45.948256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:46.948697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:47.949032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:48.949952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:49.951396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:50.951896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:51.953016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:52.953166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:53.953947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:54.954527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:55.955368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:56.955046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:57.955866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:58.956159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:32:59.956986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:00.957457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:01.957731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:02.958476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:03.958585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:04.959194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:05.959260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:06.959855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:07.960599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:08.961158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:09.961559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:10.961772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:11.961873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:12.962005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:13.962844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:14.963019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:15.963241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:16.964403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:17.964141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:18.964254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:19.964847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:20.965581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:21.372: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/24/23 12:33:21.378
  Aug 24 12:33:21.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/24/23 12:33:21.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:21.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:21.42
  Aug 24 12:33:21.454: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Aug 24 12:33:21.463: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Aug 24 12:33:21.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:33:21.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4460" for this suite. @ 08/24/23 12:33:21.658
  STEP: Destroying namespace "sched-preemption-1618" for this suite. @ 08/24/23 12:33:21.67
• [60.426 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 08/24/23 12:33:21.685
  Aug 24 12:33:21.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:33:21.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:21.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:21.719
  STEP: Create a ReplicaSet @ 08/24/23 12:33:21.726
  STEP: Verify that the required pods have come up @ 08/24/23 12:33:21.74
  Aug 24 12:33:21.746: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0824 12:33:21.966340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:22.966808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:23.967047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:24.967492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:25.968039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:26.757: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 08/24/23 12:33:26.758
  Aug 24 12:33:26.766: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 08/24/23 12:33:26.766
  STEP: DeleteCollection of the ReplicaSets @ 08/24/23 12:33:26.775
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 08/24/23 12:33:26.792
  Aug 24 12:33:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8696" for this suite. @ 08/24/23 12:33:26.813
• [5.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 08/24/23 12:33:26.827
  Aug 24 12:33:26.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:33:26.83
  E0824 12:33:26.968798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:27.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:27.032
  STEP: Creating service test in namespace statefulset-2743 @ 08/24/23 12:33:27.042
  STEP: Looking for a node to schedule stateful set and pod @ 08/24/23 12:33:27.088
  STEP: Creating pod with conflicting port in namespace statefulset-2743 @ 08/24/23 12:33:27.108
  STEP: Waiting until pod test-pod will start running in namespace statefulset-2743 @ 08/24/23 12:33:27.259
  E0824 12:33:27.969139      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:28.969107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-2743 @ 08/24/23 12:33:29.306
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2743 @ 08/24/23 12:33:29.321
  Aug 24 12:33:29.377: INFO: Observed stateful pod in namespace: statefulset-2743, name: ss-0, uid: 34ce0ee6-1775-4eaa-aab3-6bc28e6220c8, status phase: Pending. Waiting for statefulset controller to delete.
  Aug 24 12:33:29.401: INFO: Observed stateful pod in namespace: statefulset-2743, name: ss-0, uid: 34ce0ee6-1775-4eaa-aab3-6bc28e6220c8, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 24 12:33:29.422: INFO: Observed stateful pod in namespace: statefulset-2743, name: ss-0, uid: 34ce0ee6-1775-4eaa-aab3-6bc28e6220c8, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 24 12:33:29.432: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2743
  STEP: Removing pod with conflicting port in namespace statefulset-2743 @ 08/24/23 12:33:29.433
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2743 and will be in running state @ 08/24/23 12:33:29.475
  E0824 12:33:29.969322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:30.970172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:31.490: INFO: Deleting all statefulset in ns statefulset-2743
  Aug 24 12:33:31.496: INFO: Scaling statefulset ss to 0
  E0824 12:33:31.970999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:32.972208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:33.972055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:34.972544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:35.972894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:36.973254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:37.973966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:38.975012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:39.977779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:40.978716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:41.574: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:33:41.585: INFO: Deleting statefulset ss
  Aug 24 12:33:41.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2743" for this suite. @ 08/24/23 12:33:41.622
• [14.810 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 08/24/23 12:33:41.641
  Aug 24 12:33:41.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sysctl @ 08/24/23 12:33:41.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:41.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:41.681
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 08/24/23 12:33:41.686
  STEP: Watching for error events or started pod @ 08/24/23 12:33:41.703
  E0824 12:33:41.976099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:42.976287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 08/24/23 12:33:43.713
  E0824 12:33:43.976866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:44.977219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 08/24/23 12:33:45.737
  STEP: Getting logs from the pod @ 08/24/23 12:33:45.737
  STEP: Checking that the sysctl is actually updated @ 08/24/23 12:33:45.756
  Aug 24 12:33:45.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6683" for this suite. @ 08/24/23 12:33:45.775
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 08/24/23 12:33:45.794
  Aug 24 12:33:45.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 12:33:45.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:45.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:45.838
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 08/24/23 12:33:45.866
  E0824 12:33:45.977696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:46.977942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:47.978685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:48.979793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:49.980363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:50.981234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:51.981402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:52.981951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:53.982723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:54.983116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:55.982961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:56.983996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:57.984472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:58.985303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:59.985527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:00.985777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:01.986174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:02.986261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:03.987064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 08/24/23 12:34:04.054
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 08/24/23 12:34:04.06
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 08/24/23 12:34:04.073
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 08/24/23 12:34:04.073
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 08/24/23 12:34:04.123
  E0824 12:34:04.988662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:05.989579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:06.989804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 08/24/23 12:34:07.156
  E0824 12:34:07.990260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 08/24/23 12:34:08.17
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 08/24/23 12:34:08.184
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 08/24/23 12:34:08.184
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 08/24/23 12:34:08.233
  E0824 12:34:08.991610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 08/24/23 12:34:09.252
  E0824 12:34:09.991709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:10.992978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 08/24/23 12:34:11.275
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 08/24/23 12:34:11.293
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 08/24/23 12:34:11.294
  Aug 24 12:34:11.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-327" for this suite. @ 08/24/23 12:34:11.351
• [25.571 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 08/24/23 12:34:11.365
  Aug 24 12:34:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:34:11.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:11.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:11.41
  STEP: Creating configMap with name projected-configmap-test-volume-6273b7fa-2cdf-4487-af2f-4276286be22c @ 08/24/23 12:34:11.415
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:34:11.425
  E0824 12:34:11.999257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:12.993930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:13.994089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:14.994849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:34:15.477
  Aug 24 12:34:15.483: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-1bdf0173-38dd-42b7-9034-31912261f7a0 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:34:15.497
  Aug 24 12:34:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2398" for this suite. @ 08/24/23 12:34:15.532
• [4.180 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 08/24/23 12:34:15.546
  Aug 24 12:34:15.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename podtemplate @ 08/24/23 12:34:15.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:15.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:15.589
  STEP: Create set of pod templates @ 08/24/23 12:34:15.597
  Aug 24 12:34:15.611: INFO: created test-podtemplate-1
  Aug 24 12:34:15.621: INFO: created test-podtemplate-2
  Aug 24 12:34:15.630: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 08/24/23 12:34:15.631
  STEP: delete collection of pod templates @ 08/24/23 12:34:15.638
  Aug 24 12:34:15.639: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 08/24/23 12:34:15.679
  Aug 24 12:34:15.679: INFO: requesting list of pod templates to confirm quantity
  Aug 24 12:34:15.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2809" for this suite. @ 08/24/23 12:34:15.697
• [0.164 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 08/24/23 12:34:15.715
  Aug 24 12:34:15.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:34:15.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:15.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:15.757
  STEP: Creating projection with secret that has name secret-emptykey-test-dbe8df47-90e2-47d7-b2e1-87907ed982ae @ 08/24/23 12:34:15.762
  Aug 24 12:34:15.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1820" for this suite. @ 08/24/23 12:34:15.777
• [0.074 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 08/24/23 12:34:15.793
  Aug 24 12:34:15.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:34:15.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:15.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:15.84
  STEP: Deleting RuntimeClass runtimeclass-5736-delete-me @ 08/24/23 12:34:15.857
  STEP: Waiting for the RuntimeClass to disappear @ 08/24/23 12:34:15.873
  Aug 24 12:34:15.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5736" for this suite. @ 08/24/23 12:34:15.908
• [0.128 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 08/24/23 12:34:15.923
  Aug 24 12:34:15.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:34:15.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:15.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:15.969
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:34:15.978
  E0824 12:34:15.996114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:16.996791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:17.997174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:18.998019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:19.998449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:34:20.023
  Aug 24 12:34:20.031: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-1d6e5fd8-1241-4b66-8bab-559c8d5e627c container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:34:20.045
  Aug 24 12:34:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8441" for this suite. @ 08/24/23 12:34:20.089
• [4.179 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 08/24/23 12:34:20.104
  Aug 24 12:34:20.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:34:20.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:20.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:20.163
  E0824 12:34:20.999851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:22.003183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:22.999671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:24.000088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:25.002846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:26.000803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:27.000923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:28.001744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:29.002397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:30.002925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:31.003177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:32.004147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:33.004282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:34.005048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:35.006236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:36.005589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:37.005841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:38.006752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:39.007292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:40.007592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:41.007581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:42.008473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:43.008052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:44.008986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:45.009103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:46.009142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:47.009971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:48.010622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:49.011272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:50.011391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:51.012118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:52.012335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:53.013259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:54.013530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:55.013509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:56.014109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:57.014152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:58.015275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:59.015806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:00.016077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:01.016394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:02.017353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:03.017688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:04.018049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:05.019005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:06.019008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:07.020530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:08.021227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:09.022365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:10.023476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:11.024163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:12.024358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:13.024510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:14.024677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:15.024880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:16.025837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:17.026124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:18.026820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:19.026953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:20.026984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:35:20.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5523" for this suite. @ 08/24/23 12:35:20.206
• [60.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 08/24/23 12:35:20.23
  Aug 24 12:35:20.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename events @ 08/24/23 12:35:20.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:35:20.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:35:20.281
  STEP: creating a test event @ 08/24/23 12:35:20.286
  STEP: listing all events in all namespaces @ 08/24/23 12:35:20.294
  STEP: patching the test event @ 08/24/23 12:35:20.328
  STEP: fetching the test event @ 08/24/23 12:35:20.342
  STEP: updating the test event @ 08/24/23 12:35:20.347
  STEP: getting the test event @ 08/24/23 12:35:20.365
  STEP: deleting the test event @ 08/24/23 12:35:20.371
  STEP: listing all events in all namespaces @ 08/24/23 12:35:20.383
  Aug 24 12:35:20.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6793" for this suite. @ 08/24/23 12:35:20.402
• [0.183 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 08/24/23 12:35:20.415
  Aug 24 12:35:20.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:35:20.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:35:20.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:35:20.45
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:35:20.467
  E0824 12:35:21.028110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:22.028566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 08/24/23 12:35:22.481
  STEP: Waiting for all pods to be running @ 08/24/23 12:35:22.492
  Aug 24 12:35:22.501: INFO: running pods: 0 < 1
  E0824 12:35:23.030638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:24.031253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:35:24.508: INFO: running pods: 0 < 1
  E0824 12:35:25.031654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:26.031759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/24/23 12:35:26.509
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:35:26.556
  STEP: Patching PodDisruptionBudget status @ 08/24/23 12:35:26.579
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:35:26.608
  Aug 24 12:35:26.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8258" for this suite. @ 08/24/23 12:35:26.632
• [6.230 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 08/24/23 12:35:26.646
  Aug 24 12:35:26.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:35:26.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:35:26.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:35:26.687
  Aug 24 12:35:26.709: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0824 12:35:27.031866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:28.032472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:29.032973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:30.033821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:31.034276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:35:31.717: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:35:31.717
  Aug 24 12:35:31.718: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 08/24/23 12:35:31.735
  Aug 24 12:35:31.756: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4094  55d66241-7ea0-41e5-b62f-1c3eccb52958 22333 1 2023-08-24 12:35:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-24 12:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0014d9bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 24 12:35:31.761: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Aug 24 12:35:31.761: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Aug 24 12:35:31.762: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4094  42e59093-e245-4fb7-a42b-e02d9882a40e 22335 1 2023-08-24 12:35:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 55d66241-7ea0-41e5-b62f-1c3eccb52958 0xc007d39467 0xc007d39468}] [] [{e2e.test Update apps/v1 2023-08-24 12:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:35:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:35:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"55d66241-7ea0-41e5-b62f-1c3eccb52958\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007d39538 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:35:31.769: INFO: Pod "test-cleanup-controller-p5lpg" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-p5lpg test-cleanup-controller- deployment-4094  2c85f795-b6ce-4135-9fe4-d60a9c6c1d5f 22317 0 2023-08-24 12:35:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 42e59093-e245-4fb7-a42b-e02d9882a40e 0xc0014d9f27 0xc0014d9f28}] [] [{kube-controller-manager Update v1 2023-08-24 12:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42e59093-e245-4fb7-a42b-e02d9882a40e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:35:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rftj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rftj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:35:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:35:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:35:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:35:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.80,StartTime:2023-08-24 12:35:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:35:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://cdf16e3837e3742def2272d8ebcf2ea9bad7a1c6628f9e7e3b567b38859d4243,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:35:31.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4094" for this suite. @ 08/24/23 12:35:31.783
• [5.170 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 08/24/23 12:35:31.816
  Aug 24 12:35:31.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:35:31.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:35:31.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:35:31.887
  STEP: Creating service test in namespace statefulset-9955 @ 08/24/23 12:35:31.894
  STEP: Creating a new StatefulSet @ 08/24/23 12:35:31.904
  Aug 24 12:35:31.945: INFO: Found 0 stateful pods, waiting for 3
  E0824 12:35:32.035161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:33.036047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:34.037140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:35.038480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:36.038472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:37.038784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:38.038901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:39.039917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:40.039854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:41.040220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:35:41.956: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:35:41.956: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:35:41.956: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/24/23 12:35:41.979
  Aug 24 12:35:42.007: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/24/23 12:35:42.008
  E0824 12:35:42.040726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:43.041421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:44.043466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:45.043351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:46.043307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:47.043494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:48.043926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:49.044272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:50.044889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:51.045002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:52.045426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 08/24/23 12:35:52.055
  STEP: Performing a canary update @ 08/24/23 12:35:52.055
  Aug 24 12:35:52.083: INFO: Updating stateful set ss2
  Aug 24 12:35:52.098: INFO: Waiting for Pod statefulset-9955/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0824 12:35:53.046841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:54.046783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:55.047069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:56.047440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:57.047396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:58.047451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:59.047783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:00.047918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:01.048314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:02.048389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 08/24/23 12:36:02.113
  Aug 24 12:36:02.222: INFO: Found 1 stateful pods, waiting for 3
  E0824 12:36:03.048950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:04.050264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:05.050267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:06.050428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:07.051130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:08.051361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:09.052403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:10.052604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:11.052931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:12.053073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:12.233: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:36:12.233: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:36:12.233: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 08/24/23 12:36:12.257
  Aug 24 12:36:12.287: INFO: Updating stateful set ss2
  Aug 24 12:36:12.302: INFO: Waiting for Pod statefulset-9955/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0824 12:36:13.054008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:14.055102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:15.056157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:16.056368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:17.057134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:18.057465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:19.058364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:20.059058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:21.059381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:22.060028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:22.349: INFO: Updating stateful set ss2
  Aug 24 12:36:22.363: INFO: Waiting for StatefulSet statefulset-9955/ss2 to complete update
  Aug 24 12:36:22.363: INFO: Waiting for Pod statefulset-9955/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0824 12:36:23.061020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:24.061145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:25.061589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:26.062059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:27.062469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:28.063090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:29.064178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:30.064740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:31.065359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:32.065563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:32.381: INFO: Deleting all statefulset in ns statefulset-9955
  Aug 24 12:36:32.389: INFO: Scaling statefulset ss2 to 0
  E0824 12:36:33.065632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:34.066319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:35.066562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:36.067196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:37.067448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:38.067729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:39.068265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:40.068424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:41.068672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:42.069291      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:42.426: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:36:42.432: INFO: Deleting statefulset ss2
  Aug 24 12:36:42.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9955" for this suite. @ 08/24/23 12:36:42.474
• [70.678 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 08/24/23 12:36:42.496
  Aug 24 12:36:42.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:36:42.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:42.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:42.542
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/24/23 12:36:42.547
  E0824 12:36:43.070399      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:44.071371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:45.072176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:46.072796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:36:46.583
  Aug 24 12:36:46.590: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-06e1090d-b5e3-4ddc-9c95-f0aa0240667d container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:36:46.618
  Aug 24 12:36:46.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8948" for this suite. @ 08/24/23 12:36:46.658
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 08/24/23 12:36:46.675
  Aug 24 12:36:46.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:36:46.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:46.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:46.711
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:36:46.717
  E0824 12:36:47.073898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:48.074089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:49.074655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:50.081465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:36:50.761
  Aug 24 12:36:50.767: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-568c638d-1311-4eac-b2f1-b63e9792e173 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:36:50.777
  Aug 24 12:36:50.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5166" for this suite. @ 08/24/23 12:36:50.813
• [4.151 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 08/24/23 12:36:50.83
  Aug 24 12:36:50.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:36:50.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:50.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:50.865
  STEP: creating secret secrets-1831/secret-test-d2a71b83-626e-4d42-8215-1076a25a53e0 @ 08/24/23 12:36:50.869
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:36:50.879
  E0824 12:36:51.076589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:52.077995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:53.077217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:54.080082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:36:54.921
  Aug 24 12:36:54.931: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-ef0b32e9-dc64-41bc-a476-947e83bb2268 container env-test: <nil>
  STEP: delete the pod @ 08/24/23 12:36:54.95
  Aug 24 12:36:55.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1831" for this suite. @ 08/24/23 12:36:55.019
• [4.213 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 08/24/23 12:36:55.044
  Aug 24 12:36:55.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:36:55.047
  E0824 12:36:55.079283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:55.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:55.089
  Aug 24 12:36:55.097: INFO: Creating deployment "webserver-deployment"
  Aug 24 12:36:55.109: INFO: Waiting for observed generation 1
  E0824 12:36:56.106811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:57.107901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:57.136: INFO: Waiting for all required pods to come up
  Aug 24 12:36:57.150: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 08/24/23 12:36:57.15
  E0824 12:36:58.108898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:59.109551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:59.204: INFO: Waiting for deployment "webserver-deployment" to complete
  Aug 24 12:36:59.216: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Aug 24 12:36:59.233: INFO: Updating deployment webserver-deployment
  Aug 24 12:36:59.233: INFO: Waiting for observed generation 2
  E0824 12:37:00.172580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:01.146057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:37:01.249: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Aug 24 12:37:01.256: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Aug 24 12:37:01.263: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 24 12:37:01.285: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Aug 24 12:37:01.286: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Aug 24 12:37:01.291: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 24 12:37:01.309: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Aug 24 12:37:01.310: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Aug 24 12:37:01.335: INFO: Updating deployment webserver-deployment
  Aug 24 12:37:01.335: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Aug 24 12:37:01.350: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Aug 24 12:37:01.360: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Aug 24 12:37:01.381: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-6254  6cae8320-19a3-4c83-aa00-2836833d32fc 23182 3 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-24 12:37:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00671bb48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:36:58 +0000 UTC,LastTransitionTime:2023-08-24 12:36:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-08-24 12:36:59 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Aug 24 12:37:01.391: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-6254  afe6e907-b706-48d4-8936-dfd36a5576fa 23186 3 2023-08-24 12:36:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6cae8320-19a3-4c83-aa00-2836833d32fc 0xc004c4a027 0xc004c4a028}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:37:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cae8320-19a3-4c83-aa00-2836833d32fc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c4a0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:37:01.392: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Aug 24 12:37:01.393: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-6254  7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 23183 3 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6cae8320-19a3-4c83-aa00-2836833d32fc 0xc002269f37 0xc002269f38}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:37:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cae8320-19a3-4c83-aa00-2836833d32fc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002269fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:37:01.410: INFO: Pod "webserver-deployment-67bd4bf6dc-2xfg5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2xfg5 webserver-deployment-67bd4bf6dc- deployment-6254  f12f6db3-7bd8-4aae-ad8e-e7b569ae53af 23090 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc00671bf67 0xc00671bf68}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sg72d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sg72d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.188,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d842c3f3e0a9c7ce6fd04e61fd2b5925ab78bf8ea294b85ce402805e8d743995,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.188,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.410: INFO: Pod "webserver-deployment-67bd4bf6dc-7t9ls" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7t9ls webserver-deployment-67bd4bf6dc- deployment-6254  b18d4db5-c9e6-4f00-b315-c2f1b422e9e5 23185 0 2023-08-24 12:37:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b2157 0xc0064b2158}] [] [{kube-controller-manager Update v1 2023-08-24 12:37:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbmsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbmsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.411: INFO: Pod "webserver-deployment-67bd4bf6dc-8dsbq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8dsbq webserver-deployment-67bd4bf6dc- deployment-6254  7c1681a3-9ca6-4ace-be04-6b4840dc1d62 23088 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b22a7 0xc0064b22a8}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r2k28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r2k28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.37,PodIP:10.233.64.231,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://afaaa6dc1beff6c7c74c522a07a9221271718e03b8d1fc5229ec2390d5fed155,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.231,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.411: INFO: Pod "webserver-deployment-67bd4bf6dc-mdph9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mdph9 webserver-deployment-67bd4bf6dc- deployment-6254  5d213e73-0991-4ac0-9c3c-0be9130159ca 23079 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b2497 0xc0064b2498}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55hmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55hmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.187,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://93fa9c85d274f8da963a8a27be24a74c545b24d7d16ce20e06504eba26e7df0c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.187,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.412: INFO: Pod "webserver-deployment-67bd4bf6dc-mf98s" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mf98s webserver-deployment-67bd4bf6dc- deployment-6254  6c55b03d-12e7-4e50-8e59-824159681fd9 23070 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b26f7 0xc0064b26f8}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qsg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qsg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.65.225,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d6523ca77f635f0538a3fe5cf11b5ca70054f0084835343fe7f3107f88ae4592,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.225,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.412: INFO: Pod "webserver-deployment-67bd4bf6dc-pgjd4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pgjd4 webserver-deployment-67bd4bf6dc- deployment-6254  b1e8227b-26ce-40d3-9290-596d3e981956 23187 0 2023-08-24 12:37:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b28f7 0xc0064b28f8}] [] [{kube-controller-manager Update v1 2023-08-24 12:37:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbfnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbfnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.412: INFO: Pod "webserver-deployment-67bd4bf6dc-rtsgl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rtsgl webserver-deployment-67bd4bf6dc- deployment-6254  c9e4aaf4-9159-4208-9dde-a20033f1d876 23188 0 2023-08-24 12:37:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b2a37 0xc0064b2a38}] [] [{kube-controller-manager Update v1 2023-08-24 12:37:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8h96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8h96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.414: INFO: Pod "webserver-deployment-67bd4bf6dc-s5wb5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s5wb5 webserver-deployment-67bd4bf6dc- deployment-6254  35e63b89-fe19-4ab1-961d-c4895ebb4d8f 23092 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b2b97 0xc0064b2b98}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lxwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lxwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.37,PodIP:10.233.64.71,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5941490c9e68f4b62f6244fb42d774b109434fe3d93a02565046e951d4a3f003,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.71,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.414: INFO: Pod "webserver-deployment-67bd4bf6dc-xmpcp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xmpcp webserver-deployment-67bd4bf6dc- deployment-6254  9f55aca5-644b-4c21-8829-0b06c6513d67 23073 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b2d87 0xc0064b2d88}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nr8zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nr8zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.65.11,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://900924e4f7e21ac53b3556f1c643963b505cde00fc8c0407621302742ba528c0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.11,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.415: INFO: Pod "webserver-deployment-67bd4bf6dc-z757j" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z757j webserver-deployment-67bd4bf6dc- deployment-6254  41fa6715-dcbc-433f-8df0-72441f46bbe4 23044 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b2f87 0xc0064b2f88}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjt7f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjt7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.65.184,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ddc171c597f8546cd388ecc806b55f1f131c9771ea87c6faaeb828a1aa4c8320,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.184,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.416: INFO: Pod "webserver-deployment-67bd4bf6dc-zdpgd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zdpgd webserver-deployment-67bd4bf6dc- deployment-6254  99e21a85-b251-4fcc-b140-991ccf0ae34f 23100 0 2023-08-24 12:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 7931bbbe-37eb-49ef-9f9b-4ebdb17ef771 0xc0064b3177 0xc0064b3178}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7931bbbe-37eb-49ef-9f9b-4ebdb17ef771\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5qpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5qpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.37,PodIP:10.233.64.97,StartTime:2023-08-24 12:36:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:36:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4cf4b30a6cec33d97d4b28b9bc27059d926d0f9bbb6d3caabb3d97a0ca712a57,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.97,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.416: INFO: Pod "webserver-deployment-7b75d79cf5-5fl8s" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5fl8s webserver-deployment-7b75d79cf5- deployment-6254  00e83578-3ab2-4acc-a2b3-0bfe37ee7a30 23133 0 2023-08-24 12:36:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 afe6e907-b706-48d4-8936-dfd36a5576fa 0xc0064b3367 0xc0064b3368}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe6e907-b706-48d4-8936-dfd36a5576fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwrfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwrfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.37,PodIP:,StartTime:2023-08-24 12:36:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.418: INFO: Pod "webserver-deployment-7b75d79cf5-bq5nd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bq5nd webserver-deployment-7b75d79cf5- deployment-6254  765759e0-e2fa-438d-9f41-436e9d6cd8dc 23127 0 2023-08-24 12:36:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 afe6e907-b706-48d4-8936-dfd36a5576fa 0xc0064b3557 0xc0064b3558}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe6e907-b706-48d4-8936-dfd36a5576fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rs92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rs92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2023-08-24 12:36:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.422: INFO: Pod "webserver-deployment-7b75d79cf5-kbf4v" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kbf4v webserver-deployment-7b75d79cf5- deployment-6254  afb0a018-5326-4028-9d9d-e367a2715126 23115 0 2023-08-24 12:36:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 afe6e907-b706-48d4-8936-dfd36a5576fa 0xc0064b3747 0xc0064b3748}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe6e907-b706-48d4-8936-dfd36a5576fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vg2k7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vg2k7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2023-08-24 12:36:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.422: INFO: Pod "webserver-deployment-7b75d79cf5-lwdvg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lwdvg webserver-deployment-7b75d79cf5- deployment-6254  58753436-1841-416d-85b2-8bed853ccdb9 23156 0 2023-08-24 12:36:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 afe6e907-b706-48d4-8936-dfd36a5576fa 0xc0064b3937 0xc0064b3938}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe6e907-b706-48d4-8936-dfd36a5576fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prw5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prw5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.37,PodIP:,StartTime:2023-08-24 12:36:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.423: INFO: Pod "webserver-deployment-7b75d79cf5-pw6nw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pw6nw webserver-deployment-7b75d79cf5- deployment-6254  dfee67da-9ee1-4e8d-b7b8-3e79abd98eb9 23150 0 2023-08-24 12:36:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 afe6e907-b706-48d4-8936-dfd36a5576fa 0xc0064b3b27 0xc0064b3b28}] [] [{kube-controller-manager Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe6e907-b706-48d4-8936-dfd36a5576fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:36:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlsrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlsrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:36:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2023-08-24 12:36:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:37:01.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6254" for this suite. @ 08/24/23 12:37:01.465
• [6.445 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 08/24/23 12:37:01.498
  Aug 24 12:37:01.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:37:01.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:01.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:01.818
  Aug 24 12:37:01.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:37:02.146995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:03.147210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:04.148083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:37:04.598939      13 warnings.go:70] unknown field "alpha"
  W0824 12:37:04.599157      13 warnings.go:70] unknown field "beta"
  W0824 12:37:04.599270      13 warnings.go:70] unknown field "delta"
  W0824 12:37:04.599377      13 warnings.go:70] unknown field "epsilon"
  W0824 12:37:04.599387      13 warnings.go:70] unknown field "gamma"
  E0824 12:37:05.148702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:37:05.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7967" for this suite. @ 08/24/23 12:37:05.19
• [3.702 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 08/24/23 12:37:05.201
  Aug 24 12:37:05.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:37:05.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:05.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:05.243
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/24/23 12:37:05.249
  E0824 12:37:06.149023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:07.149697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:08.152426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:09.153448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:37:09.307
  Aug 24 12:37:09.357: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-a9d4ff1d-1a1f-4014-bda3-620e1228291a container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:37:09.389
  Aug 24 12:37:09.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8698" for this suite. @ 08/24/23 12:37:09.529
• [4.341 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 08/24/23 12:37:09.547
  Aug 24 12:37:09.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:37:09.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:09.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:09.68
  STEP: Creating a pdb that targets all three pods in a test replica set @ 08/24/23 12:37:09.687
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:37:09.699
  E0824 12:37:10.153955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:11.154447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 08/24/23 12:37:11.727
  STEP: Waiting for all pods to be running @ 08/24/23 12:37:11.727
  Aug 24 12:37:11.737: INFO: pods: 0 < 3
  E0824 12:37:12.154739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:13.155358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/24/23 12:37:13.748
  STEP: Updating the pdb to allow a pod to be evicted @ 08/24/23 12:37:13.764
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:37:13.777
  E0824 12:37:14.155351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:15.155435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/24/23 12:37:15.791
  STEP: Waiting for all pods to be running @ 08/24/23 12:37:15.791
  STEP: Waiting for the pdb to observed all healthy pods @ 08/24/23 12:37:15.798
  STEP: Patching the pdb to disallow a pod to be evicted @ 08/24/23 12:37:15.838
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:37:15.888
  STEP: Waiting for all pods to be running @ 08/24/23 12:37:15.901
  Aug 24 12:37:15.911: INFO: running pods: 2 < 3
  E0824 12:37:16.156644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:17.157171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/24/23 12:37:17.92
  STEP: Deleting the pdb to allow a pod to be evicted @ 08/24/23 12:37:17.939
  STEP: Waiting for the pdb to be deleted @ 08/24/23 12:37:17.95
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/24/23 12:37:17.955
  STEP: Waiting for all pods to be running @ 08/24/23 12:37:17.955
  Aug 24 12:37:17.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5422" for this suite. @ 08/24/23 12:37:18.01
• [8.606 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 08/24/23 12:37:18.156
  Aug 24 12:37:18.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:37:18.157789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:37:18.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:18.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:18.249
  STEP: creating the pod @ 08/24/23 12:37:18.255
  STEP: submitting the pod to kubernetes @ 08/24/23 12:37:18.256
  E0824 12:37:19.158189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:20.158275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/24/23 12:37:20.308
  STEP: updating the pod @ 08/24/23 12:37:20.313
  Aug 24 12:37:20.835: INFO: Successfully updated pod "pod-update-5066dfcc-de62-4ee4-9d61-754def0a1556"
  STEP: verifying the updated pod is in kubernetes @ 08/24/23 12:37:20.853
  Aug 24 12:37:20.892: INFO: Pod update OK
  Aug 24 12:37:20.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9876" for this suite. @ 08/24/23 12:37:20.9
• [2.756 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 08/24/23 12:37:20.914
  Aug 24 12:37:20.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:37:20.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:20.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:20.957
  STEP: Creating namespace "e2e-ns-b6f4v" @ 08/24/23 12:37:20.962
  Aug 24 12:37:21.013: INFO: Namespace "e2e-ns-b6f4v-8932" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-b6f4v-8932" @ 08/24/23 12:37:21.013
  Aug 24 12:37:21.027: INFO: Namespace "e2e-ns-b6f4v-8932" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-b6f4v-8932" @ 08/24/23 12:37:21.027
  Aug 24 12:37:21.043: INFO: Namespace "e2e-ns-b6f4v-8932" has []v1.FinalizerName{"kubernetes"}
  Aug 24 12:37:21.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6089" for this suite. @ 08/24/23 12:37:21.053
  STEP: Destroying namespace "e2e-ns-b6f4v-8932" for this suite. @ 08/24/23 12:37:21.068
• [0.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 08/24/23 12:37:21.087
  Aug 24 12:37:21.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:37:21.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:21.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:21.129
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/24/23 12:37:21.135
  E0824 12:37:21.159382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:22.159488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:23.159651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:24.161444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:25.160321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:37:25.19
  Aug 24 12:37:25.199: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-69b0d783-0d06-46f0-a33b-b4c443a1b9de container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:37:25.216
  Aug 24 12:37:25.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3201" for this suite. @ 08/24/23 12:37:25.248
• [4.173 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 08/24/23 12:37:25.261
  Aug 24 12:37:25.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:37:25.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:25.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:25.311
  STEP: Creating projection with secret that has name projected-secret-test-map-647fbd03-b8a9-43ec-b418-0c6de0083ab4 @ 08/24/23 12:37:25.317
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:37:25.326
  E0824 12:37:26.166371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:27.166044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:28.166629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:29.167709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:37:29.367
  Aug 24 12:37:29.374: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-secrets-ce6c4dcd-5a7d-4c86-81b7-58fa67ddf1fd container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:37:29.386
  Aug 24 12:37:29.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4784" for this suite. @ 08/24/23 12:37:29.42
• [4.169 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 08/24/23 12:37:29.432
  Aug 24 12:37:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:37:29.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:37:29.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:37:29.476
  STEP: Creating pod liveness-de03f404-ccc2-4870-8842-6b61246effea in namespace container-probe-1486 @ 08/24/23 12:37:29.481
  E0824 12:37:30.173006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:31.168967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:37:31.511: INFO: Started pod liveness-de03f404-ccc2-4870-8842-6b61246effea in namespace container-probe-1486
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:37:31.512
  Aug 24 12:37:31.518: INFO: Initial restart count of pod liveness-de03f404-ccc2-4870-8842-6b61246effea is 0
  E0824 12:37:32.169224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:33.169965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:34.170941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:35.171507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:36.172057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:37.172151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:38.172378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:39.172663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:40.172640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:41.173088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:42.173452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:43.173419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:44.173568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:45.174060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:46.174496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:47.174541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:48.174784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:49.174801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:50.175861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:51.176440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:52.180042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:53.180994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:54.180712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:55.181228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:56.181864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:57.182163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:58.182285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:59.182908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:00.183716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:01.183868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:02.184645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:03.184756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:04.184978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:05.185138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:06.185345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:07.185472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:08.186256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:09.186477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:10.186501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:11.186815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:12.188735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:13.189392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:14.189940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:15.190443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:16.191478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:17.191627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:18.192430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:19.192618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:20.193368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:21.193981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:22.194168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:23.194687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:24.194806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:25.194959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:26.195109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:27.195277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:28.196155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:29.196335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:30.197101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:31.197940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:32.198756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:33.199240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:34.199508      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:35.201161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:36.201405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:37.204564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:38.203660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:39.205585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:40.204631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:41.205313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:42.205755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:43.209652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:44.207307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:45.208041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:46.208339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:47.208752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:48.209605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:49.210466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:50.210901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:51.211195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:52.211318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:53.211479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:54.212476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:55.212754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:56.212771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:57.218351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:58.219406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:59.220412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:00.221039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:01.221251      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:02.221635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:03.222325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:04.223470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:05.223607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:06.224682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:07.224744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:08.225208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:09.225444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:10.226167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:11.226652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:12.227503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:13.228296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:14.229161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:15.229847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:16.230455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:17.230866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:18.231514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:19.232230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:20.232455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:21.232541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:22.232954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:23.233699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:24.234642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:25.235352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:26.235986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:27.236960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:28.237946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:29.238146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:30.239059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:31.239161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:32.239532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:33.240222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:34.240619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:35.241106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:36.241807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:37.242365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:38.243129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:39.243229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:40.243856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:41.244137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:42.244829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:43.245670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:44.246383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:45.247092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:46.247270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:47.248347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:48.249240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:49.251899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:50.250239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:51.250604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:52.251255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:53.253140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:54.252278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:55.254862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:56.254585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:57.254704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:58.255173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:59.255772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:00.255822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:01.256255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:02.256572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:03.256776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:04.257817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:05.258443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:06.258839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:07.260425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:08.259968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:09.260053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:10.261100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:11.261436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:12.261993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:13.262228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:14.262748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:15.262904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:16.263163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:17.263659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:18.263830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:19.264010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:20.265611      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:21.265794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:22.266342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:23.267439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:24.268349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:25.268952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:26.269597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:27.269715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:28.269861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:29.270687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:30.270882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:31.271047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:32.271256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:33.271430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:34.272339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:35.272589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:36.273310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:37.273471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:38.273997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:39.274040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:40.274955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:41.275219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:42.276058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:43.276171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:44.277046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:45.277226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:46.277460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:47.278742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:48.279352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:49.279649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:50.280285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:51.280443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:52.281248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:53.281286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:54.282112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:55.282396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:56.282903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:57.285669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:58.283497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:59.283353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:00.284011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:01.284662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:02.284787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:03.285313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:04.285220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:05.286067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:06.287170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:07.287351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:08.288124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:09.289435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:10.292588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:11.290763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:12.290924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:13.293020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:14.293123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:15.293379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:16.294110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:17.295149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:18.295804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:19.295848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:20.296079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:21.296666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:22.297151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:23.297835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:24.297889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:25.298370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:26.299232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:27.299872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:28.300398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:29.301235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:30.301518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:31.301838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:32.302938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:32.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:41:32.663
  STEP: Destroying namespace "container-probe-1486" for this suite. @ 08/24/23 12:41:32.72
• [243.315 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 08/24/23 12:41:32.75
  Aug 24 12:41:32.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 12:41:32.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:32.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:32.816
  Aug 24 12:41:32.831: INFO: Got root ca configmap in namespace "svcaccounts-2545"
  Aug 24 12:41:32.902: INFO: Deleted root ca configmap in namespace "svcaccounts-2545"
  E0824 12:41:33.303708      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 08/24/23 12:41:33.404
  Aug 24 12:41:33.415: INFO: Recreated root ca configmap in namespace "svcaccounts-2545"
  Aug 24 12:41:33.428: INFO: Updated root ca configmap in namespace "svcaccounts-2545"
  STEP: waiting for the root ca configmap reconciled @ 08/24/23 12:41:33.928
  Aug 24 12:41:33.936: INFO: Reconciled root ca configmap in namespace "svcaccounts-2545"
  Aug 24 12:41:33.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2545" for this suite. @ 08/24/23 12:41:33.947
• [1.215 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 08/24/23 12:41:33.972
  Aug 24 12:41:33.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:41:33.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:34.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:34.027
  STEP: validating cluster-info @ 08/24/23 12:41:34.034
  Aug 24 12:41:34.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-1306 cluster-info'
  Aug 24 12:41:34.215: INFO: stderr: ""
  Aug 24 12:41:34.215: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Aug 24 12:41:34.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1306" for this suite. @ 08/24/23 12:41:34.226
• [0.270 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 08/24/23 12:41:34.242
  Aug 24 12:41:34.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:41:34.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:34.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:34.295
  E0824 12:41:34.303720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a Deployment @ 08/24/23 12:41:34.309
  Aug 24 12:41:34.309: INFO: Creating simple deployment test-deployment-8v59z
  Aug 24 12:41:34.350: INFO: deployment "test-deployment-8v59z" doesn't have the required revision set
  E0824 12:41:35.304161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:36.305214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 08/24/23 12:41:36.378
  Aug 24 12:41:36.386: INFO: Deployment test-deployment-8v59z has Conditions: [{Available True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8v59z-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 08/24/23 12:41:36.386
  Aug 24 12:41:36.402: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 41, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 41, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 41, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 41, 34, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8v59z-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 08/24/23 12:41:36.403
  Aug 24 12:41:36.410: INFO: Observed &Deployment event: ADDED
  Aug 24 12:41:36.410: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8v59z-5994cf9475"}
  Aug 24 12:41:36.410: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.411: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8v59z-5994cf9475"}
  Aug 24 12:41:36.411: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:41:36.411: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.411: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:41:36.411: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8v59z-5994cf9475" is progressing.}
  Aug 24 12:41:36.412: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.412: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:41:36.412: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8v59z-5994cf9475" has successfully progressed.}
  Aug 24 12:41:36.413: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.413: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:41:36.413: INFO: Observed Deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8v59z-5994cf9475" has successfully progressed.}
  Aug 24 12:41:36.413: INFO: Found Deployment test-deployment-8v59z in namespace deployment-2025 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:41:36.413: INFO: Deployment test-deployment-8v59z has an updated status
  STEP: patching the Statefulset Status @ 08/24/23 12:41:36.413
  Aug 24 12:41:36.413: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 24 12:41:36.441: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 08/24/23 12:41:36.441
  Aug 24 12:41:36.447: INFO: Observed &Deployment event: ADDED
  Aug 24 12:41:36.447: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8v59z-5994cf9475"}
  Aug 24 12:41:36.448: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.449: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8v59z-5994cf9475"}
  Aug 24 12:41:36.449: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:41:36.450: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.450: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:41:36.451: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:34 +0000 UTC 2023-08-24 12:41:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8v59z-5994cf9475" is progressing.}
  Aug 24 12:41:36.451: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.452: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:41:36.453: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8v59z-5994cf9475" has successfully progressed.}
  Aug 24 12:41:36.454: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.454: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:41:36.455: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:41:36 +0000 UTC 2023-08-24 12:41:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8v59z-5994cf9475" has successfully progressed.}
  Aug 24 12:41:36.456: INFO: Observed deployment test-deployment-8v59z in namespace deployment-2025 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:41:36.456: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:41:36.457: INFO: Found deployment test-deployment-8v59z in namespace deployment-2025 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Aug 24 12:41:36.457: INFO: Deployment test-deployment-8v59z has a patched status
  Aug 24 12:41:36.467: INFO: Deployment "test-deployment-8v59z":
  &Deployment{ObjectMeta:{test-deployment-8v59z  deployment-2025  06ba1a6c-7414-4bbc-bc80-dce18867ded2 24405 1 2023-08-24 12:41:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-24 12:41:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-24 12:41:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:41:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c905d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-8v59z-5994cf9475",LastUpdateTime:2023-08-24 12:41:36 +0000 UTC,LastTransitionTime:2023-08-24 12:41:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 12:41:36.477: INFO: New ReplicaSet "test-deployment-8v59z-5994cf9475" of Deployment "test-deployment-8v59z":
  &ReplicaSet{ObjectMeta:{test-deployment-8v59z-5994cf9475  deployment-2025  f8a1e664-c256-4a1a-b02d-883167b887ea 24401 1 2023-08-24 12:41:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8v59z 06ba1a6c-7414-4bbc-bc80-dce18867ded2 0xc006410e60 0xc006410e61}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:41:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06ba1a6c-7414-4bbc-bc80-dce18867ded2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:41:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006411038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:41:36.488: INFO: Pod "test-deployment-8v59z-5994cf9475-dd7jm" is available:
  &Pod{ObjectMeta:{test-deployment-8v59z-5994cf9475-dd7jm test-deployment-8v59z-5994cf9475- deployment-2025  482ca825-0d43-4068-af73-5768aa62970a 24400 0 2023-08-24 12:41:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-8v59z-5994cf9475 f8a1e664-c256-4a1a-b02d-883167b887ea 0xc002c911c0 0xc002c911c1}] [] [{kube-controller-manager Update v1 2023-08-24 12:41:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8a1e664-c256-4a1a-b02d-883167b887ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:41:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2jzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2jzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:41:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:41:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:41:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:41:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.195,StartTime:2023-08-24 12:41:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:41:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d69f1ec485e29e7bdf43c32ccf17c4774fac38a13294d72c73b85ef60b52e043,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.195,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:41:36.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2025" for this suite. @ 08/24/23 12:41:36.5
• [2.279 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 08/24/23 12:41:36.531
  Aug 24 12:41:36.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:41:36.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:36.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:36.574
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 12:41:36.58
  Aug 24 12:41:36.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-1850 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Aug 24 12:41:36.762: INFO: stderr: ""
  Aug 24 12:41:36.762: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/24/23 12:41:36.762
  Aug 24 12:41:36.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-1850 delete pods e2e-test-httpd-pod'
  E0824 12:41:37.306925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:38.307208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:39.234: INFO: stderr: ""
  Aug 24 12:41:39.235: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 24 12:41:39.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1850" for this suite. @ 08/24/23 12:41:39.244
• [2.729 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 08/24/23 12:41:39.261
  Aug 24 12:41:39.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:41:39.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:39.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:39.304
  E0824 12:41:39.307365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 08/24/23 12:41:39.366
  E0824 12:41:40.308000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:41:40.449
  STEP: Deploying the webhook pod @ 08/24/23 12:41:40.465
  STEP: Wait for the deployment to be ready @ 08/24/23 12:41:40.492
  Aug 24 12:41:40.512: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:41:41.309060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:42.309952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:41:42.537
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:41:42.56
  E0824 12:41:43.309697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:43.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 08/24/23 12:41:43.572
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:41:43.572
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 08/24/23 12:41:43.604
  E0824 12:41:44.311575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 08/24/23 12:41:44.619
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:41:44.619
  E0824 12:41:45.311868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 08/24/23 12:41:45.684
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:41:45.685
  E0824 12:41:46.312281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:47.313333      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:48.313218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:49.313849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:50.314032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 08/24/23 12:41:50.764
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:41:50.765
  E0824 12:41:51.315022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:52.315788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:53.316664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:54.317615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:55.317814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:55.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-93" for this suite. @ 08/24/23 12:41:55.981
  STEP: Destroying namespace "webhook-markers-1162" for this suite. @ 08/24/23 12:41:56.016
• [16.773 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 08/24/23 12:41:56.034
  Aug 24 12:41:56.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:41:56.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:56.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:56.079
  STEP: Creating secret with name secret-test-f0b50333-4cad-4a57-a757-f284582c7c0e @ 08/24/23 12:41:56.086
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:41:56.103
  E0824 12:41:56.318922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:57.319323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:58.319885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:59.320133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:42:00.151
  Aug 24 12:42:00.157: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-187a1e84-359f-4f18-8fa2-b9053ebd3fa0 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:42:00.186
  Aug 24 12:42:00.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-339" for this suite. @ 08/24/23 12:42:00.218
• [4.201 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 08/24/23 12:42:00.237
  Aug 24 12:42:00.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:42:00.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:00.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:00.278
  STEP: Counting existing ResourceQuota @ 08/24/23 12:42:00.284
  E0824 12:42:00.320608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:01.321415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:02.321411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:03.321990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:04.322495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 12:42:05.297
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:42:05.31
  E0824 12:42:05.323334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:06.323558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 08/24/23 12:42:07.322
  E0824 12:42:07.323476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status captures replication controller creation @ 08/24/23 12:42:07.344
  E0824 12:42:08.338404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:09.331108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 08/24/23 12:42:09.356
  STEP: Ensuring resource quota status released usage @ 08/24/23 12:42:09.371
  E0824 12:42:10.330938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:11.339975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:42:11.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9570" for this suite. @ 08/24/23 12:42:11.403
• [11.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 08/24/23 12:42:11.42
  Aug 24 12:42:11.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:42:11.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:11.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:11.465
  STEP: Setting up data @ 08/24/23 12:42:11.469
  STEP: Creating pod pod-subpath-test-projected-rdjn @ 08/24/23 12:42:11.488
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:42:11.488
  E0824 12:42:12.334453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:13.333870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:14.334095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:15.335035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:16.335033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:17.335797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:18.336858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:19.336888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:20.338065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:21.338777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:22.339182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:23.339374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:24.340274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:25.341183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:26.342018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:27.342036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:28.342158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:29.342656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:30.342930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:31.343576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:32.344456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:33.344466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:34.345367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:35.345527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:42:35.682
  Aug 24 12:42:35.690: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-subpath-test-projected-rdjn container test-container-subpath-projected-rdjn: <nil>
  STEP: delete the pod @ 08/24/23 12:42:35.709
  STEP: Deleting pod pod-subpath-test-projected-rdjn @ 08/24/23 12:42:35.735
  Aug 24 12:42:35.735: INFO: Deleting pod "pod-subpath-test-projected-rdjn" in namespace "subpath-6678"
  Aug 24 12:42:35.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6678" for this suite. @ 08/24/23 12:42:35.747
• [24.337 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 08/24/23 12:42:35.765
  Aug 24 12:42:35.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename init-container @ 08/24/23 12:42:35.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:35.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:35.805
  STEP: creating the pod @ 08/24/23 12:42:35.809
  Aug 24 12:42:35.809: INFO: PodSpec: initContainers in spec.initContainers
  E0824 12:42:36.345884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:37.346428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:38.347003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:39.347335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:42:39.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9599" for this suite. @ 08/24/23 12:42:39.623
• [3.872 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 08/24/23 12:42:39.639
  Aug 24 12:42:39.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:42:39.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:39.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:39.697
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 08/24/23 12:42:39.706
  Aug 24 12:42:39.720: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9311  38c0bcf1-1f28-4692-ab6f-6468cc3fb8ad 24778 0 2023-08-24 12:42:39 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-24 12:42:39 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzsbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzsbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0824 12:42:40.348849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:41.349046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 08/24/23 12:42:41.74
  Aug 24 12:42:41.740: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9311 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:42:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:42:41.741: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:42:41.741: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9311/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 08/24/23 12:42:41.918
  Aug 24 12:42:41.918: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9311 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:42:41.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:42:41.920: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:42:41.920: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-9311/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 12:42:42.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:42:42.089: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-9311" for this suite. @ 08/24/23 12:42:42.117
• [2.490 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 08/24/23 12:42:42.132
  Aug 24 12:42:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:42:42.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:42.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:42.179
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/24/23 12:42:42.185
  E0824 12:42:42.349343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:43.349924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:44.350822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:45.351450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:42:46.223
  Aug 24 12:42:46.231: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-ed277ab1-0c84-4316-a766-fe715a63f5a0 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:42:46.247
  Aug 24 12:42:46.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8840" for this suite. @ 08/24/23 12:42:46.278
• [4.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 08/24/23 12:42:46.304
  Aug 24 12:42:46.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:42:46.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:46.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:46.342
  E0824 12:42:46.352136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a Service @ 08/24/23 12:42:46.352
  STEP: watching for the Service to be added @ 08/24/23 12:42:46.371
  Aug 24 12:42:46.376: INFO: Found Service test-service-wwz69 in namespace services-2704 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Aug 24 12:42:46.376: INFO: Service test-service-wwz69 created
  STEP: Getting /status @ 08/24/23 12:42:46.376
  Aug 24 12:42:46.382: INFO: Service test-service-wwz69 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 08/24/23 12:42:46.382
  STEP: watching for the Service to be patched @ 08/24/23 12:42:46.394
  Aug 24 12:42:46.397: INFO: observed Service test-service-wwz69 in namespace services-2704 with annotations: map[] & LoadBalancer: {[]}
  Aug 24 12:42:46.397: INFO: Found Service test-service-wwz69 in namespace services-2704 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Aug 24 12:42:46.398: INFO: Service test-service-wwz69 has service status patched
  STEP: updating the ServiceStatus @ 08/24/23 12:42:46.398
  Aug 24 12:42:46.415: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 08/24/23 12:42:46.415
  Aug 24 12:42:46.420: INFO: Observed Service test-service-wwz69 in namespace services-2704 with annotations: map[] & Conditions: {[]}
  Aug 24 12:42:46.420: INFO: Observed event: &Service{ObjectMeta:{test-service-wwz69  services-2704  46187b4c-a31a-4af5-92a4-61444890459e 24854 0 2023-08-24 12:42:46 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-24 12:42:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-24 12:42:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.63.27,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.63.27],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Aug 24 12:42:46.421: INFO: Found Service test-service-wwz69 in namespace services-2704 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 12:42:46.421: INFO: Service test-service-wwz69 has service status updated
  STEP: patching the service @ 08/24/23 12:42:46.421
  STEP: watching for the Service to be patched @ 08/24/23 12:42:46.444
  Aug 24 12:42:46.447: INFO: observed Service test-service-wwz69 in namespace services-2704 with labels: map[test-service-static:true]
  Aug 24 12:42:46.447: INFO: observed Service test-service-wwz69 in namespace services-2704 with labels: map[test-service-static:true]
  Aug 24 12:42:46.448: INFO: observed Service test-service-wwz69 in namespace services-2704 with labels: map[test-service-static:true]
  Aug 24 12:42:46.448: INFO: Found Service test-service-wwz69 in namespace services-2704 with labels: map[test-service:patched test-service-static:true]
  Aug 24 12:42:46.449: INFO: Service test-service-wwz69 patched
  STEP: deleting the service @ 08/24/23 12:42:46.449
  STEP: watching for the Service to be deleted @ 08/24/23 12:42:46.478
  Aug 24 12:42:46.481: INFO: Observed event: ADDED
  Aug 24 12:42:46.481: INFO: Observed event: MODIFIED
  Aug 24 12:42:46.481: INFO: Observed event: MODIFIED
  Aug 24 12:42:46.481: INFO: Observed event: MODIFIED
  Aug 24 12:42:46.482: INFO: Found Service test-service-wwz69 in namespace services-2704 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Aug 24 12:42:46.482: INFO: Service test-service-wwz69 deleted
  Aug 24 12:42:46.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2704" for this suite. @ 08/24/23 12:42:46.491
• [0.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 08/24/23 12:42:46.514
  Aug 24 12:42:46.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:42:46.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:42:46.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:42:46.55
  STEP: Creating service test in namespace statefulset-4096 @ 08/24/23 12:42:46.555
  STEP: Creating statefulset ss in namespace statefulset-4096 @ 08/24/23 12:42:46.563
  Aug 24 12:42:46.585: INFO: Found 0 stateful pods, waiting for 1
  E0824 12:42:47.355368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:48.354013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:49.355001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:50.355091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:51.355345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:52.355842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:53.356587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:54.357085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:55.357373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:56.357988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:42:56.595: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 08/24/23 12:42:56.607
  STEP: updating a scale subresource @ 08/24/23 12:42:56.617
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/24/23 12:42:56.63
  STEP: Patch a scale subresource @ 08/24/23 12:42:56.637
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/24/23 12:42:56.654
  Aug 24 12:42:56.662: INFO: Deleting all statefulset in ns statefulset-4096
  Aug 24 12:42:56.676: INFO: Scaling statefulset ss to 0
  E0824 12:42:57.359603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:58.358388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:59.358627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:00.358817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:01.359252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:02.360459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:03.361467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:04.361504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:05.362404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:06.372657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:06.748: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:43:06.757: INFO: Deleting statefulset ss
  Aug 24 12:43:06.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4096" for this suite. @ 08/24/23 12:43:06.81
• [20.309 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 08/24/23 12:43:06.826
  Aug 24 12:43:06.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename job @ 08/24/23 12:43:06.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:06.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:06.876
  STEP: Creating a suspended job @ 08/24/23 12:43:06.885
  STEP: Patching the Job @ 08/24/23 12:43:06.895
  STEP: Watching for Job to be patched @ 08/24/23 12:43:06.918
  Aug 24 12:43:06.921: INFO: Event ADDED observed for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug 24 12:43:06.922: INFO: Event MODIFIED found for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 08/24/23 12:43:06.922
  STEP: Watching for Job to be updated @ 08/24/23 12:43:06.948
  Aug 24 12:43:06.951: INFO: Event MODIFIED found for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:43:06.951: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 08/24/23 12:43:06.951
  Aug 24 12:43:06.959: INFO: Job: e2e-c8p65 as labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65]
  STEP: Waiting for job to complete @ 08/24/23 12:43:06.959
  E0824 12:43:07.363516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:08.364408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:09.365100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:10.369020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:11.366870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:12.367093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:13.367289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:14.367568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 08/24/23 12:43:14.967
  STEP: Watching for Job to be deleted @ 08/24/23 12:43:14.993
  Aug 24 12:43:14.997: INFO: Event MODIFIED observed for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:43:14.997: INFO: Event MODIFIED observed for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:43:14.997: INFO: Event MODIFIED observed for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:43:14.998: INFO: Event MODIFIED observed for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:43:14.998: INFO: Event MODIFIED observed for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:43:14.998: INFO: Event DELETED found for Job e2e-c8p65 in namespace job-6004 with labels: map[e2e-c8p65:patched e2e-job-label:e2e-c8p65] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 08/24/23 12:43:14.998
  Aug 24 12:43:15.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6004" for this suite. @ 08/24/23 12:43:15.035
• [8.227 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 08/24/23 12:43:15.083
  Aug 24 12:43:15.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:43:15.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:15.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:15.123
  STEP: Setting up server cert @ 08/24/23 12:43:15.177
  E0824 12:43:15.368527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:16.368913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:43:16.37
  STEP: Deploying the webhook pod @ 08/24/23 12:43:16.381
  STEP: Wait for the deployment to be ready @ 08/24/23 12:43:16.398
  Aug 24 12:43:16.415: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 12:43:17.369208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:18.369636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:43:18.437
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:43:18.467
  E0824 12:43:19.370003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:19.468: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 08/24/23 12:43:19.479
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 08/24/23 12:43:19.482
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 08/24/23 12:43:19.482
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 08/24/23 12:43:19.482
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 08/24/23 12:43:19.484
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/24/23 12:43:19.484
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/24/23 12:43:19.486
  Aug 24 12:43:19.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7917" for this suite. @ 08/24/23 12:43:19.603
  STEP: Destroying namespace "webhook-markers-3332" for this suite. @ 08/24/23 12:43:19.62
• [4.555 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 08/24/23 12:43:19.64
  Aug 24 12:43:19.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:43:19.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:19.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:19.69
  STEP: Creating configMap with name configmap-test-volume-map-479834a5-8a41-4562-97a8-75390973d394 @ 08/24/23 12:43:19.696
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:43:19.706
  E0824 12:43:20.371802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:21.370350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:22.370366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:23.370614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:43:23.762
  Aug 24 12:43:23.772: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-f0408bb2-3322-4106-8d3e-e73939fe3bc1 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:43:23.788
  Aug 24 12:43:23.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5438" for this suite. @ 08/24/23 12:43:23.832
• [4.214 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 08/24/23 12:43:23.858
  Aug 24 12:43:23.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:43:23.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:23.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:23.895
  STEP: Creating configMap with name configmap-test-volume-cd69ea5b-a2a7-4152-9ce1-81a53aa7783a @ 08/24/23 12:43:23.899
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:43:23.907
  E0824 12:43:24.371058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:25.371598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:26.372053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:27.372679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:43:27.941
  Aug 24 12:43:27.949: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-8490ba66-1643-431d-bb9a-f9dbdcffc580 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:43:27.963
  Aug 24 12:43:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7297" for this suite. @ 08/24/23 12:43:28.043
• [4.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 08/24/23 12:43:28.059
  Aug 24 12:43:28.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:43:28.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:28.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:28.093
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:43:28.097
  E0824 12:43:28.372578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:29.372762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:30.373533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:31.373949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:43:32.138
  Aug 24 12:43:32.143: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-b259ad3b-8199-4d6d-aff0-f712e177ae54 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:43:32.157
  Aug 24 12:43:32.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-722" for this suite. @ 08/24/23 12:43:32.186
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 08/24/23 12:43:32.21
  Aug 24 12:43:32.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:43:32.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:32.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:32.253
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/24/23 12:43:32.258
  E0824 12:43:32.374495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:33.374760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:34.375801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:35.376565      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:43:36.296
  Aug 24 12:43:36.304: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-4602b997-09dc-4035-8e87-d7ec1b8ed565 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:43:36.317
  Aug 24 12:43:36.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9441" for this suite. @ 08/24/23 12:43:36.354
• [4.158 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 08/24/23 12:43:36.369
  Aug 24 12:43:36.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 12:43:36.371
  E0824 12:43:36.376808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:36.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:36.405
  E0824 12:43:37.377934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:38.378422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:39.378994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:40.379779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:40.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9269" for this suite. @ 08/24/23 12:43:40.465
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 08/24/23 12:43:40.491
  Aug 24 12:43:40.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:43:40.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:40.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:40.538
  STEP: Setting up data @ 08/24/23 12:43:40.543
  STEP: Creating pod pod-subpath-test-secret-j8b4 @ 08/24/23 12:43:40.563
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:43:40.563
  E0824 12:43:41.382538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:42.381986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:43.382298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:44.382789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:45.382715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:46.383093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:47.383593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:48.384187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:49.384541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:50.385059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:51.385787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:52.385812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:53.386086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:54.387903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:55.387134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:56.387168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:57.388140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:58.388272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:59.388612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:00.389819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:01.390095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:02.390546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:03.393465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:04.391736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:44:04.715
  Aug 24 12:44:04.721: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-subpath-test-secret-j8b4 container test-container-subpath-secret-j8b4: <nil>
  STEP: delete the pod @ 08/24/23 12:44:04.733
  STEP: Deleting pod pod-subpath-test-secret-j8b4 @ 08/24/23 12:44:04.758
  Aug 24 12:44:04.759: INFO: Deleting pod "pod-subpath-test-secret-j8b4" in namespace "subpath-3633"
  Aug 24 12:44:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3633" for this suite. @ 08/24/23 12:44:04.774
• [24.300 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 08/24/23 12:44:04.804
  Aug 24 12:44:04.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:44:04.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:04.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:04.864
  STEP: fetching services @ 08/24/23 12:44:04.87
  Aug 24 12:44:04.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-177" for this suite. @ 08/24/23 12:44:04.886
• [0.105 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 08/24/23 12:44:04.91
  Aug 24 12:44:04.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:44:04.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:04.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:04.957
  STEP: creating the pdb @ 08/24/23 12:44:04.962
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:44:04.97
  E0824 12:44:05.392793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:06.393576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 08/24/23 12:44:06.989
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:44:07.044
  STEP: patching the pdb @ 08/24/23 12:44:07.064
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:44:07.085
  E0824 12:44:07.393938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:08.395598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 08/24/23 12:44:09.114
  Aug 24 12:44:09.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3628" for this suite. @ 08/24/23 12:44:09.139
• [4.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 08/24/23 12:44:09.161
  Aug 24 12:44:09.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 12:44:09.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:09.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:09.208
  STEP: creating service in namespace services-4164 @ 08/24/23 12:44:09.212
  STEP: creating service affinity-clusterip in namespace services-4164 @ 08/24/23 12:44:09.212
  STEP: creating replication controller affinity-clusterip in namespace services-4164 @ 08/24/23 12:44:09.235
  I0824 12:44:09.249735      13 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-4164, replica count: 3
  E0824 12:44:09.395911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:10.397676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:11.398378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:44:12.305784      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:44:12.324: INFO: Creating new exec pod
  E0824 12:44:12.399073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:13.399243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:14.399368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:44:15.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-4164 exec execpod-affinityn4rff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  E0824 12:44:15.400034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:44:15.675: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Aug 24 12:44:15.675: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:44:15.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-4164 exec execpod-affinityn4rff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.54 80'
  Aug 24 12:44:15.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.54 80\nConnection to 10.233.24.54 80 port [tcp/http] succeeded!\n"
  Aug 24 12:44:15.947: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:44:15.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-4164 exec execpod-affinityn4rff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.24.54:80/ ; done'
  Aug 24 12:44:16.389: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.54:80/\n"
  Aug 24 12:44:16.389: INFO: stdout: "\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c\naffinity-clusterip-xx49c"
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Received response from host: affinity-clusterip-xx49c
  Aug 24 12:44:16.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:44:16.400: INFO: Cleaning up the exec pod
  E0824 12:44:16.400975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController affinity-clusterip in namespace services-4164, will wait for the garbage collector to delete the pods @ 08/24/23 12:44:16.43
  Aug 24 12:44:16.502: INFO: Deleting ReplicationController affinity-clusterip took: 13.664918ms
  Aug 24 12:44:16.604: INFO: Terminating ReplicationController affinity-clusterip pods took: 102.367533ms
  E0824 12:44:17.401431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:18.402503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4164" for this suite. @ 08/24/23 12:44:18.644
• [9.501 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 08/24/23 12:44:18.679
  Aug 24 12:44:18.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:44:18.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:18.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:18.712
  Aug 24 12:44:18.717: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:44:18.735: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:44:18.740: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-1 before test
  Aug 24 12:44:18.763: INFO: cilium-5bz85 from kube-system started at 2023-08-24 11:30:24 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.763: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:44:18.763: INFO: cilium-node-init-js6v2 from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.763: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:44:18.763: INFO: coredns-5d78c9869d-4zkjt from kube-system started at 2023-08-24 12:00:11 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: kube-addon-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: kube-apiserver-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: kube-controller-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: kube-proxy-l6rtn from kube-system started at 2023-08-24 11:27:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: kube-scheduler-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-z7825 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:44:18.764: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:44:18.764: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-2 before test
  Aug 24 12:44:18.783: INFO: cilium-node-init-xrb2l from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.783: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:44:18.783: INFO: cilium-zx72t from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.783: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:44:18.783: INFO: coredns-5d78c9869d-znmdb from kube-system started at 2023-08-24 11:31:21 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.783: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:44:18.783: INFO: kube-addon-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.783: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:44:18.783: INFO: kube-apiserver-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.784: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:44:18.784: INFO: kube-controller-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.784: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:44:18.784: INFO: kube-proxy-nz65t from kube-system started at 2023-08-24 11:28:08 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.784: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:44:18.784: INFO: kube-scheduler-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.784: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:44:18.784: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-8jtw4 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:44:18.784: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:44:18.784: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:44:18.784: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-3 before test
  Aug 24 12:44:18.802: INFO: cilium-node-init-42bmw from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.802: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:44:18.802: INFO: cilium-operator-b8f479cd9-gv7jv from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.803: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:44:18.803: INFO: cilium-xptxb from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.803: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:44:18.803: INFO: kube-proxy-vtcsn from kube-system started at 2023-08-24 11:28:50 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.804: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:44:18.804: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:39:28 +0000 UTC (1 container statuses recorded)
  Aug 24 12:44:18.804: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:44:18.804: INFO: sonobuoy-e2e-job-ee97c55b29594c3a from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:44:18.805: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:44:18.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:44:18.805: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-4l69j from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:44:18.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:44:18.806: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/24/23 12:44:18.806
  E0824 12:44:19.404742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:20.404894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/24/23 12:44:20.86
  STEP: Trying to apply a random label on the found node. @ 08/24/23 12:44:20.902
  STEP: verifying the node has the label kubernetes.io/e2e-b1c83816-c981-4c83-970b-01d3630e1304 42 @ 08/24/23 12:44:20.922
  STEP: Trying to relaunch the pod, now with labels. @ 08/24/23 12:44:20.93
  E0824 12:44:21.405958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:22.406065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-b1c83816-c981-4c83-970b-01d3630e1304 off the node pohje9aimahx-3 @ 08/24/23 12:44:23.045
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-b1c83816-c981-4c83-970b-01d3630e1304 @ 08/24/23 12:44:23.074
  Aug 24 12:44:23.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7958" for this suite. @ 08/24/23 12:44:23.103
• [4.441 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 08/24/23 12:44:23.121
  Aug 24 12:44:23.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename server-version @ 08/24/23 12:44:23.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:23.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:23.166
  STEP: Request ServerVersion @ 08/24/23 12:44:23.171
  STEP: Confirm major version @ 08/24/23 12:44:23.173
  Aug 24 12:44:23.175: INFO: Major version: 1
  STEP: Confirm minor version @ 08/24/23 12:44:23.176
  Aug 24 12:44:23.177: INFO: cleanMinorVersion: 27
  Aug 24 12:44:23.177: INFO: Minor version: 27
  Aug 24 12:44:23.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-1738" for this suite. @ 08/24/23 12:44:23.189
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 08/24/23 12:44:23.217
  Aug 24 12:44:23.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:44:23.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:23.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:23.255
  STEP: Creating projection with secret that has name projected-secret-test-327aabda-2ba8-4c40-866b-0e43143af6aa @ 08/24/23 12:44:23.264
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:44:23.275
  E0824 12:44:23.406586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:24.407513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:25.408195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:26.408274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:44:27.328
  Aug 24 12:44:27.333: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-secrets-6d4df835-4699-4f92-82d4-e247f9a364c3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:44:27.353
  Aug 24 12:44:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2745" for this suite. @ 08/24/23 12:44:27.397
• [4.192 seconds]
------------------------------
  E0824 12:44:27.409357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 08/24/23 12:44:27.414
  Aug 24 12:44:27.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:44:27.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:27.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:27.452
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:44:27.457
  E0824 12:44:28.410304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:29.411361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:30.412432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:31.413722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:44:31.498
  Aug 24 12:44:31.506: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-82aaf9e8-95a1-4e93-a602-9c2dab75257b container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:44:31.52
  Aug 24 12:44:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-630" for this suite. @ 08/24/23 12:44:31.558
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 08/24/23 12:44:31.575
  Aug 24 12:44:31.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:44:31.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:31.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:31.613
  E0824 12:44:32.417183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:33.417588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:34.417612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:35.417917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:36.417983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:37.419266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:38.418903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:39.419021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:40.419394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:41.419613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:42.419864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:43.420185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:44.420311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:45.420481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:46.420855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:47.421672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:48.422580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:49.422697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:50.423114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:51.423419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:52.423675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:53.426580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:44:53.752: INFO: Container started at 2023-08-24 12:44:32 +0000 UTC, pod became ready at 2023-08-24 12:44:52 +0000 UTC
  Aug 24 12:44:53.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3598" for this suite. @ 08/24/23 12:44:53.763
• [22.198 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 08/24/23 12:44:53.775
  Aug 24 12:44:53.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:44:53.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:44:53.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:44:53.81
  Aug 24 12:44:53.833: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0824 12:44:54.427652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:55.427898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:56.428439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:57.430079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:58.428803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:44:58.855: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:44:58.855
  Aug 24 12:44:58.855: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0824 12:44:59.429888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:00.429897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:00.864: INFO: Creating deployment "test-rollover-deployment"
  Aug 24 12:45:00.881: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0824 12:45:01.431158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:02.431665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:02.901: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Aug 24 12:45:02.919: INFO: Ensure that both replica sets have 1 created replica
  Aug 24 12:45:02.937: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Aug 24 12:45:02.957: INFO: Updating deployment test-rollover-deployment
  Aug 24 12:45:02.957: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0824 12:45:03.432137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:04.432729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:04.975: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Aug 24 12:45:04.988: INFO: Make sure deployment "test-rollover-deployment" is complete
  Aug 24 12:45:05.002: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 12:45:05.003: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 12:45:05.433405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:06.433695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:07.016: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 12:45:07.016: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 12:45:07.434597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:08.435102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:09.014: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 12:45:09.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 12:45:09.436220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:10.436783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:11.041: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 12:45:11.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 12:45:11.437471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:12.438052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:13.016: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 12:45:13.016: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 45, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 45, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 12:45:13.438914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:14.439554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:15.014: INFO: 
  Aug 24 12:45:15.014: INFO: Ensure that both old replica sets have no replicas
  Aug 24 12:45:15.053: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8738  25a8dadb-fc75-46d9-bcd7-805be36797e8 26020 2 2023-08-24 12:45:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:45:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064b3028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:45:00 +0000 UTC,LastTransitionTime:2023-08-24 12:45:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-08-24 12:45:14 +0000 UTC,LastTransitionTime:2023-08-24 12:45:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 12:45:15.065: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-8738  9075c6cd-efd7-4ba1-8989-666876f4e83b 26010 2 2023-08-24 12:45:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 25a8dadb-fc75-46d9-bcd7-805be36797e8 0xc0049379a7 0xc0049379a8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25a8dadb-fc75-46d9-bcd7-805be36797e8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:45:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004937a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:45:15.065: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Aug 24 12:45:15.065: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8738  8a607977-db3d-4bf9-b5b5-2eb33d6e56e3 26019 2 2023-08-24 12:44:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 25a8dadb-fc75-46d9-bcd7-805be36797e8 0xc004937877 0xc004937878}] [] [{e2e.test Update apps/v1 2023-08-24 12:44:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:45:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25a8dadb-fc75-46d9-bcd7-805be36797e8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:45:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004937938 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:45:15.065: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-8738  c9df7b5a-d6da-4231-8724-7420dc390678 25971 2 2023-08-24 12:45:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 25a8dadb-fc75-46d9-bcd7-805be36797e8 0xc004937ac7 0xc004937ac8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:45:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25a8dadb-fc75-46d9-bcd7-805be36797e8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:45:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004937b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:45:15.074: INFO: Pod "test-rollover-deployment-57777854c9-p7kgg" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-p7kgg test-rollover-deployment-57777854c9- deployment-8738  e5ed5027-4639-423f-81eb-6cbe92b4db4a 25983 0 2023-08-24 12:45:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 9075c6cd-efd7-4ba1-8989-666876f4e83b 0xc005bd20e7 0xc005bd20e8}] [] [{kube-controller-manager Update v1 2023-08-24 12:45:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9075c6cd-efd7-4ba1-8989-666876f4e83b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:45:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxsp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxsp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:45:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:45:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:45:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.237,StartTime:2023-08-24 12:45:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:45:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://d5f33ebb955f3d196d794291fa9135686dfb12833cca0ad303de209b3860a515,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.237,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:45:15.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8738" for this suite. @ 08/24/23 12:45:15.083
• [21.323 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 08/24/23 12:45:15.1
  Aug 24 12:45:15.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:45:15.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:45:15.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:45:15.149
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 08/24/23 12:45:15.155
  Aug 24 12:45:15.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:45:15.440020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:16.439919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:17.440158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:18.440925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:19.441324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:20.442536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:21.443443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:22.443995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:23.444371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 08/24/23 12:45:23.811
  Aug 24 12:45:23.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:45:24.445010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:25.446237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:26.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:45:26.446996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:27.447927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:28.447991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:29.449090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:30.448647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:31.449766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:32.450100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:33.449950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:45:34.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3962" for this suite. @ 08/24/23 12:45:34.13
• [19.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 08/24/23 12:45:34.149
  Aug 24 12:45:34.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:45:34.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:45:34.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:45:34.192
  STEP: create the rc @ 08/24/23 12:45:34.205
  W0824 12:45:34.218010      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0824 12:45:34.451201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:35.463754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:36.653879      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:37.676541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:38.817506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:39.822404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/24/23 12:45:40.57
  STEP: wait for the rc to be deleted @ 08/24/23 12:45:40.664
  E0824 12:45:40.823261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:41.868752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:42.855418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:43.855914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:44.856019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 08/24/23 12:45:45.771
  E0824 12:45:45.858631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:46.955786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:47.958035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:48.965017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:50.015468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:51.015623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:52.015810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:53.016275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:54.021668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:55.023059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:56.023628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:57.024293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:58.024597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:59.025629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:00.025770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:01.025893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:02.026543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:03.026723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:04.027448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:05.028014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:06.028724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:07.028837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:08.029845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:09.030688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:10.031026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:11.031991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:12.031388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:13.031772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:14.032582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:15.032726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/24/23 12:46:15.799
  Aug 24 12:46:15.974: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 12:46:15.974: INFO: Deleting pod "simpletest.rc-2kvgm" in namespace "gc-1469"
  Aug 24 12:46:16.012: INFO: Deleting pod "simpletest.rc-2nf52" in namespace "gc-1469"
  E0824 12:46:16.032889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:16.040: INFO: Deleting pod "simpletest.rc-2nsdz" in namespace "gc-1469"
  Aug 24 12:46:16.088: INFO: Deleting pod "simpletest.rc-2prbz" in namespace "gc-1469"
  Aug 24 12:46:16.183: INFO: Deleting pod "simpletest.rc-2spp5" in namespace "gc-1469"
  Aug 24 12:46:16.303: INFO: Deleting pod "simpletest.rc-44c6t" in namespace "gc-1469"
  Aug 24 12:46:16.341: INFO: Deleting pod "simpletest.rc-4kj67" in namespace "gc-1469"
  Aug 24 12:46:16.378: INFO: Deleting pod "simpletest.rc-4txcr" in namespace "gc-1469"
  Aug 24 12:46:16.516: INFO: Deleting pod "simpletest.rc-586bj" in namespace "gc-1469"
  Aug 24 12:46:16.620: INFO: Deleting pod "simpletest.rc-599wr" in namespace "gc-1469"
  Aug 24 12:46:16.680: INFO: Deleting pod "simpletest.rc-67kwk" in namespace "gc-1469"
  Aug 24 12:46:16.869: INFO: Deleting pod "simpletest.rc-6895d" in namespace "gc-1469"
  Aug 24 12:46:17.015: INFO: Deleting pod "simpletest.rc-6rd2b" in namespace "gc-1469"
  E0824 12:46:17.034041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:17.248: INFO: Deleting pod "simpletest.rc-6rhgt" in namespace "gc-1469"
  Aug 24 12:46:17.290: INFO: Deleting pod "simpletest.rc-6s5nw" in namespace "gc-1469"
  Aug 24 12:46:17.358: INFO: Deleting pod "simpletest.rc-6xsld" in namespace "gc-1469"
  Aug 24 12:46:17.427: INFO: Deleting pod "simpletest.rc-7h75n" in namespace "gc-1469"
  Aug 24 12:46:17.506: INFO: Deleting pod "simpletest.rc-7kpww" in namespace "gc-1469"
  Aug 24 12:46:17.809: INFO: Deleting pod "simpletest.rc-8v4wk" in namespace "gc-1469"
  Aug 24 12:46:17.926: INFO: Deleting pod "simpletest.rc-958wj" in namespace "gc-1469"
  Aug 24 12:46:18.020: INFO: Deleting pod "simpletest.rc-98gw8" in namespace "gc-1469"
  E0824 12:46:18.034238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:18.114: INFO: Deleting pod "simpletest.rc-9k4js" in namespace "gc-1469"
  Aug 24 12:46:18.194: INFO: Deleting pod "simpletest.rc-9rtp2" in namespace "gc-1469"
  Aug 24 12:46:18.244: INFO: Deleting pod "simpletest.rc-b2bgc" in namespace "gc-1469"
  Aug 24 12:46:18.345: INFO: Deleting pod "simpletest.rc-b4frc" in namespace "gc-1469"
  Aug 24 12:46:18.544: INFO: Deleting pod "simpletest.rc-b7kh9" in namespace "gc-1469"
  Aug 24 12:46:18.704: INFO: Deleting pod "simpletest.rc-blqcf" in namespace "gc-1469"
  Aug 24 12:46:18.822: INFO: Deleting pod "simpletest.rc-cn789" in namespace "gc-1469"
  Aug 24 12:46:18.892: INFO: Deleting pod "simpletest.rc-d42bn" in namespace "gc-1469"
  Aug 24 12:46:18.949: INFO: Deleting pod "simpletest.rc-d4p4w" in namespace "gc-1469"
  E0824 12:46:19.034723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:19.267: INFO: Deleting pod "simpletest.rc-d76nq" in namespace "gc-1469"
  Aug 24 12:46:19.331: INFO: Deleting pod "simpletest.rc-dcmlc" in namespace "gc-1469"
  Aug 24 12:46:19.422: INFO: Deleting pod "simpletest.rc-dd8cc" in namespace "gc-1469"
  Aug 24 12:46:19.516: INFO: Deleting pod "simpletest.rc-f6j9d" in namespace "gc-1469"
  Aug 24 12:46:19.583: INFO: Deleting pod "simpletest.rc-fln5r" in namespace "gc-1469"
  Aug 24 12:46:19.632: INFO: Deleting pod "simpletest.rc-fzrmw" in namespace "gc-1469"
  Aug 24 12:46:19.724: INFO: Deleting pod "simpletest.rc-g6jqf" in namespace "gc-1469"
  Aug 24 12:46:19.814: INFO: Deleting pod "simpletest.rc-g8n8t" in namespace "gc-1469"
  Aug 24 12:46:19.923: INFO: Deleting pod "simpletest.rc-grr5p" in namespace "gc-1469"
  Aug 24 12:46:19.999: INFO: Deleting pod "simpletest.rc-gzgqr" in namespace "gc-1469"
  E0824 12:46:20.034899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:20.101: INFO: Deleting pod "simpletest.rc-hf5rn" in namespace "gc-1469"
  Aug 24 12:46:20.328: INFO: Deleting pod "simpletest.rc-hg5bz" in namespace "gc-1469"
  Aug 24 12:46:20.451: INFO: Deleting pod "simpletest.rc-hlkdh" in namespace "gc-1469"
  Aug 24 12:46:20.552: INFO: Deleting pod "simpletest.rc-hrd5t" in namespace "gc-1469"
  Aug 24 12:46:20.646: INFO: Deleting pod "simpletest.rc-jf2mk" in namespace "gc-1469"
  Aug 24 12:46:20.774: INFO: Deleting pod "simpletest.rc-jkzzg" in namespace "gc-1469"
  Aug 24 12:46:20.875: INFO: Deleting pod "simpletest.rc-kkx2l" in namespace "gc-1469"
  Aug 24 12:46:21.026: INFO: Deleting pod "simpletest.rc-klcx6" in namespace "gc-1469"
  E0824 12:46:21.034762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:21.103: INFO: Deleting pod "simpletest.rc-knrdp" in namespace "gc-1469"
  Aug 24 12:46:21.247: INFO: Deleting pod "simpletest.rc-ks7zc" in namespace "gc-1469"
  Aug 24 12:46:21.401: INFO: Deleting pod "simpletest.rc-l7v94" in namespace "gc-1469"
  Aug 24 12:46:21.484: INFO: Deleting pod "simpletest.rc-lgf44" in namespace "gc-1469"
  Aug 24 12:46:21.559: INFO: Deleting pod "simpletest.rc-lk7l4" in namespace "gc-1469"
  Aug 24 12:46:21.671: INFO: Deleting pod "simpletest.rc-m2sfs" in namespace "gc-1469"
  Aug 24 12:46:21.844: INFO: Deleting pod "simpletest.rc-m7wwd" in namespace "gc-1469"
  Aug 24 12:46:21.894: INFO: Deleting pod "simpletest.rc-m9qqz" in namespace "gc-1469"
  Aug 24 12:46:21.966: INFO: Deleting pod "simpletest.rc-n4nwd" in namespace "gc-1469"
  E0824 12:46:22.040986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:22.042: INFO: Deleting pod "simpletest.rc-n58nj" in namespace "gc-1469"
  Aug 24 12:46:22.161: INFO: Deleting pod "simpletest.rc-nkggp" in namespace "gc-1469"
  Aug 24 12:46:22.263: INFO: Deleting pod "simpletest.rc-nldx5" in namespace "gc-1469"
  Aug 24 12:46:22.460: INFO: Deleting pod "simpletest.rc-nq2tv" in namespace "gc-1469"
  Aug 24 12:46:22.594: INFO: Deleting pod "simpletest.rc-nzg7q" in namespace "gc-1469"
  Aug 24 12:46:22.734: INFO: Deleting pod "simpletest.rc-p2jjj" in namespace "gc-1469"
  Aug 24 12:46:22.830: INFO: Deleting pod "simpletest.rc-p5gnf" in namespace "gc-1469"
  Aug 24 12:46:22.992: INFO: Deleting pod "simpletest.rc-p9cqp" in namespace "gc-1469"
  E0824 12:46:23.041997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:23.094: INFO: Deleting pod "simpletest.rc-pjmmb" in namespace "gc-1469"
  Aug 24 12:46:23.372: INFO: Deleting pod "simpletest.rc-pxhp7" in namespace "gc-1469"
  Aug 24 12:46:23.515: INFO: Deleting pod "simpletest.rc-q8ksx" in namespace "gc-1469"
  Aug 24 12:46:23.592: INFO: Deleting pod "simpletest.rc-qjlrk" in namespace "gc-1469"
  Aug 24 12:46:23.640: INFO: Deleting pod "simpletest.rc-qkh68" in namespace "gc-1469"
  Aug 24 12:46:23.719: INFO: Deleting pod "simpletest.rc-qmhrv" in namespace "gc-1469"
  Aug 24 12:46:23.776: INFO: Deleting pod "simpletest.rc-qpbpl" in namespace "gc-1469"
  Aug 24 12:46:23.833: INFO: Deleting pod "simpletest.rc-r4q2s" in namespace "gc-1469"
  Aug 24 12:46:23.892: INFO: Deleting pod "simpletest.rc-r6gws" in namespace "gc-1469"
  Aug 24 12:46:23.950: INFO: Deleting pod "simpletest.rc-r87bs" in namespace "gc-1469"
  Aug 24 12:46:23.996: INFO: Deleting pod "simpletest.rc-rk48d" in namespace "gc-1469"
  E0824 12:46:24.043243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:24.062: INFO: Deleting pod "simpletest.rc-rmbw9" in namespace "gc-1469"
  Aug 24 12:46:24.196: INFO: Deleting pod "simpletest.rc-scj9h" in namespace "gc-1469"
  Aug 24 12:46:24.245: INFO: Deleting pod "simpletest.rc-sqkfz" in namespace "gc-1469"
  Aug 24 12:46:24.389: INFO: Deleting pod "simpletest.rc-sv9tz" in namespace "gc-1469"
  Aug 24 12:46:24.527: INFO: Deleting pod "simpletest.rc-t4t5m" in namespace "gc-1469"
  Aug 24 12:46:24.615: INFO: Deleting pod "simpletest.rc-tcq8g" in namespace "gc-1469"
  Aug 24 12:46:24.707: INFO: Deleting pod "simpletest.rc-tf6bb" in namespace "gc-1469"
  Aug 24 12:46:24.795: INFO: Deleting pod "simpletest.rc-tvgwr" in namespace "gc-1469"
  Aug 24 12:46:25.026: INFO: Deleting pod "simpletest.rc-vpbqv" in namespace "gc-1469"
  E0824 12:46:25.044038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:25.105: INFO: Deleting pod "simpletest.rc-vpkrd" in namespace "gc-1469"
  Aug 24 12:46:25.218: INFO: Deleting pod "simpletest.rc-vtlv4" in namespace "gc-1469"
  Aug 24 12:46:25.293: INFO: Deleting pod "simpletest.rc-wb94x" in namespace "gc-1469"
  Aug 24 12:46:25.434: INFO: Deleting pod "simpletest.rc-wbtmf" in namespace "gc-1469"
  Aug 24 12:46:25.536: INFO: Deleting pod "simpletest.rc-wjlh6" in namespace "gc-1469"
  Aug 24 12:46:25.747: INFO: Deleting pod "simpletest.rc-wxndd" in namespace "gc-1469"
  Aug 24 12:46:26.026: INFO: Deleting pod "simpletest.rc-x9nkv" in namespace "gc-1469"
  E0824 12:46:26.045346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:26.167: INFO: Deleting pod "simpletest.rc-xfcwx" in namespace "gc-1469"
  Aug 24 12:46:26.272: INFO: Deleting pod "simpletest.rc-xm8kr" in namespace "gc-1469"
  Aug 24 12:46:26.467: INFO: Deleting pod "simpletest.rc-zb745" in namespace "gc-1469"
  Aug 24 12:46:26.598: INFO: Deleting pod "simpletest.rc-zc2wf" in namespace "gc-1469"
  Aug 24 12:46:26.695: INFO: Deleting pod "simpletest.rc-zd5gd" in namespace "gc-1469"
  Aug 24 12:46:26.771: INFO: Deleting pod "simpletest.rc-zgvvw" in namespace "gc-1469"
  Aug 24 12:46:26.928: INFO: Deleting pod "simpletest.rc-zshxh" in namespace "gc-1469"
  Aug 24 12:46:27.002: INFO: Deleting pod "simpletest.rc-zvfdn" in namespace "gc-1469"
  E0824 12:46:27.046160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:27.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1469" for this suite. @ 08/24/23 12:46:27.292
• [53.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 08/24/23 12:46:27.356
  Aug 24 12:46:27.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:46:27.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:46:27.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:46:27.479
  STEP: Creating configMap with name projected-configmap-test-volume-map-916348ca-9c54-47f8-bf1e-fad3a07d9284 @ 08/24/23 12:46:27.483
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:46:27.522
  E0824 12:46:28.047354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:29.047614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:30.048686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:31.048832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:46:31.627
  Aug 24 12:46:31.633: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-10a48124-1fb0-4bf8-9815-55bf12afcafa container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:46:31.662
  Aug 24 12:46:31.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4847" for this suite. @ 08/24/23 12:46:31.704
• [4.361 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 08/24/23 12:46:31.739
  Aug 24 12:46:31.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:46:31.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:46:31.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:46:31.788
  Aug 24 12:46:31.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:46:32.049380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:33.049300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:34.049415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:46:34.548699      13 warnings.go:70] unknown field "alpha"
  W0824 12:46:34.549148      13 warnings.go:70] unknown field "beta"
  W0824 12:46:34.549616      13 warnings.go:70] unknown field "delta"
  W0824 12:46:34.550013      13 warnings.go:70] unknown field "epsilon"
  W0824 12:46:34.550434      13 warnings.go:70] unknown field "gamma"
  E0824 12:46:35.050190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:46:35.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8520" for this suite. @ 08/24/23 12:46:35.161
• [3.434 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 08/24/23 12:46:35.178
  Aug 24 12:46:35.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 12:46:35.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:46:35.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:46:35.218
  Aug 24 12:46:35.247: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 12:46:36.050766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:37.051664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:38.051775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:39.052832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:40.053408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:41.053595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:42.054365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:43.055180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:44.055893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:45.056612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:46.056943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:47.057447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:48.057712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:49.058610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:50.058888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:51.058865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:52.058959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:53.059269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:54.060113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:55.060784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:56.060915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:57.061681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:58.061963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:59.062182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:00.062423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:01.062541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:02.062779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:03.063526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:04.064594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:05.064294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:06.065353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:07.065523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:08.065697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:09.066256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:10.066426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:11.067095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:12.067992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:13.068192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:14.069187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:15.069810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:16.070349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:17.070579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:18.071310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:19.071448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:20.071845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:21.072649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:22.072949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:23.073187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:24.073812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:25.074550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:26.075033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:27.075108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:28.075921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:29.076564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:30.076645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:31.077833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:32.078278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:33.078739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:34.079046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:35.079272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:47:35.291: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/24/23 12:47:35.297
  Aug 24 12:47:35.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/24/23 12:47:35.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:47:35.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:47:35.333
  STEP: Finding an available node @ 08/24/23 12:47:35.338
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/24/23 12:47:35.339
  E0824 12:47:36.079584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:37.079978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/24/23 12:47:37.384
  Aug 24 12:47:37.402: INFO: found a healthy node: pohje9aimahx-3
  E0824 12:47:38.096997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:39.097531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:40.098677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:41.099442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:42.099860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:43.099896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:44.104852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:45.102877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:47:45.562: INFO: pods created so far: [1 1 1]
  Aug 24 12:47:45.563: INFO: length of pods created so far: 3
  E0824 12:47:46.102087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:47.102885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:47:47.604: INFO: pods created so far: [2 2 1]
  E0824 12:47:48.103787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:49.104367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:50.104455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:51.104953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:52.104980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:53.105061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:54.105876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:47:54.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:47:54.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6814" for this suite. @ 08/24/23 12:47:54.78
  STEP: Destroying namespace "sched-preemption-5912" for this suite. @ 08/24/23 12:47:54.794
• [79.629 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 08/24/23 12:47:54.815
  Aug 24 12:47:54.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:47:54.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:47:54.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:47:54.856
  STEP: Setting up server cert @ 08/24/23 12:47:54.898
  E0824 12:47:55.106409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:47:55.328
  STEP: Deploying the webhook pod @ 08/24/23 12:47:55.347
  STEP: Wait for the deployment to be ready @ 08/24/23 12:47:55.376
  Aug 24 12:47:55.394: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:47:56.108970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:57.108947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:47:57.414
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:47:57.432
  E0824 12:47:58.109274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:47:58.433: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 08/24/23 12:47:58.441
  STEP: create a pod that should be updated by the webhook @ 08/24/23 12:47:58.48
  Aug 24 12:47:58.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7475" for this suite. @ 08/24/23 12:47:58.638
  STEP: Destroying namespace "webhook-markers-9741" for this suite. @ 08/24/23 12:47:58.654
• [3.851 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 08/24/23 12:47:58.667
  Aug 24 12:47:58.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 12:47:58.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:47:58.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:47:58.721
  STEP: fetching the /apis discovery document @ 08/24/23 12:47:58.727
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 08/24/23 12:47:58.729
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 08/24/23 12:47:58.729
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 08/24/23 12:47:58.73
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 08/24/23 12:47:58.731
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 08/24/23 12:47:58.731
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 08/24/23 12:47:58.735
  Aug 24 12:47:58.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5523" for this suite. @ 08/24/23 12:47:58.744
• [0.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 08/24/23 12:47:58.756
  Aug 24 12:47:58.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename conformance-tests @ 08/24/23 12:47:58.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:47:58.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:47:58.79
  STEP: Getting node addresses @ 08/24/23 12:47:58.797
  Aug 24 12:47:58.798: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Aug 24 12:47:58.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-750" for this suite. @ 08/24/23 12:47:58.821
• [0.079 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 08/24/23 12:47:58.837
  Aug 24 12:47:58.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:47:58.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:47:58.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:47:58.872
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/24/23 12:47:58.876
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/24/23 12:47:58.876
  STEP: creating a pod to probe DNS @ 08/24/23 12:47:58.876
  STEP: submitting the pod to kubernetes @ 08/24/23 12:47:58.876
  E0824 12:47:59.110143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:00.110516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 12:48:00.927
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:48:00.932
  Aug 24 12:48:00.976: INFO: DNS probes using dns-9057/dns-test-f5bd289c-b2b8-4bf6-817a-6274d4df29f1 succeeded

  Aug 24 12:48:00.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:48:00.984
  STEP: Destroying namespace "dns-9057" for this suite. @ 08/24/23 12:48:01.021
• [2.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 08/24/23 12:48:01.065
  Aug 24 12:48:01.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:48:01.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:48:01.109
  E0824 12:48:01.110722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:48:01.116
  Aug 24 12:48:01.171: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 08/24/23 12:48:01.185
  Aug 24 12:48:01.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:01.197: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 08/24/23 12:48:01.197
  Aug 24 12:48:01.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:01.269: INFO: Node pohje9aimahx-3 is running 0 daemon pod, expected 1
  E0824 12:48:02.255547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:02.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:02.302: INFO: Node pohje9aimahx-3 is running 0 daemon pod, expected 1
  E0824 12:48:03.152814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:03.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 12:48:03.278: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 08/24/23 12:48:03.286
  Aug 24 12:48:03.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 12:48:03.322: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0824 12:48:04.160744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:04.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:04.341: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 08/24/23 12:48:04.342
  Aug 24 12:48:04.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:04.374: INFO: Node pohje9aimahx-3 is running 0 daemon pod, expected 1
  E0824 12:48:05.161213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:05.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:05.382: INFO: Node pohje9aimahx-3 is running 0 daemon pod, expected 1
  E0824 12:48:06.162088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:06.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 12:48:06.383: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:48:06.398
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3132, will wait for the garbage collector to delete the pods @ 08/24/23 12:48:06.398
  Aug 24 12:48:06.469: INFO: Deleting DaemonSet.extensions daemon-set took: 12.643712ms
  Aug 24 12:48:06.570: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.962352ms
  E0824 12:48:07.161981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:08.162987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:08.178: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:48:08.178: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:48:08.183: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28924"},"items":null}

  Aug 24 12:48:08.187: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28924"},"items":null}

  Aug 24 12:48:08.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3132" for this suite. @ 08/24/23 12:48:08.258
• [7.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 08/24/23 12:48:08.284
  Aug 24 12:48:08.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:48:08.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:48:08.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:48:08.326
  STEP: Creating pod busybox-c6aec617-ee19-4fbf-a5d3-6be08d537b5f in namespace container-probe-2340 @ 08/24/23 12:48:08.335
  E0824 12:48:09.181746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:10.164922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:48:10.381: INFO: Started pod busybox-c6aec617-ee19-4fbf-a5d3-6be08d537b5f in namespace container-probe-2340
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:48:10.381
  Aug 24 12:48:10.389: INFO: Initial restart count of pod busybox-c6aec617-ee19-4fbf-a5d3-6be08d537b5f is 0
  E0824 12:48:11.165220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:12.165529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:13.165596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:14.166701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:15.166886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:16.167444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:17.167535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:18.168444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:19.169429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:20.169909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:21.171302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:22.171422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:23.171649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:24.172747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:25.172846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:26.172974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:27.173981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:28.174757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:29.175019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:30.175571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:31.175648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:32.175786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:33.176549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:34.176974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:35.177187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:36.177828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:37.178002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:38.178219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:39.179054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:40.180177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:41.180596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:42.181226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:43.182278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:44.183149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:45.183248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:46.184012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:47.184072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:48.184320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:49.185382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:50.185977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:51.185995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:52.187311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:53.186481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:54.186985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:55.187783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:56.187973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:57.188168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:58.188426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:59.189494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:00.190216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:01.190491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:02.191158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:03.191297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:04.192244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:05.192592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:06.192859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:07.192967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:08.194129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:09.194663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:10.195198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:11.195962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:12.196356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:13.196443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:14.196575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:15.196909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:16.197174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:17.198182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:18.198398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:19.198491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:20.199003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:21.199880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:22.199994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:23.200160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:24.201257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:25.201479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:26.201514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:27.201716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:28.202499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:29.203502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:30.203665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:31.204652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:32.205508      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:33.206537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:34.207490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:35.208127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:36.208321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:37.209326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:38.209771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:39.210791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:40.211058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:41.211745      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:42.212361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:43.212670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:44.212915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:45.213890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:46.214458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:47.215281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:48.215862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:49.216083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:50.216645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:51.216740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:52.217097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:53.217850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:54.218172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:55.218378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:56.219128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:57.219547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:58.220089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:59.220259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:00.220781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:01.221076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:02.221353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:03.221963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:04.222475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:05.223299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:06.223696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:07.223880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:08.224147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:09.224630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:10.224759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:11.225354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:12.225869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:13.226079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:14.226847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:15.227487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:16.227960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:17.228166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:18.228712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:19.229252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:20.229854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:21.230624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:22.231214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:23.232231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:24.232986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:25.233617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:26.234343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:27.234536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:28.235229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:29.236181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:30.236386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:31.237113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:32.237312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:33.238375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:34.239156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:35.239214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:36.239324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:37.240334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:38.240954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:39.241201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:40.241296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:41.241732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:42.241870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:43.242074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:44.242430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:45.243198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:46.243704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:47.244162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:48.244430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:49.245069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:50.245729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:51.245755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:52.247366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:53.247970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:54.249034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:55.249405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:56.249603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:57.249825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:58.250096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:59.250381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:00.250525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:01.251235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:02.251307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:03.252419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:04.252734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:05.252653      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:06.252921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:07.253943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:08.254774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:09.254870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:10.255467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:11.255841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:12.255823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:13.256789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:14.258113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:15.258361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:16.259355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:17.259978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:18.260276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:19.260377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:20.260887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:21.260920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:22.261219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:23.261840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:24.261908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:25.263045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:26.263661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:27.264447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:28.264771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:29.265549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:30.265774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:31.266438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:32.266593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:33.267505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:34.268302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:35.269058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:36.269420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:37.269385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:38.270125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:39.270274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:40.270585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:41.271443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:42.271622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:43.272290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:44.272989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:45.273116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:46.273933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:47.274692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:48.274950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:49.275146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:50.275353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:51.275852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:52.275937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:53.275993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:54.276018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:55.276362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:56.276632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:57.276775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:58.277029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:59.278071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:00.278133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:01.278598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:02.278816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:03.278918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:04.279148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:05.279255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:06.279416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:07.279636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:08.279871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:09.279992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:10.280281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:11.280364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:52:11.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:52:11.401
  STEP: Destroying namespace "container-probe-2340" for this suite. @ 08/24/23 12:52:11.437
• [243.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 08/24/23 12:52:11.461
  Aug 24 12:52:11.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:52:11.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:52:11.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:52:11.506
  STEP: Creating a pod to test downward api env vars @ 08/24/23 12:52:11.511
  E0824 12:52:12.280860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:13.281607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:14.281606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:15.281860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:52:15.578
  Aug 24 12:52:15.583: INFO: Trying to get logs from node pohje9aimahx-3 pod downward-api-95eff212-55f9-41bb-ab06-59cd8922796e container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:52:15.617
  Aug 24 12:52:15.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5447" for this suite. @ 08/24/23 12:52:15.664
• [4.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 08/24/23 12:52:15.686
  Aug 24 12:52:15.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:52:15.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:52:15.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:52:15.729
  STEP: creating a Pod with a static label @ 08/24/23 12:52:15.749
  STEP: watching for Pod to be ready @ 08/24/23 12:52:15.765
  Aug 24 12:52:15.768: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Aug 24 12:52:15.783: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC  }]
  Aug 24 12:52:15.799: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC  }]
  E0824 12:52:16.282573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:17.283063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:52:17.666: INFO: Found Pod pod-test in namespace pods-7733 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:52:15 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 08/24/23 12:52:17.673
  STEP: getting the Pod and ensuring that it's patched @ 08/24/23 12:52:17.692
  STEP: replacing the Pod's status Ready condition to False @ 08/24/23 12:52:17.699
  STEP: check the Pod again to ensure its Ready conditions are False @ 08/24/23 12:52:17.742
  STEP: deleting the Pod via a Collection with a LabelSelector @ 08/24/23 12:52:17.742
  STEP: watching for the Pod to be deleted @ 08/24/23 12:52:17.758
  Aug 24 12:52:17.763: INFO: observed event type MODIFIED
  E0824 12:52:18.283947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:19.285442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:52:19.685: INFO: observed event type MODIFIED
  Aug 24 12:52:19.847: INFO: observed event type MODIFIED
  E0824 12:52:20.286099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:52:20.680: INFO: observed event type MODIFIED
  Aug 24 12:52:20.717: INFO: observed event type MODIFIED
  Aug 24 12:52:20.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7733" for this suite. @ 08/24/23 12:52:20.741
• [5.065 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 08/24/23 12:52:20.754
  Aug 24 12:52:20.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:52:20.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:52:20.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:52:20.798
  STEP: Creating secret with name secret-test-map-014fd863-bd07-493d-8def-a51ca411e4c1 @ 08/24/23 12:52:20.803
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:52:20.815
  E0824 12:52:21.286658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:22.287232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:23.287412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:24.288323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:52:24.887
  Aug 24 12:52:24.894: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-39d8781d-a8e4-4d45-9823-5c670b3c78b8 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:52:24.909
  Aug 24 12:52:24.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-424" for this suite. @ 08/24/23 12:52:24.947
• [4.205 seconds]
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 08/24/23 12:52:24.959
  Aug 24 12:52:24.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename taint-multiple-pods @ 08/24/23 12:52:24.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:52:24.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:52:24.999
  Aug 24 12:52:25.003: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 12:52:25.288793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:26.288984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:27.289606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:28.290189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:29.291009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:30.291586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:31.291905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:32.292900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:33.294103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:34.294067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:35.294709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:36.294903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:37.295672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:38.296118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:39.296718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:40.296839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:41.297330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:42.297548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:43.297844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:44.298538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:45.298992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:46.299428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:47.300234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:48.300438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:49.301198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:50.301919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:51.301980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:52.303998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:53.304365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:54.305237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:55.305900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:56.306115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:57.306760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:58.306887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:59.308043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:00.308458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:01.309034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:02.309184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:03.310236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:04.310556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:05.311343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:06.312716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:07.312739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:08.312768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:09.313303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:10.313817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:11.314309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:12.314542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:13.314790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:14.315899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:15.316333      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:16.316542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:17.316672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:18.317327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:19.318277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:20.318367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:21.318631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:22.318775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:23.319478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:24.320365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:25.059: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:53:25.071: INFO: Starting informer...
  STEP: Starting pods... @ 08/24/23 12:53:25.072
  Aug 24 12:53:25.312: INFO: Pod1 is running on pohje9aimahx-3. Tainting Node
  E0824 12:53:25.320757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:26.320805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:27.320907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:27.594: INFO: Pod2 is running on pohje9aimahx-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/24/23 12:53:27.594
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 12:53:27.618
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 08/24/23 12:53:27.625
  E0824 12:53:28.321338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:29.321993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:30.322789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:31.323400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:32.323580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:33.324675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:34.006: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0824 12:53:34.325405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:35.325533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:36.326217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:37.326445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:38.326628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:39.326892      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:40.327758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:41.328608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:42.328654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:43.329411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:44.330436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:45.330584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:46.330737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:47.331076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:48.331317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:49.332038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:50.332205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:51.332578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:52.332630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:53.333491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:53.357: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Aug 24 12:53:53.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 12:53:53.393
  STEP: Destroying namespace "taint-multiple-pods-7958" for this suite. @ 08/24/23 12:53:53.404
• [88.460 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 08/24/23 12:53:53.427
  Aug 24 12:53:53.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename hostport @ 08/24/23 12:53:53.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:53.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:53.475
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 08/24/23 12:53:53.491
  E0824 12:53:54.333764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:55.334481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.248 on the node which pod1 resides and expect scheduled @ 08/24/23 12:53:55.536
  E0824 12:53:56.335086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:57.334788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.248 but use UDP protocol on the node which pod2 resides @ 08/24/23 12:53:57.583
  E0824 12:53:58.336085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:59.336201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:00.336090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:01.336380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 08/24/23 12:54:01.676
  Aug 24 12:54:01.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.248 http://127.0.0.1:54323/hostname] Namespace:hostport-4572 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:54:01.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:54:01.679: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:54:01.679: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4572/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.248+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.248, port: 54323 @ 08/24/23 12:54:01.835
  Aug 24 12:54:01.836: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.248:54323/hostname] Namespace:hostport-4572 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:54:01.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:54:01.838: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:54:01.839: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4572/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.248%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.248, port: 54323 UDP @ 08/24/23 12:54:01.957
  Aug 24 12:54:01.958: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.248 54323] Namespace:hostport-4572 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:54:01.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 12:54:01.960: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:54:01.961: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4572/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.248+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0824 12:54:02.337216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:03.337349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:04.338341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:05.338584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:06.338629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:07.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-4572" for this suite. @ 08/24/23 12:54:07.121
• [13.706 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 08/24/23 12:54:07.15
  Aug 24 12:54:07.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:54:07.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:54:07.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:54:07.185
  Aug 24 12:54:07.191: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:54:07.207: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:54:07.213: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-1 before test
  Aug 24 12:54:07.230: INFO: cilium-5bz85 from kube-system started at 2023-08-24 11:30:24 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.230: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:54:07.230: INFO: cilium-node-init-js6v2 from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.230: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:54:07.230: INFO: coredns-5d78c9869d-4zkjt from kube-system started at 2023-08-24 12:00:11 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: kube-addon-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: kube-apiserver-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: kube-controller-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: kube-proxy-l6rtn from kube-system started at 2023-08-24 11:27:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: kube-scheduler-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-z7825 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:54:07.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:54:07.231: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-2 before test
  Aug 24 12:54:07.247: INFO: e2e-host-exec from hostport-4572 started at 2023-08-24 12:53:59 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container e2e-host-exec ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: pod1 from hostport-4572 started at 2023-08-24 12:53:53 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container agnhost ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: pod2 from hostport-4572 started at 2023-08-24 12:53:55 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container agnhost ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: pod3 from hostport-4572 started at 2023-08-24 12:53:57 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container agnhost ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: cilium-node-init-xrb2l from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: cilium-zx72t from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: coredns-5d78c9869d-znmdb from kube-system started at 2023-08-24 11:31:21 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: kube-addon-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.247: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:54:07.247: INFO: kube-apiserver-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.248: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:54:07.248: INFO: kube-controller-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.248: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:54:07.248: INFO: kube-proxy-nz65t from kube-system started at 2023-08-24 11:28:08 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.248: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:54:07.248: INFO: kube-scheduler-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.248: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:54:07.248: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-8jtw4 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:54:07.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:54:07.248: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:54:07.248: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-3 before test
  Aug 24 12:54:07.264: INFO: cilium-node-init-42bmw from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: cilium-operator-b8f479cd9-gv7jv from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: cilium-xptxb from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: kube-proxy-vtcsn from kube-system started at 2023-08-24 11:28:50 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:39:28 +0000 UTC (1 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: sonobuoy-e2e-job-ee97c55b29594c3a from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:54:07.264: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-4l69j from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 12:54:07.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:54:07.265: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node pohje9aimahx-1 @ 08/24/23 12:54:07.332
  E0824 12:54:07.338941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node has the label node pohje9aimahx-2 @ 08/24/23 12:54:07.358
  STEP: verifying the node has the label node pohje9aimahx-3 @ 08/24/23 12:54:07.389
  Aug 24 12:54:07.451: INFO: Pod e2e-host-exec requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.454: INFO: Pod pod1 requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.454: INFO: Pod pod2 requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.454: INFO: Pod pod3 requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.455: INFO: Pod cilium-5bz85 requesting resource cpu=0m on Node pohje9aimahx-1
  Aug 24 12:54:07.455: INFO: Pod cilium-node-init-42bmw requesting resource cpu=100m on Node pohje9aimahx-3
  Aug 24 12:54:07.456: INFO: Pod cilium-node-init-js6v2 requesting resource cpu=100m on Node pohje9aimahx-1
  Aug 24 12:54:07.456: INFO: Pod cilium-node-init-xrb2l requesting resource cpu=100m on Node pohje9aimahx-2
  Aug 24 12:54:07.457: INFO: Pod cilium-operator-b8f479cd9-gv7jv requesting resource cpu=0m on Node pohje9aimahx-3
  Aug 24 12:54:07.459: INFO: Pod cilium-xptxb requesting resource cpu=0m on Node pohje9aimahx-3
  Aug 24 12:54:07.460: INFO: Pod cilium-zx72t requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.461: INFO: Pod coredns-5d78c9869d-4zkjt requesting resource cpu=100m on Node pohje9aimahx-1
  Aug 24 12:54:07.461: INFO: Pod coredns-5d78c9869d-znmdb requesting resource cpu=100m on Node pohje9aimahx-2
  Aug 24 12:54:07.462: INFO: Pod kube-addon-manager-pohje9aimahx-1 requesting resource cpu=5m on Node pohje9aimahx-1
  Aug 24 12:54:07.463: INFO: Pod kube-addon-manager-pohje9aimahx-2 requesting resource cpu=5m on Node pohje9aimahx-2
  Aug 24 12:54:07.463: INFO: Pod kube-apiserver-pohje9aimahx-1 requesting resource cpu=250m on Node pohje9aimahx-1
  Aug 24 12:54:07.463: INFO: Pod kube-apiserver-pohje9aimahx-2 requesting resource cpu=250m on Node pohje9aimahx-2
  Aug 24 12:54:07.464: INFO: Pod kube-controller-manager-pohje9aimahx-1 requesting resource cpu=200m on Node pohje9aimahx-1
  Aug 24 12:54:07.464: INFO: Pod kube-controller-manager-pohje9aimahx-2 requesting resource cpu=200m on Node pohje9aimahx-2
  Aug 24 12:54:07.465: INFO: Pod kube-proxy-l6rtn requesting resource cpu=0m on Node pohje9aimahx-1
  Aug 24 12:54:07.465: INFO: Pod kube-proxy-nz65t requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.466: INFO: Pod kube-proxy-vtcsn requesting resource cpu=0m on Node pohje9aimahx-3
  Aug 24 12:54:07.466: INFO: Pod kube-scheduler-pohje9aimahx-1 requesting resource cpu=100m on Node pohje9aimahx-1
  Aug 24 12:54:07.466: INFO: Pod kube-scheduler-pohje9aimahx-2 requesting resource cpu=100m on Node pohje9aimahx-2
  Aug 24 12:54:07.467: INFO: Pod sonobuoy requesting resource cpu=0m on Node pohje9aimahx-3
  Aug 24 12:54:07.467: INFO: Pod sonobuoy-e2e-job-ee97c55b29594c3a requesting resource cpu=0m on Node pohje9aimahx-3
  Aug 24 12:54:07.467: INFO: Pod sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-4l69j requesting resource cpu=0m on Node pohje9aimahx-3
  Aug 24 12:54:07.468: INFO: Pod sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-8jtw4 requesting resource cpu=0m on Node pohje9aimahx-2
  Aug 24 12:54:07.468: INFO: Pod sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-z7825 requesting resource cpu=0m on Node pohje9aimahx-1
  STEP: Starting Pods to consume most of the cluster CPU. @ 08/24/23 12:54:07.468
  Aug 24 12:54:07.469: INFO: Creating a pod which consumes cpu=591m on Node pohje9aimahx-1
  Aug 24 12:54:07.489: INFO: Creating a pod which consumes cpu=591m on Node pohje9aimahx-2
  Aug 24 12:54:07.504: INFO: Creating a pod which consumes cpu=1050m on Node pohje9aimahx-3
  E0824 12:54:08.500827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:09.487420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 08/24/23 12:54:09.582
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb.177e52e39eb38b61], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4065/filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb to pohje9aimahx-1] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb.177e52e3e5efeded], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb.177e52e3f7608555], Reason = [Created], Message = [Created container filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb.177e52e3fa495110], Reason = [Started], Message = [Started container filler-pod-08b4ded8-33ea-483b-9579-3b5c4ee2a2fb] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b.177e52e3a1b4847c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4065/filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b to pohje9aimahx-2] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b.177e52e3e296c0e0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b.177e52e3f074e752], Reason = [Created], Message = [Created container filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b.177e52e3f4fadd66], Reason = [Started], Message = [Started container filler-pod-2786ee16-7f77-4f3a-94f5-af7780a6a51b] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a0853908-09aa-423e-9e23-802293be2c79.177e52e3a25bbc55], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4065/filler-pod-a0853908-09aa-423e-9e23-802293be2c79 to pohje9aimahx-3] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a0853908-09aa-423e-9e23-802293be2c79.177e52e3db6a56c2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a0853908-09aa-423e-9e23-802293be2c79.177e52e3e940dff0], Reason = [Created], Message = [Created container filler-pod-a0853908-09aa-423e-9e23-802293be2c79] @ 08/24/23 12:54:09.592
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a0853908-09aa-423e-9e23-802293be2c79.177e52e3ebfc7f55], Reason = [Started], Message = [Started container filler-pod-a0853908-09aa-423e-9e23-802293be2c79] @ 08/24/23 12:54:09.593
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177e52e41b6c4f74], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 08/24/23 12:54:09.619
  E0824 12:54:10.487775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node pohje9aimahx-1 @ 08/24/23 12:54:10.615
  STEP: verifying the node doesn't have the label node @ 08/24/23 12:54:10.638
  STEP: removing the label node off the node pohje9aimahx-2 @ 08/24/23 12:54:10.644
  STEP: verifying the node doesn't have the label node @ 08/24/23 12:54:10.674
  STEP: removing the label node off the node pohje9aimahx-3 @ 08/24/23 12:54:10.681
  STEP: verifying the node doesn't have the label node @ 08/24/23 12:54:10.725
  Aug 24 12:54:10.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4065" for this suite. @ 08/24/23 12:54:10.757
• [3.645 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 08/24/23 12:54:10.796
  Aug 24 12:54:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:54:10.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:54:10.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:54:10.916
  STEP: Setting up server cert @ 08/24/23 12:54:11.023
  E0824 12:54:11.488150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:54:12.308
  STEP: Deploying the webhook pod @ 08/24/23 12:54:12.326
  STEP: Wait for the deployment to be ready @ 08/24/23 12:54:12.357
  Aug 24 12:54:12.370: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 12:54:12.489214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:13.489821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:54:14.396
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:54:14.421
  E0824 12:54:14.490350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:15.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 08/24/23 12:54:15.435
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 08/24/23 12:54:15.484
  E0824 12:54:15.491002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a configMap that should not be mutated @ 08/24/23 12:54:15.496
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 08/24/23 12:54:15.518
  STEP: Creating a configMap that should be mutated @ 08/24/23 12:54:15.536
  Aug 24 12:54:15.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-887" for this suite. @ 08/24/23 12:54:15.694
  STEP: Destroying namespace "webhook-markers-6142" for this suite. @ 08/24/23 12:54:15.71
• [4.930 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 08/24/23 12:54:15.728
  Aug 24 12:54:15.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename job @ 08/24/23 12:54:15.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:54:15.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:54:15.764
  STEP: Creating a job @ 08/24/23 12:54:15.768
  STEP: Ensure pods equal to parallelism count is attached to the job @ 08/24/23 12:54:15.783
  E0824 12:54:16.491983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:17.492650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:18.493714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:19.493776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/24/23 12:54:19.793
  STEP: updating /status @ 08/24/23 12:54:19.807
  STEP: get /status @ 08/24/23 12:54:19.821
  Aug 24 12:54:19.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2738" for this suite. @ 08/24/23 12:54:19.836
• [4.124 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 08/24/23 12:54:19.855
  Aug 24 12:54:19.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename containers @ 08/24/23 12:54:19.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:54:19.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:54:19.897
  STEP: Creating a pod to test override command @ 08/24/23 12:54:19.901
  E0824 12:54:20.494851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:21.494901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:22.495048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:23.495230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:54:23.961
  Aug 24 12:54:23.966: INFO: Trying to get logs from node pohje9aimahx-3 pod client-containers-b75a5375-a4c7-4fc2-ac24-fbac78958e0c container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:54:23.997
  Aug 24 12:54:24.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2177" for this suite. @ 08/24/23 12:54:24.032
• [4.188 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 08/24/23 12:54:24.044
  Aug 24 12:54:24.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename csistoragecapacity @ 08/24/23 12:54:24.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:54:24.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:54:24.077
  STEP: getting /apis @ 08/24/23 12:54:24.081
  STEP: getting /apis/storage.k8s.io @ 08/24/23 12:54:24.089
  STEP: getting /apis/storage.k8s.io/v1 @ 08/24/23 12:54:24.09
  STEP: creating @ 08/24/23 12:54:24.092
  STEP: watching @ 08/24/23 12:54:24.122
  Aug 24 12:54:24.122: INFO: starting watch
  STEP: getting @ 08/24/23 12:54:24.135
  STEP: listing in namespace @ 08/24/23 12:54:24.14
  STEP: listing across namespaces @ 08/24/23 12:54:24.145
  STEP: patching @ 08/24/23 12:54:24.149
  STEP: updating @ 08/24/23 12:54:24.16
  Aug 24 12:54:24.171: INFO: waiting for watch events with expected annotations in namespace
  Aug 24 12:54:24.172: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 08/24/23 12:54:24.172
  STEP: deleting a collection @ 08/24/23 12:54:24.189
  Aug 24 12:54:24.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1566" for this suite. @ 08/24/23 12:54:24.219
• [0.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 08/24/23 12:54:24.232
  Aug 24 12:54:24.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:54:24.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:54:24.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:54:24.27
  STEP: Creating a test headless service @ 08/24/23 12:54:24.274
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local;sleep 1; done
   @ 08/24/23 12:54:24.283
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4713.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local;sleep 1; done
   @ 08/24/23 12:54:24.284
  STEP: creating a pod to probe DNS @ 08/24/23 12:54:24.285
  STEP: submitting the pod to kubernetes @ 08/24/23 12:54:24.286
  E0824 12:54:24.496229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:25.496973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 12:54:26.329
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:54:26.336
  Aug 24 12:54:26.348: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.356: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.363: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.370: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.376: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.382: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.388: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.394: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:26.394: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:26.498179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:27.498226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:28.498486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:29.498921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:30.498955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:31.420: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:31.426: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:31.448: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:31.455: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:31.455: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:31.500152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:32.500128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:33.500800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:34.501800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:35.501714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:36.415: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:36.421: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:36.442: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:36.450: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:36.450: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:36.502881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:37.503141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:38.503361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:39.503441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:40.504713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:41.418: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:41.428: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:41.449: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:41.460: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:41.461: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:41.505663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:42.505583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:43.505742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:44.506385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:45.506815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:46.415: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:46.425: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:46.447: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:46.457: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:46.457: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:46.507301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:47.507527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:48.507817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:49.507749      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:50.508250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:51.416: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:51.422: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:51.442: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:51.449: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:51.449: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:51.508664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:52.510406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:53.509168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:54.510144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:55.511000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:54:56.418: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:56.450: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:56.458: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local from pod dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56: the server could not find the requested resource (get pods dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56)
  Aug 24 12:54:56.458: INFO: Lookups using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 failed for: [wheezy_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_udp@dns-test-service-2.dns-4713.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4713.svc.cluster.local]

  E0824 12:54:56.511532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:57.511790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:58.512648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:59.513588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:00.514176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:01.470: INFO: DNS probes using dns-4713/dns-test-ed2a1e1b-a43b-419f-81df-84b45a343c56 succeeded

  Aug 24 12:55:01.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:55:01.482
  E0824 12:55:01.517372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test headless service @ 08/24/23 12:55:01.55
  STEP: Destroying namespace "dns-4713" for this suite. @ 08/24/23 12:55:01.603
• [37.422 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 08/24/23 12:55:01.66
  Aug 24 12:55:01.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 12:55:01.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:01.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:01.757
  Aug 24 12:55:01.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:55:02.517488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:02.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2324" for this suite. @ 08/24/23 12:55:02.823
• [1.174 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 08/24/23 12:55:02.836
  Aug 24 12:55:02.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:55:02.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:02.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:02.871
  STEP: Creating projection with secret that has name projected-secret-test-map-51dcd289-701e-46e6-8210-995cc176ff75 @ 08/24/23 12:55:02.877
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:55:02.886
  E0824 12:55:03.520751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:04.521286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:05.521802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:06.522175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:55:06.948
  Aug 24 12:55:06.953: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-secrets-792898db-bd61-45d3-86dc-f5f6715b3a73 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:55:06.968
  Aug 24 12:55:07.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-756" for this suite. @ 08/24/23 12:55:07.014
• [4.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 08/24/23 12:55:07.038
  Aug 24 12:55:07.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:55:07.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:07.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:07.123
  STEP: creating Agnhost RC @ 08/24/23 12:55:07.13
  Aug 24 12:55:07.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2067 create -f -'
  E0824 12:55:07.522232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:08.095: INFO: stderr: ""
  Aug 24 12:55:08.095: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/24/23 12:55:08.095
  E0824 12:55:08.522362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:09.104: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:55:09.104: INFO: Found 0 / 1
  E0824 12:55:09.522686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:10.101: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:55:10.101: INFO: Found 1 / 1
  Aug 24 12:55:10.101: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 08/24/23 12:55:10.102
  Aug 24 12:55:10.106: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:55:10.107: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 12:55:10.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-2067 patch pod agnhost-primary-jwn5b -p {"metadata":{"annotations":{"x":"y"}}}'
  Aug 24 12:55:10.268: INFO: stderr: ""
  Aug 24 12:55:10.268: INFO: stdout: "pod/agnhost-primary-jwn5b patched\n"
  STEP: checking annotations @ 08/24/23 12:55:10.268
  Aug 24 12:55:10.275: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:55:10.275: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 12:55:10.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2067" for this suite. @ 08/24/23 12:55:10.284
• [3.259 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 08/24/23 12:55:10.316
  Aug 24 12:55:10.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:55:10.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:10.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:10.366
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 08/24/23 12:55:10.372
  Aug 24 12:55:10.390: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0824 12:55:10.523199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:11.524270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:12.524848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:13.525815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:14.526305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:15.399: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:55:15.399
  STEP: getting scale subresource @ 08/24/23 12:55:15.399
  STEP: updating a scale subresource @ 08/24/23 12:55:15.409
  STEP: verifying the replicaset Spec.Replicas was modified @ 08/24/23 12:55:15.427
  STEP: Patch a scale subresource @ 08/24/23 12:55:15.439
  Aug 24 12:55:15.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4955" for this suite. @ 08/24/23 12:55:15.485
• [5.188 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 08/24/23 12:55:15.507
  Aug 24 12:55:15.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:55:15.509
  E0824 12:55:15.527232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:15.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:15.552
  STEP: Setting up server cert @ 08/24/23 12:55:15.596
  E0824 12:55:16.527521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:55:17.023
  STEP: Deploying the webhook pod @ 08/24/23 12:55:17.038
  STEP: Wait for the deployment to be ready @ 08/24/23 12:55:17.056
  Aug 24 12:55:17.080: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:55:17.527671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:18.527827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:55:19.096
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:55:19.112
  E0824 12:55:19.527930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:20.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 08/24/23 12:55:20.121
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:55:20.15
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 08/24/23 12:55:20.167
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:55:20.184
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 08/24/23 12:55:20.205
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:55:20.219
  Aug 24 12:55:20.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6245" for this suite. @ 08/24/23 12:55:20.34
  STEP: Destroying namespace "webhook-markers-6883" for this suite. @ 08/24/23 12:55:20.352
• [4.857 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 08/24/23 12:55:20.365
  Aug 24 12:55:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:55:20.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:20.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:20.411
  STEP: Creating a test namespace @ 08/24/23 12:55:20.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:20.44
  STEP: Creating a service in the namespace @ 08/24/23 12:55:20.445
  STEP: Deleting the namespace @ 08/24/23 12:55:20.473
  STEP: Waiting for the namespace to be removed. @ 08/24/23 12:55:20.486
  E0824 12:55:20.528436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:21.528411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:22.529000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:23.529525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:24.530438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:25.530658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/24/23 12:55:26.496
  STEP: Verifying there is no service in the namespace @ 08/24/23 12:55:26.53
  E0824 12:55:26.530438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:26.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8463" for this suite. @ 08/24/23 12:55:26.546
  STEP: Destroying namespace "nsdeletetest-2662" for this suite. @ 08/24/23 12:55:26.562
  Aug 24 12:55:26.568: INFO: Namespace nsdeletetest-2662 was already deleted
  STEP: Destroying namespace "nsdeletetest-2647" for this suite. @ 08/24/23 12:55:26.568
• [6.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 08/24/23 12:55:26.586
  Aug 24 12:55:26.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:55:26.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:26.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:26.625
  Aug 24 12:55:26.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 12:55:27.531311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:28.531547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:55:29.337258      13 warnings.go:70] unknown field "alpha"
  W0824 12:55:29.337610      13 warnings.go:70] unknown field "beta"
  W0824 12:55:29.337835      13 warnings.go:70] unknown field "delta"
  W0824 12:55:29.338118      13 warnings.go:70] unknown field "epsilon"
  W0824 12:55:29.338327      13 warnings.go:70] unknown field "gamma"
  E0824 12:55:29.532031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:29.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2350" for this suite. @ 08/24/23 12:55:29.983
• [3.415 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 08/24/23 12:55:30.007
  Aug 24 12:55:30.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 12:55:30.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:30.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:30.063
  Aug 24 12:55:30.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1384" for this suite. @ 08/24/23 12:55:30.239
• [0.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 08/24/23 12:55:30.286
  Aug 24 12:55:30.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:55:30.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:30.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:30.341
  STEP: Creating secret with name s-test-opt-del-b873a087-842c-4749-ad9e-b6147cb597a4 @ 08/24/23 12:55:30.357
  STEP: Creating secret with name s-test-opt-upd-15b956c3-8480-4927-97b2-36f353c67dd3 @ 08/24/23 12:55:30.369
  STEP: Creating the pod @ 08/24/23 12:55:30.379
  E0824 12:55:30.532750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:31.533483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:32.534209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:33.534433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-b873a087-842c-4749-ad9e-b6147cb597a4 @ 08/24/23 12:55:34.487
  STEP: Updating secret s-test-opt-upd-15b956c3-8480-4927-97b2-36f353c67dd3 @ 08/24/23 12:55:34.501
  STEP: Creating secret with name s-test-opt-create-87d9f333-56ef-4129-80e6-2190e56d03d8 @ 08/24/23 12:55:34.513
  STEP: waiting to observe update in volume @ 08/24/23 12:55:34.521
  E0824 12:55:34.535496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:35.535619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:36.535798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:37.536395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:38.536454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:39.538213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:40.537857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:41.537665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:42.538505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:43.538338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:44.540588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:45.540673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:46.539855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:47.539455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:48.539750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:49.539884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:50.540632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:51.541446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:52.541511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:53.542111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:54.542227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:55.542352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:56.542596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:57.542923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:58.543250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:59.543503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:00.544155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:01.545025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:02.545572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:03.545762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:04.546751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:05.547081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:06.547357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:07.549869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:08.550125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:09.550416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:10.550617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:11.550649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:12.550844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:13.551258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:14.552409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:15.553126      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:16.553537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:17.553777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:18.554159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:19.554970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:20.555235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:21.555529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:22.556121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:23.557038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:24.557119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:25.558087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:26.558275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:27.558450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:28.559129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:29.559433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:30.559715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:31.559923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:32.560223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:33.560518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:34.561460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:35.562412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:36.562771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:37.563689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:38.564794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:39.565512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:40.566154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:41.567201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:42.567616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:43.567903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:44.568639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:45.569574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:46.569718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:47.570345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:48.570460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:49.570547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:50.571035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:51.571149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:52.571453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:53.571607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:54.572058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:55.572844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:56.573179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:57.573369      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:58.573590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:59.574563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:00.575640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:01.576667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:02.576874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:03.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-504" for this suite. @ 08/24/23 12:57:03.476
• [93.208 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 08/24/23 12:57:03.496
  Aug 24 12:57:03.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:57:03.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:03.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:03.548
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/24/23 12:57:03.552
  E0824 12:57:03.577357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:04.578546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:05.578665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:06.578746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:07.579008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:57:07.598
  Aug 24 12:57:07.605: INFO: Trying to get logs from node pohje9aimahx-1 pod pod-73ba0660-83db-412b-9e98-f21c80faa649 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:57:07.637
  Aug 24 12:57:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9457" for this suite. @ 08/24/23 12:57:07.682
• [4.199 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 08/24/23 12:57:07.695
  Aug 24 12:57:07.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename events @ 08/24/23 12:57:07.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:07.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:07.737
  STEP: Create set of events @ 08/24/23 12:57:07.742
  Aug 24 12:57:07.759: INFO: created test-event-1
  Aug 24 12:57:07.768: INFO: created test-event-2
  Aug 24 12:57:07.778: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 08/24/23 12:57:07.778
  STEP: delete collection of events @ 08/24/23 12:57:07.784
  Aug 24 12:57:07.784: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/24/23 12:57:07.828
  Aug 24 12:57:07.828: INFO: requesting list of events to confirm quantity
  Aug 24 12:57:07.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6423" for this suite. @ 08/24/23 12:57:07.843
• [0.163 seconds]
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 08/24/23 12:57:07.858
  Aug 24 12:57:07.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 12:57:07.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:07.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:07.894
  E0824 12:57:08.579819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:09.580694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:10.580253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:11.580943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:11.963: INFO: Got logs for pod "busybox-privileged-false-e738d82e-eaa8-4c29-a3f7-77bb9c761e47": "ip: RTNETLINK answers: Operation not permitted\n"
  Aug 24 12:57:11.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4333" for this suite. @ 08/24/23 12:57:11.972
• [4.124 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 08/24/23 12:57:11.984
  Aug 24 12:57:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:57:11.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:12.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:12.019
  STEP: Setting up server cert @ 08/24/23 12:57:12.062
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:57:12.531
  STEP: Deploying the webhook pod @ 08/24/23 12:57:12.544
  STEP: Wait for the deployment to be ready @ 08/24/23 12:57:12.562
  Aug 24 12:57:12.573: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 12:57:12.581876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:13.583330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:14.583223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:57:14.616
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:57:14.644
  E0824 12:57:15.583639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:15.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:57:15.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5563-crds.webhook.example.com via the AdmissionRegistration API @ 08/24/23 12:57:16.182
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/24/23 12:57:16.208
  E0824 12:57:16.584256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:17.584780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:18.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 12:57:18.585635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9156" for this suite. @ 08/24/23 12:57:19.104
  STEP: Destroying namespace "webhook-markers-3081" for this suite. @ 08/24/23 12:57:19.119
• [7.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 08/24/23 12:57:19.138
  Aug 24 12:57:19.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename job @ 08/24/23 12:57:19.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:19.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:19.175
  STEP: Creating Indexed job @ 08/24/23 12:57:19.18
  STEP: Ensuring job reaches completions @ 08/24/23 12:57:19.188
  E0824 12:57:19.586313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:20.586542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:21.586645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:22.586885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:23.595082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:24.595258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:25.595875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:26.596549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 08/24/23 12:57:27.196
  Aug 24 12:57:27.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3165" for this suite. @ 08/24/23 12:57:27.212
• [8.085 seconds]
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 08/24/23 12:57:27.223
  Aug 24 12:57:27.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:57:27.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:27.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:27.267
  STEP: Creating a pod to test downward api env vars @ 08/24/23 12:57:27.271
  E0824 12:57:27.596853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:28.597138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:29.597673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:30.597980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:57:31.304
  Aug 24 12:57:31.310: INFO: Trying to get logs from node pohje9aimahx-3 pod downward-api-5a1c97d5-9c1c-4fea-a0d6-00ef9110baf5 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:57:31.329
  Aug 24 12:57:31.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5167" for this suite. @ 08/24/23 12:57:31.364
• [4.155 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 08/24/23 12:57:31.381
  Aug 24 12:57:31.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 12:57:31.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:31.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:31.416
  STEP: Creating a cronjob @ 08/24/23 12:57:31.42
  STEP: Ensuring more than one job is running at a time @ 08/24/23 12:57:31.429
  E0824 12:57:31.597980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:32.598828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:33.599791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:34.599805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:35.599909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:36.600708      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:37.601395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:38.601559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:39.601664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:40.602625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:41.603271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:42.603458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:43.603847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:44.604838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:45.605334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:46.606107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:47.606606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:48.606896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:49.607194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:50.607519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:51.607668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:52.609144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:53.608847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:54.608979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:55.609255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:56.609337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:57.610316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:58.610792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:59.611561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:00.612161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:01.613171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:02.613424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:03.614260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:04.614273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:05.615329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:06.615553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:07.617647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:08.617899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:09.618563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:10.618662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:11.619409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:12.619579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:13.620284      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:14.621310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:15.621603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:16.621799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:17.622997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:18.623290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:19.623908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:20.624081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:21.624309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:22.624535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:23.625355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:24.626152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:25.626311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:26.626533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:27.627122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:28.627351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:29.627518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:30.628306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:31.628108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:32.628869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:33.629557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:34.630589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:35.631831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:36.631901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:37.631888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:38.632180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:39.633798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:40.633034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:41.633464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:42.634789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:43.634504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:44.635567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:45.636077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:46.636168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:47.637040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:48.637221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:49.637574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:50.638223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:51.639243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:52.639371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:53.639943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:54.640668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:55.641601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:56.642001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:57.642191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:58.642825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:59.643303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:00.643215      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 08/24/23 12:59:01.437
  STEP: Removing cronjob @ 08/24/23 12:59:01.444
  Aug 24 12:59:01.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1200" for this suite. @ 08/24/23 12:59:01.47
• [90.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 08/24/23 12:59:01.515
  Aug 24 12:59:01.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:59:01.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:01.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:01.622
  STEP: apply creating a deployment @ 08/24/23 12:59:01.629
  Aug 24 12:59:01.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 12:59:01.643261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-9032" for this suite. @ 08/24/23 12:59:01.669
• [0.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 08/24/23 12:59:01.694
  Aug 24 12:59:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:59:01.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:01.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:01.73
  STEP: Counting existing ResourceQuota @ 08/24/23 12:59:01.735
  E0824 12:59:02.644005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:03.644253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:04.644384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:05.644633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:06.644797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 12:59:06.746
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:59:06.763
  E0824 12:59:07.645827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:08.645827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 08/24/23 12:59:08.77
  STEP: Ensuring resource quota status captures replicaset creation @ 08/24/23 12:59:08.803
  E0824 12:59:09.646526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:10.647421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 08/24/23 12:59:10.811
  STEP: Ensuring resource quota status released usage @ 08/24/23 12:59:10.823
  E0824 12:59:11.647861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:12.647872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:59:12.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8000" for this suite. @ 08/24/23 12:59:12.845
• [11.167 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 08/24/23 12:59:12.861
  Aug 24 12:59:12.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:59:12.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:12.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:12.9
  STEP: creating a Namespace @ 08/24/23 12:59:12.905
  STEP: patching the Namespace @ 08/24/23 12:59:12.938
  STEP: get the Namespace and ensuring it has the label @ 08/24/23 12:59:12.949
  Aug 24 12:59:12.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9906" for this suite. @ 08/24/23 12:59:12.964
  STEP: Destroying namespace "nspatchtest-4c5ba6c5-c446-49c5-bfb9-105b9e0ddef5-8991" for this suite. @ 08/24/23 12:59:12.977
• [0.129 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 08/24/23 12:59:12.991
  Aug 24 12:59:12.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:59:12.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:13.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:13.026
  STEP: Creating a pod to test env composition @ 08/24/23 12:59:13.03
  E0824 12:59:13.647896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:14.648289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:15.648296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:16.648892      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:59:17.1
  Aug 24 12:59:17.105: INFO: Trying to get logs from node pohje9aimahx-3 pod var-expansion-b85ae554-a607-4c32-b5a0-56ab4260376a container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:59:17.134
  Aug 24 12:59:17.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-904" for this suite. @ 08/24/23 12:59:17.176
• [4.199 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 08/24/23 12:59:17.191
  Aug 24 12:59:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:59:17.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:17.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:17.231
  STEP: Discovering how many secrets are in namespace by default @ 08/24/23 12:59:17.236
  E0824 12:59:17.649624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:18.650165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:19.650148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:20.650445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:21.650737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/24/23 12:59:22.244
  E0824 12:59:22.651843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:23.651925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:24.652219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:25.652382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:26.653350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 12:59:27.251
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:59:27.263
  E0824 12:59:27.653882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:28.654977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 08/24/23 12:59:29.271
  STEP: Ensuring resource quota status captures secret creation @ 08/24/23 12:59:29.296
  E0824 12:59:29.655513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:30.656043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 08/24/23 12:59:31.304
  STEP: Ensuring resource quota status released usage @ 08/24/23 12:59:31.315
  E0824 12:59:31.656737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:32.657474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:59:33.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4723" for this suite. @ 08/24/23 12:59:33.332
• [16.166 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 08/24/23 12:59:33.359
  Aug 24 12:59:33.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:59:33.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:33.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:33.411
  STEP: Creating projection with secret that has name projected-secret-test-34edd07f-117e-420e-82f4-1b9c420d3113 @ 08/24/23 12:59:33.417
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:59:33.433
  E0824 12:59:33.657947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:34.658395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:35.659019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:36.659365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:59:37.477
  Aug 24 12:59:37.483: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-secrets-59bbcf4a-addb-40e1-8758-2ade4a170547 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:59:37.497
  Aug 24 12:59:37.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5945" for this suite. @ 08/24/23 12:59:37.57
• [4.226 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 08/24/23 12:59:37.588
  Aug 24 12:59:37.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:59:37.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:37.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:37.629
  STEP: Counting existing ResourceQuota @ 08/24/23 12:59:37.634
  E0824 12:59:37.660086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:38.660732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:39.661104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:40.661280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:41.662551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 12:59:42.644
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:59:42.66
  E0824 12:59:42.663340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:43.663770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:44.664083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 08/24/23 12:59:44.667
  STEP: Ensuring ResourceQuota status captures the pod usage @ 08/24/23 12:59:44.691
  E0824 12:59:45.664377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:46.664560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 08/24/23 12:59:46.702
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 08/24/23 12:59:46.712
  STEP: Ensuring a pod cannot update its resource requirements @ 08/24/23 12:59:46.719
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 08/24/23 12:59:46.729
  E0824 12:59:47.664752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:48.664992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/24/23 12:59:48.74
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 12:59:48.77
  E0824 12:59:49.665457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:50.665788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:59:50.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-287" for this suite. @ 08/24/23 12:59:50.79
• [13.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 08/24/23 12:59:50.805
  Aug 24 12:59:50.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 12:59:50.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:50.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:50.849
  E0824 12:59:51.667213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:52.667300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:59:52.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5540" for this suite. @ 08/24/23 12:59:52.983
• [2.196 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 08/24/23 12:59:53.001
  Aug 24 12:59:53.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:59:53.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:53.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:53.046
  STEP: Creating a kubernetes client @ 08/24/23 12:59:53.051
  Aug 24 12:59:53.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename disruption-2 @ 08/24/23 12:59:53.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:53.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:53.105
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:59:53.117
  E0824 12:59:53.668428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:54.668862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:59:55.139
  E0824 12:59:55.669631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:56.670484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:59:57.167
  E0824 12:59:57.671507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:58.672593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 08/24/23 12:59:59.182
  STEP: listing a collection of PDBs in namespace disruption-7749 @ 08/24/23 12:59:59.189
  STEP: deleting a collection of PDBs @ 08/24/23 12:59:59.198
  STEP: Waiting for the PDB collection to be deleted @ 08/24/23 12:59:59.22
  Aug 24 12:59:59.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:59:59.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-4901" for this suite. @ 08/24/23 12:59:59.252
  STEP: Destroying namespace "disruption-7749" for this suite. @ 08/24/23 12:59:59.264
• [6.280 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 08/24/23 12:59:59.302
  Aug 24 12:59:59.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:59:59.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:59.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:59.344
  STEP: creating the pod @ 08/24/23 12:59:59.349
  STEP: waiting for pod running @ 08/24/23 12:59:59.366
  E0824 12:59:59.672933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:00.673759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 08/24/23 13:00:01.388
  Aug 24 13:00:01.395: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3278 PodName:var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:00:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:00:01.398: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:00:01.398: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3278/pods/var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 08/24/23 13:00:01.535
  Aug 24 13:00:01.572: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3278 PodName:var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:00:01.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:00:01.574: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:00:01.574: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3278/pods/var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  E0824 13:00:01.673878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the annotation value @ 08/24/23 13:00:01.694
  Aug 24 13:00:02.229: INFO: Successfully updated pod "var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8"
  STEP: waiting for annotated pod running @ 08/24/23 13:00:02.229
  STEP: deleting the pod gracefully @ 08/24/23 13:00:02.236
  Aug 24 13:00:02.236: INFO: Deleting pod "var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8" in namespace "var-expansion-3278"
  Aug 24 13:00:02.253: INFO: Wait up to 5m0s for pod "var-expansion-60b4a548-f6d0-4359-b865-6c02e11490c8" to be fully deleted
  E0824 13:00:02.674514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:03.674486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:04.674831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:05.674909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:06.675693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:07.678546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:08.678288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:09.678406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:10.679493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:11.679651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:12.680048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:13.680777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:14.680915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:15.681789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:16.682465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:17.687256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:18.684730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:19.684823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:20.685395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:21.687267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:22.687672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:23.688275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:24.688182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:25.688663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:26.688370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:27.689029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:28.689408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:29.689799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:30.690109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:31.690342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:32.691113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:33.691282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:00:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3278" for this suite. @ 08/24/23 13:00:34.415
• [35.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 08/24/23 13:00:34.439
  Aug 24 13:00:34.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename lease-test @ 08/24/23 13:00:34.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:00:34.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:00:34.494
  Aug 24 13:00:34.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-5332" for this suite. @ 08/24/23 13:00:34.643
• [0.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 08/24/23 13:00:34.669
  Aug 24 13:00:34.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:00:34.672
  E0824 13:00:34.691448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:00:34.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:00:34.716
  Aug 24 13:00:34.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1926" for this suite. @ 08/24/23 13:00:34.833
• [0.187 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 08/24/23 13:00:34.856
  Aug 24 13:00:34.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:00:34.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:00:34.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:00:34.908
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:00:34.916
  E0824 13:00:35.708135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:36.693340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:37.693836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:38.694479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:00:38.978
  Aug 24 13:00:38.989: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-e1590d07-e002-40c4-8cc7-069e7d554b60 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:00:39.043
  Aug 24 13:00:39.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1433" for this suite. @ 08/24/23 13:00:39.088
• [4.245 seconds]
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 08/24/23 13:00:39.102
  Aug 24 13:00:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename job @ 08/24/23 13:00:39.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:00:39.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:00:39.148
  STEP: Creating a job @ 08/24/23 13:00:39.156
  STEP: Ensuring job reaches completions @ 08/24/23 13:00:39.171
  E0824 13:00:39.695675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:40.696303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:41.696915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:42.698651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:43.698748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:44.699600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:45.743604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:46.713638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:47.714098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:48.714084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:49.715245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:50.715707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:00:51.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5827" for this suite. @ 08/24/23 13:00:51.192
• [12.103 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 08/24/23 13:00:51.207
  Aug 24 13:00:51.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:00:51.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:00:51.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:00:51.254
  STEP: Creating pod liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e in namespace container-probe-9241 @ 08/24/23 13:00:51.261
  E0824 13:00:51.715858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:52.716106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:00:53.298: INFO: Started pod liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e in namespace container-probe-9241
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:00:53.298
  Aug 24 13:00:53.305: INFO: Initial restart count of pod liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e is 0
  E0824 13:00:53.716569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:54.716865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:55.718268      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:56.718366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:57.718929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:58.719273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:59.719487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:00.719792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:01.720779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:02.721525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:03.722090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:04.722329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:05.723350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:06.723738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:07.723691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:08.723946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:09.724870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:10.725510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:11.725838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:12.728433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:01:13.402: INFO: Restart count of pod container-probe-9241/liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e is now 1 (20.09649065s elapsed)
  E0824 13:01:13.728195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:14.729358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:15.729853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:16.730093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:17.731191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:18.732360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:19.733140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:20.733265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:21.734212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:22.735022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:23.736049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:24.736264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:25.737323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:26.737971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:27.739350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:28.743099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:29.741306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:30.745473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:31.742499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:32.743099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:01:33.509: INFO: Restart count of pod container-probe-9241/liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e is now 2 (40.203720739s elapsed)
  E0824 13:01:33.744330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:34.744989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:35.746188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:36.746267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:37.746851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:38.747813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:39.748945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:40.749842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:41.749423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:42.749557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:43.751545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:44.750211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:45.750485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:46.752322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:47.752306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:48.753044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:49.753343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:50.754015      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:51.754295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:52.754641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:01:53.608: INFO: Restart count of pod container-probe-9241/liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e is now 3 (1m0.303254817s elapsed)
  E0824 13:01:53.754940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:54.754944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:55.755163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:56.756262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:57.756810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:58.757882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:59.758922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:00.759702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:01.760246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:02.761169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:03.761485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:04.762671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:05.762871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:06.763060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:07.765863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:08.766082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:09.766405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:10.766919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:11.767832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:12.768168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:02:13.701: INFO: Restart count of pod container-probe-9241/liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e is now 4 (1m20.396121241s elapsed)
  E0824 13:02:13.768723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:14.769759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:15.770178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:16.770573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:17.771280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:18.771370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:19.771970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:20.772137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:21.772807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:22.773030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:23.773326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:24.774076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:25.774558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:26.775370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:27.776244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:28.776623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:29.776622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:30.776832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:31.777268      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:32.777912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:33.778410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:34.778732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:35.778876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:36.780016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:37.780794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:38.780736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:39.781418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:40.781486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:41.781905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:42.782420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:43.782830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:44.782871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:45.783715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:46.784120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:47.786065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:48.786481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:49.787079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:50.787424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:51.788147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:52.788750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:53.789397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:54.789270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:55.790003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:56.790324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:57.798498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:58.793900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:59.801181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:00.795542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:01.796128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:02.796788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:03.797463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:04.797940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:05.798569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:06.800747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:07.799332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:08.799471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:09.799512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:10.799632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:11.800662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:12.801824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:13.801726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:14.006: INFO: Restart count of pod container-probe-9241/liveness-f33f2136-cd5e-47ec-8ed8-4b36d0b6860e is now 5 (2m20.700922608s elapsed)
  Aug 24 13:03:14.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:03:14.018
  STEP: Destroying namespace "container-probe-9241" for this suite. @ 08/24/23 13:03:14.047
• [142.862 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 08/24/23 13:03:14.078
  Aug 24 13:03:14.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:03:14.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:03:14.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:03:14.12
  STEP: Creating secret with name secret-test-2b18cf2c-9a0e-41dc-b2e0-71921a815d8c @ 08/24/23 13:03:14.126
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:03:14.139
  E0824 13:03:14.801774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:15.802107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:16.802464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:17.802512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:03:18.186
  Aug 24 13:03:18.193: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-55305ebe-f778-4c1b-a7dc-a3a31b1defe6 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:03:18.228
  Aug 24 13:03:18.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9444" for this suite. @ 08/24/23 13:03:18.272
• [4.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 08/24/23 13:03:18.293
  Aug 24 13:03:18.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:03:18.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:03:18.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:03:18.352
  STEP: creating a replication controller @ 08/24/23 13:03:18.357
  Aug 24 13:03:18.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 create -f -'
  E0824 13:03:18.803494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:19.804569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:20.224: INFO: stderr: ""
  Aug 24 13:03:20.224: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 13:03:20.224
  Aug 24 13:03:20.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:03:20.480: INFO: stderr: ""
  Aug 24 13:03:20.480: INFO: stdout: "update-demo-nautilus-926nt update-demo-nautilus-llzv8 "
  Aug 24 13:03:20.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:20.626: INFO: stderr: ""
  Aug 24 13:03:20.626: INFO: stdout: ""
  Aug 24 13:03:20.626: INFO: update-demo-nautilus-926nt is created but not running
  E0824 13:03:20.805022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:21.805315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:22.805510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:23.805839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:24.805975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:25.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0824 13:03:25.806149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:25.825: INFO: stderr: ""
  Aug 24 13:03:25.825: INFO: stdout: "update-demo-nautilus-926nt update-demo-nautilus-llzv8 "
  Aug 24 13:03:25.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:26.044: INFO: stderr: ""
  Aug 24 13:03:26.044: INFO: stdout: ""
  Aug 24 13:03:26.044: INFO: update-demo-nautilus-926nt is created but not running
  E0824 13:03:26.806406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:27.806966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:28.807868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:29.808784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:30.808842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:31.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:03:31.207: INFO: stderr: ""
  Aug 24 13:03:31.207: INFO: stdout: "update-demo-nautilus-926nt update-demo-nautilus-llzv8 "
  Aug 24 13:03:31.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:31.370: INFO: stderr: ""
  Aug 24 13:03:31.370: INFO: stdout: ""
  Aug 24 13:03:31.370: INFO: update-demo-nautilus-926nt is created but not running
  E0824 13:03:31.809790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:32.810206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:33.810559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:34.810742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:35.810903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:36.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:03:36.517: INFO: stderr: ""
  Aug 24 13:03:36.517: INFO: stdout: "update-demo-nautilus-926nt update-demo-nautilus-llzv8 "
  Aug 24 13:03:36.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:36.662: INFO: stderr: ""
  Aug 24 13:03:36.662: INFO: stdout: "true"
  Aug 24 13:03:36.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0824 13:03:36.811394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:36.839: INFO: stderr: ""
  Aug 24 13:03:36.839: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:03:36.839: INFO: validating pod update-demo-nautilus-926nt
  Aug 24 13:03:36.881: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:03:36.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:03:36.881: INFO: update-demo-nautilus-926nt is verified up and running
  Aug 24 13:03:36.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-llzv8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:37.017: INFO: stderr: ""
  Aug 24 13:03:37.017: INFO: stdout: ""
  Aug 24 13:03:37.017: INFO: update-demo-nautilus-llzv8 is created but not running
  E0824 13:03:37.811850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:38.811951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:39.812344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:40.813447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:41.812823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:42.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:03:42.204: INFO: stderr: ""
  Aug 24 13:03:42.204: INFO: stdout: "update-demo-nautilus-926nt update-demo-nautilus-llzv8 "
  Aug 24 13:03:42.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:42.369: INFO: stderr: ""
  Aug 24 13:03:42.369: INFO: stdout: "true"
  Aug 24 13:03:42.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:03:42.517: INFO: stderr: ""
  Aug 24 13:03:42.517: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:03:42.517: INFO: validating pod update-demo-nautilus-926nt
  Aug 24 13:03:42.528: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:03:42.528: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:03:42.528: INFO: update-demo-nautilus-926nt is verified up and running
  Aug 24 13:03:42.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-llzv8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:42.706: INFO: stderr: ""
  Aug 24 13:03:42.706: INFO: stdout: ""
  Aug 24 13:03:42.706: INFO: update-demo-nautilus-llzv8 is created but not running
  E0824 13:03:42.813779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:43.814238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:44.814461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:45.814645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:46.814841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:47.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0824 13:03:47.815868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:47.889: INFO: stderr: ""
  Aug 24 13:03:47.890: INFO: stdout: "update-demo-nautilus-926nt update-demo-nautilus-llzv8 "
  Aug 24 13:03:47.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:48.069: INFO: stderr: ""
  Aug 24 13:03:48.069: INFO: stdout: "true"
  Aug 24 13:03:48.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-926nt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:03:48.226: INFO: stderr: ""
  Aug 24 13:03:48.227: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:03:48.227: INFO: validating pod update-demo-nautilus-926nt
  Aug 24 13:03:48.236: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:03:48.236: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:03:48.236: INFO: update-demo-nautilus-926nt is verified up and running
  Aug 24 13:03:48.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-llzv8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:03:48.395: INFO: stderr: ""
  Aug 24 13:03:48.395: INFO: stdout: "true"
  Aug 24 13:03:48.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods update-demo-nautilus-llzv8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:03:48.569: INFO: stderr: ""
  Aug 24 13:03:48.569: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:03:48.569: INFO: validating pod update-demo-nautilus-llzv8
  Aug 24 13:03:48.711: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:03:48.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:03:48.711: INFO: update-demo-nautilus-llzv8 is verified up and running
  STEP: using delete to clean up resources @ 08/24/23 13:03:48.711
  Aug 24 13:03:48.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 delete --grace-period=0 --force -f -'
  E0824 13:03:48.816040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:48.909: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:03:48.909: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 24 13:03:48.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get rc,svc -l name=update-demo --no-headers'
  Aug 24 13:03:49.152: INFO: stderr: "No resources found in kubectl-5637 namespace.\n"
  Aug 24 13:03:49.153: INFO: stdout: ""
  Aug 24 13:03:49.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5637 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 24 13:03:49.416: INFO: stderr: ""
  Aug 24 13:03:49.416: INFO: stdout: ""
  Aug 24 13:03:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5637" for this suite. @ 08/24/23 13:03:49.427
• [31.148 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 08/24/23 13:03:49.443
  Aug 24 13:03:49.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:03:49.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:03:49.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:03:49.487
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:03:49.492
  E0824 13:03:49.817217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:50.817909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:51.818567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:52.818827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:03:53.56
  Aug 24 13:03:53.568: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-6208ec12-5ddc-4641-9cb1-6444e3754f32 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:03:53.583
  Aug 24 13:03:53.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7254" for this suite. @ 08/24/23 13:03:53.646
• [4.218 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 08/24/23 13:03:53.664
  Aug 24 13:03:53.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:03:53.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:03:53.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:03:53.714
  STEP: creating all guestbook components @ 08/24/23 13:03:53.719
  Aug 24 13:03:53.719: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Aug 24 13:03:53.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 create -f -'
  E0824 13:03:53.820392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:54.312: INFO: stderr: ""
  Aug 24 13:03:54.312: INFO: stdout: "service/agnhost-replica created\n"
  Aug 24 13:03:54.313: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Aug 24 13:03:54.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 create -f -'
  E0824 13:03:54.821476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:54.981: INFO: stderr: ""
  Aug 24 13:03:54.981: INFO: stdout: "service/agnhost-primary created\n"
  Aug 24 13:03:54.982: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Aug 24 13:03:54.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 create -f -'
  Aug 24 13:03:55.648: INFO: stderr: ""
  Aug 24 13:03:55.648: INFO: stdout: "service/frontend created\n"
  Aug 24 13:03:55.649: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Aug 24 13:03:55.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 create -f -'
  E0824 13:03:55.821966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:56.165: INFO: stderr: ""
  Aug 24 13:03:56.165: INFO: stdout: "deployment.apps/frontend created\n"
  Aug 24 13:03:56.165: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 24 13:03:56.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 create -f -'
  E0824 13:03:56.824202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:57.156: INFO: stderr: ""
  Aug 24 13:03:57.156: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Aug 24 13:03:57.156: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 24 13:03:57.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 create -f -'
  E0824 13:03:57.824951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:03:58.233: INFO: stderr: ""
  Aug 24 13:03:58.233: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 08/24/23 13:03:58.233
  Aug 24 13:03:58.233: INFO: Waiting for all frontend pods to be Running.
  E0824 13:03:58.824945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:59.825568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:00.825885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:01.826188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:02.826789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:03.284: INFO: Waiting for frontend to serve content.
  Aug 24 13:04:03.304: INFO: Trying to add a new entry to the guestbook.
  Aug 24 13:04:03.334: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 08/24/23 13:04:03.355
  Aug 24 13:04:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  Aug 24 13:04:03.553: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:04:03.553: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 13:04:03.553
  Aug 24 13:04:03.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  Aug 24 13:04:03.739: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:04:03.739: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 13:04:03.739
  Aug 24 13:04:03.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  E0824 13:04:03.829529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:03.939: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:04:03.939: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 13:04:03.94
  Aug 24 13:04:03.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  Aug 24 13:04:04.097: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:04:04.097: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 13:04:04.097
  Aug 24 13:04:04.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  Aug 24 13:04:04.311: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:04:04.311: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 13:04:04.311
  Aug 24 13:04:04.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  Aug 24 13:04:04.556: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:04:04.556: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Aug 24 13:04:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-564" for this suite. @ 08/24/23 13:04:04.574
• [10.926 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 08/24/23 13:04:04.592
  Aug 24 13:04:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:04:04.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:04.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:04.714
  STEP: Creating pod liveness-c4fd2bfc-19b9-4a21-95df-8fdf02a1ae4d in namespace container-probe-9263 @ 08/24/23 13:04:04.721
  E0824 13:04:04.828037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:05.828247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:06.818: INFO: Started pod liveness-c4fd2bfc-19b9-4a21-95df-8fdf02a1ae4d in namespace container-probe-9263
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:04:06.818
  Aug 24 13:04:06.823: INFO: Initial restart count of pod liveness-c4fd2bfc-19b9-4a21-95df-8fdf02a1ae4d is 0
  E0824 13:04:06.828825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:07.829114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:08.832083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:09.832455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:10.832854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:11.833005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:12.833845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:13.835093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:14.835549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:15.835593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:16.835803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:17.836893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:18.837310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:19.838765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:20.838712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:21.839142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:22.839963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:23.840794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:24.841045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:25.842121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:26.842379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:26.919: INFO: Restart count of pod container-probe-9263/liveness-c4fd2bfc-19b9-4a21-95df-8fdf02a1ae4d is now 1 (20.095081921s elapsed)
  Aug 24 13:04:26.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:04:26.928
  STEP: Destroying namespace "container-probe-9263" for this suite. @ 08/24/23 13:04:26.961
• [22.403 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 08/24/23 13:04:26.997
  Aug 24 13:04:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:04:26.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:27.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:27.066
  E0824 13:04:27.843125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:28.843410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:29.843568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:30.843961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:31.844330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:32.844851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:33.845143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:34.845885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:35.846021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:36.846976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:37.847547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:38.848385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:39.848419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:40.848613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:41.848924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:42.849592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:43.849921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/24/23 13:04:44.079
  E0824 13:04:44.849935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:45.850286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:46.851363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:47.851521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:48.852589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 13:04:49.088
  STEP: Ensuring resource quota status is calculated @ 08/24/23 13:04:49.101
  E0824 13:04:49.852822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:50.853292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 08/24/23 13:04:51.115
  STEP: Ensuring resource quota status captures configMap creation @ 08/24/23 13:04:51.139
  E0824 13:04:51.853382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:52.853786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 08/24/23 13:04:53.147
  STEP: Ensuring resource quota status released usage @ 08/24/23 13:04:53.164
  E0824 13:04:53.853971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:54.854066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:55.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-495" for this suite. @ 08/24/23 13:04:55.182
• [28.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 08/24/23 13:04:55.209
  Aug 24 13:04:55.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename controllerrevisions @ 08/24/23 13:04:55.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:55.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:55.247
  STEP: Creating DaemonSet "e2e-shq8s-daemon-set" @ 08/24/23 13:04:55.288
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 13:04:55.298
  Aug 24 13:04:55.316: INFO: Number of nodes with available pods controlled by daemonset e2e-shq8s-daemon-set: 0
  Aug 24 13:04:55.316: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:04:55.855160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:56.344: INFO: Number of nodes with available pods controlled by daemonset e2e-shq8s-daemon-set: 0
  Aug 24 13:04:56.344: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:04:56.856300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:57.338: INFO: Number of nodes with available pods controlled by daemonset e2e-shq8s-daemon-set: 3
  Aug 24 13:04:57.338: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-shq8s-daemon-set
  STEP: Confirm DaemonSet "e2e-shq8s-daemon-set" successfully created with "daemonset-name=e2e-shq8s-daemon-set" label @ 08/24/23 13:04:57.345
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-shq8s-daemon-set" @ 08/24/23 13:04:57.362
  Aug 24 13:04:57.374: INFO: Located ControllerRevision: "e2e-shq8s-daemon-set-574446cc54"
  STEP: Patching ControllerRevision "e2e-shq8s-daemon-set-574446cc54" @ 08/24/23 13:04:57.38
  Aug 24 13:04:57.398: INFO: e2e-shq8s-daemon-set-574446cc54 has been patched
  STEP: Create a new ControllerRevision @ 08/24/23 13:04:57.399
  Aug 24 13:04:57.413: INFO: Created ControllerRevision: e2e-shq8s-daemon-set-5cbbb54cc6
  STEP: Confirm that there are two ControllerRevisions @ 08/24/23 13:04:57.414
  Aug 24 13:04:57.414: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 13:04:57.421: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-shq8s-daemon-set-574446cc54" @ 08/24/23 13:04:57.421
  STEP: Confirm that there is only one ControllerRevision @ 08/24/23 13:04:57.433
  Aug 24 13:04:57.433: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 13:04:57.439: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-shq8s-daemon-set-5cbbb54cc6" @ 08/24/23 13:04:57.447
  Aug 24 13:04:57.462: INFO: e2e-shq8s-daemon-set-5cbbb54cc6 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 08/24/23 13:04:57.462
  W0824 13:04:57.476339      13 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 08/24/23 13:04:57.476
  Aug 24 13:04:57.476: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0824 13:04:57.856386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:58.485: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 13:04:58.501: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-shq8s-daemon-set-5cbbb54cc6=updated" @ 08/24/23 13:04:58.501
  STEP: Confirm that there is only one ControllerRevision @ 08/24/23 13:04:58.528
  Aug 24 13:04:58.528: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 13:04:58.539: INFO: Found 1 ControllerRevisions
  Aug 24 13:04:58.548: INFO: ControllerRevision "e2e-shq8s-daemon-set-7c8dfc7d57" has revision 3
  STEP: Deleting DaemonSet "e2e-shq8s-daemon-set" @ 08/24/23 13:04:58.555
  STEP: deleting DaemonSet.extensions e2e-shq8s-daemon-set in namespace controllerrevisions-6712, will wait for the garbage collector to delete the pods @ 08/24/23 13:04:58.555
  Aug 24 13:04:58.634: INFO: Deleting DaemonSet.extensions e2e-shq8s-daemon-set took: 20.232151ms
  Aug 24 13:04:58.736: INFO: Terminating DaemonSet.extensions e2e-shq8s-daemon-set pods took: 101.412596ms
  E0824 13:04:58.857644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:59.858277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:00.858922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:01.245: INFO: Number of nodes with available pods controlled by daemonset e2e-shq8s-daemon-set: 0
  Aug 24 13:05:01.245: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-shq8s-daemon-set
  Aug 24 13:05:01.252: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33427"},"items":null}

  Aug 24 13:05:01.264: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33427"},"items":null}

  Aug 24 13:05:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-6712" for this suite. @ 08/24/23 13:05:01.325
• [6.133 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 08/24/23 13:05:01.342
  Aug 24 13:05:01.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 13:05:01.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:05:01.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:05:01.385
  STEP: Create a Replicaset @ 08/24/23 13:05:01.402
  STEP: Verify that the required pods have come up. @ 08/24/23 13:05:01.412
  Aug 24 13:05:01.419: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0824 13:05:01.859500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:02.859984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:03.859694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:04.860171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:05.860298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:06.430: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 13:05:06.43
  STEP: Getting /status @ 08/24/23 13:05:06.43
  Aug 24 13:05:06.441: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 08/24/23 13:05:06.442
  Aug 24 13:05:06.464: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 08/24/23 13:05:06.464
  Aug 24 13:05:06.473: INFO: Observed &ReplicaSet event: ADDED
  Aug 24 13:05:06.473: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.474: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.474: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.474: INFO: Found replicaset test-rs in namespace replicaset-1174 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 13:05:06.474: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 08/24/23 13:05:06.474
  Aug 24 13:05:06.474: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 24 13:05:06.490: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 08/24/23 13:05:06.49
  Aug 24 13:05:06.493: INFO: Observed &ReplicaSet event: ADDED
  Aug 24 13:05:06.493: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.494: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.494: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.494: INFO: Observed replicaset test-rs in namespace replicaset-1174 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 13:05:06.494: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 13:05:06.494: INFO: Found replicaset test-rs in namespace replicaset-1174 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Aug 24 13:05:06.494: INFO: Replicaset test-rs has a patched status
  Aug 24 13:05:06.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1174" for this suite. @ 08/24/23 13:05:06.508
• [5.200 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 08/24/23 13:05:06.545
  Aug 24 13:05:06.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:05:06.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:05:06.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:05:06.621
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:05:06.627
  E0824 13:05:06.860352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:07.861042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:08.861228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:09.862814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:05:10.679
  Aug 24 13:05:10.688: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-81eea74b-d61b-474f-8166-f6956e9e10db container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:05:10.7
  Aug 24 13:05:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-876" for this suite. @ 08/24/23 13:05:10.75
• [4.219 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 08/24/23 13:05:10.767
  Aug 24 13:05:10.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 13:05:10.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:05:10.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:05:10.819
  STEP: create the rc @ 08/24/23 13:05:10.84
  W0824 13:05:10.856654      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0824 13:05:10.863007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:11.963520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:12.982512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:13.997436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:15.056473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:16.057377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:17.062202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/24/23 13:05:17.194
  STEP: wait for the rc to be deleted @ 08/24/23 13:05:17.493
  E0824 13:05:18.073667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:18.997: INFO: 85 pods remaining
  Aug 24 13:05:18.997: INFO: 78 pods has nil DeletionTimestamp
  Aug 24 13:05:18.997: INFO: 
  E0824 13:05:19.080766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:20.006: INFO: 79 pods remaining
  Aug 24 13:05:20.013: INFO: 69 pods has nil DeletionTimestamp
  Aug 24 13:05:20.014: INFO: 
  E0824 13:05:20.082000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:20.727: INFO: 71 pods remaining
  Aug 24 13:05:20.733: INFO: 55 pods has nil DeletionTimestamp
  Aug 24 13:05:20.733: INFO: 
  E0824 13:05:21.086437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:22.093288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:22.115: INFO: 66 pods remaining
  Aug 24 13:05:22.123: INFO: 39 pods has nil DeletionTimestamp
  Aug 24 13:05:22.148: INFO: 
  Aug 24 13:05:22.841: INFO: 61 pods remaining
  Aug 24 13:05:22.842: INFO: 27 pods has nil DeletionTimestamp
  Aug 24 13:05:22.842: INFO: 
  E0824 13:05:23.088825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:23.790: INFO: 53 pods remaining
  Aug 24 13:05:23.799: INFO: 14 pods has nil DeletionTimestamp
  Aug 24 13:05:23.799: INFO: 
  E0824 13:05:24.092828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:24.882: INFO: 48 pods remaining
  Aug 24 13:05:24.882: INFO: 1 pods has nil DeletionTimestamp
  Aug 24 13:05:24.882: INFO: 
  E0824 13:05:25.176789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:26.028: INFO: 44 pods remaining
  Aug 24 13:05:26.028: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:26.028: INFO: 
  E0824 13:05:26.109867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:26.691: INFO: 37 pods remaining
  Aug 24 13:05:26.691: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:26.691: INFO: 
  E0824 13:05:27.124792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:28.055: INFO: 32 pods remaining
  Aug 24 13:05:28.055: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:28.055: INFO: 
  E0824 13:05:28.117437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:28.532: INFO: 26 pods remaining
  Aug 24 13:05:28.532: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:28.533: INFO: 
  E0824 13:05:29.118401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:29.651: INFO: 19 pods remaining
  Aug 24 13:05:29.652: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:29.652: INFO: 
  E0824 13:05:30.118566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:30.584: INFO: 15 pods remaining
  Aug 24 13:05:30.584: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:30.584: INFO: 
  E0824 13:05:31.119621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:31.539: INFO: 7 pods remaining
  Aug 24 13:05:31.539: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:31.539: INFO: 
  E0824 13:05:32.119869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:32.599: INFO: 1 pods remaining
  Aug 24 13:05:32.599: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 13:05:32.599: INFO: 
  E0824 13:05:33.119982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/24/23 13:05:33.536
  E0824 13:05:34.120079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:34.248: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 13:05:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7232" for this suite. @ 08/24/23 13:05:34.335
• [23.693 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 08/24/23 13:05:34.464
  Aug 24 13:05:34.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 13:05:34.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:05:34.561
  E0824 13:05:35.244095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:05:35.244
  Aug 24 13:05:35.255: INFO: Creating ReplicaSet my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f
  Aug 24 13:05:35.453: INFO: Pod name my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f: Found 0 pods out of 1
  E0824 13:05:36.277074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:37.439744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:38.444193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:39.443178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:40.444321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:40.799: INFO: Pod name my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f: Found 1 pods out of 1
  Aug 24 13:05:40.800: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f" is running
  E0824 13:05:41.448956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:42.449310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:43.450373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:44.450583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:05:44.921: INFO: Pod "my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f-2xtvt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:05:35 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:05:35 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:05:35 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-37b14748-b7f2-49aa-9bee-8f346f48cb6f]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:05:35 +0000 UTC Reason: Message:}])
  Aug 24 13:05:44.922: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/24/23 13:05:44.922
  Aug 24 13:05:44.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3334" for this suite. @ 08/24/23 13:05:44.978
• [10.557 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 08/24/23 13:05:45.03
  Aug 24 13:05:45.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:05:45.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:05:45.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:05:45.136
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/24/23 13:05:45.146
  E0824 13:05:45.450613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:46.451410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:47.451371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:48.452095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:05:49.312
  Aug 24 13:05:49.449: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-6937c0da-9551-4164-8fbc-9e321ac26d4a container test-container: <nil>
  E0824 13:05:49.451948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 08/24/23 13:05:49.708
  Aug 24 13:05:49.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7109" for this suite. @ 08/24/23 13:05:49.937
• [4.930 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 08/24/23 13:05:49.961
  Aug 24 13:05:49.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 13:05:49.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:05:50.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:05:50.041
  STEP: set up a multi version CRD @ 08/24/23 13:05:50.049
  Aug 24 13:05:50.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:05:50.452879      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:51.454413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:52.455485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:53.456568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:54.457736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:55.459561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 08/24/23 13:05:56.257
  STEP: check the new version name is served @ 08/24/23 13:05:56.304
  E0824 13:05:56.460190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:57.461457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 08/24/23 13:05:57.699
  E0824 13:05:58.461747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 08/24/23 13:05:58.699
  E0824 13:05:59.462364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:00.463167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:01.465891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:02.466359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:02.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3267" for this suite. @ 08/24/23 13:06:03.017
• [13.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 08/24/23 13:06:03.053
  Aug 24 13:06:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 13:06:03.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:03.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:03.086
  STEP: Creating configMap configmap-7066/configmap-test-0dcb098c-0bfd-4208-b417-5bb2abef908d @ 08/24/23 13:06:03.092
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:06:03.101
  E0824 13:06:03.467355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:04.467713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:05.468310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:06.468598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:06:07.141
  Aug 24 13:06:07.150: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-e8f2af38-89e5-4eb0-835f-d9e091d54e8f container env-test: <nil>
  STEP: delete the pod @ 08/24/23 13:06:07.184
  Aug 24 13:06:07.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7066" for this suite. @ 08/24/23 13:06:07.231
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 08/24/23 13:06:07.264
  Aug 24 13:06:07.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:06:07.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:07.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:07.312
  STEP: Creating the pod @ 08/24/23 13:06:07.319
  E0824 13:06:07.468594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:08.469028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:09.469352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:09.913: INFO: Successfully updated pod "annotationupdatecd5bcd14-b02d-44cc-b798-bfbea3aac575"
  E0824 13:06:10.470094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:11.470274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:11.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8121" for this suite. @ 08/24/23 13:06:11.958
• [4.713 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 08/24/23 13:06:11.98
  Aug 24 13:06:11.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 13:06:11.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:12.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:12.022
  STEP: Creating replication controller my-hostname-basic-899aa622-d1d5-49f9-b1b1-9b10d020f8bb @ 08/24/23 13:06:12.027
  Aug 24 13:06:12.045: INFO: Pod name my-hostname-basic-899aa622-d1d5-49f9-b1b1-9b10d020f8bb: Found 0 pods out of 1
  E0824 13:06:12.471573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:13.471602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:14.472027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:15.472173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:16.473030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:17.070: INFO: Pod name my-hostname-basic-899aa622-d1d5-49f9-b1b1-9b10d020f8bb: Found 1 pods out of 1
  Aug 24 13:06:17.070: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-899aa622-d1d5-49f9-b1b1-9b10d020f8bb" are running
  Aug 24 13:06:17.079: INFO: Pod "my-hostname-basic-899aa622-d1d5-49f9-b1b1-9b10d020f8bb-p2mgg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:06:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:06:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:06:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 13:06:12 +0000 UTC Reason: Message:}])
  Aug 24 13:06:17.079: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/24/23 13:06:17.079
  Aug 24 13:06:17.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4594" for this suite. @ 08/24/23 13:06:17.139
• [5.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 08/24/23 13:06:17.168
  Aug 24 13:06:17.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:06:17.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:17.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:17.22
  STEP: creating a secret @ 08/24/23 13:06:17.228
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 08/24/23 13:06:17.244
  STEP: patching the secret @ 08/24/23 13:06:17.251
  STEP: deleting the secret using a LabelSelector @ 08/24/23 13:06:17.278
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 08/24/23 13:06:17.305
  Aug 24 13:06:17.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4157" for this suite. @ 08/24/23 13:06:17.327
• [0.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 08/24/23 13:06:17.345
  Aug 24 13:06:17.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:06:17.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:17.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:17.404
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/24/23 13:06:17.415
  E0824 13:06:17.473386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:18.473896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:19.474788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:20.475098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:06:21.474
  E0824 13:06:21.474958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:21.482: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-5984efd2-9b72-4ea3-a7de-16f74e171e5e container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:06:21.496
  Aug 24 13:06:21.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3241" for this suite. @ 08/24/23 13:06:21.538
• [4.241 seconds]
------------------------------
SS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 08/24/23 13:06:21.587
  Aug 24 13:06:21.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename events @ 08/24/23 13:06:21.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:21.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:21.625
  STEP: Create set of events @ 08/24/23 13:06:21.631
  STEP: get a list of Events with a label in the current namespace @ 08/24/23 13:06:21.663
  STEP: delete a list of events @ 08/24/23 13:06:21.671
  Aug 24 13:06:21.672: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/24/23 13:06:21.711
  Aug 24 13:06:21.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5630" for this suite. @ 08/24/23 13:06:21.727
• [0.154 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 08/24/23 13:06:21.742
  Aug 24 13:06:21.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 13:06:21.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:21.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:21.773
  E0824 13:06:22.475388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:23.476148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 08/24/23 13:06:23.825
  Aug 24 13:06:23.825: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3076 pod-service-account-4b213884-f5d8-4ca1-91d3-b8e65dc6b0de -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 08/24/23 13:06:24.273
  Aug 24 13:06:24.274: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3076 pod-service-account-4b213884-f5d8-4ca1-91d3-b8e65dc6b0de -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  E0824 13:06:24.476709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 08/24/23 13:06:24.529
  Aug 24 13:06:24.530: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3076 pod-service-account-4b213884-f5d8-4ca1-91d3-b8e65dc6b0de -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Aug 24 13:06:24.853: INFO: Got root ca configmap in namespace "svcaccounts-3076"
  Aug 24 13:06:24.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3076" for this suite. @ 08/24/23 13:06:24.871
• [3.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 08/24/23 13:06:24.895
  Aug 24 13:06:24.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 08/24/23 13:06:24.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:24.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:24.959
  STEP: Setting up the test @ 08/24/23 13:06:24.97
  STEP: Creating hostNetwork=false pod @ 08/24/23 13:06:24.97
  E0824 13:06:25.476961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:26.477089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 08/24/23 13:06:27.053
  E0824 13:06:27.478152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:28.478760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 08/24/23 13:06:29.102
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 08/24/23 13:06:29.102
  Aug 24 13:06:29.102: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.105: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.105: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 13:06:29.243: INFO: Exec stderr: ""
  Aug 24 13:06:29.243: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.246: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.247: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 13:06:29.404: INFO: Exec stderr: ""
  Aug 24 13:06:29.405: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.411: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.412: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0824 13:06:29.479551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:29.558: INFO: Exec stderr: ""
  Aug 24 13:06:29.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.561: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.562: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 24 13:06:29.701: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 08/24/23 13:06:29.701
  Aug 24 13:06:29.701: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.703: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.703: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 24 13:06:29.834: INFO: Exec stderr: ""
  Aug 24 13:06:29.835: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.837: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.837: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 24 13:06:29.969: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 08/24/23 13:06:29.969
  Aug 24 13:06:29.969: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:29.971: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:29.971: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 13:06:30.158: INFO: Exec stderr: ""
  Aug 24 13:06:30.159: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:30.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:30.161: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:30.162: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 13:06:30.302: INFO: Exec stderr: ""
  Aug 24 13:06:30.302: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:30.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:30.304: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:30.305: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 24 13:06:30.440: INFO: Exec stderr: ""
  Aug 24 13:06:30.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5027 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:06:30.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:06:30.445: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:06:30.445: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5027/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0824 13:06:30.480033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:30.618: INFO: Exec stderr: ""
  Aug 24 13:06:30.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-5027" for this suite. @ 08/24/23 13:06:30.632
• [5.755 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 08/24/23 13:06:30.651
  Aug 24 13:06:30.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 13:06:30.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:30.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:30.7
  Aug 24 13:06:30.706: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 13:06:30.724: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 13:06:30.730: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-1 before test
  Aug 24 13:06:30.750: INFO: cilium-5bz85 from kube-system started at 2023-08-24 11:30:24 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.750: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 13:06:30.750: INFO: cilium-node-init-js6v2 from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.750: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 13:06:30.750: INFO: coredns-5d78c9869d-4zkjt from kube-system started at 2023-08-24 12:00:11 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.750: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 13:06:30.750: INFO: kube-addon-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.750: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 13:06:30.750: INFO: kube-apiserver-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.751: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 13:06:30.751: INFO: kube-controller-manager-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.751: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 13:06:30.751: INFO: kube-proxy-l6rtn from kube-system started at 2023-08-24 11:27:37 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.751: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 13:06:30.751: INFO: kube-scheduler-pohje9aimahx-1 from kube-system started at 2023-08-24 11:28:36 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.751: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 13:06:30.751: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-z7825 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 13:06:30.751: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 13:06:30.751: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 13:06:30.751: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-2 before test
  Aug 24 13:06:30.777: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-5027 started at 2023-08-24 13:06:27 +0000 UTC (2 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container busybox-1 ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: 	Container busybox-2 ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: cilium-node-init-xrb2l from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: cilium-zx72t from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: coredns-5d78c9869d-znmdb from kube-system started at 2023-08-24 11:31:21 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: kube-addon-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:30:10 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: kube-apiserver-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: kube-controller-manager-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: kube-proxy-nz65t from kube-system started at 2023-08-24 11:28:08 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: kube-scheduler-pohje9aimahx-2 from kube-system started at 2023-08-24 11:28:37 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-8jtw4 from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 13:06:30.777: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 13:06:30.777: INFO: 
  Logging pods the apiserver thinks is on node pohje9aimahx-3 before test
  Aug 24 13:06:30.800: INFO: test-pod from e2e-kubelet-etc-hosts-5027 started at 2023-08-24 13:06:25 +0000 UTC (3 container statuses recorded)
  Aug 24 13:06:30.800: INFO: 	Container busybox-1 ready: true, restart count 0
  Aug 24 13:06:30.801: INFO: 	Container busybox-2 ready: true, restart count 0
  Aug 24 13:06:30.801: INFO: 	Container busybox-3 ready: true, restart count 0
  Aug 24 13:06:30.801: INFO: cilium-node-init-42bmw from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.802: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 13:06:30.802: INFO: cilium-operator-b8f479cd9-gv7jv from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.802: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 13:06:30.803: INFO: cilium-xptxb from kube-system started at 2023-08-24 11:30:25 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.803: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 13:06:30.803: INFO: kube-proxy-vtcsn from kube-system started at 2023-08-24 11:28:50 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.803: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 13:06:30.804: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:39:28 +0000 UTC (1 container statuses recorded)
  Aug 24 13:06:30.804: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 13:06:30.804: INFO: sonobuoy-e2e-job-ee97c55b29594c3a from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 13:06:30.805: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 13:06:30.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 13:06:30.805: INFO: sonobuoy-systemd-logs-daemon-set-03224e7216b846cb-4l69j from sonobuoy started at 2023-08-24 11:39:59 +0000 UTC (2 container statuses recorded)
  Aug 24 13:06:30.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 13:06:30.806: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 08/24/23 13:06:30.806
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.177e5390b2586944], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 08/24/23 13:06:30.891
  E0824 13:06:31.479976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:31.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8007" for this suite. @ 08/24/23 13:06:31.891
• [1.253 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 08/24/23 13:06:31.907
  Aug 24 13:06:31.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 13:06:31.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:31.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:31.961
  STEP: Creating service test in namespace statefulset-822 @ 08/24/23 13:06:31.967
  STEP: Creating statefulset ss in namespace statefulset-822 @ 08/24/23 13:06:31.991
  Aug 24 13:06:32.023: INFO: Found 0 stateful pods, waiting for 1
  E0824 13:06:32.481672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:33.480997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:34.482717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:35.482540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:36.483907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:37.483886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:38.484552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:39.485214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:40.485391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:41.487008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:42.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 08/24/23 13:06:42.057
  STEP: Getting /status @ 08/24/23 13:06:42.081
  Aug 24 13:06:42.095: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 08/24/23 13:06:42.095
  Aug 24 13:06:42.120: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 08/24/23 13:06:42.12
  Aug 24 13:06:42.130: INFO: Observed &StatefulSet event: ADDED
  Aug 24 13:06:42.131: INFO: Found Statefulset ss in namespace statefulset-822 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 13:06:42.131: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 08/24/23 13:06:42.131
  Aug 24 13:06:42.131: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 24 13:06:42.148: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 08/24/23 13:06:42.148
  Aug 24 13:06:42.153: INFO: Observed &StatefulSet event: ADDED
  Aug 24 13:06:42.154: INFO: Deleting all statefulset in ns statefulset-822
  Aug 24 13:06:42.161: INFO: Scaling statefulset ss to 0
  E0824 13:06:42.486716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:43.487319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:44.489656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:45.489272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:46.489481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:47.489689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:48.490447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:49.490615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:50.491038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:51.491166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:06:52.195: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 13:06:52.199: INFO: Deleting statefulset ss
  Aug 24 13:06:52.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-822" for this suite. @ 08/24/23 13:06:52.253
• [20.360 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 08/24/23 13:06:52.275
  Aug 24 13:06:52.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 13:06:52.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:52.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:52.313
  STEP: Creating configMap with name configmap-test-volume-map-eef37120-112d-49b9-97bb-5c2d491f8987 @ 08/24/23 13:06:52.318
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:06:52.327
  E0824 13:06:52.491722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:53.492228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:54.492837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:55.492948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:06:56.375
  Aug 24 13:06:56.381: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-configmaps-04523633-7853-4cb6-a147-e23ca1982db5 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:06:56.399
  Aug 24 13:06:56.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4181" for this suite. @ 08/24/23 13:06:56.44
• [4.178 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 08/24/23 13:06:56.454
  Aug 24 13:06:56.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 13:06:56.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:06:56.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:06:56.488
  E0824 13:06:56.492849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7126.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7126.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 08/24/23 13:06:56.494
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7126.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7126.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 08/24/23 13:06:56.495
  STEP: creating a pod to probe /etc/hosts @ 08/24/23 13:06:56.495
  STEP: submitting the pod to kubernetes @ 08/24/23 13:06:56.495
  E0824 13:06:57.493127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:58.494107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:59.494193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:00.494345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:07:00.551
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:07:00.56
  Aug 24 13:07:00.597: INFO: DNS probes using dns-7126/dns-test-607372cd-3d2d-4a2f-88d3-811c978e7d32 succeeded

  Aug 24 13:07:00.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:07:00.612
  STEP: Destroying namespace "dns-7126" for this suite. @ 08/24/23 13:07:00.644
• [4.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 08/24/23 13:07:00.67
  Aug 24 13:07:00.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:07:00.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:07:00.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:07:00.714
  STEP: Creating pod test-grpc-2b47f08e-ec59-4131-a42b-be1700bdae99 in namespace container-probe-2890 @ 08/24/23 13:07:00.72
  E0824 13:07:01.497133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:02.497997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:07:02.758: INFO: Started pod test-grpc-2b47f08e-ec59-4131-a42b-be1700bdae99 in namespace container-probe-2890
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:07:02.758
  Aug 24 13:07:02.763: INFO: Initial restart count of pod test-grpc-2b47f08e-ec59-4131-a42b-be1700bdae99 is 0
  E0824 13:07:03.498403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:04.498378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:05.498659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:06.499286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:07.499152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:08.499863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:09.500939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:10.501483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:11.501991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:12.502681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:13.503112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:14.503857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:15.504294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:16.504918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:17.506071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:18.506671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:19.506970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:20.507829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:21.507877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:22.509112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:23.509404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:24.509903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:25.510673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:26.510670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:27.511806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:28.512211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:29.512262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:30.512903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:31.513059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:32.513665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:33.513783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:34.514953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:35.515104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:36.516249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:37.517394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:38.517731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:39.518471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:40.518850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:41.519310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:42.519875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:43.519709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:44.519995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:45.520566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:46.521302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:47.521786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:48.522180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:49.522923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:50.523630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:51.524845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:52.525524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:53.525938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:54.526122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:55.526411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:56.526621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:57.526921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:58.527645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:59.531030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:00.528025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:01.528241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:02.528919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:03.529915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:04.532184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:05.531279      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:06.531698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:07.537049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:08.533128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:09.128: INFO: Restart count of pod container-probe-2890/test-grpc-2b47f08e-ec59-4131-a42b-be1700bdae99 is now 1 (1m6.364042839s elapsed)
  Aug 24 13:08:09.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:08:09.146
  STEP: Destroying namespace "container-probe-2890" for this suite. @ 08/24/23 13:08:09.186
• [68.531 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 08/24/23 13:08:09.205
  Aug 24 13:08:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:08:09.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:09.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:09.316
  STEP: Counting existing ResourceQuota @ 08/24/23 13:08:09.323
  E0824 13:08:09.533852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:10.535675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:11.535700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:12.535988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:13.536955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 13:08:14.33
  STEP: Ensuring resource quota status is calculated @ 08/24/23 13:08:14.345
  E0824 13:08:14.537700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:15.538724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:16.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9277" for this suite. @ 08/24/23 13:08:16.37
• [7.182 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 08/24/23 13:08:16.388
  Aug 24 13:08:16.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 13:08:16.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:16.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:16.457
  STEP: apply creating a deployment @ 08/24/23 13:08:16.462
  Aug 24 13:08:16.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-436" for this suite. @ 08/24/23 13:08:16.507
  E0824 13:08:16.539183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.151 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 08/24/23 13:08:16.541
  Aug 24 13:08:16.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename containers @ 08/24/23 13:08:16.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:16.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:16.612
  STEP: Creating a pod to test override arguments @ 08/24/23 13:08:16.62
  E0824 13:08:17.540072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:18.541866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:19.542714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:20.543230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:08:20.682
  Aug 24 13:08:20.689: INFO: Trying to get logs from node pohje9aimahx-3 pod client-containers-3a3c0cce-eff6-47b9-a6e8-2352416c26db container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:08:20.708
  Aug 24 13:08:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4999" for this suite. @ 08/24/23 13:08:20.745
• [4.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 08/24/23 13:08:20.767
  Aug 24 13:08:20.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:08:20.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:20.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:20.816
  STEP: Create a pod @ 08/24/23 13:08:20.821
  E0824 13:08:21.552871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:22.550545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/24/23 13:08:22.892
  Aug 24 13:08:22.907: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Aug 24 13:08:22.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5011" for this suite. @ 08/24/23 13:08:22.918
• [2.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 08/24/23 13:08:22.937
  Aug 24 13:08:22.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:08:22.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:22.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:22.998
  STEP: Creating the pod @ 08/24/23 13:08:23.003
  E0824 13:08:23.550830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:24.552118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:25.552365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:25.598: INFO: Successfully updated pod "labelsupdate0da96f16-2dbe-4a25-bc18-494f03619b15"
  E0824 13:08:26.553464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:27.553578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:27.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8053" for this suite. @ 08/24/23 13:08:27.666
• [4.743 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 08/24/23 13:08:27.685
  Aug 24 13:08:27.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 13:08:27.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:27.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:27.728
  STEP: creating service nodeport-test with type=NodePort in namespace services-7833 @ 08/24/23 13:08:27.733
  STEP: creating replication controller nodeport-test in namespace services-7833 @ 08/24/23 13:08:27.758
  I0824 13:08:27.774348      13 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-7833, replica count: 2
  E0824 13:08:28.554053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:29.555412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:30.558996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:08:30.826325      13 runners.go:194] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 13:08:31.559723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:32.560304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:33.560605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:08:33.827541      13 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 13:08:33.827: INFO: Creating new exec pod
  E0824 13:08:34.561230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:35.562464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:36.562412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:36.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7833 exec execpodr9xjs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Aug 24 13:08:37.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug 24 13:08:37.219: INFO: stdout: ""
  E0824 13:08:37.562735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:38.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7833 exec execpodr9xjs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  E0824 13:08:38.563346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:38.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug 24 13:08:38.573: INFO: stdout: "nodeport-test-2j2z2"
  Aug 24 13:08:38.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7833 exec execpodr9xjs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.113 80'
  Aug 24 13:08:38.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.113 80\nConnection to 10.233.16.113 80 port [tcp/http] succeeded!\n"
  Aug 24 13:08:38.938: INFO: stdout: "nodeport-test-2j2z2"
  Aug 24 13:08:38.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7833 exec execpodr9xjs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.5 30612'
  Aug 24 13:08:39.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.5 30612\nConnection to 192.168.121.5 30612 port [tcp/*] succeeded!\n"
  Aug 24 13:08:39.236: INFO: stdout: ""
  E0824 13:08:39.564286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:40.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7833 exec execpodr9xjs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.5 30612'
  E0824 13:08:40.565002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:40.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.5 30612\nConnection to 192.168.121.5 30612 port [tcp/*] succeeded!\n"
  Aug 24 13:08:40.569: INFO: stdout: "nodeport-test-5pdvl"
  Aug 24 13:08:40.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-7833 exec execpodr9xjs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.37 30612'
  Aug 24 13:08:40.984: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.37 30612\nConnection to 192.168.121.37 30612 port [tcp/*] succeeded!\n"
  Aug 24 13:08:40.984: INFO: stdout: "nodeport-test-5pdvl"
  Aug 24 13:08:40.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7833" for this suite. @ 08/24/23 13:08:40.999
• [13.332 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 08/24/23 13:08:41.028
  Aug 24 13:08:41.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 13:08:41.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:41.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:41.062
  E0824 13:08:41.565922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:42.566240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:43.566562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:44.567388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:45.567907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 08/24/23 13:08:46.286
  E0824 13:08:46.567915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:47.568075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:48.568238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:49.568391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:50.568784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 08/24/23 13:08:51.303
  E0824 13:08:51.569260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:52.570006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:53.570788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:54.571232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:55.571868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 08/24/23 13:08:56.319
  E0824 13:08:56.572698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:57.573724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:58.573715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:59.574630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:00.574997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 08/24/23 13:09:01.337
  Aug 24 13:09:01.393: INFO: EndpointSlice for Service endpointslice-4189/example-named-port not found
  E0824 13:09:01.575697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:02.576070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:03.576366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:04.577133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:05.577332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:06.577726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:07.578187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:08.578614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:09.579294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:10.579486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:11.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4189" for this suite. @ 08/24/23 13:09:11.42
• [30.406 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 08/24/23 13:09:11.44
  Aug 24 13:09:11.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 13:09:11.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:11.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:11.49
  E0824 13:09:11.579876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:12.580313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 13:09:13.574: INFO: Deleting pod "var-expansion-7fa333ef-e196-423f-9e71-89e568b2cc30" in namespace "var-expansion-8009"
  E0824 13:09:13.580864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:13.589: INFO: Wait up to 5m0s for pod "var-expansion-7fa333ef-e196-423f-9e71-89e568b2cc30" to be fully deleted
  E0824 13:09:14.581490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:15.582270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-8009" for this suite. @ 08/24/23 13:09:15.609
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 08/24/23 13:09:15.638
  Aug 24 13:09:15.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-watch @ 08/24/23 13:09:15.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:15.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:15.683
  Aug 24 13:09:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:09:16.583956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:17.583578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 08/24/23 13:09:18.477
  Aug 24 13:09:18.491: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T13:09:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T13:09:18Z]] name:name1 resourceVersion:36307 uid:2a0f27fc-50d8-47a0-bfed-33f183476b63] num:map[num1:9223372036854775807 num2:1000000]]}
  E0824 13:09:18.583954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:19.583986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:20.585727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:21.585152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:22.585760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:23.586348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:24.587043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:25.589837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:26.589740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:27.590306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 08/24/23 13:09:28.492
  Aug 24 13:09:28.512: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T13:09:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T13:09:28Z]] name:name2 resourceVersion:36345 uid:c8135fb9-1dda-4391-af57-680a2927a4c4] num:map[num1:9223372036854775807 num2:1000000]]}
  E0824 13:09:28.590578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:29.590781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:30.591855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:31.592437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:32.593009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:33.593955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:34.594770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:35.595240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:36.595670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:37.597747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 08/24/23 13:09:38.514
  Aug 24 13:09:38.531: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T13:09:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T13:09:38Z]] name:name1 resourceVersion:36368 uid:2a0f27fc-50d8-47a0-bfed-33f183476b63] num:map[num1:9223372036854775807 num2:1000000]]}
  E0824 13:09:38.597328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:39.598085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:40.598941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:41.599310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:42.599881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:43.600170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:44.601455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:45.601377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:46.601596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:47.602421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 08/24/23 13:09:48.532
  Aug 24 13:09:48.549: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T13:09:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T13:09:48Z]] name:name2 resourceVersion:36391 uid:c8135fb9-1dda-4391-af57-680a2927a4c4] num:map[num1:9223372036854775807 num2:1000000]]}
  E0824 13:09:48.603058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:49.603683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:50.603720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:51.604006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:52.604173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:53.604777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:54.605303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:55.605365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:56.605823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:57.606646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 08/24/23 13:09:58.55
  Aug 24 13:09:58.567: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T13:09:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T13:09:38Z]] name:name1 resourceVersion:36414 uid:2a0f27fc-50d8-47a0-bfed-33f183476b63] num:map[num1:9223372036854775807 num2:1000000]]}
  E0824 13:09:58.606743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:59.607197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:00.607125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:01.609935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:02.607653      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:03.607914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:04.609937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:05.609827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:06.609961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:07.610934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 08/24/23 13:10:08.568
  Aug 24 13:10:08.592: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T13:09:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T13:09:48Z]] name:name2 resourceVersion:36436 uid:c8135fb9-1dda-4391-af57-680a2927a4c4] num:map[num1:9223372036854775807 num2:1000000]]}
  E0824 13:10:08.612004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:09.612198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:10.612545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:11.612415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:12.612645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:13.612793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:14.613123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:15.613446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:16.613941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:17.614270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:18.614892      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:19.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-1212" for this suite. @ 08/24/23 13:10:19.143
• [63.519 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 08/24/23 13:10:19.158
  Aug 24 13:10:19.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:10:19.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:19.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:19.194
  STEP: Creating the pod @ 08/24/23 13:10:19.199
  E0824 13:10:19.615830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:20.616583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:21.616981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:21.817: INFO: Successfully updated pod "labelsupdate86780a88-1fcb-4fb9-b73a-84703c8abc4d"
  E0824 13:10:22.616847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:23.617161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:23.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7339" for this suite. @ 08/24/23 13:10:23.881
• [4.741 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 08/24/23 13:10:23.905
  Aug 24 13:10:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:10:23.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:23.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:23.952
  STEP: Creating secret with name secret-test-map-4f49a515-fb47-42d9-bcf1-aec8b744c73e @ 08/24/23 13:10:23.961
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:10:23.974
  E0824 13:10:24.661449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:25.641710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:26.642060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:27.642739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:10:28.055
  Aug 24 13:10:28.063: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-879f1f5c-cf7d-4fe3-a52f-b150c5fee49f container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:10:28.095
  Aug 24 13:10:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-285" for this suite. @ 08/24/23 13:10:28.18
• [4.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 08/24/23 13:10:28.202
  Aug 24 13:10:28.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:10:28.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:28.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:28.243
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/24/23 13:10:28.249
  E0824 13:10:28.644096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:29.644401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:30.645039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:31.646526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:10:32.302
  Aug 24 13:10:32.309: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-7d4abc38-ff3d-4278-a40f-d91c222bb579 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:10:32.324
  Aug 24 13:10:32.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1924" for this suite. @ 08/24/23 13:10:32.367
• [4.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 08/24/23 13:10:32.394
  Aug 24 13:10:32.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:10:32.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:32.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:32.433
  STEP: validating api versions @ 08/24/23 13:10:32.44
  Aug 24 13:10:32.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-3566 api-versions'
  E0824 13:10:32.646080      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:32.649: INFO: stderr: ""
  Aug 24 13:10:32.649: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Aug 24 13:10:32.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3566" for this suite. @ 08/24/23 13:10:32.663
• [0.288 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 08/24/23 13:10:32.683
  Aug 24 13:10:32.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:10:32.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:32.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:32.732
  STEP: Counting existing ResourceQuota @ 08/24/23 13:10:32.74
  E0824 13:10:33.646451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:34.646732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:35.646883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:36.647217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:37.647168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 13:10:37.753
  STEP: Ensuring resource quota status is calculated @ 08/24/23 13:10:37.77
  E0824 13:10:38.647602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:39.647822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 08/24/23 13:10:39.783
  STEP: Creating a NodePort Service @ 08/24/23 13:10:39.837
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 08/24/23 13:10:39.896
  STEP: Ensuring resource quota status captures service creation @ 08/24/23 13:10:39.972
  E0824 13:10:40.648472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:41.648265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 08/24/23 13:10:41.981
  STEP: Ensuring resource quota status released usage @ 08/24/23 13:10:42.081
  E0824 13:10:42.649344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:43.649418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:44.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7959" for this suite. @ 08/24/23 13:10:44.096
• [11.429 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 08/24/23 13:10:44.117
  Aug 24 13:10:44.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/24/23 13:10:44.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:44.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:44.153
  STEP: creating @ 08/24/23 13:10:44.16
  STEP: getting @ 08/24/23 13:10:44.19
  STEP: listing @ 08/24/23 13:10:44.201
  STEP: deleting @ 08/24/23 13:10:44.207
  Aug 24 13:10:44.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-7709" for this suite. @ 08/24/23 13:10:44.247
• [0.140 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 08/24/23 13:10:44.26
  Aug 24 13:10:44.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 13:10:44.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:44.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:44.295
  STEP: Given a ReplicationController is created @ 08/24/23 13:10:44.302
  STEP: When the matched label of one of its pods change @ 08/24/23 13:10:44.316
  Aug 24 13:10:44.323: INFO: Pod name pod-release: Found 0 pods out of 1
  E0824 13:10:44.650132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:45.650767      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:46.651483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:47.651620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:48.652381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:49.329: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/24/23 13:10:49.351
  E0824 13:10:49.653321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:50.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7712" for this suite. @ 08/24/23 13:10:50.386
• [6.137 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 08/24/23 13:10:50.398
  Aug 24 13:10:50.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:10:50.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:50.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:50.427
  E0824 13:10:50.653769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:51.654437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:52.654684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:53.654993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:54.655257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:55.655979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:10:56.579
  Aug 24 13:10:56.585: INFO: Trying to get logs from node pohje9aimahx-3 pod client-envvars-db85fed8-e363-40f5-a668-7a9354f49c5c container env3cont: <nil>
  STEP: delete the pod @ 08/24/23 13:10:56.6
  Aug 24 13:10:56.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:10:56.656014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "pods-8962" for this suite. @ 08/24/23 13:10:56.656
• [6.272 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 08/24/23 13:10:56.673
  Aug 24 13:10:56.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 13:10:56.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:56.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:56.734
  STEP: Creating service test in namespace statefulset-5277 @ 08/24/23 13:10:56.746
  Aug 24 13:10:56.816: INFO: Found 0 stateful pods, waiting for 1
  E0824 13:10:57.690228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:58.689780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:59.689324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:00.689597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:01.689709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:02.690319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:03.691346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:04.691346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:05.692198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:06.692119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:06.825: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 08/24/23 13:11:06.845
  W0824 13:11:06.867289      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 24 13:11:06.887: INFO: Found 1 stateful pods, waiting for 2
  E0824 13:11:07.692882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:08.693009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:09.692857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:10.692980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:11.693530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:12.693728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:13.693928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:14.694863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:15.695086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:16.695302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:16.897: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 13:11:16.897: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 08/24/23 13:11:16.913
  STEP: Delete all of the StatefulSets @ 08/24/23 13:11:16.921
  STEP: Verify that StatefulSets have been deleted @ 08/24/23 13:11:16.941
  Aug 24 13:11:16.951: INFO: Deleting all statefulset in ns statefulset-5277
  Aug 24 13:11:16.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5277" for this suite. @ 08/24/23 13:11:17.039
• [20.386 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 08/24/23 13:11:17.079
  Aug 24 13:11:17.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 13:11:17.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:17.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:17.141
  STEP: create the rc @ 08/24/23 13:11:17.156
  W0824 13:11:17.178230      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0824 13:11:17.718446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:18.718144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:19.718315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:20.718827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:21.718934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/24/23 13:11:22.188
  STEP: wait for all pods to be garbage collected @ 08/24/23 13:11:22.206
  E0824 13:11:22.719318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:23.719886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:24.720769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:25.720758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:26.720935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/24/23 13:11:27.222
  Aug 24 13:11:27.399: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 13:11:27.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1579" for this suite. @ 08/24/23 13:11:27.411
• [10.344 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 08/24/23 13:11:27.429
  Aug 24 13:11:27.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 13:11:27.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:27.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:27.481
  Aug 24 13:11:27.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:11:27.721998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:28.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4467" for this suite. @ 08/24/23 13:11:28.092
• [0.690 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 08/24/23 13:11:28.123
  Aug 24 13:11:28.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-webhook @ 08/24/23 13:11:28.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:28.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:28.159
  STEP: Setting up server cert @ 08/24/23 13:11:28.167
  E0824 13:11:28.722238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/24/23 13:11:29.299
  STEP: Deploying the custom resource conversion webhook pod @ 08/24/23 13:11:29.309
  STEP: Wait for the deployment to be ready @ 08/24/23 13:11:29.331
  Aug 24 13:11:29.351: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0824 13:11:29.722774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:30.722630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:31.387: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 13, 11, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 11, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 11, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 11, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:11:31.722846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:32.723048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:11:33.394
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:11:33.417
  E0824 13:11:33.723664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:34.417: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 24 13:11:34.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:11:34.724387      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:35.724866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:36.725283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/24/23 13:11:37.261
  STEP: v2 custom resource should be converted @ 08/24/23 13:11:37.27
  Aug 24 13:11:37.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:11:37.725631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-6415" for this suite. @ 08/24/23 13:11:37.947
• [9.838 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 08/24/23 13:11:37.962
  Aug 24 13:11:37.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 13:11:37.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:38.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:38.061
  E0824 13:11:38.728865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:39.730077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:40.730477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:41.731351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:42.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7237" for this suite. @ 08/24/23 13:11:42.116
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 08/24/23 13:11:42.142
  Aug 24 13:11:42.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:11:42.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:42.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:42.176
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:11:42.183
  E0824 13:11:42.732147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:43.732621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:44.733517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:45.734336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:11:46.23
  Aug 24 13:11:46.237: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-87789663-9540-44b1-9940-5d9b1463491a container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:11:46.25
  Aug 24 13:11:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9279" for this suite. @ 08/24/23 13:11:46.29
• [4.158 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 08/24/23 13:11:46.304
  Aug 24 13:11:46.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 13:11:46.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:46.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:46.338
  Aug 24 13:11:46.399: INFO: Create a RollingUpdate DaemonSet
  Aug 24 13:11:46.409: INFO: Check that daemon pods launch on every node of the cluster
  Aug 24 13:11:46.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:11:46.426: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:11:46.734515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:47.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:11:47.470: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:11:47.735543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:48.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 13:11:48.444: INFO: Node pohje9aimahx-3 is running 0 daemon pod, expected 1
  E0824 13:11:48.736379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:49.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 13:11:49.445: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Aug 24 13:11:49.445: INFO: Update the DaemonSet to trigger a rollout
  Aug 24 13:11:49.463: INFO: Updating DaemonSet daemon-set
  E0824 13:11:49.737382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:50.501: INFO: Roll back the DaemonSet before rollout is complete
  Aug 24 13:11:50.519: INFO: Updating DaemonSet daemon-set
  Aug 24 13:11:50.519: INFO: Make sure DaemonSet rollback is complete
  Aug 24 13:11:50.537: INFO: Wrong image for pod: daemon-set-s95hx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Aug 24 13:11:50.537: INFO: Pod daemon-set-s95hx is not available
  E0824 13:11:50.738147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:51.739591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:52.739338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:53.740046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:54.741145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:55.742013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:56.558: INFO: Pod daemon-set-f2bc4 is not available
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 13:11:56.579
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3398, will wait for the garbage collector to delete the pods @ 08/24/23 13:11:56.579
  Aug 24 13:11:56.649: INFO: Deleting DaemonSet.extensions daemon-set took: 12.926405ms
  E0824 13:11:56.742839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:56.750: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.394288ms
  E0824 13:11:57.743930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:58.744368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:11:59.361: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:11:59.361: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 13:11:59.368: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37364"},"items":null}

  Aug 24 13:11:59.374: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37364"},"items":null}

  Aug 24 13:11:59.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3398" for this suite. @ 08/24/23 13:11:59.411
• [13.126 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 08/24/23 13:11:59.433
  Aug 24 13:11:59.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:11:59.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:11:59.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:11:59.473
  STEP: Creating pod test-webserver-caf14d16-8957-4a5c-afaf-e43c29838705 in namespace container-probe-2439 @ 08/24/23 13:11:59.481
  E0824 13:11:59.745076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:00.746515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:01.746691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:02.746649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:03.536: INFO: Started pod test-webserver-caf14d16-8957-4a5c-afaf-e43c29838705 in namespace container-probe-2439
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:12:03.536
  Aug 24 13:12:03.544: INFO: Initial restart count of pod test-webserver-caf14d16-8957-4a5c-afaf-e43c29838705 is 0
  E0824 13:12:03.747296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:04.747848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:05.748555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:06.748633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:07.748994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:08.749975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:09.750490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:10.750803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:11.751863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:12.752977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:13.753588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:14.754278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:15.754949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:16.755786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:17.757345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:18.757469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:19.757972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:20.762525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:21.759872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:22.759396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:23.759704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:24.760730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:25.763087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:26.761949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:27.762506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:28.763114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:29.763031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:30.763340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:31.764973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:32.766147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:33.766538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:34.766773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:35.767473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:36.767997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:37.768707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:38.768979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:39.770044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:40.770747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:41.771604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:42.772477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:43.773753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:44.773683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:45.773811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:46.773984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:47.774209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:48.774363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:49.775881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:50.775769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:51.775856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:52.776301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:53.777438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:54.778159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:55.778956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:56.779644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:57.780273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:58.780902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:59.781398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:00.781872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:01.783104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:02.783816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:03.783746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:04.784725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:05.784267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:06.785195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:07.785546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:08.785920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:09.786516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:10.787056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:11.787881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:12.788202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:13.788305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:14.789057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:15.789707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:16.790262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:17.790264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:18.790615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:19.790908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:20.791105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:21.791177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:22.791293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:23.791614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:24.791555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:25.792195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:26.792727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:27.793062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:28.793761      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:29.794134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:30.794283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:31.794700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:32.794711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:33.794956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:34.795533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:35.795503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:36.796663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:37.796891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:38.797173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:39.797512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:40.797556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:41.798421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:42.799137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:43.799486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:44.799649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:45.800737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:46.800791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:47.800965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:48.802060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:49.802996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:50.804073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:51.805462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:52.804566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:53.804884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:54.805054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:55.805185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:56.806136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:57.806743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:58.806893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:59.807254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:00.807442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:01.808435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:02.809230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:03.809368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:04.810467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:05.810946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:06.811071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:07.811288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:08.811432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:09.811569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:10.812794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:11.812944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:12.812988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:13.813095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:14.813313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:15.813465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:16.813691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:17.813778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:18.813990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:19.814151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:20.816864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:21.817664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:22.817703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:23.818430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:24.818932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:25.819161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:26.820256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:27.820780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:28.821877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:29.822414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:30.822488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:31.823448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:32.826878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:33.825579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:34.826447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:35.826556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:36.826699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:37.826931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:38.828101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:39.834880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:40.832193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:41.832660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:42.832554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:43.833208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:44.832839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:45.833075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:46.833447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:47.834208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:48.834859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:49.835001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:50.835889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:51.836865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:52.837466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:53.837941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:54.838796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:55.839131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:56.839294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:57.839865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:58.840023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:59.840954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:00.841985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:01.842324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:02.842787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:03.842822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:04.843994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:05.844053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:06.844923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:07.845389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:08.845518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:09.845794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:10.846391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:11.847046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:12.848084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:13.848713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:14.848295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:15.849074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:16.849452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:17.849810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:18.849908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:19.850109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:20.850493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:21.851173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:22.851410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:23.851715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:24.852560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:25.853075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:26.853951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:27.854623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:28.855594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:29.857662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:30.857065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:31.857608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:32.858348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:33.858637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:34.859301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:35.859916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:36.860941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:37.861272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:38.862043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:39.862154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:40.862974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:41.863236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:42.864385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:43.864466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:44.865269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:45.865952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:46.866695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:47.867353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:48.867600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:49.868739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:50.869315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:51.869918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:52.870657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:53.871414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:54.872138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:55.872910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:56.873160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:57.873384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:58.874405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:59.874615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:00.875350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:01.875601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:02.876240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:03.876898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:04.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:16:04.68
  STEP: Destroying namespace "container-probe-2439" for this suite. @ 08/24/23 13:16:04.707
• [245.329 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 08/24/23 13:16:04.767
  Aug 24 13:16:04.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 13:16:04.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:04.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:04.813
  STEP: create the container @ 08/24/23 13:16:04.819
  W0824 13:16:04.831901      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/24/23 13:16:04.832
  E0824 13:16:04.877958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:05.878744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:06.878762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/24/23 13:16:07.869
  STEP: the container should be terminated @ 08/24/23 13:16:07.875
  STEP: the termination message should be set @ 08/24/23 13:16:07.877
  Aug 24 13:16:07.877: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 08/24/23 13:16:07.878
  E0824 13:16:07.879201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:07.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1198" for this suite. @ 08/24/23 13:16:07.923
• [3.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 08/24/23 13:16:07.939
  Aug 24 13:16:07.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 13:16:07.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:07.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:07.973
  Aug 24 13:16:07.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:16:08.879414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:09.880432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:10.884787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:11.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1617" for this suite. @ 08/24/23 13:16:11.412
• [3.483 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 08/24/23 13:16:11.427
  Aug 24 13:16:11.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:16:11.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:11.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:11.464
  STEP: Creating pod test-grpc-0bf79ac4-30fa-459a-a93f-7a9a01464286 in namespace container-probe-1497 @ 08/24/23 13:16:11.474
  E0824 13:16:11.885709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:12.886056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:13.517: INFO: Started pod test-grpc-0bf79ac4-30fa-459a-a93f-7a9a01464286 in namespace container-probe-1497
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:16:13.517
  Aug 24 13:16:13.525: INFO: Initial restart count of pod test-grpc-0bf79ac4-30fa-459a-a93f-7a9a01464286 is 0
  E0824 13:16:13.886326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:14.886780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:15.887579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:16.887928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:17.888325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:18.889601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:19.889673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:20.889926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:21.890074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:22.890503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:23.891483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:24.891621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:25.892667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:26.893437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:27.894075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:28.894199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:29.894870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:30.894753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:31.896597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:32.895290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:33.896283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:34.896208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:35.897037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:36.897267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:37.897861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:38.898234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:39.898283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:40.898540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:41.899215      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:42.900001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:43.900700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:44.900766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:45.900918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:46.901140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:47.901719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:48.901735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:49.902852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:50.903154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:51.903989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:52.904204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:53.904447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:54.904662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:55.905320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:56.905558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:57.906604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:58.907251      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:59.907443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:00.907644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:01.908592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:02.909276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:03.909741      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:04.910223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:05.911129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:06.911634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:07.912189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:08.913240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:09.914814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:10.916184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:11.916682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:12.916544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:13.917368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:14.918067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:15.918371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:16.919124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:17.919127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:18.920306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:19.920952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:20.921645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:21.922374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:22.922696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:23.923008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:24.927138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:25.925471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:26.926165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:27.926909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:28.927657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:29.928280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:30.929239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:31.928809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:32.929796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:33.929943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:34.930224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:35.931046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:36.931285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:37.932934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:38.933189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:39.934124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:40.935670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:41.935841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:42.936252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:43.936394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:44.936594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:45.936990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:46.937023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:47.937244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:48.937295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:49.937796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:50.938163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:51.938386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:52.938582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:53.938911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:54.939501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:55.940095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:56.940434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:57.940638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:58.941476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:59.941728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:00.942331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:01.942510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:02.942648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:03.943764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:04.943877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:05.944244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:06.944703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:07.945661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:08.945587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:09.945908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:10.946436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:11.946583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:12.946807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:13.946867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:14.947097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:15.947197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:16.947496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:17.948103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:18.948700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:19.949319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:20.950351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:21.951304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:22.951346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:23.952102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:24.952314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:25.952709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:26.952932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:27.953814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:28.954167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:29.954797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:30.954922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:31.955073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:32.955504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:33.955825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:34.956473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:35.956936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:36.957284      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:37.957921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:38.958639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:39.959025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:40.959270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:41.959750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:42.961362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:43.960390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:44.960798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:45.961000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:46.961979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:47.962619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:48.964052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:49.964789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:50.965102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:51.965377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:52.965383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:53.966594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:54.966712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:55.967117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:56.967524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:57.967621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:58.968279      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:59.969078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:00.969691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:01.969876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:02.970329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:03.971205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:04.971575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:05.972213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:06.972660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:07.972942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:08.972884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:09.973142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:10.973986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:11.974447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:12.974914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:13.975308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:14.976351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:15.977433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:16.982258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:17.981192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:18.982150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:19.982356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:20.983384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:21.983970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:22.983947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:23.984974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:24.986098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:25.986437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:26.986756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:27.986951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:28.987788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:29.987718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:30.988779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:31.988885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:32.992276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:33.990343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:34.990513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:35.990802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:36.991775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:37.992039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:38.992748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:39.993319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:40.993424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:41.993979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:42.995204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:43.995473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:44.996347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:45.996911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:46.997104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:47.997288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:48.997665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:49.997867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:50.998823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:51.999298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:52.999459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:54.000209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:55.000546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:56.000783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:57.001021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:58.001979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:59.002367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:00.002594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:01.003000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:02.003455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:03.003705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:04.003861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:05.004582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:06.005240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:07.006167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:08.006099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:09.006669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:10.006810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:11.007498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:12.007809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:13.008245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:14.008903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:14.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:20:14.746
  STEP: Destroying namespace "container-probe-1497" for this suite. @ 08/24/23 13:20:14.78
• [243.374 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 08/24/23 13:20:14.805
  Aug 24 13:20:14.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 13:20:14.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:14.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:14.878
  E0824 13:20:15.009756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:16.010119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:16.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3126" for this suite. @ 08/24/23 13:20:16.986
• [2.196 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 08/24/23 13:20:17.006
  Aug 24 13:20:17.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:20:17.009
  E0824 13:20:17.010068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:17.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:17.051
  STEP: Setting up server cert @ 08/24/23 13:20:17.107
  E0824 13:20:18.010342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:20:18.742
  STEP: Deploying the webhook pod @ 08/24/23 13:20:18.764
  STEP: Wait for the deployment to be ready @ 08/24/23 13:20:18.86
  Aug 24 13:20:18.882: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 13:20:19.010585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:20.010583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:20:20.904
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:20:20.932
  E0824 13:20:21.011633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:21.932: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 08/24/23 13:20:21.94
  STEP: create a namespace for the webhook @ 08/24/23 13:20:21.982
  E0824 13:20:22.012076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap should be unconditionally rejected by the webhook @ 08/24/23 13:20:22.022
  Aug 24 13:20:22.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3456" for this suite. @ 08/24/23 13:20:22.188
  STEP: Destroying namespace "webhook-markers-3376" for this suite. @ 08/24/23 13:20:22.225
  STEP: Destroying namespace "fail-closed-namespace-9388" for this suite. @ 08/24/23 13:20:22.269
• [5.288 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 08/24/23 13:20:22.297
  Aug 24 13:20:22.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:20:22.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:22.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:22.34
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:20:22.349
  E0824 13:20:23.020032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:24.013679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:25.013555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:26.014380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:20:26.406
  Aug 24 13:20:26.416: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-9681066e-fa14-4c2e-9681-45a25d7cfe01 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:20:26.428
  Aug 24 13:20:26.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6903" for this suite. @ 08/24/23 13:20:26.485
• [4.206 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 08/24/23 13:20:26.504
  Aug 24 13:20:26.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename discovery @ 08/24/23 13:20:26.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:26.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:26.56
  STEP: Setting up server cert @ 08/24/23 13:20:26.574
  E0824 13:20:27.015149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:27.236: INFO: Checking APIGroup: apiregistration.k8s.io
  Aug 24 13:20:27.239: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Aug 24 13:20:27.239: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Aug 24 13:20:27.239: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Aug 24 13:20:27.239: INFO: Checking APIGroup: apps
  Aug 24 13:20:27.244: INFO: PreferredVersion.GroupVersion: apps/v1
  Aug 24 13:20:27.244: INFO: Versions found [{apps/v1 v1}]
  Aug 24 13:20:27.244: INFO: apps/v1 matches apps/v1
  Aug 24 13:20:27.244: INFO: Checking APIGroup: events.k8s.io
  Aug 24 13:20:27.248: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Aug 24 13:20:27.248: INFO: Versions found [{events.k8s.io/v1 v1}]
  Aug 24 13:20:27.248: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Aug 24 13:20:27.248: INFO: Checking APIGroup: authentication.k8s.io
  Aug 24 13:20:27.251: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Aug 24 13:20:27.251: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Aug 24 13:20:27.251: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Aug 24 13:20:27.251: INFO: Checking APIGroup: authorization.k8s.io
  Aug 24 13:20:27.255: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Aug 24 13:20:27.255: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Aug 24 13:20:27.255: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Aug 24 13:20:27.255: INFO: Checking APIGroup: autoscaling
  Aug 24 13:20:27.265: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Aug 24 13:20:27.265: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Aug 24 13:20:27.265: INFO: autoscaling/v2 matches autoscaling/v2
  Aug 24 13:20:27.265: INFO: Checking APIGroup: batch
  Aug 24 13:20:27.270: INFO: PreferredVersion.GroupVersion: batch/v1
  Aug 24 13:20:27.270: INFO: Versions found [{batch/v1 v1}]
  Aug 24 13:20:27.270: INFO: batch/v1 matches batch/v1
  Aug 24 13:20:27.270: INFO: Checking APIGroup: certificates.k8s.io
  Aug 24 13:20:27.279: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Aug 24 13:20:27.279: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Aug 24 13:20:27.279: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Aug 24 13:20:27.280: INFO: Checking APIGroup: networking.k8s.io
  Aug 24 13:20:27.283: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Aug 24 13:20:27.283: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Aug 24 13:20:27.283: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Aug 24 13:20:27.284: INFO: Checking APIGroup: policy
  Aug 24 13:20:27.289: INFO: PreferredVersion.GroupVersion: policy/v1
  Aug 24 13:20:27.289: INFO: Versions found [{policy/v1 v1}]
  Aug 24 13:20:27.289: INFO: policy/v1 matches policy/v1
  Aug 24 13:20:27.289: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Aug 24 13:20:27.291: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Aug 24 13:20:27.291: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Aug 24 13:20:27.291: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Aug 24 13:20:27.291: INFO: Checking APIGroup: storage.k8s.io
  Aug 24 13:20:27.303: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Aug 24 13:20:27.304: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Aug 24 13:20:27.304: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Aug 24 13:20:27.304: INFO: Checking APIGroup: admissionregistration.k8s.io
  Aug 24 13:20:27.307: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Aug 24 13:20:27.307: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Aug 24 13:20:27.307: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Aug 24 13:20:27.307: INFO: Checking APIGroup: apiextensions.k8s.io
  Aug 24 13:20:27.310: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Aug 24 13:20:27.310: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Aug 24 13:20:27.310: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Aug 24 13:20:27.310: INFO: Checking APIGroup: scheduling.k8s.io
  Aug 24 13:20:27.315: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Aug 24 13:20:27.315: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Aug 24 13:20:27.315: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Aug 24 13:20:27.315: INFO: Checking APIGroup: coordination.k8s.io
  Aug 24 13:20:27.320: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Aug 24 13:20:27.320: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Aug 24 13:20:27.320: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Aug 24 13:20:27.320: INFO: Checking APIGroup: node.k8s.io
  Aug 24 13:20:27.326: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Aug 24 13:20:27.326: INFO: Versions found [{node.k8s.io/v1 v1}]
  Aug 24 13:20:27.326: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Aug 24 13:20:27.326: INFO: Checking APIGroup: discovery.k8s.io
  Aug 24 13:20:27.329: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Aug 24 13:20:27.331: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Aug 24 13:20:27.331: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Aug 24 13:20:27.332: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Aug 24 13:20:27.335: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Aug 24 13:20:27.335: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Aug 24 13:20:27.335: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Aug 24 13:20:27.335: INFO: Checking APIGroup: cilium.io
  Aug 24 13:20:27.337: INFO: PreferredVersion.GroupVersion: cilium.io/v2
  Aug 24 13:20:27.337: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
  Aug 24 13:20:27.337: INFO: cilium.io/v2 matches cilium.io/v2
  Aug 24 13:20:27.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1602" for this suite. @ 08/24/23 13:20:27.355
• [0.882 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 08/24/23 13:20:27.387
  Aug 24 13:20:27.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 13:20:27.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:27.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:27.463
  E0824 13:20:28.016023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:29.016375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:30.016537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:31.016908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:32.017019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:33.019736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:33.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3188" for this suite. @ 08/24/23 13:20:33.627
• [6.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 08/24/23 13:20:33.648
  Aug 24 13:20:33.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:20:33.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:33.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:33.69
  STEP: Creating a pod to test downward api env vars @ 08/24/23 13:20:33.696
  E0824 13:20:34.017575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:35.017872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:36.018247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:37.018480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:20:37.738
  Aug 24 13:20:37.749: INFO: Trying to get logs from node pohje9aimahx-3 pod downward-api-d246a9ca-089f-4870-b00a-08208a7b0b7c container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 13:20:37.762
  Aug 24 13:20:37.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2772" for this suite. @ 08/24/23 13:20:37.802
• [4.166 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 08/24/23 13:20:37.815
  Aug 24 13:20:37.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 13:20:37.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:37.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:37.865
  STEP: create the container @ 08/24/23 13:20:37.871
  W0824 13:20:37.892937      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/24/23 13:20:37.893
  E0824 13:20:38.019543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:39.020034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:40.019967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/24/23 13:20:40.932
  STEP: the container should be terminated @ 08/24/23 13:20:40.938
  STEP: the termination message should be set @ 08/24/23 13:20:40.938
  Aug 24 13:20:40.938: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 08/24/23 13:20:40.938
  Aug 24 13:20:40.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1859" for this suite. @ 08/24/23 13:20:40.985
• [3.201 seconds]
------------------------------
SSSSSSSSS  E0824 13:20:41.020083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 08/24/23 13:20:41.024
  Aug 24 13:20:41.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename containers @ 08/24/23 13:20:41.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:41.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:41.065
  STEP: Creating a pod to test override all @ 08/24/23 13:20:41.069
  E0824 13:20:42.020300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:43.020912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:44.022068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:45.022654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:20:45.116
  Aug 24 13:20:45.128: INFO: Trying to get logs from node pohje9aimahx-3 pod client-containers-6ecfc5c4-cf70-48f0-9382-7ccedc915029 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:20:45.154
  Aug 24 13:20:45.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8005" for this suite. @ 08/24/23 13:20:45.216
• [4.215 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 08/24/23 13:20:45.241
  Aug 24 13:20:45.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 13:20:45.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:45.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:45.324
  Aug 24 13:20:45.385: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 13:20:45.405
  Aug 24 13:20:45.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:20:45.430: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:20:46.023004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:46.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:20:46.461: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:20:47.024435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:47.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 13:20:47.454: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:20:48.024256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:48.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 13:20:48.455: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:20:49.024415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:49.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 13:20:49.454: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 08/24/23 13:20:49.485
  STEP: Check that daemon pods images are updated. @ 08/24/23 13:20:49.51
  Aug 24 13:20:49.532: INFO: Wrong image for pod: daemon-set-5gsdx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 13:20:49.532: INFO: Wrong image for pod: daemon-set-cn68z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 13:20:49.532: INFO: Wrong image for pod: daemon-set-qxhbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0824 13:20:50.024833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:50.571: INFO: Pod daemon-set-8gvnb is not available
  Aug 24 13:20:50.572: INFO: Wrong image for pod: daemon-set-cn68z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 13:20:50.572: INFO: Wrong image for pod: daemon-set-qxhbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0824 13:20:51.025093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:51.572: INFO: Pod daemon-set-8gvnb is not available
  Aug 24 13:20:51.572: INFO: Wrong image for pod: daemon-set-cn68z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 13:20:51.573: INFO: Wrong image for pod: daemon-set-qxhbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0824 13:20:52.025299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:52.586: INFO: Wrong image for pod: daemon-set-cn68z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 13:20:52.586: INFO: Pod daemon-set-hk46p is not available
  E0824 13:20:53.026267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:53.572: INFO: Wrong image for pod: daemon-set-cn68z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 13:20:53.572: INFO: Pod daemon-set-hk46p is not available
  E0824 13:20:54.026400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:55.026208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:55.572: INFO: Pod daemon-set-2vqnd is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 08/24/23 13:20:55.585
  Aug 24 13:20:55.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 13:20:55.618: INFO: Node pohje9aimahx-2 is running 0 daemon pod, expected 1
  E0824 13:20:56.026983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:56.639: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 13:20:56.639: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 13:20:56.671
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7850, will wait for the garbage collector to delete the pods @ 08/24/23 13:20:56.671
  Aug 24 13:20:56.742: INFO: Deleting DaemonSet.extensions daemon-set took: 16.044126ms
  Aug 24 13:20:56.843: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.875021ms
  E0824 13:20:57.027201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:58.028200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:20:58.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:20:58.551: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 13:20:58.557: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39155"},"items":null}

  Aug 24 13:20:58.564: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39155"},"items":null}

  Aug 24 13:20:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7850" for this suite. @ 08/24/23 13:20:58.634
• [13.415 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 08/24/23 13:20:58.659
  Aug 24 13:20:58.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 13:20:58.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:20:58.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:20:58.686
  STEP: creating service multi-endpoint-test in namespace services-9393 @ 08/24/23 13:20:58.69
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[] @ 08/24/23 13:20:58.714
  Aug 24 13:20:58.738: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-9393 @ 08/24/23 13:20:58.738
  E0824 13:20:59.028966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:00.029762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[pod1:[100]] @ 08/24/23 13:21:00.784
  Aug 24 13:21:00.841: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-9393 @ 08/24/23 13:21:00.841
  E0824 13:21:01.030075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:02.030144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:03.030725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:04.031556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[pod1:[100] pod2:[101]] @ 08/24/23 13:21:04.913
  Aug 24 13:21:04.950: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 08/24/23 13:21:04.95
  Aug 24 13:21:04.950: INFO: Creating new exec pod
  E0824 13:21:05.032807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:06.032659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:07.033044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:07.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9393 exec execpodbhz4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  E0824 13:21:08.033526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:08.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Aug 24 13:21:08.316: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:21:08.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9393 exec execpodbhz4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.41 80'
  Aug 24 13:21:08.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.41 80\nConnection to 10.233.5.41 80 port [tcp/http] succeeded!\n"
  Aug 24 13:21:08.663: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:21:08.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9393 exec execpodbhz4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  E0824 13:21:09.034270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:09.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Aug 24 13:21:09.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:21:09.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=services-9393 exec execpodbhz4r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.5.41 81'
  Aug 24 13:21:09.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.5.41 81\nConnection to 10.233.5.41 81 port [tcp/*] succeeded!\n"
  Aug 24 13:21:09.347: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-9393 @ 08/24/23 13:21:09.347
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[pod2:[101]] @ 08/24/23 13:21:09.371
  E0824 13:21:10.034675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:10.469: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-9393 @ 08/24/23 13:21:10.469
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9393 to expose endpoints map[] @ 08/24/23 13:21:10.552
  E0824 13:21:11.034820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:11.607: INFO: successfully validated that service multi-endpoint-test in namespace services-9393 exposes endpoints map[]
  Aug 24 13:21:11.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9393" for this suite. @ 08/24/23 13:21:11.66
• [13.026 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 08/24/23 13:21:11.688
  Aug 24 13:21:11.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:21:11.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:11.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:11.727
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 13:21:11.732
  Aug 24 13:21:11.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-342 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 24 13:21:11.920: INFO: stderr: ""
  Aug 24 13:21:11.920: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 08/24/23 13:21:11.92
  Aug 24 13:21:11.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-342 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  E0824 13:21:12.035454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:12.121: INFO: stderr: ""
  Aug 24 13:21:12.121: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 13:21:12.121
  Aug 24 13:21:12.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-342 delete pods e2e-test-httpd-pod'
  E0824 13:21:13.036793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:14.037598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:14.815: INFO: stderr: ""
  Aug 24 13:21:14.815: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 24 13:21:14.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-342" for this suite. @ 08/24/23 13:21:14.828
• [3.159 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 08/24/23 13:21:14.848
  Aug 24 13:21:14.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename configmap @ 08/24/23 13:21:14.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:14.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:14.886
  Aug 24 13:21:15.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8988" for this suite. @ 08/24/23 13:21:15.014
• [0.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSS  E0824 13:21:15.037560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 08/24/23 13:21:15.046
  Aug 24 13:21:15.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename events @ 08/24/23 13:21:15.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:15.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:15.093
  STEP: creating a test event @ 08/24/23 13:21:15.098
  STEP: listing events in all namespaces @ 08/24/23 13:21:15.11
  STEP: listing events in test namespace @ 08/24/23 13:21:15.121
  STEP: listing events with field selection filtering on source @ 08/24/23 13:21:15.128
  STEP: listing events with field selection filtering on reportingController @ 08/24/23 13:21:15.135
  STEP: getting the test event @ 08/24/23 13:21:15.142
  STEP: patching the test event @ 08/24/23 13:21:15.147
  STEP: getting the test event @ 08/24/23 13:21:15.163
  STEP: updating the test event @ 08/24/23 13:21:15.169
  STEP: getting the test event @ 08/24/23 13:21:15.184
  STEP: deleting the test event @ 08/24/23 13:21:15.189
  STEP: listing events in all namespaces @ 08/24/23 13:21:15.204
  STEP: listing events in test namespace @ 08/24/23 13:21:15.212
  Aug 24 13:21:15.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8893" for this suite. @ 08/24/23 13:21:15.236
• [0.207 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 08/24/23 13:21:15.259
  Aug 24 13:21:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 13:21:15.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:15.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:15.294
  Aug 24 13:21:15.301: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0824 13:21:16.038371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 08/24/23 13:21:16.336
  STEP: Checking rc "condition-test" has the desired failure condition set @ 08/24/23 13:21:16.351
  E0824 13:21:17.039053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 08/24/23 13:21:17.368
  Aug 24 13:21:17.399: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 08/24/23 13:21:17.4
  E0824 13:21:18.039445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:18.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7350" for this suite. @ 08/24/23 13:21:18.425
• [3.177 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 08/24/23 13:21:18.441
  Aug 24 13:21:18.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 13:21:18.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:18.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:18.476
  STEP: Performing setup for networking test in namespace pod-network-test-1595 @ 08/24/23 13:21:18.48
  STEP: creating a selector @ 08/24/23 13:21:18.48
  STEP: Creating the service pods in kubernetes @ 08/24/23 13:21:18.48
  Aug 24 13:21:18.481: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0824 13:21:19.039629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:20.040141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:21.040414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:22.041132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:23.042074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:24.042844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:25.043094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:26.043458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:27.043924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:28.044996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:29.045017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:30.045196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:31.046038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:32.046202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:33.046558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:34.047305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:35.047354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:36.047614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:37.048984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:38.049008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:39.049883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:40.050049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/24/23 13:21:40.749
  E0824 13:21:41.050743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:42.051051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:43.051175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:44.051266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:44.886: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 13:21:44.886: INFO: Going to poll 10.233.64.52 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 13:21:44.893: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1595 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:21:44.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:21:44.895: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:21:44.896: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1595/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0824 13:21:45.052275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:45.074: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 24 13:21:45.074: INFO: Going to poll 10.233.65.205 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 13:21:45.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.205:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1595 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:21:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:21:45.086: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:21:45.086: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1595/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.205%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 13:21:45.223: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 24 13:21:45.224: INFO: Going to poll 10.233.66.234 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 13:21:45.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1595 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:21:45.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:21:45.233: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:21:45.233: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1595/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 13:21:45.372: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 24 13:21:45.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1595" for this suite. @ 08/24/23 13:21:45.38
• [26.952 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 08/24/23 13:21:45.393
  Aug 24 13:21:45.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 13:21:45.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:45.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:45.432
  Aug 24 13:21:45.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7989" for this suite. @ 08/24/23 13:21:45.451
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 08/24/23 13:21:45.469
  Aug 24 13:21:45.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:21:45.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:45.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:45.513
  STEP: Creating a pod to test emptydir volume type on node default medium @ 08/24/23 13:21:45.517
  E0824 13:21:46.053321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:47.053516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:48.054561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:49.055519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:21:49.591
  Aug 24 13:21:49.597: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-fe291471-eaf8-4a7f-8291-28a36bbcf894 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:21:49.613
  Aug 24 13:21:49.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7359" for this suite. @ 08/24/23 13:21:49.658
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 08/24/23 13:21:49.674
  Aug 24 13:21:49.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:21:49.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:49.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:49.71
  STEP: Creating configMap with name projected-configmap-test-volume-968c432e-569c-46ae-9f2a-bc4f4b05ad95 @ 08/24/23 13:21:49.716
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:21:49.725
  E0824 13:21:50.056201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:51.055685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:52.056636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:53.059112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:21:53.792
  Aug 24 13:21:53.799: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-56914fd0-796a-4d60-877c-798010b45021 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:21:53.813
  Aug 24 13:21:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7849" for this suite. @ 08/24/23 13:21:53.856
• [4.197 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 08/24/23 13:21:53.877
  Aug 24 13:21:53.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:21:53.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:53.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:53.918
  STEP: Setting up server cert @ 08/24/23 13:21:53.97
  E0824 13:21:54.058137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:21:54.496
  STEP: Deploying the webhook pod @ 08/24/23 13:21:54.506
  STEP: Wait for the deployment to be ready @ 08/24/23 13:21:54.537
  Aug 24 13:21:54.567: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 13:21:55.058312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:56.059271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:21:56.607
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:21:56.653
  E0824 13:21:57.060393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:57.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 13:21:57.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:21:58.059932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8064-crds.webhook.example.com via the AdmissionRegistration API @ 08/24/23 13:21:58.189
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/24/23 13:21:58.231
  E0824 13:21:59.060514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:00.060719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:00.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:22:01.060965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2067" for this suite. @ 08/24/23 13:22:01.075
  STEP: Destroying namespace "webhook-markers-3012" for this suite. @ 08/24/23 13:22:01.092
• [7.237 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 08/24/23 13:22:01.119
  Aug 24 13:22:01.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:22:01.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:01.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:01.176
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 13:22:01.194
  Aug 24 13:22:01.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-7583 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 24 13:22:01.357: INFO: stderr: ""
  Aug 24 13:22:01.357: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 08/24/23 13:22:01.357
  E0824 13:22:02.061123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:03.061114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:04.061973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:05.062283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:06.062581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/24/23 13:22:06.409
  Aug 24 13:22:06.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-7583 get pod e2e-test-httpd-pod -o json'
  Aug 24 13:22:06.641: INFO: stderr: ""
  Aug 24 13:22:06.641: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-24T13:22:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7583\",\n        \"resourceVersion\": \"39800\",\n        \"uid\": \"b59be086-4fec-49f4-9a5b-98c4d5409044\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-jf4f7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pohje9aimahx-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-jf4f7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T13:22:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T13:22:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T13:22:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T13:22:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://1eb392e3c84a5bbf9768e4251fb82741ce29bce30d4472ca538eb6a0c7dbc8ea\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-24T13:22:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.61\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.61\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-24T13:22:01Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 08/24/23 13:22:06.641
  Aug 24 13:22:06.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-7583 replace -f -'
  E0824 13:22:07.063367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:07.518: INFO: stderr: ""
  Aug 24 13:22:07.518: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 08/24/23 13:22:07.518
  Aug 24 13:22:07.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-7583 delete pods e2e-test-httpd-pod'
  E0824 13:22:08.063833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:09.063972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:09.104: INFO: stderr: ""
  Aug 24 13:22:09.104: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 24 13:22:09.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7583" for this suite. @ 08/24/23 13:22:09.121
• [8.014 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 08/24/23 13:22:09.134
  Aug 24 13:22:09.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 13:22:09.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:09.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:09.172
  STEP: create the container @ 08/24/23 13:22:09.177
  W0824 13:22:09.194493      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 08/24/23 13:22:09.194
  E0824 13:22:10.064741      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:11.064727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:12.065735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/24/23 13:22:12.245
  STEP: the container should be terminated @ 08/24/23 13:22:12.252
  STEP: the termination message should be set @ 08/24/23 13:22:12.253
  Aug 24 13:22:12.253: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/24/23 13:22:12.253
  Aug 24 13:22:12.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7468" for this suite. @ 08/24/23 13:22:12.293
• [3.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 08/24/23 13:22:12.315
  Aug 24 13:22:12.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:22:12.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:12.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:12.358
  STEP: creating a replication controller @ 08/24/23 13:22:12.364
  Aug 24 13:22:12.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 create -f -'
  E0824 13:22:13.066569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:13.155: INFO: stderr: ""
  Aug 24 13:22:13.155: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 13:22:13.155
  Aug 24 13:22:13.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:22:13.372: INFO: stderr: ""
  Aug 24 13:22:13.372: INFO: stdout: "update-demo-nautilus-dldq9 update-demo-nautilus-j8kpc "
  Aug 24 13:22:13.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-dldq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:13.572: INFO: stderr: ""
  Aug 24 13:22:13.572: INFO: stdout: ""
  Aug 24 13:22:13.572: INFO: update-demo-nautilus-dldq9 is created but not running
  E0824 13:22:14.067254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:15.067428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:16.067718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:17.067824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:18.068072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:18.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:22:18.801: INFO: stderr: ""
  Aug 24 13:22:18.801: INFO: stdout: "update-demo-nautilus-dldq9 update-demo-nautilus-j8kpc "
  Aug 24 13:22:18.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-dldq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:18.975: INFO: stderr: ""
  Aug 24 13:22:18.975: INFO: stdout: "true"
  Aug 24 13:22:18.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-dldq9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0824 13:22:19.068331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:19.136: INFO: stderr: ""
  Aug 24 13:22:19.136: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:22:19.136: INFO: validating pod update-demo-nautilus-dldq9
  Aug 24 13:22:19.153: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:22:19.153: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:22:19.153: INFO: update-demo-nautilus-dldq9 is verified up and running
  Aug 24 13:22:19.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:19.295: INFO: stderr: ""
  Aug 24 13:22:19.295: INFO: stdout: "true"
  Aug 24 13:22:19.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:22:19.422: INFO: stderr: ""
  Aug 24 13:22:19.422: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:22:19.422: INFO: validating pod update-demo-nautilus-j8kpc
  Aug 24 13:22:19.438: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:22:19.438: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:22:19.438: INFO: update-demo-nautilus-j8kpc is verified up and running
  STEP: scaling down the replication controller @ 08/24/23 13:22:19.438
  Aug 24 13:22:19.456: INFO: scanned /root for discovery docs: <nil>
  Aug 24 13:22:19.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0824 13:22:20.068616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:20.641: INFO: stderr: ""
  Aug 24 13:22:20.641: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 13:22:20.641
  Aug 24 13:22:20.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:22:20.790: INFO: stderr: ""
  Aug 24 13:22:20.790: INFO: stdout: "update-demo-nautilus-j8kpc "
  Aug 24 13:22:20.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:20.952: INFO: stderr: ""
  Aug 24 13:22:20.952: INFO: stdout: "true"
  Aug 24 13:22:20.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0824 13:22:21.069119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:21.082: INFO: stderr: ""
  Aug 24 13:22:21.082: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:22:21.082: INFO: validating pod update-demo-nautilus-j8kpc
  Aug 24 13:22:21.089: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:22:21.089: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:22:21.089: INFO: update-demo-nautilus-j8kpc is verified up and running
  STEP: scaling up the replication controller @ 08/24/23 13:22:21.089
  Aug 24 13:22:21.103: INFO: scanned /root for discovery docs: <nil>
  Aug 24 13:22:21.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0824 13:22:22.069323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:22.298: INFO: stderr: ""
  Aug 24 13:22:22.298: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 13:22:22.298
  Aug 24 13:22:22.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:22:22.429: INFO: stderr: ""
  Aug 24 13:22:22.429: INFO: stdout: "update-demo-nautilus-j8kpc update-demo-nautilus-vfbn7 "
  Aug 24 13:22:22.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:22.559: INFO: stderr: ""
  Aug 24 13:22:22.560: INFO: stdout: "true"
  Aug 24 13:22:22.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:22:22.693: INFO: stderr: ""
  Aug 24 13:22:22.693: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:22:22.693: INFO: validating pod update-demo-nautilus-j8kpc
  Aug 24 13:22:22.701: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:22:22.701: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:22:22.701: INFO: update-demo-nautilus-j8kpc is verified up and running
  Aug 24 13:22:22.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-vfbn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:22.846: INFO: stderr: ""
  Aug 24 13:22:22.846: INFO: stdout: ""
  Aug 24 13:22:22.846: INFO: update-demo-nautilus-vfbn7 is created but not running
  E0824 13:22:23.070204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:24.071274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:25.071388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:26.071862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:27.071967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:27.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 13:22:28.013: INFO: stderr: ""
  Aug 24 13:22:28.013: INFO: stdout: "update-demo-nautilus-j8kpc update-demo-nautilus-vfbn7 "
  Aug 24 13:22:28.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0824 13:22:28.072886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:28.189: INFO: stderr: ""
  Aug 24 13:22:28.189: INFO: stdout: "true"
  Aug 24 13:22:28.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-j8kpc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:22:28.331: INFO: stderr: ""
  Aug 24 13:22:28.331: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:22:28.331: INFO: validating pod update-demo-nautilus-j8kpc
  Aug 24 13:22:28.346: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:22:28.346: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:22:28.346: INFO: update-demo-nautilus-j8kpc is verified up and running
  Aug 24 13:22:28.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-vfbn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 13:22:28.488: INFO: stderr: ""
  Aug 24 13:22:28.488: INFO: stdout: "true"
  Aug 24 13:22:28.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods update-demo-nautilus-vfbn7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 13:22:28.623: INFO: stderr: ""
  Aug 24 13:22:28.624: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 13:22:28.624: INFO: validating pod update-demo-nautilus-vfbn7
  Aug 24 13:22:28.648: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 13:22:28.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 13:22:28.648: INFO: update-demo-nautilus-vfbn7 is verified up and running
  STEP: using delete to clean up resources @ 08/24/23 13:22:28.649
  Aug 24 13:22:28.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 delete --grace-period=0 --force -f -'
  Aug 24 13:22:28.825: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 13:22:28.825: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 24 13:22:28.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get rc,svc -l name=update-demo --no-headers'
  Aug 24 13:22:29.062: INFO: stderr: "No resources found in kubectl-586 namespace.\n"
  Aug 24 13:22:29.062: INFO: stdout: ""
  Aug 24 13:22:29.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-586 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  E0824 13:22:29.073320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:29.255: INFO: stderr: ""
  Aug 24 13:22:29.255: INFO: stdout: ""
  Aug 24 13:22:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-586" for this suite. @ 08/24/23 13:22:29.269
• [16.975 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 08/24/23 13:22:29.291
  Aug 24 13:22:29.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/24/23 13:22:29.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:29.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:29.346
  E0824 13:22:30.073697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:31.074042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:31.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 08/24/23 13:22:31.438
  STEP: Cleaning up the configmap @ 08/24/23 13:22:31.45
  STEP: Cleaning up the pod @ 08/24/23 13:22:31.461
  STEP: Destroying namespace "emptydir-wrapper-546" for this suite. @ 08/24/23 13:22:31.483
• [2.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 08/24/23 13:22:31.51
  Aug 24 13:22:31.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:22:31.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:31.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:31.546
  STEP: Creating resourceQuota "e2e-rq-status-77sgc" @ 08/24/23 13:22:31.559
  Aug 24 13:22:31.579: INFO: Resource quota "e2e-rq-status-77sgc" reports spec: hard cpu limit of 500m
  Aug 24 13:22:31.579: INFO: Resource quota "e2e-rq-status-77sgc" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-77sgc" /status @ 08/24/23 13:22:31.579
  STEP: Confirm /status for "e2e-rq-status-77sgc" resourceQuota via watch @ 08/24/23 13:22:31.635
  Aug 24 13:22:31.638: INFO: observed resourceQuota "e2e-rq-status-77sgc" in namespace "resourcequota-4056" with hard status: v1.ResourceList(nil)
  Aug 24 13:22:31.638: INFO: Found resourceQuota "e2e-rq-status-77sgc" in namespace "resourcequota-4056" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 24 13:22:31.638: INFO: ResourceQuota "e2e-rq-status-77sgc" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 08/24/23 13:22:31.644
  Aug 24 13:22:31.655: INFO: Resource quota "e2e-rq-status-77sgc" reports spec: hard cpu limit of 1
  Aug 24 13:22:31.655: INFO: Resource quota "e2e-rq-status-77sgc" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-77sgc" /status @ 08/24/23 13:22:31.656
  STEP: Confirm /status for "e2e-rq-status-77sgc" resourceQuota via watch @ 08/24/23 13:22:31.669
  Aug 24 13:22:31.672: INFO: observed resourceQuota "e2e-rq-status-77sgc" in namespace "resourcequota-4056" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 24 13:22:31.672: INFO: Found resourceQuota "e2e-rq-status-77sgc" in namespace "resourcequota-4056" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug 24 13:22:31.672: INFO: ResourceQuota "e2e-rq-status-77sgc" /status was patched
  STEP: Get "e2e-rq-status-77sgc" /status @ 08/24/23 13:22:31.672
  Aug 24 13:22:31.679: INFO: Resourcequota "e2e-rq-status-77sgc" reports status: hard cpu of 1
  Aug 24 13:22:31.679: INFO: Resourcequota "e2e-rq-status-77sgc" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-77sgc" /status before checking Spec is unchanged @ 08/24/23 13:22:31.684
  Aug 24 13:22:31.693: INFO: Resourcequota "e2e-rq-status-77sgc" reports status: hard cpu of 2
  Aug 24 13:22:31.693: INFO: Resourcequota "e2e-rq-status-77sgc" reports status: hard memory of 2Gi
  Aug 24 13:22:31.695: INFO: Found resourceQuota "e2e-rq-status-77sgc" in namespace "resourcequota-4056" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0824 13:22:32.075005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:33.075148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:34.075397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:35.075566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:36.075686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:37.075781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:38.076010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:39.076117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:40.076407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:41.076592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:41.723: INFO: ResourceQuota "e2e-rq-status-77sgc" Spec was unchanged and /status reset
  Aug 24 13:22:41.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4056" for this suite. @ 08/24/23 13:22:41.733
• [10.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 08/24/23 13:22:41.756
  Aug 24 13:22:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:22:41.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:41.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:41.801
  STEP: Creating a pod to test downward api env vars @ 08/24/23 13:22:41.806
  E0824 13:22:42.077537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:43.078109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:44.078423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:45.078510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:22:45.859
  Aug 24 13:22:45.867: INFO: Trying to get logs from node pohje9aimahx-3 pod downward-api-819027c6-09a7-4485-a79a-49d547e868db container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 13:22:45.887
  Aug 24 13:22:45.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9989" for this suite. @ 08/24/23 13:22:45.934
• [4.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 08/24/23 13:22:45.966
  Aug 24 13:22:45.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename dns @ 08/24/23 13:22:45.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:46.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:46.026
  STEP: Creating a test headless service @ 08/24/23 13:22:46.031
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6908.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6908.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 08/24/23 13:22:46.04
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6908.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6908.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 08/24/23 13:22:46.04
  STEP: creating a pod to probe DNS @ 08/24/23 13:22:46.04
  STEP: submitting the pod to kubernetes @ 08/24/23 13:22:46.04
  E0824 13:22:46.078994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:47.079486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:48.080340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:49.080309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:50.080636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:22:50.101
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:22:50.108
  Aug 24 13:22:50.143: INFO: DNS probes using dns-6908/dns-test-434deaf0-0519-4f59-a99c-9e72868b2af1 succeeded

  Aug 24 13:22:50.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:22:50.152
  STEP: deleting the test headless service @ 08/24/23 13:22:50.181
  STEP: Destroying namespace "dns-6908" for this suite. @ 08/24/23 13:22:50.249
• [4.301 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 08/24/23 13:22:50.268
  Aug 24 13:22:50.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename deployment @ 08/24/23 13:22:50.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:50.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:50.333
  Aug 24 13:22:50.337: INFO: Creating simple deployment test-new-deployment
  Aug 24 13:22:50.361: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0824 13:22:51.082293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:52.081874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 08/24/23 13:22:52.395
  STEP: updating a scale subresource @ 08/24/23 13:22:52.4
  STEP: verifying the deployment Spec.Replicas was modified @ 08/24/23 13:22:52.412
  STEP: Patch a scale subresource @ 08/24/23 13:22:52.426
  Aug 24 13:22:52.484: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-2923  cafe9f06-8c34-43e0-a9e0-1fe5dce87293 40204 3 2023-08-24 13:22:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-24 13:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00344b8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-08-24 13:22:52 +0000 UTC,LastTransitionTime:2023-08-24 13:22:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 13:22:52 +0000 UTC,LastTransitionTime:2023-08-24 13:22:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 13:22:52.511: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-2923  17a173c1-fc2b-4aa2-ae5d-9a0043827f1d 40208 3 2023-08-24 13:22:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment cafe9f06-8c34-43e0-a9e0-1fe5dce87293 0xc00478f6a7 0xc00478f6a8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 13:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cafe9f06-8c34-43e0-a9e0-1fe5dce87293\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:22:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00478f738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:22:52.523: INFO: Pod "test-new-deployment-67bd4bf6dc-r4n27" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-r4n27 test-new-deployment-67bd4bf6dc- deployment-2923  fe5e844e-7e3b-4e91-853d-0fe02eea9fab 40196 0 2023-08-24 13:22:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 17a173c1-fc2b-4aa2-ae5d-9a0043827f1d 0xc00344bc87 0xc00344bc88}] [] [{kube-controller-manager Update v1 2023-08-24 13:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17a173c1-fc2b-4aa2-ae5d-9a0043827f1d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:22:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c59c8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c59c8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:22:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.66.200,StartTime:2023-08-24 13:22:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:22:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6903fe54d253be3f0aadc95ec0bbda790af09764c504bc7c88d63f5b3600ed46,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.200,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:22:52.523: INFO: Pod "test-new-deployment-67bd4bf6dc-s7fsw" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-s7fsw test-new-deployment-67bd4bf6dc- deployment-2923  ceddbb5c-87d5-4637-a9ca-0f174f8a1951 40207 0 2023-08-24 13:22:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 17a173c1-fc2b-4aa2-ae5d-9a0043827f1d 0xc00344be77 0xc00344be78}] [] [{kube-controller-manager Update v1 2023-08-24 13:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17a173c1-fc2b-4aa2-ae5d-9a0043827f1d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99sbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99sbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pohje9aimahx-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:22:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:22:52.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2923" for this suite. @ 08/24/23 13:22:52.535
• [2.296 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 08/24/23 13:22:52.567
  Aug 24 13:22:52.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:22:52.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:52.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:52.633
  STEP: Setting up server cert @ 08/24/23 13:22:52.695
  E0824 13:22:53.081797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:22:53.159
  STEP: Deploying the webhook pod @ 08/24/23 13:22:53.169
  STEP: Wait for the deployment to be ready @ 08/24/23 13:22:53.196
  Aug 24 13:22:53.213: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 13:22:54.082141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:55.083033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:22:55.243
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:22:55.26
  E0824 13:22:56.083381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:56.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/24/23 13:22:56.379
  STEP: Creating a configMap that should be mutated @ 08/24/23 13:22:56.43
  STEP: Deleting the collection of validation webhooks @ 08/24/23 13:22:56.487
  STEP: Creating a configMap that should not be mutated @ 08/24/23 13:22:56.588
  Aug 24 13:22:56.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4322" for this suite. @ 08/24/23 13:22:56.724
  STEP: Destroying namespace "webhook-markers-8370" for this suite. @ 08/24/23 13:22:56.741
• [4.212 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 08/24/23 13:22:56.782
  Aug 24 13:22:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename limitrange @ 08/24/23 13:22:56.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:22:56.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:22:56.828
  STEP: Creating a LimitRange @ 08/24/23 13:22:56.834
  STEP: Setting up watch @ 08/24/23 13:22:56.834
  STEP: Submitting a LimitRange @ 08/24/23 13:22:56.941
  STEP: Verifying LimitRange creation was observed @ 08/24/23 13:22:56.954
  STEP: Fetching the LimitRange to ensure it has proper values @ 08/24/23 13:22:56.955
  Aug 24 13:22:56.961: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 24 13:22:56.962: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 08/24/23 13:22:56.962
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 08/24/23 13:22:56.972
  Aug 24 13:22:56.978: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 24 13:22:56.979: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 08/24/23 13:22:56.979
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 08/24/23 13:22:57.002
  Aug 24 13:22:57.018: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Aug 24 13:22:57.018: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 08/24/23 13:22:57.018
  STEP: Failing to create a Pod with more than max resources @ 08/24/23 13:22:57.024
  STEP: Updating a LimitRange @ 08/24/23 13:22:57.03
  STEP: Verifying LimitRange updating is effective @ 08/24/23 13:22:57.041
  E0824 13:22:57.084055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:58.085504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 08/24/23 13:22:59.047
  STEP: Failing to create a Pod with more than max resources @ 08/24/23 13:22:59.059
  STEP: Deleting a LimitRange @ 08/24/23 13:22:59.064
  E0824 13:22:59.086459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the LimitRange was deleted @ 08/24/23 13:22:59.091
  E0824 13:23:00.086934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:01.087185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:02.087334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:03.087956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:04.088839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:04.100: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 08/24/23 13:23:04.1
  Aug 24 13:23:04.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7884" for this suite. @ 08/24/23 13:23:04.133
• [7.367 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 08/24/23 13:23:04.15
  Aug 24 13:23:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 13:23:04.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:04.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:04.204
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 13:23:04.221
  E0824 13:23:05.089832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:06.090954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:07.090698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:08.090694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/24/23 13:23:08.272
  E0824 13:23:09.091690      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:10.092055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:11.092640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:12.092898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/24/23 13:23:12.314
  E0824 13:23:13.093076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:14.093734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/24/23 13:23:14.352
  Aug 24 13:23:14.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-844" for this suite. @ 08/24/23 13:23:14.377
• [10.240 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 08/24/23 13:23:14.396
  Aug 24 13:23:14.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:23:14.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:14.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:14.434
  STEP: starting the proxy server @ 08/24/23 13:23:14.438
  Aug 24 13:23:14.439: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=kubectl-5783 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 08/24/23 13:23:14.543
  Aug 24 13:23:14.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5783" for this suite. @ 08/24/23 13:23:14.573
• [0.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 08/24/23 13:23:14.585
  Aug 24 13:23:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename services @ 08/24/23 13:23:14.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:14.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:14.623
  STEP: creating a collection of services @ 08/24/23 13:23:14.629
  Aug 24 13:23:14.629: INFO: Creating e2e-svc-a-zghxq
  Aug 24 13:23:14.655: INFO: Creating e2e-svc-b-mqtdx
  Aug 24 13:23:14.683: INFO: Creating e2e-svc-c-bbpsl
  STEP: deleting service collection @ 08/24/23 13:23:14.712
  Aug 24 13:23:14.796: INFO: Collection of services has been deleted
  Aug 24 13:23:14.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2299" for this suite. @ 08/24/23 13:23:14.812
• [0.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 08/24/23 13:23:14.83
  Aug 24 13:23:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:23:14.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:14.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:14.883
  STEP: Setting up server cert @ 08/24/23 13:23:14.933
  E0824 13:23:15.096176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:16.095488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:23:16.242
  STEP: Deploying the webhook pod @ 08/24/23 13:23:16.257
  STEP: Wait for the deployment to be ready @ 08/24/23 13:23:16.277
  Aug 24 13:23:16.315: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 13:23:17.097064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:18.097623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:23:18.34
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:23:18.368
  E0824 13:23:19.098446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:19.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 13:23:19.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4037-crds.webhook.example.com via the AdmissionRegistration API @ 08/24/23 13:23:19.997
  STEP: Creating a custom resource while v1 is storage version @ 08/24/23 13:23:20.074
  E0824 13:23:20.099033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:21.098961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:22.098968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 08/24/23 13:23:22.117
  STEP: Patching the custom resource while v2 is storage version @ 08/24/23 13:23:22.146
  Aug 24 13:23:22.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-163" for this suite. @ 08/24/23 13:23:22.829
  STEP: Destroying namespace "webhook-markers-2786" for this suite. @ 08/24/23 13:23:22.844
• [8.025 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 08/24/23 13:23:22.862
  Aug 24 13:23:22.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 13:23:22.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:22.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:22.903
  STEP: Creating a test namespace @ 08/24/23 13:23:22.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:22.939
  STEP: Creating a pod in the namespace @ 08/24/23 13:23:22.943
  STEP: Waiting for the pod to have running status @ 08/24/23 13:23:22.981
  E0824 13:23:23.099301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:24.099849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 08/24/23 13:23:25.009
  STEP: Waiting for the namespace to be removed. @ 08/24/23 13:23:25.022
  E0824 13:23:25.099779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:26.099938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:27.099981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:28.101114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:29.101995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:30.102866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:31.103129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:32.103448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:33.103658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:34.103688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:35.104681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/24/23 13:23:36.034
  STEP: Verifying there are no pods in the namespace @ 08/24/23 13:23:36.06
  Aug 24 13:23:36.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8403" for this suite. @ 08/24/23 13:23:36.076
  STEP: Destroying namespace "nsdeletetest-9067" for this suite. @ 08/24/23 13:23:36.09
  Aug 24 13:23:36.096: INFO: Namespace nsdeletetest-9067 was already deleted
  STEP: Destroying namespace "nsdeletetest-7307" for this suite. @ 08/24/23 13:23:36.096
  E0824 13:23:36.104841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
• [13.245 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 08/24/23 13:23:36.107
  Aug 24 13:23:36.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 13:23:36.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:36.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:36.147
  STEP: Creating service test in namespace statefulset-8234 @ 08/24/23 13:23:36.15
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 08/24/23 13:23:36.159
  STEP: Creating stateful set ss in namespace statefulset-8234 @ 08/24/23 13:23:36.165
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8234 @ 08/24/23 13:23:36.176
  Aug 24 13:23:36.180: INFO: Found 0 stateful pods, waiting for 1
  E0824 13:23:37.105779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:38.106171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:39.106349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:40.106456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:41.106554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:42.107362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:43.107591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:44.108655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:45.109214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:46.110123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:46.189: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 08/24/23 13:23:46.189
  Aug 24 13:23:46.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 13:23:46.507: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 13:23:46.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 13:23:46.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 13:23:46.520: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0824 13:23:47.110245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:48.110471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:49.111226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:50.111464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:51.111673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:52.112350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:53.112206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:54.113088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:55.113343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:56.113615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:56.533: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 13:23:56.533: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 13:23:56.579: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999343s
  E0824 13:23:57.113915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:57.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983678456s
  E0824 13:23:58.114703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:58.601: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.968104366s
  E0824 13:23:59.115634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:59.610: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.960763084s
  E0824 13:24:00.116606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:00.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.951642864s
  E0824 13:24:01.116821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:01.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.941223797s
  E0824 13:24:02.117261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:02.643: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.929189974s
  E0824 13:24:03.117362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:03.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.91885508s
  E0824 13:24:04.117549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:04.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.909905903s
  E0824 13:24:05.118579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:05.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 899.563342ms
  E0824 13:24:06.119423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8234 @ 08/24/23 13:24:06.673
  Aug 24 13:24:06.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:24:06.986: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 13:24:06.986: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 13:24:06.986: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 13:24:07.000: INFO: Found 1 stateful pods, waiting for 3
  E0824 13:24:07.120114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:08.120431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:09.120964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:10.121093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:11.121402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:12.121648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:13.122091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:14.122902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:15.123344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:16.126873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:17.010: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 13:24:17.010: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 13:24:17.010: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 08/24/23 13:24:17.01
  STEP: Scale down will halt with unhealthy stateful pod @ 08/24/23 13:24:17.01
  Aug 24 13:24:17.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0824 13:24:17.125122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:17.377: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 13:24:17.377: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 13:24:17.377: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 13:24:17.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 13:24:17.734: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 13:24:17.734: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 13:24:17.734: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 13:24:17.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 13:24:18.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 13:24:18.052: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 13:24:18.052: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 13:24:18.052: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 13:24:18.082: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  E0824 13:24:18.126257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:19.126423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:20.126615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:21.126960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:22.128014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:23.127496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:24.128123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:25.129365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:26.128535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:27.128845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:28.104: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 13:24:28.104: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 13:24:28.104: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  E0824 13:24:28.130374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:28.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999138s
  E0824 13:24:29.130128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:29.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985031844s
  E0824 13:24:30.130702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:30.169: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973500298s
  E0824 13:24:31.130805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:31.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965497711s
  E0824 13:24:32.131120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:32.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.955153933s
  E0824 13:24:33.131327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:33.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944780734s
  E0824 13:24:34.133906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:34.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.934023507s
  E0824 13:24:35.132083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:35.222: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.922525299s
  E0824 13:24:36.131985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:36.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.912847368s
  E0824 13:24:37.132191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:37.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.958882ms
  E0824 13:24:38.132394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8234 @ 08/24/23 13:24:38.239
  Aug 24 13:24:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:24:38.546: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 13:24:38.546: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 13:24:38.546: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 13:24:38.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:24:38.878: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 13:24:38.878: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 13:24:38.878: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 13:24:38.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 13:24:39.132981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:39.449: INFO: rc: 1
  Aug 24 13:24:39.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  error: Internal error occurred: error executing command in container: container is not created or running

  error:
  exit status 1
  E0824 13:24:40.133434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:41.134404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:42.134781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:43.135341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:44.136625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:45.136702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:46.137006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:47.137452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:48.138393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:49.139456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:49.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:24:49.670: INFO: rc: 1
  Aug 24 13:24:49.670: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:24:50.139833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:51.140408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:52.141064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:53.141069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:54.142045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:55.142262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:56.142647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:57.142771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:58.143042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:59.143329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:59.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:24:59.847: INFO: rc: 1
  Aug 24 13:24:59.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:25:00.144116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:01.144423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:02.144513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:03.144874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:04.145934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:05.146104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:06.146185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:07.146407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:08.146777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:09.146838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:09.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:25:10.009: INFO: rc: 1
  Aug 24 13:25:10.009: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:25:10.146931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:11.147459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:12.147544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:13.147829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:14.147995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:15.148426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:16.148459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:17.148853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:18.149545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:19.149613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:20.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 13:25:20.149932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:20.158: INFO: rc: 1
  Aug 24 13:25:20.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:25:21.150233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:22.150472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:23.150788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:24.150948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:25.151226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:26.151302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:27.151719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:28.151878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:29.152686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:30.153140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:30.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:25:30.304: INFO: rc: 1
  Aug 24 13:25:30.304: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:25:31.153543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:32.153766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:33.154044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:34.155006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:35.155147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:36.155317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:37.155480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:38.156331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:39.157333      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:40.157534      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:40.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:25:40.483: INFO: rc: 1
  Aug 24 13:25:40.483: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:25:41.158161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:42.158332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:43.159439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:44.160216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:45.160417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:46.160945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:47.161320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:48.162103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:49.162213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:50.162457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:50.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:25:50.663: INFO: rc: 1
  Aug 24 13:25:50.663: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:25:51.163375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:52.163523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:53.163853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:54.164682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:55.165135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:56.165598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:57.165769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:58.166064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:59.167159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:00.167422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:00.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:26:00.859: INFO: rc: 1
  Aug 24 13:26:00.859: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:26:01.168065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:02.168161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:03.168379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:04.169152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:05.170235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:06.170929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:07.171136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:08.172647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:09.173528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:10.174141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:10.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:26:11.089: INFO: rc: 1
  Aug 24 13:26:11.089: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:26:11.174910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:12.175725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:13.176564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:14.178750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:15.179097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:16.179238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:17.179520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:18.179789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:19.179913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:20.180236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:21.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 13:26:21.180973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:21.246: INFO: rc: 1
  Aug 24 13:26:21.247: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:26:22.181244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:23.181763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:24.182270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:25.182564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:26.182745      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:27.183329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:28.183621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:29.183895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:30.184157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:31.185301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:31.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:26:31.424: INFO: rc: 1
  Aug 24 13:26:31.424: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:26:32.185649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:33.186748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:34.186744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:35.189830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:36.189932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:37.190162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:38.190740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:39.191399      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:40.192125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:41.193434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:41.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:26:41.593: INFO: rc: 1
  Aug 24 13:26:41.593: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:26:42.192645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:43.192738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:44.193494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:45.194347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:46.195196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:47.195436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:48.195684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:49.196566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:50.197025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:51.197277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:51.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:26:51.786: INFO: rc: 1
  Aug 24 13:26:51.787: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:26:52.197645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:53.200065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:54.199994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:55.200448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:56.200736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:57.200976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:58.201161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:59.201338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:00.201517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:01.201797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:01.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:27:01.960: INFO: rc: 1
  Aug 24 13:27:01.960: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:27:02.202622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:03.202850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:04.203164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:05.203454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:06.203635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:07.203797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:08.204667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:09.205366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:10.205819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:11.205897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:11.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:27:12.129: INFO: rc: 1
  Aug 24 13:27:12.129: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:27:12.205987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:13.206331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:14.207137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:15.207214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:16.207908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:17.207900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:18.208018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:19.209155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:20.209579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:21.209932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:22.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 13:27:22.210407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:22.280: INFO: rc: 1
  Aug 24 13:27:22.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:27:23.210990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:24.211150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:25.211327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:26.211526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:27.211775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:28.211917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:29.212582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:30.213333      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:31.213826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:32.214592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:32.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:27:32.450: INFO: rc: 1
  Aug 24 13:27:32.450: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:27:33.215337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:34.216253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:35.216424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:36.217009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:37.217095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:38.217313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:39.218318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:40.218805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:41.219913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:42.219194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:42.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:27:42.604: INFO: rc: 1
  Aug 24 13:27:42.604: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:27:43.219469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:44.220264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:45.220751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:46.220953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:47.221154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:48.221444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:49.222407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:50.222563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:51.222903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:52.223027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:27:52.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:27:52.750: INFO: rc: 1
  Aug 24 13:27:52.750: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:27:53.223785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:54.224249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:55.224707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:56.224869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:57.225068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:58.225302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:27:59.225502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:00.225833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:01.225981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:02.226218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:02.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:28:02.907: INFO: rc: 1
  Aug 24 13:28:02.907: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:28:03.226352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:04.227162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:05.227506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:06.227671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:07.227889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:08.228212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:09.229288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:10.229383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:11.230638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:12.229774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:12.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:28:13.062: INFO: rc: 1
  Aug 24 13:28:13.062: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:28:13.230613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:14.230807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:15.231339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:16.231419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:17.231648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:18.231835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:19.232939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:20.233086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:21.233377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:22.233512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:23.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:28:23.209: INFO: rc: 1
  Aug 24 13:28:23.210: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:28:23.234128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:24.234687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:25.234887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:26.235141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:27.235468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:28.235589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:29.237929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:30.236789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:31.237074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:32.237198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:33.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 13:28:33.237628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:33.364: INFO: rc: 1
  Aug 24 13:28:33.364: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:28:34.238093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:35.238357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:36.238571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:37.238981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:38.238980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:39.239767      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:40.239934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:41.240124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:42.240376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:43.240788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:43.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:28:43.547: INFO: rc: 1
  Aug 24 13:28:43.547: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:28:44.241561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:45.242327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:46.242796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:47.242968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:48.243225      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:49.243601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:50.243643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:51.243835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:52.244153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:53.244330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:28:53.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:28:53.722: INFO: rc: 1
  Aug 24 13:28:53.722: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:28:54.244728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:55.244915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:56.245160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:57.245351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:58.245562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:28:59.245690      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:00.245871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:01.246349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:02.246488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:03.247653      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:29:03.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:29:03.895: INFO: rc: 1
  Aug 24 13:29:03.896: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:29:04.247069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:05.247316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:06.247774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:07.247881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:08.248040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:09.249165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:10.249458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:11.249650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:12.249919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:13.250141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:29:13.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:29:14.037: INFO: rc: 1
  Aug 24 13:29:14.037: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:29:14.250430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:15.250561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:16.250920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:17.251123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:18.251592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:19.252460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:20.252918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:21.253100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:22.253914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:23.254142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:29:24.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:29:24.213: INFO: rc: 1
  Aug 24 13:29:24.213: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:29:24.254961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:25.255382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:26.255789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:27.256152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:28.256264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:29.257124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:30.257478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:31.257628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:32.257818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:33.258115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:29:34.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0824 13:29:34.258666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:29:34.364: INFO: rc: 1
  Aug 24 13:29:34.364: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  E0824 13:29:35.258907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:36.259425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:37.259564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:38.259889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:39.260662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:40.260947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:41.261085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:42.261516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:43.261694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:44.261927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:29:44.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3669880393 --namespace=statefulset-8234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:29:44.515: INFO: rc: 1
  Aug 24 13:29:44.515: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
  Aug 24 13:29:44.515: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 08/24/23 13:29:44.55
  Aug 24 13:29:44.550: INFO: Deleting all statefulset in ns statefulset-8234
  Aug 24 13:29:44.557: INFO: Scaling statefulset ss to 0
  Aug 24 13:29:44.576: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 13:29:44.584: INFO: Deleting statefulset ss
  Aug 24 13:29:44.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8234" for this suite. @ 08/24/23 13:29:44.631
• [368.537 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 08/24/23 13:29:44.647
  Aug 24 13:29:44.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 13:29:44.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:29:44.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:29:44.695
  STEP: creating a ServiceAccount @ 08/24/23 13:29:44.699
  STEP: watching for the ServiceAccount to be added @ 08/24/23 13:29:44.713
  STEP: patching the ServiceAccount @ 08/24/23 13:29:44.72
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 08/24/23 13:29:44.732
  STEP: deleting the ServiceAccount @ 08/24/23 13:29:44.74
  Aug 24 13:29:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7815" for this suite. @ 08/24/23 13:29:44.793
• [0.159 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 08/24/23 13:29:44.807
  Aug 24 13:29:44.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:29:44.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:29:44.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:29:44.867
  STEP: Creating projection with secret that has name projected-secret-test-f7f78afe-5655-4d6d-af0a-06088343a2b7 @ 08/24/23 13:29:44.873
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:29:44.887
  E0824 13:29:45.262936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:46.263212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:47.263330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:48.263553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:29:48.933
  Aug 24 13:29:48.942: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-secrets-d4e564a1-eec5-48d9-ac66-b665baeac6bf container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:29:48.978
  Aug 24 13:29:49.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5090" for this suite. @ 08/24/23 13:29:49.037
• [4.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 08/24/23 13:29:49.066
  Aug 24 13:29:49.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename tables @ 08/24/23 13:29:49.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:29:49.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:29:49.106
  Aug 24 13:29:49.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-9084" for this suite. @ 08/24/23 13:29:49.131
• [0.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 08/24/23 13:29:49.153
  Aug 24 13:29:49.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename init-container @ 08/24/23 13:29:49.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:29:49.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:29:49.193
  STEP: creating the pod @ 08/24/23 13:29:49.199
  Aug 24 13:29:49.199: INFO: PodSpec: initContainers in spec.initContainers
  E0824 13:29:49.264075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:50.265118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:51.264962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:52.265381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:53.266278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:54.267475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:55.267341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:56.267442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:57.267881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:58.268121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:29:59.268556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:00.268908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:01.269289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:02.269694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:03.269763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:04.270902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:05.271277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:06.271455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:07.271579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:08.271748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:09.271863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:10.271917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:11.272103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:12.272885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:13.273137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:14.273953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:15.274539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:16.275429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:17.275568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:18.275799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:19.275960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:20.276089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:21.276293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:22.276625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:23.277556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:24.278550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:25.278973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:26.279303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:27.280677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:28.280872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:29.281770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:30.281891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:31.283863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:32.282938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:33.283072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:30:34.000: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d7ef8a0c-87d7-435c-8195-60a0262784f4", GenerateName:"", Namespace:"init-container-6529", SelfLink:"", UID:"dce12904-7dde-4747-bd38-8b26699e0159", ResourceVersion:"41920", Generation:0, CreationTimestamp:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"199378408"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005cbb4d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 13, 30, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005cbb500), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-467qp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004b397a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-467qp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-467qp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-467qp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0056b1a30), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pohje9aimahx-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0042ced20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0056b1ac0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0056b1ae0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0056b1ae8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0056b1aec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006eeae00), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.5", PodIP:"10.233.66.12", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.12"}}, StartTime:time.Date(2023, time.August, 24, 13, 29, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0042cee00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0042cee70)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://31ac92695402354e85ff5a5d543d83f19b57c241d22afa83a0d8967d2ae9b5b1", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b39840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b39800), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0056b1b6f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Aug 24 13:30:34.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6529" for this suite. @ 08/24/23 13:30:34.018
• [44.881 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 08/24/23 13:30:34.044
  Aug 24 13:30:34.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 13:30:34.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:34.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:34.096
  Aug 24 13:30:34.172: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e8bd4845-8a1a-4704-b98e-d701d25e34c6", Controller:(*bool)(0xc004b5ef26), BlockOwnerDeletion:(*bool)(0xc004b5ef27)}}
  Aug 24 13:30:34.195: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ae9eaaa5-27c9-4ae5-8aca-4ef4740a9cde", Controller:(*bool)(0xc00356b76e), BlockOwnerDeletion:(*bool)(0xc00356b76f)}}
  Aug 24 13:30:34.211: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2c148ec4-59a2-42f3-a7e0-57603533db8b", Controller:(*bool)(0xc00356b97a), BlockOwnerDeletion:(*bool)(0xc00356b97b)}}
  E0824 13:30:34.284286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:35.285228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:36.286214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:37.286222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:38.286684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:30:39.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3684" for this suite. @ 08/24/23 13:30:39.257
• [5.231 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 08/24/23 13:30:39.277
  Aug 24 13:30:39.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:30:39.28
  E0824 13:30:39.287289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:39.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:39.338
  STEP: Creating a pod to test downward api env vars @ 08/24/23 13:30:39.343
  E0824 13:30:40.287960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:41.288162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:42.288298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:43.288421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:30:43.393
  Aug 24 13:30:43.398: INFO: Trying to get logs from node pohje9aimahx-3 pod downward-api-b0d16b2c-1c63-4b5e-9017-c7abe57091b4 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 13:30:43.417
  Aug 24 13:30:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3743" for this suite. @ 08/24/23 13:30:43.454
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 08/24/23 13:30:43.474
  Aug 24 13:30:43.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 13:30:43.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:43.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:43.535
  Aug 24 13:30:43.571: INFO: Endpoints addresses: [192.168.121.248 192.168.121.37] , ports: [6443]
  Aug 24 13:30:43.572: INFO: EndpointSlices addresses: [192.168.121.248 192.168.121.37] , ports: [6443]
  Aug 24 13:30:43.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4916" for this suite. @ 08/24/23 13:30:43.589
• [0.132 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 08/24/23 13:30:43.61
  Aug 24 13:30:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:30:43.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:43.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:43.646
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:30:43.653
  E0824 13:30:44.288815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:45.291084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:46.291825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:47.292123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:30:47.701
  Aug 24 13:30:47.708: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-a6431c70-aa73-4212-8c9c-d29cbeec2201 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:30:47.723
  Aug 24 13:30:47.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4401" for this suite. @ 08/24/23 13:30:47.773
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 08/24/23 13:30:47.791
  Aug 24 13:30:47.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:30:47.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:47.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:47.84
  STEP: Creating secret with name secret-test-aa309d3c-5191-4ed9-a663-3f9300524958 @ 08/24/23 13:30:47.846
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:30:47.86
  E0824 13:30:48.292995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:49.293555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:50.293348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:51.293593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:30:51.903
  Aug 24 13:30:51.910: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-secrets-8030586a-9173-4582-9092-20f932735c7d container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:30:51.924
  Aug 24 13:30:51.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8253" for this suite. @ 08/24/23 13:30:51.968
• [4.190 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 08/24/23 13:30:51.985
  Aug 24 13:30:51.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename gc @ 08/24/23 13:30:51.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:52.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:52.032
  STEP: create the deployment @ 08/24/23 13:30:52.037
  W0824 13:30:52.050390      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/24/23 13:30:52.05
  E0824 13:30:52.294203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 08/24/23 13:30:52.572
  STEP: wait for all rs to be garbage collected @ 08/24/23 13:30:52.589
  STEP: expected 0 pods, got 2 pods @ 08/24/23 13:30:52.746
  STEP: Gathering metrics @ 08/24/23 13:30:53.282
  E0824 13:30:53.294656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:30:53.558: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 13:30:53.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2661" for this suite. @ 08/24/23 13:30:53.633
• [1.676 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 08/24/23 13:30:53.662
  Aug 24 13:30:53.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 13:30:53.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:53.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:53.732
  STEP: Waiting for pod completion @ 08/24/23 13:30:53.753
  E0824 13:30:54.295161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:55.295588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:56.296776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:57.296337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:30:57.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8834" for this suite. @ 08/24/23 13:30:57.811
• [4.164 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 08/24/23 13:30:57.828
  Aug 24 13:30:57.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 13:30:57.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:30:57.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:30:57.868
  Aug 24 13:30:57.908: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 13:30:58.297111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:30:59.297240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:00.298172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:01.298566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:02.299351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:03.299940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:04.300297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:05.300946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:06.301150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:07.301254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:08.301448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:09.301690      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:10.301765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:11.302116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:12.302685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:13.303422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:14.304145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:15.304849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:16.305576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:17.305794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:18.306462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:19.307227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:20.307332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:21.307732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:22.308561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:23.308820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:24.309163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:25.308810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:26.309095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:27.309760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:28.310641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:29.311591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:30.312053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:31.312231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:32.312581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:33.312964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:34.313418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:35.313658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:36.314096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:37.314298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:38.315024      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:39.315267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:40.315817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:41.316604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:42.317304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:43.319392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:44.319499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:45.319527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:46.319735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:47.320342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:48.320584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:49.320877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:50.320999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:51.321109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:52.321968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:53.322723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:54.323772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:55.323936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:56.324241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:57.324943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:31:57.971: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/24/23 13:31:57.977
  Aug 24 13:31:58.019: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 24 13:31:58.031: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 24 13:31:58.071: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 24 13:31:58.085: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 24 13:31:58.156: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 24 13:31:58.191: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/24/23 13:31:58.191
  E0824 13:31:58.325946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:31:59.326448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:00.327237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:01.327832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 08/24/23 13:32:02.261
  E0824 13:32:02.328731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:03.329180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:04.329788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:05.330087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:06.330226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:06.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7962" for this suite. @ 08/24/23 13:32:06.467
• [68.659 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 08/24/23 13:32:06.488
  Aug 24 13:32:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:32:06.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:06.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:06.535
  STEP: creating the pod @ 08/24/23 13:32:06.54
  STEP: setting up watch @ 08/24/23 13:32:06.54
  STEP: submitting the pod to kubernetes @ 08/24/23 13:32:06.648
  STEP: verifying the pod is in kubernetes @ 08/24/23 13:32:06.672
  STEP: verifying pod creation was observed @ 08/24/23 13:32:06.684
  E0824 13:32:07.330384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:08.331670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/24/23 13:32:08.722
  STEP: verifying pod deletion was observed @ 08/24/23 13:32:08.742
  E0824 13:32:09.331042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:10.331808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:10.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6297" for this suite. @ 08/24/23 13:32:10.465
• [3.994 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 08/24/23 13:32:10.488
  Aug 24 13:32:10.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:32:10.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:10.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:10.533
  STEP: creating the pod @ 08/24/23 13:32:10.537
  STEP: submitting the pod to kubernetes @ 08/24/23 13:32:10.537
  W0824 13:32:10.554779      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0824 13:32:11.333046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:12.333042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/24/23 13:32:12.61
  STEP: updating the pod @ 08/24/23 13:32:12.617
  Aug 24 13:32:13.151: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ed1f679f-3b6d-4f63-b03b-bce9249928d9"
  E0824 13:32:13.333733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:14.333929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:15.334953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:16.335298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:17.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9245" for this suite. @ 08/24/23 13:32:17.187
• [6.711 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 08/24/23 13:32:17.205
  Aug 24 13:32:17.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename init-container @ 08/24/23 13:32:17.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:17.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:17.247
  STEP: creating the pod @ 08/24/23 13:32:17.256
  Aug 24 13:32:17.257: INFO: PodSpec: initContainers in spec.initContainers
  E0824 13:32:17.335578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:18.336135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:19.337251      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:20.337283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:20.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3683" for this suite. @ 08/24/23 13:32:20.492
• [3.297 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 08/24/23 13:32:20.504
  Aug 24 13:32:20.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename crd-webhook @ 08/24/23 13:32:20.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:20.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:20.54
  STEP: Setting up server cert @ 08/24/23 13:32:20.544
  E0824 13:32:21.338158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/24/23 13:32:21.401
  STEP: Deploying the custom resource conversion webhook pod @ 08/24/23 13:32:21.447
  STEP: Wait for the deployment to be ready @ 08/24/23 13:32:21.468
  Aug 24 13:32:21.483: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0824 13:32:22.339137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:23.338972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:23.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 13, 32, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 32, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 32, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 32, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:32:24.339226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:25.339544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:32:25.523
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:32:25.576
  E0824 13:32:26.339954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:26.577: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 24 13:32:26.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  E0824 13:32:27.340518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:28.340821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:29.341664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/24/23 13:32:29.387
  STEP: Create a v2 custom resource @ 08/24/23 13:32:29.424
  STEP: List CRs in v1 @ 08/24/23 13:32:29.436
  STEP: List CRs in v2 @ 08/24/23 13:32:29.655
  Aug 24 13:32:29.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4467" for this suite. @ 08/24/23 13:32:30.281
• [9.789 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 08/24/23 13:32:30.298
  Aug 24 13:32:30.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:32:30.302
  E0824 13:32:30.341756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:30.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:30.418
  STEP: Setting up server cert @ 08/24/23 13:32:30.498
  E0824 13:32:31.342590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:32:32.057
  STEP: Deploying the webhook pod @ 08/24/23 13:32:32.07
  STEP: Wait for the deployment to be ready @ 08/24/23 13:32:32.092
  Aug 24 13:32:32.122: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 13:32:32.343301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:33.343431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:32:34.143
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:32:34.16
  E0824 13:32:34.344304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:35.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/24/23 13:32:35.168
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/24/23 13:32:35.2
  STEP: Creating a dummy validating-webhook-configuration object @ 08/24/23 13:32:35.229
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 08/24/23 13:32:35.245
  STEP: Creating a dummy mutating-webhook-configuration object @ 08/24/23 13:32:35.257
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 08/24/23 13:32:35.273
  Aug 24 13:32:35.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:32:35.346576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7505" for this suite. @ 08/24/23 13:32:35.423
  STEP: Destroying namespace "webhook-markers-5024" for this suite. @ 08/24/23 13:32:35.442
• [5.154 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 08/24/23 13:32:35.455
  Aug 24 13:32:35.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:32:35.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:35.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:35.557
  STEP: Creating secret with name projected-secret-test-cf5dffc1-3d6c-468b-977c-ef360b4d0da0 @ 08/24/23 13:32:35.562
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:32:35.571
  E0824 13:32:36.345731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:37.346312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:38.346222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:39.346930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:32:39.611
  Aug 24 13:32:39.618: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-secrets-0bc31905-6e73-40ef-a6b1-aafb2034ee8d container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:32:39.648
  Aug 24 13:32:39.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3618" for this suite. @ 08/24/23 13:32:39.696
• [4.262 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 08/24/23 13:32:39.721
  Aug 24 13:32:39.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:32:39.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:39.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:39.755
  STEP: Creating Pod @ 08/24/23 13:32:39.763
  E0824 13:32:40.348410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:41.348941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 08/24/23 13:32:41.802
  Aug 24 13:32:41.803: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4847 PodName:pod-sharedvolume-b507a4a6-4137-4675-974a-ae4ce333fbd2 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:32:41.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:32:41.805: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:32:41.805: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4847/pods/pod-sharedvolume-b507a4a6-4137-4675-974a-ae4ce333fbd2/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Aug 24 13:32:41.953: INFO: Exec stderr: ""
  Aug 24 13:32:41.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4847" for this suite. @ 08/24/23 13:32:41.963
• [2.256 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 08/24/23 13:32:41.98
  Aug 24 13:32:41.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 13:32:41.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:42.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:42.02
  E0824 13:32:42.348949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:43.349012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:44.349013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:45.349358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:46.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8115" for this suite. @ 08/24/23 13:32:46.082
• [4.114 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 08/24/23 13:32:46.096
  Aug 24 13:32:46.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:32:46.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:46.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:46.135
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:32:46.14
  E0824 13:32:46.350273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:47.351277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:48.351830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:49.351924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:32:50.181
  Aug 24 13:32:50.189: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-ada5b31a-18c4-48f8-893a-803ec3487845 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:32:50.204
  Aug 24 13:32:50.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-159" for this suite. @ 08/24/23 13:32:50.256
• [4.177 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 08/24/23 13:32:50.278
  Aug 24 13:32:50.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 13:32:50.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:50.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:50.327
  E0824 13:32:50.352819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating simple DaemonSet "daemon-set" @ 08/24/23 13:32:50.381
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 13:32:50.394
  Aug 24 13:32:50.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:32:50.413: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:32:51.352971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:51.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 13:32:51.445: INFO: Node pohje9aimahx-1 is running 0 daemon pod, expected 1
  E0824 13:32:52.353232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:32:52.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 13:32:52.431: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 08/24/23 13:32:52.438
  STEP: DeleteCollection of the DaemonSets @ 08/24/23 13:32:52.446
  STEP: Verify that ReplicaSets have been deleted @ 08/24/23 13:32:52.459
  Aug 24 13:32:52.488: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43109"},"items":null}

  Aug 24 13:32:52.508: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43112"},"items":[{"metadata":{"name":"daemon-set-bqjr5","generateName":"daemon-set-","namespace":"daemonsets-3230","uid":"9f3ff2d0-b2cb-4fb1-a8f5-880d06a2de98","resourceVersion":"43112","creationTimestamp":"2023-08-24T13:32:50Z","deletionTimestamp":"2023-08-24T13:33:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f4ef968c-d041-4f73-95b9-cf909a43c686","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T13:32:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ef968c-d041-4f73-95b9-cf909a43c686\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T13:32:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2hd2j","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2hd2j","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pohje9aimahx-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pohje9aimahx-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:52Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:52Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:50Z"}],"hostIP":"192.168.121.248","podIP":"10.233.65.225","podIPs":[{"ip":"10.233.65.225"}],"startTime":"2023-08-24T13:32:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T13:32:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://84d1990c8697c3d41922e2ca17a5bd6e949f9f6d55bda7b15d1c8e2352af1741","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kd6z4","generateName":"daemon-set-","namespace":"daemonsets-3230","uid":"33fec685-162f-4617-a853-ec84c0fd41f6","resourceVersion":"43105","creationTimestamp":"2023-08-24T13:32:50Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f4ef968c-d041-4f73-95b9-cf909a43c686","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T13:32:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ef968c-d041-4f73-95b9-cf909a43c686\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T13:32:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gdv6c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gdv6c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pohje9aimahx-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pohje9aimahx-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:50Z"}],"hostIP":"192.168.121.37","podIP":"10.233.64.156","podIPs":[{"ip":"10.233.64.156"}],"startTime":"2023-08-24T13:32:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T13:32:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://999ed721f24645040a9d417a10e2e79a60ef83600d58347cb5d6b6992f8fb74e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kjvfd","generateName":"daemon-set-","namespace":"daemonsets-3230","uid":"7476b8f4-0cb7-4888-be69-415cc50f8573","resourceVersion":"43111","creationTimestamp":"2023-08-24T13:32:50Z","deletionTimestamp":"2023-08-24T13:33:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f4ef968c-d041-4f73-95b9-cf909a43c686","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T13:32:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4ef968c-d041-4f73-95b9-cf909a43c686\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T13:32:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-crx9n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-crx9n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pohje9aimahx-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pohje9aimahx-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T13:32:50Z"}],"hostIP":"192.168.121.5","podIP":"10.233.66.130","podIPs":[{"ip":"10.233.66.130"}],"startTime":"2023-08-24T13:32:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T13:32:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://d05db7651a57255ac752161b22cec41d3017b622a2c3301e8c390c7bdc54a0d5","started":true}],"qosClass":"BestEffort"}}]}

  Aug 24 13:32:52.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3230" for this suite. @ 08/24/23 13:32:52.55
• [2.284 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 08/24/23 13:32:52.569
  Aug 24 13:32:52.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:32:52.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:52.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:52.609
  STEP: Creating configMap with name projected-configmap-test-volume-fce2e40b-3359-4634-84fa-5b13916b55c2 @ 08/24/23 13:32:52.614
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:32:52.627
  E0824 13:32:53.354704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:54.355056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:55.355244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:56.355390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:32:56.678
  Aug 24 13:32:56.687: INFO: Trying to get logs from node pohje9aimahx-3 pod pod-projected-configmaps-3d700789-e55e-4ae2-bd59-eaca22c89921 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:32:56.706
  Aug 24 13:32:56.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6457" for this suite. @ 08/24/23 13:32:56.75
• [4.199 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 08/24/23 13:32:56.771
  Aug 24 13:32:56.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 13:32:56.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:32:56.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:32:56.823
  STEP: Performing setup for networking test in namespace pod-network-test-1641 @ 08/24/23 13:32:56.828
  STEP: creating a selector @ 08/24/23 13:32:56.829
  STEP: Creating the service pods in kubernetes @ 08/24/23 13:32:56.829
  Aug 24 13:32:56.829: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0824 13:32:57.356243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:58.357040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:32:59.358117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:00.358756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:01.359056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:02.359455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:03.360325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:04.360781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:05.360777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:06.361413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:07.361812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:08.362290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:09.363060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:10.362893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:11.363126      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:12.363426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:13.363925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:14.364467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:15.364603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:16.364929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:17.365484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:18.366362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/24/23 13:33:19.12
  E0824 13:33:19.366624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:20.366638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:33:21.163: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 13:33:21.164: INFO: Breadth first check of 10.233.64.77 on host 192.168.121.37...
  Aug 24 13:33:21.171: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.99:9080/dial?request=hostname&protocol=http&host=10.233.64.77&port=8083&tries=1'] Namespace:pod-network-test-1641 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:33:21.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:33:21.173: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:33:21.173: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1641/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.99%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.77%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 13:33:21.362: INFO: Waiting for responses: map[]
  Aug 24 13:33:21.362: INFO: reached 10.233.64.77 after 0/1 tries
  Aug 24 13:33:21.362: INFO: Breadth first check of 10.233.65.171 on host 192.168.121.248...
  E0824 13:33:21.367314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:33:21.373: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.99:9080/dial?request=hostname&protocol=http&host=10.233.65.171&port=8083&tries=1'] Namespace:pod-network-test-1641 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:33:21.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:33:21.375: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:33:21.375: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1641/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.99%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.171%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 13:33:21.532: INFO: Waiting for responses: map[]
  Aug 24 13:33:21.532: INFO: reached 10.233.65.171 after 0/1 tries
  Aug 24 13:33:21.532: INFO: Breadth first check of 10.233.66.157 on host 192.168.121.5...
  Aug 24 13:33:21.567: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.99:9080/dial?request=hostname&protocol=http&host=10.233.66.157&port=8083&tries=1'] Namespace:pod-network-test-1641 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:33:21.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  Aug 24 13:33:21.569: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:33:21.569: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1641/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.99%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.157%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 13:33:21.697: INFO: Waiting for responses: map[]
  Aug 24 13:33:21.697: INFO: reached 10.233.66.157 after 0/1 tries
  Aug 24 13:33:21.697: INFO: Going to retry 0 out of 3 pods....
  Aug 24 13:33:21.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1641" for this suite. @ 08/24/23 13:33:21.708
• [24.952 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 08/24/23 13:33:21.724
  Aug 24 13:33:21.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3669880393
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:33:21.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:33:21.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:33:21.773
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:33:21.778
  E0824 13:33:22.369254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:23.370206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:24.371122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:33:25.372180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:33:25.823
  Aug 24 13:33:25.830: INFO: Trying to get logs from node pohje9aimahx-3 pod downwardapi-volume-fab4dad1-7b97-4e4d-863a-f8c5b6821569 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:33:25.843
  Aug 24 13:33:25.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1942" for this suite. @ 08/24/23 13:33:25.886
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Aug 24 13:33:25.907: INFO: Running AfterSuite actions on node 1
  Aug 24 13:33:25.907: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.139 seconds]
------------------------------

Ran 378 of 7207 Specs in 6766.602 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h52m47.615063205s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m


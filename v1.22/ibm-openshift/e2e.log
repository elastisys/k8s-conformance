I0621 22:43:55.878558      22 e2e.go:129] Starting e2e run "32b5b847-8c64-4d96-bcfe-c7d2136dd517" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1655851435 - Will randomize all specs
Will run 346 of 6434 specs

Jun 21 22:43:58.080: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
E0621 22:43:58.084516      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun 21 22:43:58.091: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 21 22:43:58.181: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 21 22:43:58.293: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 21 22:43:58.294: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jun 21 22:43:58.294: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 21 22:43:58.320: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun 21 22:43:58.320: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Jun 21 22:43:58.320: INFO: e2e test version: v1.22.8
Jun 21 22:43:58.335: INFO: kube-apiserver version: v1.22.8+f34b40c
Jun 21 22:43:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 22:43:58.367: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:43:58.367: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
W0621 22:43:58.594735      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jun 21 22:43:58.594: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 22:43:58.685: INFO: Waiting up to 5m0s for pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0" in namespace "downward-api-610" to be "Succeeded or Failed"
Jun 21 22:43:58.705: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.392551ms
Jun 21 22:44:00.727: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040848304s
Jun 21 22:44:02.745: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059083075s
Jun 21 22:44:04.768: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081760976s
Jun 21 22:44:06.813: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.127149511s
Jun 21 22:44:08.831: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.145193767s
Jun 21 22:44:10.856: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.1699751s
Jun 21 22:44:12.880: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.194385402s
Jun 21 22:44:14.901: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.215374982s
STEP: Saw pod success
Jun 21 22:44:14.902: INFO: Pod "downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0" satisfied condition "Succeeded or Failed"
Jun 21 22:44:14.932: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0 container client-container: <nil>
STEP: delete the pod
Jun 21 22:44:15.103: INFO: Waiting for pod downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0 to disappear
Jun 21 22:44:15.120: INFO: Pod downwardapi-volume-379a7991-3581-4341-b52e-57d0c812eef0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:44:15.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-610" for this suite.

• [SLOW TEST:16.804 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:44:15.172: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 21 22:44:15.473: INFO: Waiting up to 5m0s for pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339" in namespace "emptydir-7517" to be "Succeeded or Failed"
Jun 21 22:44:15.503: INFO: Pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339": Phase="Pending", Reason="", readiness=false. Elapsed: 29.493305ms
Jun 21 22:44:17.518: INFO: Pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044779249s
Jun 21 22:44:19.543: INFO: Pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06970212s
Jun 21 22:44:21.560: INFO: Pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086442224s
Jun 21 22:44:23.576: INFO: Pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.10261064s
STEP: Saw pod success
Jun 21 22:44:23.576: INFO: Pod "pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339" satisfied condition "Succeeded or Failed"
Jun 21 22:44:23.591: INFO: Trying to get logs from node 10.10.24.208 pod pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339 container test-container: <nil>
STEP: delete the pod
Jun 21 22:44:23.670: INFO: Waiting for pod pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339 to disappear
Jun 21 22:44:23.684: INFO: Pod pod-06193e33-4ed1-46e0-bfaa-06dc4c55c339 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:44:23.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7517" for this suite.

• [SLOW TEST:8.560 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":24,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:44:23.733: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:44:24.024: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 21 22:44:29.053: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jun 21 22:44:39.130: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jun 21 22:44:39.165: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
Jun 21 22:44:39.232: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
Jun 21 22:44:39.304: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
Jun 21 22:44:39.329: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 1, AvailableReplicas 1
Jun 21 22:44:51.538: INFO: observed ReplicaSet test-rs in namespace replicaset-5322 with ReadyReplicas 2, AvailableReplicas 2
Jun 21 22:44:59.816: INFO: observed Replicaset test-rs in namespace replicaset-5322 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:44:59.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5322" for this suite.

• [SLOW TEST:36.129 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":3,"skipped":67,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:44:59.864: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 21 22:45:00.243: INFO: Waiting up to 5m0s for pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e" in namespace "emptydir-9425" to be "Succeeded or Failed"
Jun 21 22:45:00.266: INFO: Pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.60007ms
Jun 21 22:45:02.294: INFO: Pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051053576s
Jun 21 22:45:04.314: INFO: Pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e": Phase="Running", Reason="", readiness=false. Elapsed: 4.071542487s
Jun 21 22:45:06.334: INFO: Pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e": Phase="Running", Reason="", readiness=false. Elapsed: 6.090878331s
Jun 21 22:45:08.350: INFO: Pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.10718442s
STEP: Saw pod success
Jun 21 22:45:08.350: INFO: Pod "pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e" satisfied condition "Succeeded or Failed"
Jun 21 22:45:08.366: INFO: Trying to get logs from node 10.10.24.208 pod pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e container test-container: <nil>
STEP: delete the pod
Jun 21 22:45:08.445: INFO: Waiting for pod pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e to disappear
Jun 21 22:45:08.462: INFO: Pod pod-c85a5cc2-11c9-4585-ba79-10721cc5c30e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:45:08.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9425" for this suite.

• [SLOW TEST:8.653 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":72,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:45:08.518: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:45:08.769: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 21 22:45:18.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-182 --namespace=crd-publish-openapi-182 create -f -'
Jun 21 22:45:20.080: INFO: stderr: ""
Jun 21 22:45:20.080: INFO: stdout: "e2e-test-crd-publish-openapi-8674-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 21 22:45:20.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-182 --namespace=crd-publish-openapi-182 delete e2e-test-crd-publish-openapi-8674-crds test-cr'
Jun 21 22:45:20.257: INFO: stderr: ""
Jun 21 22:45:20.257: INFO: stdout: "e2e-test-crd-publish-openapi-8674-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 21 22:45:20.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-182 --namespace=crd-publish-openapi-182 apply -f -'
Jun 21 22:45:22.050: INFO: stderr: ""
Jun 21 22:45:22.050: INFO: stdout: "e2e-test-crd-publish-openapi-8674-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 21 22:45:22.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-182 --namespace=crd-publish-openapi-182 delete e2e-test-crd-publish-openapi-8674-crds test-cr'
Jun 21 22:45:22.248: INFO: stderr: ""
Jun 21 22:45:22.248: INFO: stdout: "e2e-test-crd-publish-openapi-8674-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 21 22:45:22.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-182 explain e2e-test-crd-publish-openapi-8674-crds'
Jun 21 22:45:23.736: INFO: stderr: ""
Jun 21 22:45:23.736: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8674-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:45:32.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-182" for this suite.

• [SLOW TEST:24.375 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":5,"skipped":77,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:45:32.894: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 22:45:33.842: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 22:45:35.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:45:37.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:45:39.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:45:41.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:45:43.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:45:45.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448333, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 22:45:48.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:45:48.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9491" for this suite.
STEP: Destroying namespace "webhook-9491-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.389 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":6,"skipped":100,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:45:49.283: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-444j
STEP: Creating a pod to test atomic-volume-subpath
Jun 21 22:45:49.677: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-444j" in namespace "subpath-3461" to be "Succeeded or Failed"
Jun 21 22:45:49.702: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Pending", Reason="", readiness=false. Elapsed: 23.855821ms
Jun 21 22:45:51.720: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042789012s
Jun 21 22:45:53.739: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 4.061024955s
Jun 21 22:45:55.757: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 6.079377948s
Jun 21 22:45:57.774: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 8.095875756s
Jun 21 22:45:59.796: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 10.118091348s
Jun 21 22:46:01.815: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 12.137654134s
Jun 21 22:46:03.832: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 14.154550028s
Jun 21 22:46:05.850: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 16.17266864s
Jun 21 22:46:07.872: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 18.19397419s
Jun 21 22:46:09.892: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 20.214772813s
Jun 21 22:46:11.908: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=true. Elapsed: 22.230546605s
Jun 21 22:46:13.928: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Running", Reason="", readiness=false. Elapsed: 24.250541135s
Jun 21 22:46:15.951: INFO: Pod "pod-subpath-test-configmap-444j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.273020151s
STEP: Saw pod success
Jun 21 22:46:15.951: INFO: Pod "pod-subpath-test-configmap-444j" satisfied condition "Succeeded or Failed"
Jun 21 22:46:15.967: INFO: Trying to get logs from node 10.10.24.214 pod pod-subpath-test-configmap-444j container test-container-subpath-configmap-444j: <nil>
STEP: delete the pod
Jun 21 22:46:16.115: INFO: Waiting for pod pod-subpath-test-configmap-444j to disappear
Jun 21 22:46:16.132: INFO: Pod pod-subpath-test-configmap-444j no longer exists
STEP: Deleting pod pod-subpath-test-configmap-444j
Jun 21 22:46:16.132: INFO: Deleting pod "pod-subpath-test-configmap-444j" in namespace "subpath-3461"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:46:16.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3461" for this suite.

• [SLOW TEST:26.916 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":7,"skipped":104,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:46:16.207: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9821.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9821.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 21 22:46:35.161: INFO: DNS probes using dns-9821/dns-test-e67fac8e-eda8-42be-aa1a-7ad8ca130101 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:46:35.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9821" for this suite.

• [SLOW TEST:19.070 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":8,"skipped":116,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:46:35.281: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jun 21 22:46:35.597: INFO: Waiting up to 5m0s for pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7" in namespace "var-expansion-8845" to be "Succeeded or Failed"
Jun 21 22:46:35.613: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.639615ms
Jun 21 22:46:37.631: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033632177s
Jun 21 22:46:39.647: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049633921s
Jun 21 22:46:41.664: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06678593s
Jun 21 22:46:43.681: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083141536s
Jun 21 22:46:45.725: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.127840202s
Jun 21 22:46:47.744: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.147017331s
Jun 21 22:46:49.764: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.166050228s
STEP: Saw pod success
Jun 21 22:46:49.764: INFO: Pod "var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7" satisfied condition "Succeeded or Failed"
Jun 21 22:46:49.780: INFO: Trying to get logs from node 10.10.24.208 pod var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7 container dapi-container: <nil>
STEP: delete the pod
Jun 21 22:46:49.921: INFO: Waiting for pod var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7 to disappear
Jun 21 22:46:49.937: INFO: Pod var-expansion-f35042a1-4ac9-4e13-bfad-bdb958dbddd7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:46:49.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8845" for this suite.

• [SLOW TEST:14.712 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":119,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:46:49.993: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 21 22:46:50.797: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 21 22:46:52.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448410, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448410, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448410, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448410, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 22:46:55.893: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:46:55.912: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:46:59.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-264" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.801 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":10,"skipped":124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:46:59.794: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun 21 22:47:00.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6509 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 21 22:47:00.252: INFO: stderr: ""
Jun 21 22:47:00.252: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 21 22:47:00.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6509 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Jun 21 22:47:02.977: INFO: stderr: ""
Jun 21 22:47:02.977: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun 21 22:47:03.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6509 delete pods e2e-test-httpd-pod'
Jun 21 22:47:06.709: INFO: stderr: ""
Jun 21 22:47:06.709: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:06.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6509" for this suite.

• [SLOW TEST:6.955 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:913
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":11,"skipped":155,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:06.749: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 22:47:07.049: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552" in namespace "projected-8912" to be "Succeeded or Failed"
Jun 21 22:47:07.066: INFO: Pod "downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552": Phase="Pending", Reason="", readiness=false. Elapsed: 17.499695ms
Jun 21 22:47:09.085: INFO: Pod "downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036450569s
Jun 21 22:47:11.107: INFO: Pod "downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05853718s
Jun 21 22:47:13.137: INFO: Pod "downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088159567s
STEP: Saw pod success
Jun 21 22:47:13.137: INFO: Pod "downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552" satisfied condition "Succeeded or Failed"
Jun 21 22:47:13.158: INFO: Trying to get logs from node 10.10.24.214 pod downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552 container client-container: <nil>
STEP: delete the pod
Jun 21 22:47:13.241: INFO: Waiting for pod downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552 to disappear
Jun 21 22:47:13.261: INFO: Pod downwardapi-volume-06d6aabf-546f-47c0-ba5c-2baee4761552 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:13.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8912" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":159,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:13.311: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:30.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6112" for this suite.

• [SLOW TEST:16.871 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":13,"skipped":163,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-466390d3-05be-46ea-9c23-588d9ff055cf
STEP: Creating a pod to test consume configMaps
Jun 21 22:47:30.492: INFO: Waiting up to 5m0s for pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725" in namespace "configmap-6287" to be "Succeeded or Failed"
Jun 21 22:47:30.508: INFO: Pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725": Phase="Pending", Reason="", readiness=false. Elapsed: 15.926223ms
Jun 21 22:47:32.524: INFO: Pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032256448s
Jun 21 22:47:34.551: INFO: Pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058706133s
Jun 21 22:47:36.567: INFO: Pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075319268s
Jun 21 22:47:38.610: INFO: Pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.117841147s
STEP: Saw pod success
Jun 21 22:47:38.610: INFO: Pod "pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725" satisfied condition "Succeeded or Failed"
Jun 21 22:47:38.650: INFO: Trying to get logs from node 10.10.24.214 pod pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 22:47:38.817: INFO: Waiting for pod pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725 to disappear
Jun 21 22:47:38.841: INFO: Pod pod-configmaps-96012582-db84-4dbd-9986-2561c99b5725 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:38.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6287" for this suite.

• [SLOW TEST:8.730 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":184,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:38.924: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a18f5000-7117-42a3-a153-5f6295c09720
STEP: Creating a pod to test consume secrets
Jun 21 22:47:39.458: INFO: Waiting up to 5m0s for pod "pod-secrets-1278b985-4375-4649-90db-ace242490f09" in namespace "secrets-8795" to be "Succeeded or Failed"
Jun 21 22:47:39.478: INFO: Pod "pod-secrets-1278b985-4375-4649-90db-ace242490f09": Phase="Pending", Reason="", readiness=false. Elapsed: 20.111479ms
Jun 21 22:47:41.497: INFO: Pod "pod-secrets-1278b985-4375-4649-90db-ace242490f09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039050891s
Jun 21 22:47:43.520: INFO: Pod "pod-secrets-1278b985-4375-4649-90db-ace242490f09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061285118s
Jun 21 22:47:45.537: INFO: Pod "pod-secrets-1278b985-4375-4649-90db-ace242490f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078240738s
STEP: Saw pod success
Jun 21 22:47:45.537: INFO: Pod "pod-secrets-1278b985-4375-4649-90db-ace242490f09" satisfied condition "Succeeded or Failed"
Jun 21 22:47:45.552: INFO: Trying to get logs from node 10.10.24.208 pod pod-secrets-1278b985-4375-4649-90db-ace242490f09 container secret-env-test: <nil>
STEP: delete the pod
Jun 21 22:47:45.653: INFO: Waiting for pod pod-secrets-1278b985-4375-4649-90db-ace242490f09 to disappear
Jun 21 22:47:45.668: INFO: Pod pod-secrets-1278b985-4375-4649-90db-ace242490f09 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:45.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8795" for this suite.

• [SLOW TEST:6.801 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":190,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:45.725: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:46.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2745" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":16,"skipped":210,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:46.357: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun 21 22:47:46.711: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:47:55.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5132" for this suite.

• [SLOW TEST:9.205 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":17,"skipped":224,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:47:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:47:55.916: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 21 22:48:00.938: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 21 22:48:00.938: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 21 22:48:02.952: INFO: Creating deployment "test-rollover-deployment"
Jun 21 22:48:03.010: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 21 22:48:05.055: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 21 22:48:05.096: INFO: Ensure that both replica sets have 1 created replica
Jun 21 22:48:05.121: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 21 22:48:05.163: INFO: Updating deployment test-rollover-deployment
Jun 21 22:48:05.163: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 21 22:48:07.205: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 21 22:48:07.231: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 21 22:48:07.256: INFO: all replica sets need to contain the pod-template-hash label
Jun 21 22:48:07.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448485, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:48:09.283: INFO: all replica sets need to contain the pod-template-hash label
Jun 21 22:48:09.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448487, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:48:11.284: INFO: all replica sets need to contain the pod-template-hash label
Jun 21 22:48:11.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448487, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:48:13.296: INFO: all replica sets need to contain the pod-template-hash label
Jun 21 22:48:13.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448487, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:48:15.299: INFO: all replica sets need to contain the pod-template-hash label
Jun 21 22:48:15.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448487, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:48:17.293: INFO: all replica sets need to contain the pod-template-hash label
Jun 21 22:48:17.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448487, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448483, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 22:48:19.298: INFO: 
Jun 21 22:48:19.298: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 21 22:48:19.335: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4232  ccb69558-2d54-432e-8e62-73965e6537a8 56026 2 2022-06-21 22:48:02 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-21 22:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:48:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a4f378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-21 22:48:03 +0000 UTC,LastTransitionTime:2022-06-21 22:48:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-06-21 22:48:17 +0000 UTC,LastTransitionTime:2022-06-21 22:48:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 21 22:48:19.346: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-4232  f59252f7-7d51-476a-884e-6974869deb8b 56015 2 2022-06-21 22:48:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ccb69558-2d54-432e-8e62-73965e6537a8 0xc001a4f960 0xc001a4f961}] []  [{kube-controller-manager Update apps/v1 2022-06-21 22:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb69558-2d54-432e-8e62-73965e6537a8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:48:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a4f9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 21 22:48:19.346: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 21 22:48:19.346: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4232  b5767c8f-aa3c-44b4-9b7b-e98c0a1afa3d 56024 2 2022-06-21 22:47:55 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ccb69558-2d54-432e-8e62-73965e6537a8 0xc001a4f717 0xc001a4f718}] []  [{e2e.test Update apps/v1 2022-06-21 22:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:48:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb69558-2d54-432e-8e62-73965e6537a8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:48:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001a4f7d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 21 22:48:19.347: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4232  2efee064-b4fb-4377-bd69-de2e076dc6fd 55930 2 2022-06-21 22:48:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ccb69558-2d54-432e-8e62-73965e6537a8 0xc001a4f847 0xc001a4f848}] []  [{kube-controller-manager Update apps/v1 2022-06-21 22:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccb69558-2d54-432e-8e62-73965e6537a8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:48:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a4f8f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 21 22:48:19.361: INFO: Pod "test-rollover-deployment-98c5f4599-2wgdb" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-2wgdb test-rollover-deployment-98c5f4599- deployment-4232  543f77fe-6989-429b-9446-3ae25a8e80bd 55968 0 2022-06-21 22:48:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:824755b59c2ebf6f1cb48db60067a67af5ee0ace5d8a59183680964658ca1c5d cni.projectcalico.org/podIP:172.30.224.53/32 cni.projectcalico.org/podIPs:172.30.224.53/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.53"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.53"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 f59252f7-7d51-476a-884e-6974869deb8b 0xc00a0b8227 0xc00a0b8228}] []  [{kube-controller-manager Update v1 2022-06-21 22:48:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f59252f7-7d51-476a-884e-6974869deb8b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 22:48:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 22:48:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 22:48:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4q6bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4q6bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c31,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-q55pw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:48:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:48:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:48:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.53,StartTime:2022-06-21 22:48:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 22:48:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://d74279304072e1e248dddd8b7570f1c3e28a860af4f575de53a7c5fe042f121b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:48:19.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4232" for this suite.

• [SLOW TEST:23.836 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":18,"skipped":232,"failed":0}
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:48:19.400: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:48:19.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2456" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":19,"skipped":232,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:48:19.839: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:48:20.090: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 21 22:48:30.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-5735 --namespace=crd-publish-openapi-5735 create -f -'
Jun 21 22:48:32.079: INFO: stderr: ""
Jun 21 22:48:32.079: INFO: stdout: "e2e-test-crd-publish-openapi-3196-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 21 22:48:32.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-5735 --namespace=crd-publish-openapi-5735 delete e2e-test-crd-publish-openapi-3196-crds test-cr'
Jun 21 22:48:32.232: INFO: stderr: ""
Jun 21 22:48:32.232: INFO: stdout: "e2e-test-crd-publish-openapi-3196-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 21 22:48:32.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-5735 --namespace=crd-publish-openapi-5735 apply -f -'
Jun 21 22:48:33.902: INFO: stderr: ""
Jun 21 22:48:33.902: INFO: stdout: "e2e-test-crd-publish-openapi-3196-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 21 22:48:33.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-5735 --namespace=crd-publish-openapi-5735 delete e2e-test-crd-publish-openapi-3196-crds test-cr'
Jun 21 22:48:34.136: INFO: stderr: ""
Jun 21 22:48:34.136: INFO: stdout: "e2e-test-crd-publish-openapi-3196-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 21 22:48:34.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-5735 explain e2e-test-crd-publish-openapi-3196-crds'
Jun 21 22:48:35.933: INFO: stderr: ""
Jun 21 22:48:35.933: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3196-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:48:45.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5735" for this suite.

• [SLOW TEST:25.355 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":20,"skipped":235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:48:45.196: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-1197/secret-test-e186681b-92b1-45a6-ba60-ae0cf497b753
STEP: Creating a pod to test consume secrets
Jun 21 22:48:45.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59" in namespace "secrets-1197" to be "Succeeded or Failed"
Jun 21 22:48:45.576: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Pending", Reason="", readiness=false. Elapsed: 14.599062ms
Jun 21 22:48:47.592: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031316929s
Jun 21 22:48:49.610: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048768783s
Jun 21 22:48:51.624: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062683074s
Jun 21 22:48:53.640: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078708835s
Jun 21 22:48:55.658: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096723927s
Jun 21 22:48:57.677: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.115739072s
STEP: Saw pod success
Jun 21 22:48:57.677: INFO: Pod "pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59" satisfied condition "Succeeded or Failed"
Jun 21 22:48:57.718: INFO: Trying to get logs from node 10.10.24.214 pod pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59 container env-test: <nil>
STEP: delete the pod
Jun 21 22:48:57.979: INFO: Waiting for pod pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59 to disappear
Jun 21 22:48:57.998: INFO: Pod pod-configmaps-78f68b14-79eb-4c87-8452-372a9e528f59 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:48:57.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1197" for this suite.

• [SLOW TEST:12.857 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":318,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:48:58.052: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun 21 22:48:58.397: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:49:00.422: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun 21 22:49:00.528: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:49:02.554: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:49:04.543: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun 21 22:49:04.595: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 21 22:49:04.609: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 21 22:49:06.609: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 21 22:49:06.631: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 21 22:49:08.609: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 21 22:49:08.629: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:49:08.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6353" for this suite.

• [SLOW TEST:10.661 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":337,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:49:08.714: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:49:08.972: INFO: Creating simple deployment test-new-deployment
Jun 21 22:49:09.027: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Jun 21 22:49:11.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448549, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448549, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448549, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791448549, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 21 22:49:13.219: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5309  3316ebca-c58c-4d1c-874d-b4e408f724b3 56853 3 2022-06-21 22:49:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-06-21 22:49:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004101438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-21 22:49:11 +0000 UTC,LastTransitionTime:2022-06-21 22:49:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-06-21 22:49:11 +0000 UTC,LastTransitionTime:2022-06-21 22:49:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 21 22:49:13.234: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-5309  e8b380a4-0967-4c9b-b22f-fe50190e5a30 56852 2 2022-06-21 22:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3316ebca-c58c-4d1c-874d-b4e408f724b3 0xc0007456b7 0xc0007456b8}] []  [{kube-controller-manager Update apps/v1 2022-06-21 22:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3316ebca-c58c-4d1c-874d-b4e408f724b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 22:49:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000745858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 21 22:49:13.256: INFO: Pod "test-new-deployment-847dcfb7fb-4qqnq" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-4qqnq test-new-deployment-847dcfb7fb- deployment-5309  0bf51ed2-5bce-4451-a546-6ac98798ac43 56858 0 2022-06-21 22:49:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb e8b380a4-0967-4c9b-b22f-fe50190e5a30 0xc0004fefd7 0xc0004fefd8}] []  [{kube-controller-manager Update v1 2022-06-21 22:49:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8b380a4-0967-4c9b-b22f-fe50190e5a30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8vbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8vbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c32,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2cmlv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:49:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 21 22:49:13.257: INFO: Pod "test-new-deployment-847dcfb7fb-lk9q5" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-lk9q5 test-new-deployment-847dcfb7fb- deployment-5309  2a398c44-ca0b-4421-923f-2b7ee78a8752 56840 0 2022-06-21 22:49:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:beab391e8065da9d03edaca70027b9531601efd353c0a44b6c282bd5529278e8 cni.projectcalico.org/podIP:172.30.47.254/32 cni.projectcalico.org/podIPs:172.30.47.254/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.254"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.254"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb e8b380a4-0967-4c9b-b22f-fe50190e5a30 0xc0004ffc67 0xc0004ffc68}] []  [{kube-controller-manager Update v1 2022-06-21 22:49:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8b380a4-0967-4c9b-b22f-fe50190e5a30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 22:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 22:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 22:49:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmdpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmdpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c32,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2cmlv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:49:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:49:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 22:49:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.254,StartTime:2022-06-21 22:49:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 22:49:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://c0990983fa26694e704f46abdbc2167f85a6a620a1755b4518dda8b8c48045ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:49:13.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5309" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":23,"skipped":351,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:49:13.308: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-884dc300-d4f1-4b1c-a76f-3b84c76b4840
STEP: Creating a pod to test consume configMaps
Jun 21 22:49:13.616: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17" in namespace "projected-9888" to be "Succeeded or Failed"
Jun 21 22:49:13.634: INFO: Pod "pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018144ms
Jun 21 22:49:15.650: INFO: Pod "pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033966521s
Jun 21 22:49:17.667: INFO: Pod "pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050239656s
Jun 21 22:49:19.683: INFO: Pod "pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066459571s
STEP: Saw pod success
Jun 21 22:49:19.683: INFO: Pod "pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17" satisfied condition "Succeeded or Failed"
Jun 21 22:49:19.694: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 21 22:49:19.847: INFO: Waiting for pod pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17 to disappear
Jun 21 22:49:19.875: INFO: Pod pod-projected-configmaps-a10550b6-42a0-4bb1-aed5-8818b520ad17 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:49:19.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9888" for this suite.

• [SLOW TEST:6.613 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":357,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:49:19.923: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 21 22:49:20.212: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 21 22:50:20.509: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jun 21 22:50:20.719: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 21 22:50:20.803: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 21 22:50:20.893: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 21 22:50:20.941: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 21 22:50:21.032: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 21 22:50:21.083: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:50:45.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4729" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:85.782 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":25,"skipped":374,"failed":0}
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:50:45.705: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 21 22:50:45.995: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 21 22:50:46.064: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 21 22:50:46.064: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 21 22:50:46.130: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 21 22:50:46.130: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 21 22:50:46.207: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 21 22:50:46.207: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 21 22:50:53.438: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:50:53.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1894" for this suite.

• [SLOW TEST:7.866 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":26,"skipped":374,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:50:53.571: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0621 22:51:34.107203      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 21 22:51:34.107: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 21 22:51:34.107: INFO: Deleting pod "simpletest.rc-6spvv" in namespace "gc-2794"
Jun 21 22:51:34.154: INFO: Deleting pod "simpletest.rc-6v95p" in namespace "gc-2794"
Jun 21 22:51:34.202: INFO: Deleting pod "simpletest.rc-88jx9" in namespace "gc-2794"
Jun 21 22:51:34.265: INFO: Deleting pod "simpletest.rc-8h2kw" in namespace "gc-2794"
Jun 21 22:51:34.302: INFO: Deleting pod "simpletest.rc-b8krf" in namespace "gc-2794"
Jun 21 22:51:34.344: INFO: Deleting pod "simpletest.rc-lgws8" in namespace "gc-2794"
Jun 21 22:51:34.399: INFO: Deleting pod "simpletest.rc-lskp2" in namespace "gc-2794"
Jun 21 22:51:34.439: INFO: Deleting pod "simpletest.rc-nmdml" in namespace "gc-2794"
Jun 21 22:51:34.485: INFO: Deleting pod "simpletest.rc-pnplc" in namespace "gc-2794"
Jun 21 22:51:34.535: INFO: Deleting pod "simpletest.rc-w4jk6" in namespace "gc-2794"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:51:34.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2794" for this suite.

• [SLOW TEST:41.097 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":27,"skipped":386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:51:34.669: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-701bea58-bf27-4645-861a-0db8c2fbdd84
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:51:34.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3208" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":28,"skipped":472,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:51:34.978: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 21 22:51:35.370: INFO: starting watch
STEP: patching
STEP: updating
Jun 21 22:51:35.433: INFO: waiting for watch events with expected annotations
Jun 21 22:51:35.433: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:51:35.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5395" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":29,"skipped":478,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:51:35.712: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 21 22:51:36.186: INFO: Waiting up to 5m0s for pod "pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf" in namespace "emptydir-9167" to be "Succeeded or Failed"
Jun 21 22:51:36.201: INFO: Pod "pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.405825ms
Jun 21 22:51:38.221: INFO: Pod "pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03512647s
Jun 21 22:51:40.244: INFO: Pod "pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057302051s
Jun 21 22:51:42.263: INFO: Pod "pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076209799s
STEP: Saw pod success
Jun 21 22:51:42.263: INFO: Pod "pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf" satisfied condition "Succeeded or Failed"
Jun 21 22:51:42.297: INFO: Trying to get logs from node 10.10.24.214 pod pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf container test-container: <nil>
STEP: delete the pod
Jun 21 22:51:42.507: INFO: Waiting for pod pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf to disappear
Jun 21 22:51:42.522: INFO: Pod pod-a0e3c4fb-81c3-433f-8fde-293973dfb7cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:51:42.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9167" for this suite.

• [SLOW TEST:6.856 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":490,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:51:42.571: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:51:43.062: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c9492e1f-c224-471f-9ba8-6c03e12503fb", Controller:(*bool)(0xc003b97a82), BlockOwnerDeletion:(*bool)(0xc003b97a83)}}
Jun 21 22:51:43.113: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"13c01049-5ec3-4a2d-8ad2-14a79fcdabf2", Controller:(*bool)(0xc003b97db2), BlockOwnerDeletion:(*bool)(0xc003b97db3)}}
Jun 21 22:51:43.140: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"56f68156-3d3d-4d73-b21d-40899bef6a24", Controller:(*bool)(0xc00a3880d6), BlockOwnerDeletion:(*bool)(0xc00a3880d7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:51:48.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3062" for this suite.

• [SLOW TEST:5.677 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":31,"skipped":499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:51:48.249: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 21 22:51:48.612: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 21 22:51:48.638: INFO: starting watch
STEP: patching
STEP: updating
Jun 21 22:51:48.716: INFO: waiting for watch events with expected annotations
Jun 21 22:51:48.718: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:51:48.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5591" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":32,"skipped":521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:51:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1293
STEP: creating service affinity-nodeport in namespace services-1293
STEP: creating replication controller affinity-nodeport in namespace services-1293
I0621 22:51:49.340110      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1293, replica count: 3
I0621 22:51:52.392709      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0621 22:51:55.394857      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0621 22:51:58.397696      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0621 22:52:01.398533      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0621 22:52:04.399491      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 22:52:04.460: INFO: Creating new exec pod
Jun 21 22:52:09.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1293 exec execpod-affinitygn5v4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun 21 22:52:10.181: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 21 22:52:10.181: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:52:10.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1293 exec execpod-affinitygn5v4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.130.185 80'
Jun 21 22:52:10.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.130.185 80\nConnection to 172.21.130.185 80 port [tcp/http] succeeded!\n"
Jun 21 22:52:10.658: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:52:10.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1293 exec execpod-affinitygn5v4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.214 32459'
Jun 21 22:52:11.203: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.214 32459\nConnection to 10.10.24.214 32459 port [tcp/*] succeeded!\n"
Jun 21 22:52:11.203: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:52:11.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1293 exec execpod-affinitygn5v4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 32459'
Jun 21 22:52:11.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 32459\nConnection to 10.10.24.206 32459 port [tcp/*] succeeded!\n"
Jun 21 22:52:11.674: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:52:11.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1293 exec execpod-affinitygn5v4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.24.206:32459/ ; done'
Jun 21 22:52:12.452: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:32459/\n"
Jun 21 22:52:12.452: INFO: stdout: "\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm\naffinity-nodeport-z26tm"
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.452: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.453: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.453: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.453: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.453: INFO: Received response from host: affinity-nodeport-z26tm
Jun 21 22:52:12.453: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1293, will wait for the garbage collector to delete the pods
Jun 21 22:52:12.599: INFO: Deleting ReplicationController affinity-nodeport took: 29.993504ms
Jun 21 22:52:12.803: INFO: Terminating ReplicationController affinity-nodeport pods took: 204.36208ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:52:16.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1293" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:27.290 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":33,"skipped":546,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:52:16.257: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 22:52:16.549: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82" in namespace "projected-973" to be "Succeeded or Failed"
Jun 21 22:52:16.561: INFO: Pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.604731ms
Jun 21 22:52:18.577: INFO: Pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028576333s
Jun 21 22:52:20.596: INFO: Pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047479023s
Jun 21 22:52:22.635: INFO: Pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086090439s
Jun 21 22:52:24.657: INFO: Pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.107835145s
STEP: Saw pod success
Jun 21 22:52:24.657: INFO: Pod "downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82" satisfied condition "Succeeded or Failed"
Jun 21 22:52:24.673: INFO: Trying to get logs from node 10.10.24.214 pod downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82 container client-container: <nil>
STEP: delete the pod
Jun 21 22:52:24.753: INFO: Waiting for pod downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82 to disappear
Jun 21 22:52:24.769: INFO: Pod downwardapi-volume-62774ce5-4a90-413a-9904-132043bcfc82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:52:24.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-973" for this suite.

• [SLOW TEST:8.561 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":566,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:52:24.819: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun 21 22:52:25.129: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:52:27.146: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun 21 22:52:27.215: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:52:29.235: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:52:31.229: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun 21 22:52:31.287: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 21 22:52:31.320: INFO: Pod pod-with-prestop-http-hook still exists
Jun 21 22:52:33.322: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 21 22:52:33.340: INFO: Pod pod-with-prestop-http-hook still exists
Jun 21 22:52:35.321: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 21 22:52:35.335: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:52:35.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9226" for this suite.

• [SLOW TEST:10.730 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":567,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:52:35.552: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7886
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7886
STEP: Waiting until pod test-pod will start running in namespace statefulset-7886
STEP: Creating statefulset with conflicting port in namespace statefulset-7886
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7886
Jun 21 22:52:40.184: INFO: Observed stateful pod in namespace: statefulset-7886, name: ss-0, uid: b3dce190-af35-4a9b-bf23-bb485fa3ccbc, status phase: Pending. Waiting for statefulset controller to delete.
Jun 21 22:52:40.238: INFO: Observed stateful pod in namespace: statefulset-7886, name: ss-0, uid: b3dce190-af35-4a9b-bf23-bb485fa3ccbc, status phase: Failed. Waiting for statefulset controller to delete.
Jun 21 22:52:40.266: INFO: Observed stateful pod in namespace: statefulset-7886, name: ss-0, uid: b3dce190-af35-4a9b-bf23-bb485fa3ccbc, status phase: Failed. Waiting for statefulset controller to delete.
Jun 21 22:52:40.286: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7886
STEP: Removing pod with conflicting port in namespace statefulset-7886
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7886 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 22:52:46.418: INFO: Deleting all statefulset in ns statefulset-7886
Jun 21 22:52:46.433: INFO: Scaling statefulset ss to 0
Jun 21 22:52:56.512: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 22:52:56.525: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:52:56.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7886" for this suite.

• [SLOW TEST:21.093 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":36,"skipped":569,"failed":0}
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:52:56.647: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 21 22:52:56.999: INFO: The status of Pod pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:52:59.014: INFO: The status of Pod pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:53:01.017: INFO: The status of Pod pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 21 22:53:01.620: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3"
Jun 21 22:53:01.620: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3" in namespace "pods-4417" to be "terminated due to deadline exceeded"
Jun 21 22:53:01.634: INFO: Pod "pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3": Phase="Running", Reason="", readiness=true. Elapsed: 13.947995ms
Jun 21 22:53:03.652: INFO: Pod "pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.03174661s
Jun 21 22:53:05.669: INFO: Pod "pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.048943783s
Jun 21 22:53:05.669: INFO: Pod "pod-update-activedeadlineseconds-6d1e2025-a2c6-4aed-bf7e-aa2a840ab1f3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:05.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4417" for this suite.

• [SLOW TEST:9.075 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":569,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:05.722: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 22:53:06.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6" in namespace "projected-6162" to be "Succeeded or Failed"
Jun 21 22:53:06.153: INFO: Pod "downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6": Phase="Pending", Reason="", readiness=false. Elapsed: 32.151371ms
Jun 21 22:53:08.169: INFO: Pod "downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048006998s
Jun 21 22:53:10.193: INFO: Pod "downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072111453s
Jun 21 22:53:12.218: INFO: Pod "downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096537855s
STEP: Saw pod success
Jun 21 22:53:12.218: INFO: Pod "downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6" satisfied condition "Succeeded or Failed"
Jun 21 22:53:12.230: INFO: Trying to get logs from node 10.10.24.206 pod downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6 container client-container: <nil>
STEP: delete the pod
Jun 21 22:53:12.331: INFO: Waiting for pod downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6 to disappear
Jun 21 22:53:12.343: INFO: Pod downwardapi-volume-fc19c335-c1a0-4384-91d5-4ba29afebeb6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:12.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6162" for this suite.

• [SLOW TEST:6.671 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":579,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:12.403: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 21 22:53:19.024: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:19.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5099" for this suite.

• [SLOW TEST:6.770 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":593,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:19.177: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jun 21 22:53:19.421: INFO: created test-event-1
Jun 21 22:53:19.443: INFO: created test-event-2
Jun 21 22:53:19.476: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 21 22:53:19.498: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 21 22:53:19.610: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:19.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8155" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":40,"skipped":598,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:19.688: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jun 21 22:53:19.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 create -f -'
Jun 21 22:53:21.903: INFO: stderr: ""
Jun 21 22:53:21.903: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 21 22:53:21.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 21 22:53:22.093: INFO: stderr: ""
Jun 21 22:53:22.093: INFO: stdout: "update-demo-nautilus-9vrvk update-demo-nautilus-ksp9j "
Jun 21 22:53:22.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-9vrvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 21 22:53:22.255: INFO: stderr: ""
Jun 21 22:53:22.255: INFO: stdout: ""
Jun 21 22:53:22.255: INFO: update-demo-nautilus-9vrvk is created but not running
Jun 21 22:53:27.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 21 22:53:27.399: INFO: stderr: ""
Jun 21 22:53:27.399: INFO: stdout: "update-demo-nautilus-9vrvk update-demo-nautilus-ksp9j "
Jun 21 22:53:27.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-9vrvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 21 22:53:27.569: INFO: stderr: ""
Jun 21 22:53:27.569: INFO: stdout: ""
Jun 21 22:53:27.569: INFO: update-demo-nautilus-9vrvk is created but not running
Jun 21 22:53:32.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 21 22:53:32.725: INFO: stderr: ""
Jun 21 22:53:32.725: INFO: stdout: "update-demo-nautilus-9vrvk update-demo-nautilus-ksp9j "
Jun 21 22:53:32.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-9vrvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 21 22:53:32.865: INFO: stderr: ""
Jun 21 22:53:32.865: INFO: stdout: ""
Jun 21 22:53:32.866: INFO: update-demo-nautilus-9vrvk is created but not running
Jun 21 22:53:37.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 21 22:53:38.043: INFO: stderr: ""
Jun 21 22:53:38.043: INFO: stdout: "update-demo-nautilus-9vrvk update-demo-nautilus-ksp9j "
Jun 21 22:53:38.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-9vrvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 21 22:53:38.227: INFO: stderr: ""
Jun 21 22:53:38.227: INFO: stdout: "true"
Jun 21 22:53:38.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-9vrvk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 21 22:53:38.352: INFO: stderr: ""
Jun 21 22:53:38.352: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 21 22:53:38.352: INFO: validating pod update-demo-nautilus-9vrvk
Jun 21 22:53:38.382: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 21 22:53:38.382: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 21 22:53:38.382: INFO: update-demo-nautilus-9vrvk is verified up and running
Jun 21 22:53:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-ksp9j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 21 22:53:38.539: INFO: stderr: ""
Jun 21 22:53:38.539: INFO: stdout: "true"
Jun 21 22:53:38.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods update-demo-nautilus-ksp9j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 21 22:53:38.719: INFO: stderr: ""
Jun 21 22:53:38.719: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 21 22:53:38.719: INFO: validating pod update-demo-nautilus-ksp9j
Jun 21 22:53:38.749: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 21 22:53:38.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 21 22:53:38.750: INFO: update-demo-nautilus-ksp9j is verified up and running
STEP: using delete to clean up resources
Jun 21 22:53:38.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 delete --grace-period=0 --force -f -'
Jun 21 22:53:38.920: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 22:53:38.920: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 21 22:53:38.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get rc,svc -l name=update-demo --no-headers'
Jun 21 22:53:39.061: INFO: stderr: "No resources found in kubectl-2914 namespace.\n"
Jun 21 22:53:39.061: INFO: stdout: ""
Jun 21 22:53:39.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2914 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 21 22:53:39.240: INFO: stderr: ""
Jun 21 22:53:39.240: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:39.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2914" for this suite.

• [SLOW TEST:19.610 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":41,"skipped":615,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:39.299: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:53:39.573: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:40.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3017" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":42,"skipped":620,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:40.698: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:53:41.005: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 21 22:53:42.207: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:53:43.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-916" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":43,"skipped":643,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:53:43.323: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7897, will wait for the garbage collector to delete the pods
Jun 21 22:53:47.749: INFO: Deleting Job.batch foo took: 26.473763ms
Jun 21 22:53:47.950: INFO: Terminating Job.batch foo pods took: 200.988912ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:54:21.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7897" for this suite.

• [SLOW TEST:38.024 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":44,"skipped":656,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:54:21.347: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 21 22:54:22.881: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0621 22:54:22.881688      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 21 22:54:22.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4113" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":45,"skipped":662,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:54:22.927: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:54:23.196: INFO: Creating pod...
Jun 21 22:54:23.277: INFO: Pod Quantity: 1 Status: Pending
Jun 21 22:54:24.326: INFO: Pod Quantity: 1 Status: Pending
Jun 21 22:54:25.301: INFO: Pod Quantity: 1 Status: Pending
Jun 21 22:54:26.295: INFO: Pod Status: Running
Jun 21 22:54:26.295: INFO: Creating service...
Jun 21 22:54:26.336: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/DELETE
Jun 21 22:54:26.365: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 21 22:54:26.367: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/GET
Jun 21 22:54:26.388: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 21 22:54:26.388: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/HEAD
Jun 21 22:54:26.409: INFO: http.Client request:HEAD | StatusCode:200
Jun 21 22:54:26.410: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/OPTIONS
Jun 21 22:54:26.437: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 21 22:54:26.437: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/PATCH
Jun 21 22:54:26.474: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 21 22:54:26.475: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/POST
Jun 21 22:54:26.498: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 21 22:54:26.498: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/pods/agnhost/proxy/some/path/with/PUT
Jun 21 22:54:26.519: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 21 22:54:26.519: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/DELETE
Jun 21 22:54:26.548: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 21 22:54:26.548: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/GET
Jun 21 22:54:26.588: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 21 22:54:26.588: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/HEAD
Jun 21 22:54:26.640: INFO: http.Client request:HEAD | StatusCode:200
Jun 21 22:54:26.640: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/OPTIONS
Jun 21 22:54:26.683: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 21 22:54:26.683: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/PATCH
Jun 21 22:54:26.714: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 21 22:54:26.715: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/POST
Jun 21 22:54:26.752: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 21 22:54:26.760: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7447/services/test-service/proxy/some/path/with/PUT
Jun 21 22:54:26.797: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:54:26.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7447" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":46,"skipped":668,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:54:26.857: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 21 22:54:27.052: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 21 22:54:27.102: INFO: Waiting for terminating namespaces to be deleted...
Jun 21 22:54:27.135: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.206 before test
Jun 21 22:54:27.252: INFO: calico-node-24dvg from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.252: INFO: 	Container calico-node ready: true, restart count 0
Jun 21 22:54:27.252: INFO: calico-typha-65cc4575bc-f88zd from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.253: INFO: 	Container calico-typha ready: true, restart count 0
Jun 21 22:54:27.253: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-21 21:14:34 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.253: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 21 22:54:27.253: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-hd8s7 from ibm-system started at 2022-06-21 21:19:53 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.253: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 21 22:54:27.253: INFO: ibm-keepalived-watcher-rpq7r from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.253: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 21 22:54:27.253: INFO: ibm-master-proxy-static-10.10.24.206 from kube-system started at 2022-06-21 21:08:38 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.254: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 21 22:54:27.254: INFO: 	Container pause ready: true, restart count 0
Jun 21 22:54:27.254: INFO: ibmcloud-block-storage-driver-l2pxj from kube-system started at 2022-06-21 21:08:46 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.254: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 21 22:54:27.254: INFO: vpn-69b96645d-qtgxb from kube-system started at 2022-06-21 21:11:45 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.254: INFO: 	Container vpn ready: true, restart count 1
Jun 21 22:54:27.254: INFO: tuned-pc9t5 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.254: INFO: 	Container tuned ready: true, restart count 0
Jun 21 22:54:27.254: INFO: csi-snapshot-controller-69d5f9c777-p6845 from openshift-cluster-storage-operator started at 2022-06-21 21:13:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.254: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 21 22:54:27.255: INFO: csi-snapshot-webhook-55d4797dc4-2c7hn from openshift-cluster-storage-operator started at 2022-06-21 21:12:56 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.255: INFO: 	Container webhook ready: true, restart count 0
Jun 21 22:54:27.255: INFO: console-67bfbdb4dc-mtf8v from openshift-console started at 2022-06-21 21:17:04 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.255: INFO: 	Container console ready: true, restart count 0
Jun 21 22:54:27.255: INFO: downloads-988dbf8f4-wqhz7 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.255: INFO: 	Container download-server ready: true, restart count 0
Jun 21 22:54:27.255: INFO: dns-default-wqq2m from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.255: INFO: 	Container dns ready: true, restart count 0
Jun 21 22:54:27.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.255: INFO: node-resolver-xjddx from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.255: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 21 22:54:27.256: INFO: node-ca-wf6f5 from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.256: INFO: 	Container node-ca ready: true, restart count 0
Jun 21 22:54:27.256: INFO: ingress-canary-mcgps from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.256: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 21 22:54:27.256: INFO: router-default-bb6fc54c5-hbbwg from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.256: INFO: 	Container router ready: true, restart count 0
Jun 21 22:54:27.256: INFO: openshift-kube-proxy-g5jv6 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.257: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 21 22:54:27.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.257: INFO: migrator-8467484867-zz9sp from openshift-kube-storage-version-migrator started at 2022-06-21 21:12:46 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.257: INFO: 	Container migrator ready: true, restart count 0
Jun 21 22:54:27.257: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-06-21 21:16:13 +0000 UTC (5 container statuses recorded)
Jun 21 22:54:27.257: INFO: 	Container alertmanager ready: true, restart count 0
Jun 21 22:54:27.257: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 21 22:54:27.257: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:54:27.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.258: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.258: INFO: kube-state-metrics-5f9b9688bc-jrnb4 from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 21 22:54:27.258: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 21 22:54:27.258: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 21 22:54:27.258: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 21 22:54:27.258: INFO: node-exporter-xvzpx from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.258: INFO: 	Container node-exporter ready: true, restart count 0
Jun 21 22:54:27.258: INFO: openshift-state-metrics-744d546498-5g75r from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 21 22:54:27.258: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 21 22:54:27.259: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 21 22:54:27.259: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 21 22:54:27.259: INFO: prometheus-adapter-5c564c9546-wz2lv from openshift-monitoring started at 2022-06-21 21:18:18 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.259: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 21 22:54:27.259: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-06-21 21:16:32 +0000 UTC (7 container statuses recorded)
Jun 21 22:54:27.259: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:54:27.259: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.259: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 21 22:54:27.260: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.260: INFO: 	Container prometheus ready: true, restart count 0
Jun 21 22:54:27.260: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 21 22:54:27.260: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 21 22:54:27.260: INFO: prometheus-operator-bf9ff66c-t6r25 from openshift-monitoring started at 2022-06-21 21:12:54 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.260: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jun 21 22:54:27.260: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 21 22:54:27.260: INFO: telemeter-client-8dfb6c944-8k9b8 from openshift-monitoring started at 2022-06-21 21:13:35 +0000 UTC (3 container statuses recorded)
Jun 21 22:54:27.260: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.260: INFO: 	Container reload ready: true, restart count 0
Jun 21 22:54:27.261: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 21 22:54:27.261: INFO: thanos-querier-75f4c69596-kmkqk from openshift-monitoring started at 2022-06-21 21:16:16 +0000 UTC (5 container statuses recorded)
Jun 21 22:54:27.261: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.261: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 21 22:54:27.261: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 21 22:54:27.261: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.261: INFO: 	Container thanos-query ready: true, restart count 0
Jun 21 22:54:27.262: INFO: multus-additional-cni-plugins-qjdfj from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.262: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 21 22:54:27.262: INFO: multus-admission-controller-sndqs from openshift-multus started at 2022-06-21 21:11:41 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.262: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.262: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 21 22:54:27.262: INFO: multus-c4bqm from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.262: INFO: 	Container kube-multus ready: true, restart count 0
Jun 21 22:54:27.262: INFO: network-metrics-daemon-l6zp7 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.262: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.262: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 21 22:54:27.263: INFO: network-check-target-4dx2q from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.263: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 21 22:54:27.263: INFO: collect-profiles-27597495--1-kj65w from openshift-operator-lifecycle-manager started at 2022-06-21 22:15:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.263: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 21 22:54:27.263: INFO: collect-profiles-27597510--1-h98k2 from openshift-operator-lifecycle-manager started at 2022-06-21 22:30:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.263: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 21 22:54:27.264: INFO: packageserver-cfb4bb4b4-kzchv from openshift-operator-lifecycle-manager started at 2022-06-21 21:13:57 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.264: INFO: 	Container packageserver ready: true, restart count 0
Jun 21 22:54:27.264: INFO: service-ca-7f5f4d65b9-gd4rm from openshift-service-ca started at 2022-06-21 21:12:50 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.264: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 21 22:54:27.264: INFO: sonobuoy-e2e-job-9869bda7abe94c50 from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.264: INFO: 	Container e2e ready: true, restart count 0
Jun 21 22:54:27.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:54:27.264: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-8pkdj from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:54:27.264: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 21 22:54:27.265: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.208 before test
Jun 21 22:54:27.396: INFO: calico-kube-controllers-fbfbb867b-d8msg from calico-system started at 2022-06-21 21:11:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.396: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 21 22:54:27.397: INFO: calico-node-fjc52 from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container calico-node ready: true, restart count 0
Jun 21 22:54:27.397: INFO: managed-storage-validation-webhooks-695b4c95d9-hglfj from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 21 22:54:27.397: INFO: managed-storage-validation-webhooks-695b4c95d9-hqvhd from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 21 22:54:27.397: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-k8tvg from ibm-system started at 2022-06-21 21:19:53 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 21 22:54:27.397: INFO: ibm-file-plugin-58d6b698d9-zrt8h from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 21 22:54:27.397: INFO: ibm-keepalived-watcher-p6mn9 from kube-system started at 2022-06-21 21:08:40 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 21 22:54:27.397: INFO: ibm-master-proxy-static-10.10.24.208 from kube-system started at 2022-06-21 21:08:36 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 21 22:54:27.397: INFO: 	Container pause ready: true, restart count 0
Jun 21 22:54:27.397: INFO: ibmcloud-block-storage-driver-7wzfz from kube-system started at 2022-06-21 21:08:45 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 21 22:54:27.397: INFO: ibmcloud-block-storage-plugin-5469669bb7-6gxw7 from kube-system started at 2022-06-21 21:11:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 21 22:54:27.397: INFO: cluster-node-tuning-operator-5f78c6cfc9-ckrp7 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 21 22:54:27.397: INFO: tuned-gxv8t from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container tuned ready: true, restart count 0
Jun 21 22:54:27.397: INFO: cluster-samples-operator-665576d565-dqhrm from openshift-cluster-samples-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 21 22:54:27.397: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 21 22:54:27.397: INFO: cluster-storage-operator-7bf9d85d5d-2cqzr from openshift-cluster-storage-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 21 22:54:27.397: INFO: csi-snapshot-controller-69d5f9c777-dbjmk from openshift-cluster-storage-operator started at 2022-06-21 21:13:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 21 22:54:27.397: INFO: csi-snapshot-controller-operator-6b5cb9d8d8-88k4j from openshift-cluster-storage-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.397: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 21 22:54:27.398: INFO: csi-snapshot-webhook-55d4797dc4-mz7ql from openshift-cluster-storage-operator started at 2022-06-21 21:12:57 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container webhook ready: true, restart count 0
Jun 21 22:54:27.398: INFO: console-operator-76fb594985-8qrdt from openshift-console-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container console-operator ready: true, restart count 1
Jun 21 22:54:27.398: INFO: dns-default-5zqvt from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container dns ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: node-resolver-2fjfv from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 21 22:54:27.398: INFO: cluster-image-registry-operator-bf8fb9fdd-k5dd9 from openshift-image-registry started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 21 22:54:27.398: INFO: node-ca-pw5vl from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container node-ca ready: true, restart count 0
Jun 21 22:54:27.398: INFO: registry-pvc-permissions--1-q7rcf from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 21 22:54:27.398: INFO: ingress-canary-xg8wv from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 21 22:54:27.398: INFO: openshift-kube-proxy-8gv94 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: kube-storage-version-migrator-operator-5b447564b8-c9kw5 from openshift-kube-storage-version-migrator-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 21 22:54:27.398: INFO: certified-operators-p4dlj from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:54:27.398: INFO: community-operators-95xlt from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:54:27.398: INFO: marketplace-operator-757fc48f95-frcn6 from openshift-marketplace started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 21 22:54:27.398: INFO: redhat-marketplace-mmvlb from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:54:27.398: INFO: redhat-operators-7jdsz from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:54:27.398: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (5 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container alertmanager ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: grafana-6db9675f77-hpcsl from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container grafana ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: node-exporter-6th4k from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.398: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.398: INFO: 	Container node-exporter ready: true, restart count 0
Jun 21 22:54:27.399: INFO: prometheus-adapter-5c564c9546-h452p from openshift-monitoring started at 2022-06-21 21:18:18 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.399: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 21 22:54:27.399: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-06-21 21:16:32 +0000 UTC (7 container statuses recorded)
Jun 21 22:54:27.399: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container prometheus ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 21 22:54:27.399: INFO: thanos-querier-75f4c69596-f82ww from openshift-monitoring started at 2022-06-21 21:16:16 +0000 UTC (5 container statuses recorded)
Jun 21 22:54:27.399: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container thanos-query ready: true, restart count 0
Jun 21 22:54:27.399: INFO: multus-additional-cni-plugins-6pjp8 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.399: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 21 22:54:27.399: INFO: multus-admission-controller-4d7xn from openshift-multus started at 2022-06-21 21:11:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.399: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.399: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 21 22:54:27.400: INFO: multus-bkbkg from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container kube-multus ready: true, restart count 0
Jun 21 22:54:27.400: INFO: network-metrics-daemon-8bt6v from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.400: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 21 22:54:27.400: INFO: network-check-source-778bbb8ccb-pprtl from openshift-network-diagnostics started at 2022-06-21 21:11:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 21 22:54:27.400: INFO: network-check-target-smj6m from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 21 22:54:27.400: INFO: catalog-operator-86b47c4c86-tjj8n from openshift-operator-lifecycle-manager started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 21 22:54:27.400: INFO: collect-profiles-27597525--1-ddgss from openshift-operator-lifecycle-manager started at 2022-06-21 22:45:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 21 22:54:27.400: INFO: olm-operator-767cc8584d-sp2r8 from openshift-operator-lifecycle-manager started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container olm-operator ready: true, restart count 0
Jun 21 22:54:27.400: INFO: package-server-manager-bbb676bfb-f9wpv from openshift-operator-lifecycle-manager started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 21 22:54:27.400: INFO: packageserver-cfb4bb4b4-tqkrw from openshift-operator-lifecycle-manager started at 2022-06-21 21:13:57 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container packageserver ready: true, restart count 0
Jun 21 22:54:27.400: INFO: metrics-6699958ffb-6wmw2 from openshift-roks-metrics started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container metrics ready: true, restart count 1
Jun 21 22:54:27.400: INFO: push-gateway-77d69cd6c6-rvfh6 from openshift-roks-metrics started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container push-gateway ready: true, restart count 0
Jun 21 22:54:27.400: INFO: sonobuoy from sonobuoy started at 2022-06-21 22:43:11 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 21 22:54:27.400: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-lrsbr from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:54:27.400: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 21 22:54:27.400: INFO: tigera-operator-844dcd89f8-z7zn8 from tigera-operator started at 2022-06-21 21:09:53 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.400: INFO: 	Container tigera-operator ready: true, restart count 2
Jun 21 22:54:27.400: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.214 before test
Jun 21 22:54:27.491: INFO: calico-node-lrfbs from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container calico-node ready: true, restart count 0
Jun 21 22:54:27.491: INFO: calico-typha-65cc4575bc-rjddl from calico-system started at 2022-06-21 21:11:06 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container calico-typha ready: true, restart count 0
Jun 21 22:54:27.491: INFO: managed-storage-validation-webhooks-695b4c95d9-xh79f from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 21 22:54:27.491: INFO: ibm-keepalived-watcher-pdlrz from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 21 22:54:27.491: INFO: ibm-master-proxy-static-10.10.24.214 from kube-system started at 2022-06-21 21:08:39 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 21 22:54:27.491: INFO: 	Container pause ready: true, restart count 0
Jun 21 22:54:27.491: INFO: ibm-storage-metrics-agent-5474c86f6c-mkwv2 from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 21 22:54:27.491: INFO: ibm-storage-watcher-79dd544684-s5lkx from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 21 22:54:27.491: INFO: ibmcloud-block-storage-driver-8w9mb from kube-system started at 2022-06-21 21:08:47 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 21 22:54:27.491: INFO: tuned-btmj7 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.491: INFO: 	Container tuned ready: true, restart count 0
Jun 21 22:54:27.492: INFO: console-67bfbdb4dc-hf9m7 from openshift-console started at 2022-06-21 21:16:16 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container console ready: true, restart count 0
Jun 21 22:54:27.492: INFO: downloads-988dbf8f4-czbb8 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container download-server ready: true, restart count 0
Jun 21 22:54:27.492: INFO: dns-operator-5d76fb554c-mcw6n from openshift-dns-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container dns-operator ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: dns-default-tdvc6 from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container dns ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: node-resolver-qlhv7 from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 21 22:54:27.492: INFO: image-registry-74b8f796c6-gq2tp from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container registry ready: true, restart count 0
Jun 21 22:54:27.492: INFO: node-ca-26tbk from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container node-ca ready: true, restart count 0
Jun 21 22:54:27.492: INFO: ingress-canary-d82cq from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 21 22:54:27.492: INFO: ingress-operator-5cdfdf4d7c-nhksh from openshift-ingress-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: router-default-bb6fc54c5-92jm4 from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container router ready: true, restart count 0
Jun 21 22:54:27.492: INFO: openshift-kube-proxy-brk2v from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (5 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container alertmanager ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: cluster-monitoring-operator-747998f9d8-tmcgs from openshift-monitoring started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 21 22:54:27.492: INFO: node-exporter-pk7wt from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.492: INFO: 	Container node-exporter ready: true, restart count 0
Jun 21 22:54:27.492: INFO: multus-additional-cni-plugins-zbkbh from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 21 22:54:27.492: INFO: multus-admission-controller-xxd4n from openshift-multus started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.492: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.493: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 21 22:54:27.493: INFO: multus-n2tzz from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container kube-multus ready: true, restart count 0
Jun 21 22:54:27.493: INFO: network-metrics-daemon-wvhtq from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:54:27.493: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 21 22:54:27.493: INFO: network-check-target-4np7s from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 21 22:54:27.493: INFO: network-operator-54477c9fc6-knql5 from openshift-network-operator started at 2022-06-21 21:09:52 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container network-operator ready: true, restart count 0
Jun 21 22:54:27.493: INFO: service-ca-operator-6d9bdb775b-ggrqz from openshift-service-ca-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 21 22:54:27.493: INFO: agnhost from proxy-7447 started at 2022-06-21 22:54:23 +0000 UTC (1 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container agnhost ready: true, restart count 0
Jun 21 22:54:27.493: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-52nfv from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:54:27.493: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:54:27.493: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16fac4aeb79af604], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:54:28.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5667" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":47,"skipped":674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:54:28.764: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf in namespace container-probe-5231
Jun 21 22:54:33.088: INFO: Started pod liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf in namespace container-probe-5231
STEP: checking the pod's current state and verifying that restartCount is present
Jun 21 22:54:33.100: INFO: Initial restart count of pod liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf is 0
Jun 21 22:54:51.294: INFO: Restart count of pod container-probe-5231/liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf is now 1 (18.194133752s elapsed)
Jun 21 22:55:11.490: INFO: Restart count of pod container-probe-5231/liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf is now 2 (38.389909479s elapsed)
Jun 21 22:55:31.704: INFO: Restart count of pod container-probe-5231/liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf is now 3 (58.604702898s elapsed)
Jun 21 22:55:51.879: INFO: Restart count of pod container-probe-5231/liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf is now 4 (1m18.779284732s elapsed)
Jun 21 22:56:52.507: INFO: Restart count of pod container-probe-5231/liveness-76230bd3-8f6c-4238-8db7-45a4bb4467bf is now 5 (2m19.407328297s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:56:52.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5231" for this suite.

• [SLOW TEST:143.833 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":703,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:56:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-b42705ad-c045-4e86-a098-8dd57da08e3b
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:56:52.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8721" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":49,"skipped":722,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:56:52.950: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 21 22:56:53.234: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 21 22:56:53.286: INFO: Waiting for terminating namespaces to be deleted...
Jun 21 22:56:53.329: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.206 before test
Jun 21 22:56:53.423: INFO: calico-node-24dvg from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.423: INFO: 	Container calico-node ready: true, restart count 0
Jun 21 22:56:53.423: INFO: calico-typha-65cc4575bc-f88zd from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.423: INFO: 	Container calico-typha ready: true, restart count 0
Jun 21 22:56:53.423: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-21 21:14:34 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.424: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 21 22:56:53.424: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-hd8s7 from ibm-system started at 2022-06-21 21:19:53 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.424: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 21 22:56:53.424: INFO: ibm-keepalived-watcher-rpq7r from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.424: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 21 22:56:53.424: INFO: ibm-master-proxy-static-10.10.24.206 from kube-system started at 2022-06-21 21:08:38 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.424: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 21 22:56:53.424: INFO: 	Container pause ready: true, restart count 0
Jun 21 22:56:53.425: INFO: ibmcloud-block-storage-driver-l2pxj from kube-system started at 2022-06-21 21:08:46 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.425: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 21 22:56:53.425: INFO: vpn-69b96645d-qtgxb from kube-system started at 2022-06-21 21:11:45 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.425: INFO: 	Container vpn ready: true, restart count 1
Jun 21 22:56:53.425: INFO: tuned-pc9t5 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.425: INFO: 	Container tuned ready: true, restart count 0
Jun 21 22:56:53.425: INFO: csi-snapshot-controller-69d5f9c777-p6845 from openshift-cluster-storage-operator started at 2022-06-21 21:13:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.425: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 21 22:56:53.426: INFO: csi-snapshot-webhook-55d4797dc4-2c7hn from openshift-cluster-storage-operator started at 2022-06-21 21:12:56 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.426: INFO: 	Container webhook ready: true, restart count 0
Jun 21 22:56:53.426: INFO: console-67bfbdb4dc-mtf8v from openshift-console started at 2022-06-21 21:17:04 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.426: INFO: 	Container console ready: true, restart count 0
Jun 21 22:56:53.426: INFO: downloads-988dbf8f4-wqhz7 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.426: INFO: 	Container download-server ready: true, restart count 0
Jun 21 22:56:53.426: INFO: dns-default-wqq2m from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.426: INFO: 	Container dns ready: true, restart count 0
Jun 21 22:56:53.426: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.427: INFO: node-resolver-xjddx from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.427: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 21 22:56:53.427: INFO: node-ca-wf6f5 from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.427: INFO: 	Container node-ca ready: true, restart count 0
Jun 21 22:56:53.427: INFO: ingress-canary-mcgps from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.427: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 21 22:56:53.427: INFO: router-default-bb6fc54c5-hbbwg from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.427: INFO: 	Container router ready: true, restart count 0
Jun 21 22:56:53.427: INFO: openshift-kube-proxy-g5jv6 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.428: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 21 22:56:53.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.428: INFO: migrator-8467484867-zz9sp from openshift-kube-storage-version-migrator started at 2022-06-21 21:12:46 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.428: INFO: 	Container migrator ready: true, restart count 0
Jun 21 22:56:53.428: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-06-21 21:16:13 +0000 UTC (5 container statuses recorded)
Jun 21 22:56:53.428: INFO: 	Container alertmanager ready: true, restart count 0
Jun 21 22:56:53.428: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 21 22:56:53.428: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:56:53.428: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.428: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.428: INFO: kube-state-metrics-5f9b9688bc-jrnb4 from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 21 22:56:53.429: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 21 22:56:53.429: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 21 22:56:53.429: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 21 22:56:53.429: INFO: node-exporter-xvzpx from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.429: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.429: INFO: 	Container node-exporter ready: true, restart count 0
Jun 21 22:56:53.429: INFO: openshift-state-metrics-744d546498-5g75r from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 21 22:56:53.429: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 21 22:56:53.429: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 21 22:56:53.429: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 21 22:56:53.429: INFO: prometheus-adapter-5c564c9546-wz2lv from openshift-monitoring started at 2022-06-21 21:18:18 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.429: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 21 22:56:53.429: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-06-21 21:16:32 +0000 UTC (7 container statuses recorded)
Jun 21 22:56:53.429: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:56:53.429: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container prometheus ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 21 22:56:53.430: INFO: prometheus-operator-bf9ff66c-t6r25 from openshift-monitoring started at 2022-06-21 21:12:54 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.430: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jun 21 22:56:53.430: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 21 22:56:53.430: INFO: telemeter-client-8dfb6c944-8k9b8 from openshift-monitoring started at 2022-06-21 21:13:35 +0000 UTC (3 container statuses recorded)
Jun 21 22:56:53.430: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container reload ready: true, restart count 0
Jun 21 22:56:53.430: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 21 22:56:53.430: INFO: thanos-querier-75f4c69596-kmkqk from openshift-monitoring started at 2022-06-21 21:16:16 +0000 UTC (5 container statuses recorded)
Jun 21 22:56:53.430: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.431: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 21 22:56:53.431: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 21 22:56:53.431: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.431: INFO: 	Container thanos-query ready: true, restart count 0
Jun 21 22:56:53.431: INFO: multus-additional-cni-plugins-qjdfj from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.431: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 21 22:56:53.431: INFO: multus-admission-controller-sndqs from openshift-multus started at 2022-06-21 21:11:41 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.431: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 21 22:56:53.431: INFO: multus-c4bqm from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.431: INFO: 	Container kube-multus ready: true, restart count 0
Jun 21 22:56:53.431: INFO: network-metrics-daemon-l6zp7 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.431: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.431: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 21 22:56:53.432: INFO: network-check-target-4dx2q from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.432: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 21 22:56:53.432: INFO: collect-profiles-27597495--1-kj65w from openshift-operator-lifecycle-manager started at 2022-06-21 22:15:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.432: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 21 22:56:53.432: INFO: collect-profiles-27597510--1-h98k2 from openshift-operator-lifecycle-manager started at 2022-06-21 22:30:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.432: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 21 22:56:53.432: INFO: packageserver-cfb4bb4b4-kzchv from openshift-operator-lifecycle-manager started at 2022-06-21 21:13:57 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.432: INFO: 	Container packageserver ready: true, restart count 0
Jun 21 22:56:53.432: INFO: service-ca-7f5f4d65b9-gd4rm from openshift-service-ca started at 2022-06-21 21:12:50 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.432: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 21 22:56:53.432: INFO: sonobuoy-e2e-job-9869bda7abe94c50 from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.432: INFO: 	Container e2e ready: true, restart count 0
Jun 21 22:56:53.432: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:56:53.433: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-8pkdj from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:56:53.433: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 21 22:56:53.433: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.208 before test
Jun 21 22:56:53.530: INFO: calico-kube-controllers-fbfbb867b-d8msg from calico-system started at 2022-06-21 21:11:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.531: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 21 22:56:53.531: INFO: calico-node-fjc52 from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.531: INFO: 	Container calico-node ready: true, restart count 0
Jun 21 22:56:53.531: INFO: managed-storage-validation-webhooks-695b4c95d9-hglfj from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.531: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 21 22:56:53.531: INFO: managed-storage-validation-webhooks-695b4c95d9-hqvhd from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.531: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 21 22:56:53.531: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-k8tvg from ibm-system started at 2022-06-21 21:19:53 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.532: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 21 22:56:53.532: INFO: ibm-file-plugin-58d6b698d9-zrt8h from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.532: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 21 22:56:53.532: INFO: ibm-keepalived-watcher-p6mn9 from kube-system started at 2022-06-21 21:08:40 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.532: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 21 22:56:53.532: INFO: ibm-master-proxy-static-10.10.24.208 from kube-system started at 2022-06-21 21:08:36 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.532: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 21 22:56:53.532: INFO: 	Container pause ready: true, restart count 0
Jun 21 22:56:53.532: INFO: ibmcloud-block-storage-driver-7wzfz from kube-system started at 2022-06-21 21:08:45 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.532: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 21 22:56:53.532: INFO: ibmcloud-block-storage-plugin-5469669bb7-6gxw7 from kube-system started at 2022-06-21 21:11:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.532: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 21 22:56:53.533: INFO: cluster-node-tuning-operator-5f78c6cfc9-ckrp7 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.533: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 21 22:56:53.533: INFO: tuned-gxv8t from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.533: INFO: 	Container tuned ready: true, restart count 0
Jun 21 22:56:53.533: INFO: cluster-samples-operator-665576d565-dqhrm from openshift-cluster-samples-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.533: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 21 22:56:53.533: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 21 22:56:53.533: INFO: cluster-storage-operator-7bf9d85d5d-2cqzr from openshift-cluster-storage-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.533: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 21 22:56:53.533: INFO: csi-snapshot-controller-69d5f9c777-dbjmk from openshift-cluster-storage-operator started at 2022-06-21 21:13:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.533: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 21 22:56:53.534: INFO: csi-snapshot-controller-operator-6b5cb9d8d8-88k4j from openshift-cluster-storage-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.534: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 21 22:56:53.534: INFO: csi-snapshot-webhook-55d4797dc4-mz7ql from openshift-cluster-storage-operator started at 2022-06-21 21:12:57 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.534: INFO: 	Container webhook ready: true, restart count 0
Jun 21 22:56:53.534: INFO: console-operator-76fb594985-8qrdt from openshift-console-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.534: INFO: 	Container console-operator ready: true, restart count 1
Jun 21 22:56:53.534: INFO: dns-default-5zqvt from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.534: INFO: 	Container dns ready: true, restart count 0
Jun 21 22:56:53.534: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.535: INFO: node-resolver-2fjfv from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.535: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 21 22:56:53.535: INFO: cluster-image-registry-operator-bf8fb9fdd-k5dd9 from openshift-image-registry started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.535: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 21 22:56:53.535: INFO: node-ca-pw5vl from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.535: INFO: 	Container node-ca ready: true, restart count 0
Jun 21 22:56:53.535: INFO: registry-pvc-permissions--1-q7rcf from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.536: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 21 22:56:53.536: INFO: ingress-canary-xg8wv from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.536: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 21 22:56:53.536: INFO: openshift-kube-proxy-8gv94 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.536: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 21 22:56:53.536: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.536: INFO: kube-storage-version-migrator-operator-5b447564b8-c9kw5 from openshift-kube-storage-version-migrator-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.536: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 21 22:56:53.536: INFO: certified-operators-p4dlj from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.536: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:56:53.536: INFO: community-operators-95xlt from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.537: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:56:53.537: INFO: marketplace-operator-757fc48f95-frcn6 from openshift-marketplace started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.537: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 21 22:56:53.537: INFO: redhat-marketplace-mmvlb from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.537: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:56:53.537: INFO: redhat-operators-7jdsz from openshift-marketplace started at 2022-06-21 21:15:37 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.537: INFO: 	Container registry-server ready: true, restart count 0
Jun 21 22:56:53.537: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (5 container statuses recorded)
Jun 21 22:56:53.537: INFO: 	Container alertmanager ready: true, restart count 0
Jun 21 22:56:53.537: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 21 22:56:53.537: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:56:53.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.538: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.538: INFO: grafana-6db9675f77-hpcsl from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.538: INFO: 	Container grafana ready: true, restart count 0
Jun 21 22:56:53.538: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 21 22:56:53.538: INFO: node-exporter-6th4k from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.538: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.538: INFO: 	Container node-exporter ready: true, restart count 0
Jun 21 22:56:53.538: INFO: prometheus-adapter-5c564c9546-h452p from openshift-monitoring started at 2022-06-21 21:18:18 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.538: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 21 22:56:53.538: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-06-21 21:16:32 +0000 UTC (7 container statuses recorded)
Jun 21 22:56:53.538: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container prometheus ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 21 22:56:53.539: INFO: thanos-querier-75f4c69596-f82ww from openshift-monitoring started at 2022-06-21 21:16:16 +0000 UTC (5 container statuses recorded)
Jun 21 22:56:53.539: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.539: INFO: 	Container thanos-query ready: true, restart count 0
Jun 21 22:56:53.540: INFO: multus-additional-cni-plugins-6pjp8 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 21 22:56:53.540: INFO: multus-admission-controller-4d7xn from openshift-multus started at 2022-06-21 21:11:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.540: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 21 22:56:53.540: INFO: multus-bkbkg from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container kube-multus ready: true, restart count 0
Jun 21 22:56:53.540: INFO: network-metrics-daemon-8bt6v from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.540: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 21 22:56:53.540: INFO: network-check-source-778bbb8ccb-pprtl from openshift-network-diagnostics started at 2022-06-21 21:11:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 21 22:56:53.540: INFO: network-check-target-smj6m from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 21 22:56:53.540: INFO: catalog-operator-86b47c4c86-tjj8n from openshift-operator-lifecycle-manager started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.540: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 21 22:56:53.540: INFO: collect-profiles-27597525--1-ddgss from openshift-operator-lifecycle-manager started at 2022-06-21 22:45:00 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 21 22:56:53.541: INFO: olm-operator-767cc8584d-sp2r8 from openshift-operator-lifecycle-manager started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container olm-operator ready: true, restart count 0
Jun 21 22:56:53.541: INFO: package-server-manager-bbb676bfb-f9wpv from openshift-operator-lifecycle-manager started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 21 22:56:53.541: INFO: packageserver-cfb4bb4b4-tqkrw from openshift-operator-lifecycle-manager started at 2022-06-21 21:13:57 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container packageserver ready: true, restart count 0
Jun 21 22:56:53.541: INFO: metrics-6699958ffb-6wmw2 from openshift-roks-metrics started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container metrics ready: true, restart count 1
Jun 21 22:56:53.541: INFO: push-gateway-77d69cd6c6-rvfh6 from openshift-roks-metrics started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container push-gateway ready: true, restart count 0
Jun 21 22:56:53.541: INFO: sonobuoy from sonobuoy started at 2022-06-21 22:43:11 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 21 22:56:53.541: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-lrsbr from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:56:53.541: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 21 22:56:53.542: INFO: tigera-operator-844dcd89f8-z7zn8 from tigera-operator started at 2022-06-21 21:09:53 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.542: INFO: 	Container tigera-operator ready: true, restart count 2
Jun 21 22:56:53.542: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.214 before test
Jun 21 22:56:53.633: INFO: calico-node-lrfbs from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.634: INFO: 	Container calico-node ready: true, restart count 0
Jun 21 22:56:53.634: INFO: calico-typha-65cc4575bc-rjddl from calico-system started at 2022-06-21 21:11:06 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.634: INFO: 	Container calico-typha ready: true, restart count 0
Jun 21 22:56:53.634: INFO: managed-storage-validation-webhooks-695b4c95d9-xh79f from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.634: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 21 22:56:53.634: INFO: ibm-keepalived-watcher-pdlrz from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.634: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 21 22:56:53.634: INFO: ibm-master-proxy-static-10.10.24.214 from kube-system started at 2022-06-21 21:08:39 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.634: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 21 22:56:53.634: INFO: 	Container pause ready: true, restart count 0
Jun 21 22:56:53.635: INFO: ibm-storage-metrics-agent-5474c86f6c-mkwv2 from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.635: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 21 22:56:53.635: INFO: ibm-storage-watcher-79dd544684-s5lkx from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.635: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 21 22:56:53.635: INFO: ibmcloud-block-storage-driver-8w9mb from kube-system started at 2022-06-21 21:08:47 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.635: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 21 22:56:53.635: INFO: tuned-btmj7 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.635: INFO: 	Container tuned ready: true, restart count 0
Jun 21 22:56:53.635: INFO: console-67bfbdb4dc-hf9m7 from openshift-console started at 2022-06-21 21:16:16 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.635: INFO: 	Container console ready: true, restart count 0
Jun 21 22:56:53.636: INFO: downloads-988dbf8f4-czbb8 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.636: INFO: 	Container download-server ready: true, restart count 0
Jun 21 22:56:53.636: INFO: dns-operator-5d76fb554c-mcw6n from openshift-dns-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.636: INFO: 	Container dns-operator ready: true, restart count 0
Jun 21 22:56:53.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.636: INFO: dns-default-tdvc6 from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.636: INFO: 	Container dns ready: true, restart count 0
Jun 21 22:56:53.636: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.636: INFO: node-resolver-qlhv7 from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.636: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 21 22:56:53.636: INFO: image-registry-74b8f796c6-gq2tp from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.637: INFO: 	Container registry ready: true, restart count 0
Jun 21 22:56:53.637: INFO: node-ca-26tbk from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.637: INFO: 	Container node-ca ready: true, restart count 0
Jun 21 22:56:53.637: INFO: ingress-canary-d82cq from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.637: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 21 22:56:53.637: INFO: ingress-operator-5cdfdf4d7c-nhksh from openshift-ingress-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.637: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 21 22:56:53.637: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.637: INFO: router-default-bb6fc54c5-92jm4 from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.637: INFO: 	Container router ready: true, restart count 0
Jun 21 22:56:53.638: INFO: openshift-kube-proxy-brk2v from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.638: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 21 22:56:53.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.638: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (5 container statuses recorded)
Jun 21 22:56:53.638: INFO: 	Container alertmanager ready: true, restart count 0
Jun 21 22:56:53.638: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 21 22:56:53.638: INFO: 	Container config-reloader ready: true, restart count 0
Jun 21 22:56:53.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.638: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 21 22:56:53.638: INFO: cluster-monitoring-operator-747998f9d8-tmcgs from openshift-monitoring started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.638: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 21 22:56:53.638: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 21 22:56:53.638: INFO: node-exporter-pk7wt from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.639: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.639: INFO: 	Container node-exporter ready: true, restart count 0
Jun 21 22:56:53.639: INFO: multus-additional-cni-plugins-zbkbh from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.639: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 21 22:56:53.639: INFO: multus-admission-controller-xxd4n from openshift-multus started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.639: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.639: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 21 22:56:53.639: INFO: multus-n2tzz from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.639: INFO: 	Container kube-multus ready: true, restart count 0
Jun 21 22:56:53.639: INFO: network-metrics-daemon-wvhtq from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.639: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 21 22:56:53.639: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 21 22:56:53.639: INFO: network-check-target-4np7s from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.640: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 21 22:56:53.640: INFO: network-operator-54477c9fc6-knql5 from openshift-network-operator started at 2022-06-21 21:09:52 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.640: INFO: 	Container network-operator ready: true, restart count 0
Jun 21 22:56:53.640: INFO: service-ca-operator-6d9bdb775b-ggrqz from openshift-service-ca-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 21 22:56:53.640: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 21 22:56:53.640: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-52nfv from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 21 22:56:53.640: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 21 22:56:53.640: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.10.24.206
STEP: verifying the node has the label node 10.10.24.208
STEP: verifying the node has the label node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod calico-kube-controllers-fbfbb867b-d8msg requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod calico-node-24dvg requesting resource cpu=250m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod calico-node-fjc52 requesting resource cpu=250m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod calico-node-lrfbs requesting resource cpu=250m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod calico-typha-65cc4575bc-f88zd requesting resource cpu=250m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod calico-typha-65cc4575bc-rjddl requesting resource cpu=250m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod managed-storage-validation-webhooks-695b4c95d9-hglfj requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod managed-storage-validation-webhooks-695b4c95d9-hqvhd requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod managed-storage-validation-webhooks-695b4c95d9-xh79f requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod ibm-cloud-provider-ip-163-73-69-51-775496d795-hd8s7 requesting resource cpu=5m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod ibm-cloud-provider-ip-163-73-69-51-775496d795-k8tvg requesting resource cpu=5m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod ibm-file-plugin-58d6b698d9-zrt8h requesting resource cpu=50m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod ibm-keepalived-watcher-p6mn9 requesting resource cpu=5m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod ibm-keepalived-watcher-pdlrz requesting resource cpu=5m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod ibm-keepalived-watcher-rpq7r requesting resource cpu=5m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod ibm-master-proxy-static-10.10.24.206 requesting resource cpu=26m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod ibm-master-proxy-static-10.10.24.208 requesting resource cpu=26m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod ibm-master-proxy-static-10.10.24.214 requesting resource cpu=26m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod ibm-storage-metrics-agent-5474c86f6c-mkwv2 requesting resource cpu=50m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod ibm-storage-watcher-79dd544684-s5lkx requesting resource cpu=50m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod ibmcloud-block-storage-driver-7wzfz requesting resource cpu=50m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod ibmcloud-block-storage-driver-8w9mb requesting resource cpu=50m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod ibmcloud-block-storage-driver-l2pxj requesting resource cpu=50m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod ibmcloud-block-storage-plugin-5469669bb7-6gxw7 requesting resource cpu=50m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod vpn-69b96645d-qtgxb requesting resource cpu=5m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod cluster-node-tuning-operator-5f78c6cfc9-ckrp7 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod tuned-btmj7 requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.042: INFO: Pod tuned-gxv8t requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.042: INFO: Pod tuned-pc9t5 requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.042: INFO: Pod cluster-samples-operator-665576d565-dqhrm requesting resource cpu=20m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod cluster-storage-operator-7bf9d85d5d-2cqzr requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod csi-snapshot-controller-69d5f9c777-dbjmk requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod csi-snapshot-controller-69d5f9c777-p6845 requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod csi-snapshot-controller-operator-6b5cb9d8d8-88k4j requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod csi-snapshot-webhook-55d4797dc4-2c7hn requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod csi-snapshot-webhook-55d4797dc4-mz7ql requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod console-operator-76fb594985-8qrdt requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod console-67bfbdb4dc-hf9m7 requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod console-67bfbdb4dc-mtf8v requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod downloads-988dbf8f4-czbb8 requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod downloads-988dbf8f4-wqhz7 requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod dns-operator-5d76fb554c-mcw6n requesting resource cpu=20m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod dns-default-5zqvt requesting resource cpu=60m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod dns-default-tdvc6 requesting resource cpu=60m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod dns-default-wqq2m requesting resource cpu=60m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod node-resolver-2fjfv requesting resource cpu=5m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod node-resolver-qlhv7 requesting resource cpu=5m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod node-resolver-xjddx requesting resource cpu=5m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod cluster-image-registry-operator-bf8fb9fdd-k5dd9 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod image-registry-74b8f796c6-gq2tp requesting resource cpu=100m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod node-ca-26tbk requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod node-ca-pw5vl requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod node-ca-wf6f5 requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod ingress-canary-d82cq requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod ingress-canary-mcgps requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod ingress-canary-xg8wv requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod ingress-operator-5cdfdf4d7c-nhksh requesting resource cpu=20m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod router-default-bb6fc54c5-92jm4 requesting resource cpu=100m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod router-default-bb6fc54c5-hbbwg requesting resource cpu=100m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod openshift-kube-proxy-8gv94 requesting resource cpu=110m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod openshift-kube-proxy-brk2v requesting resource cpu=110m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod openshift-kube-proxy-g5jv6 requesting resource cpu=110m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod kube-storage-version-migrator-operator-5b447564b8-c9kw5 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod migrator-8467484867-zz9sp requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod certified-operators-p4dlj requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod community-operators-95xlt requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod marketplace-operator-757fc48f95-frcn6 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod redhat-marketplace-mmvlb requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod redhat-operators-7jdsz requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod cluster-monitoring-operator-747998f9d8-tmcgs requesting resource cpu=11m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod grafana-6db9675f77-hpcsl requesting resource cpu=5m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod kube-state-metrics-5f9b9688bc-jrnb4 requesting resource cpu=4m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod node-exporter-6th4k requesting resource cpu=9m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod node-exporter-pk7wt requesting resource cpu=9m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod node-exporter-xvzpx requesting resource cpu=9m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod openshift-state-metrics-744d546498-5g75r requesting resource cpu=3m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod prometheus-adapter-5c564c9546-h452p requesting resource cpu=1m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod prometheus-adapter-5c564c9546-wz2lv requesting resource cpu=1m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod prometheus-operator-bf9ff66c-t6r25 requesting resource cpu=6m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod telemeter-client-8dfb6c944-8k9b8 requesting resource cpu=3m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod thanos-querier-75f4c69596-f82ww requesting resource cpu=14m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod thanos-querier-75f4c69596-kmkqk requesting resource cpu=14m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod multus-additional-cni-plugins-6pjp8 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod multus-additional-cni-plugins-qjdfj requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod multus-additional-cni-plugins-zbkbh requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod multus-admission-controller-4d7xn requesting resource cpu=20m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod multus-admission-controller-sndqs requesting resource cpu=20m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod multus-admission-controller-xxd4n requesting resource cpu=20m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod multus-bkbkg requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod multus-c4bqm requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod multus-n2tzz requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod network-metrics-daemon-8bt6v requesting resource cpu=20m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod network-metrics-daemon-l6zp7 requesting resource cpu=20m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod network-metrics-daemon-wvhtq requesting resource cpu=20m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod network-check-source-778bbb8ccb-pprtl requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod network-check-target-4dx2q requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod network-check-target-4np7s requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod network-check-target-smj6m requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod network-operator-54477c9fc6-knql5 requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod catalog-operator-86b47c4c86-tjj8n requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod olm-operator-767cc8584d-sp2r8 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod package-server-manager-bbb676bfb-f9wpv requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod packageserver-cfb4bb4b4-kzchv requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod packageserver-cfb4bb4b4-tqkrw requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod metrics-6699958ffb-6wmw2 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod push-gateway-77d69cd6c6-rvfh6 requesting resource cpu=10m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod service-ca-operator-6d9bdb775b-ggrqz requesting resource cpu=10m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod service-ca-7f5f4d65b9-gd4rm requesting resource cpu=10m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod sonobuoy-e2e-job-9869bda7abe94c50 requesting resource cpu=0m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-52nfv requesting resource cpu=0m on Node 10.10.24.214
Jun 21 22:56:54.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-8pkdj requesting resource cpu=0m on Node 10.10.24.206
Jun 21 22:56:54.043: INFO: Pod sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-lrsbr requesting resource cpu=0m on Node 10.10.24.208
Jun 21 22:56:54.043: INFO: Pod tigera-operator-844dcd89f8-z7zn8 requesting resource cpu=100m on Node 10.10.24.208
STEP: Starting Pods to consume most of the cluster CPU.
Jun 21 22:56:54.043: INFO: Creating a pod which consumes cpu=1925m on Node 10.10.24.206
Jun 21 22:56:54.102: INFO: Creating a pod which consumes cpu=1915m on Node 10.10.24.208
Jun 21 22:56:54.155: INFO: Creating a pod which consumes cpu=1845m on Node 10.10.24.214
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5.16fac4d0d26ee95d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7494/filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5 to 10.10.24.208]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5.16fac4d118e09797], Reason = [AddedInterface], Message = [Add eth0 [172.30.224.44/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5.16fac4d130064c7d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5.16fac4d1419cecf6], Reason = [Created], Message = [Created container filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5.16fac4d147e7d53c], Reason = [Started], Message = [Started container filler-pod-073d0a78-78be-4ddd-a08b-b4fb5c184ae5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f.16fac4d0cf8b360e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7494/filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f to 10.10.24.206]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f.16fac4d117ff5aba], Reason = [AddedInterface], Message = [Add eth0 [172.30.96.29/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f.16fac4d130a7bd05], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f.16fac4d140b985ba], Reason = [Created], Message = [Created container filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f.16fac4d143d2372d], Reason = [Started], Message = [Started container filler-pod-398c4327-c65d-42a9-857e-208701ff3d9f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8.16fac4d0d6dea76e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7494/filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8 to 10.10.24.214]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8.16fac4d11e7e710a], Reason = [AddedInterface], Message = [Add eth0 [172.30.47.226/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8.16fac4d1361e56a6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8.16fac4d1461db5fe], Reason = [Created], Message = [Created container filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8.16fac4d148a43419], Reason = [Started], Message = [Started container filler-pod-ada140e5-6f94-4d08-ab36-8ac474fa7ea8]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16fac4d1d57af471], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.10.24.206
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.10.24.208
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.10.24.214
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:56:59.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7494" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.877 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":50,"skipped":725,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:56:59.828: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:57:00.276: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 21 22:57:00.323: INFO: Number of nodes with available pods: 0
Jun 21 22:57:00.323: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 21 22:57:00.436: INFO: Number of nodes with available pods: 0
Jun 21 22:57:00.436: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:01.450: INFO: Number of nodes with available pods: 0
Jun 21 22:57:01.450: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:02.454: INFO: Number of nodes with available pods: 0
Jun 21 22:57:02.454: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:03.452: INFO: Number of nodes with available pods: 1
Jun 21 22:57:03.452: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 21 22:57:03.556: INFO: Number of nodes with available pods: 1
Jun 21 22:57:03.556: INFO: Number of running nodes: 0, number of available pods: 1
Jun 21 22:57:04.572: INFO: Number of nodes with available pods: 0
Jun 21 22:57:04.573: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 21 22:57:04.617: INFO: Number of nodes with available pods: 0
Jun 21 22:57:04.617: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:05.632: INFO: Number of nodes with available pods: 0
Jun 21 22:57:05.633: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:06.652: INFO: Number of nodes with available pods: 0
Jun 21 22:57:06.652: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:07.640: INFO: Number of nodes with available pods: 0
Jun 21 22:57:07.641: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:08.635: INFO: Number of nodes with available pods: 0
Jun 21 22:57:08.635: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:09.643: INFO: Number of nodes with available pods: 0
Jun 21 22:57:09.643: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:57:10.634: INFO: Number of nodes with available pods: 1
Jun 21 22:57:10.634: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8954, will wait for the garbage collector to delete the pods
Jun 21 22:57:10.753: INFO: Deleting DaemonSet.extensions daemon-set took: 25.420408ms
Jun 21 22:57:10.854: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.313638ms
Jun 21 22:57:14.466: INFO: Number of nodes with available pods: 0
Jun 21 22:57:14.466: INFO: Number of running nodes: 0, number of available pods: 0
Jun 21 22:57:14.484: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62719"},"items":null}

Jun 21 22:57:14.498: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62719"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:57:14.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8954" for this suite.

• [SLOW TEST:14.839 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":51,"skipped":734,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:57:14.669: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-6e774e41-fc10-4ad1-9925-383cce487d1e
STEP: Creating a pod to test consume configMaps
Jun 21 22:57:14.998: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad" in namespace "configmap-2940" to be "Succeeded or Failed"
Jun 21 22:57:15.013: INFO: Pod "pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad": Phase="Pending", Reason="", readiness=false. Elapsed: 15.057407ms
Jun 21 22:57:17.036: INFO: Pod "pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037395116s
Jun 21 22:57:19.049: INFO: Pod "pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051042335s
Jun 21 22:57:21.067: INFO: Pod "pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068448313s
STEP: Saw pod success
Jun 21 22:57:21.067: INFO: Pod "pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad" satisfied condition "Succeeded or Failed"
Jun 21 22:57:21.081: INFO: Trying to get logs from node 10.10.24.214 pod pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad container agnhost-container: <nil>
STEP: delete the pod
Jun 21 22:57:21.240: INFO: Waiting for pod pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad to disappear
Jun 21 22:57:21.252: INFO: Pod pod-configmaps-f7de3bac-5f71-4dcd-9ed6-a9708fcd61ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:57:21.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2940" for this suite.

• [SLOW TEST:6.637 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":755,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:57:21.306: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3545
STEP: creating service affinity-nodeport-transition in namespace services-3545
STEP: creating replication controller affinity-nodeport-transition in namespace services-3545
I0621 22:57:21.615828      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3545, replica count: 3
I0621 22:57:24.668589      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 22:57:24.753: INFO: Creating new exec pod
Jun 21 22:57:29.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3545 exec execpod-affinitydfn8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun 21 22:57:30.452: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 21 22:57:30.452: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:57:30.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3545 exec execpod-affinitydfn8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.206.64 80'
Jun 21 22:57:30.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.206.64 80\nConnection to 172.21.206.64 80 port [tcp/http] succeeded!\n"
Jun 21 22:57:30.906: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:57:30.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3545 exec execpod-affinitydfn8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 30546'
Jun 21 22:57:31.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 30546\nConnection to 10.10.24.206 30546 port [tcp/*] succeeded!\n"
Jun 21 22:57:31.366: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:57:31.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3545 exec execpod-affinitydfn8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.208 30546'
Jun 21 22:57:31.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.208 30546\nConnection to 10.10.24.208 30546 port [tcp/*] succeeded!\n"
Jun 21 22:57:31.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 22:57:31.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3545 exec execpod-affinitydfn8z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.24.206:30546/ ; done'
Jun 21 22:57:32.639: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n"
Jun 21 22:57:32.640: INFO: stdout: "\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-4w6bn\naffinity-nodeport-transition-4w6bn\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-4w6bn\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-4tkk4\naffinity-nodeport-transition-4w6bn\naffinity-nodeport-transition-4tkk4"
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4w6bn
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4w6bn
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4w6bn
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4w6bn
Jun 21 22:57:32.640: INFO: Received response from host: affinity-nodeport-transition-4tkk4
Jun 21 22:57:32.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3545 exec execpod-affinitydfn8z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.24.206:30546/ ; done'
Jun 21 22:57:33.318: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30546/\n"
Jun 21 22:57:33.318: INFO: stdout: "\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp\naffinity-nodeport-transition-s9gdp"
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Received response from host: affinity-nodeport-transition-s9gdp
Jun 21 22:57:33.318: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3545, will wait for the garbage collector to delete the pods
Jun 21 22:57:33.464: INFO: Deleting ReplicationController affinity-nodeport-transition took: 30.580173ms
Jun 21 22:57:33.666: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.932869ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:57:36.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3545" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.532 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":53,"skipped":760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:57:36.842: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8651.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8651.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8651.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8651.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8651.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 253.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.253_udp@PTR;check="$$(dig +tcp +noall +answer +search 253.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.253_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8651.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8651.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8651.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8651.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8651.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8651.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 253.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.253_udp@PTR;check="$$(dig +tcp +noall +answer +search 253.253.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.253.253_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 21 22:57:41.291: INFO: Unable to read wheezy_udp@dns-test-service.dns-8651.svc.cluster.local from pod dns-8651/dns-test-94032098-686b-4ade-b584-155a3c852d0e: the server could not find the requested resource (get pods dns-test-94032098-686b-4ade-b584-155a3c852d0e)
Jun 21 22:57:41.332: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local from pod dns-8651/dns-test-94032098-686b-4ade-b584-155a3c852d0e: the server could not find the requested resource (get pods dns-test-94032098-686b-4ade-b584-155a3c852d0e)
Jun 21 22:57:41.509: INFO: Unable to read jessie_udp@dns-test-service.dns-8651.svc.cluster.local from pod dns-8651/dns-test-94032098-686b-4ade-b584-155a3c852d0e: the server could not find the requested resource (get pods dns-test-94032098-686b-4ade-b584-155a3c852d0e)
Jun 21 22:57:41.551: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local from pod dns-8651/dns-test-94032098-686b-4ade-b584-155a3c852d0e: the server could not find the requested resource (get pods dns-test-94032098-686b-4ade-b584-155a3c852d0e)
Jun 21 22:57:41.705: INFO: Lookups using dns-8651/dns-test-94032098-686b-4ade-b584-155a3c852d0e failed for: [wheezy_udp@dns-test-service.dns-8651.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local jessie_udp@dns-test-service.dns-8651.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8651.svc.cluster.local]

Jun 21 22:57:47.140: INFO: DNS probes using dns-8651/dns-test-94032098-686b-4ade-b584-155a3c852d0e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:57:47.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8651" for this suite.

• [SLOW TEST:10.522 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":54,"skipped":812,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:57:47.365: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-k224b in namespace proxy-8083
I0621 22:57:47.746783      22 runners.go:190] Created replication controller with name: proxy-service-k224b, namespace: proxy-8083, replica count: 1
I0621 22:57:48.797814      22 runners.go:190] proxy-service-k224b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0621 22:57:49.798761      22 runners.go:190] proxy-service-k224b Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0621 22:57:50.799927      22 runners.go:190] proxy-service-k224b Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 22:57:50.823: INFO: setup took 3.17721776s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 21 22:57:50.860: INFO: (0) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 36.522177ms)
Jun 21 22:57:50.860: INFO: (0) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 36.217018ms)
Jun 21 22:57:50.863: INFO: (0) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 39.24694ms)
Jun 21 22:57:50.871: INFO: (0) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 46.874562ms)
Jun 21 22:57:50.872: INFO: (0) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 47.81761ms)
Jun 21 22:57:50.872: INFO: (0) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 48.313751ms)
Jun 21 22:57:50.873: INFO: (0) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 48.066601ms)
Jun 21 22:57:50.873: INFO: (0) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 47.387057ms)
Jun 21 22:57:50.883: INFO: (0) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 57.23739ms)
Jun 21 22:57:50.884: INFO: (0) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 59.598442ms)
Jun 21 22:57:50.884: INFO: (0) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 58.94978ms)
Jun 21 22:57:50.884: INFO: (0) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 59.659105ms)
Jun 21 22:57:50.893: INFO: (0) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 68.634941ms)
Jun 21 22:57:50.895: INFO: (0) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 70.029084ms)
Jun 21 22:57:50.903: INFO: (0) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 78.993165ms)
Jun 21 22:57:50.903: INFO: (0) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 78.705023ms)
Jun 21 22:57:50.926: INFO: (1) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 22.13562ms)
Jun 21 22:57:50.927: INFO: (1) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 22.835737ms)
Jun 21 22:57:50.928: INFO: (1) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 24.66789ms)
Jun 21 22:57:50.929: INFO: (1) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 24.389463ms)
Jun 21 22:57:50.929: INFO: (1) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 24.797459ms)
Jun 21 22:57:50.929: INFO: (1) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 25.243226ms)
Jun 21 22:57:50.931: INFO: (1) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 27.287647ms)
Jun 21 22:57:50.931: INFO: (1) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 26.691125ms)
Jun 21 22:57:50.931: INFO: (1) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 26.661721ms)
Jun 21 22:57:50.931: INFO: (1) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 26.89827ms)
Jun 21 22:57:50.936: INFO: (1) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 32.846075ms)
Jun 21 22:57:50.938: INFO: (1) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 34.036227ms)
Jun 21 22:57:50.942: INFO: (1) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 37.227232ms)
Jun 21 22:57:50.942: INFO: (1) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 37.979899ms)
Jun 21 22:57:50.942: INFO: (1) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 37.63893ms)
Jun 21 22:57:50.943: INFO: (1) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 38.844114ms)
Jun 21 22:57:50.968: INFO: (2) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 24.340274ms)
Jun 21 22:57:50.968: INFO: (2) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 24.891663ms)
Jun 21 22:57:50.969: INFO: (2) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 25.249393ms)
Jun 21 22:57:50.970: INFO: (2) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 25.87258ms)
Jun 21 22:57:50.970: INFO: (2) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 26.789161ms)
Jun 21 22:57:50.970: INFO: (2) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 26.670565ms)
Jun 21 22:57:50.971: INFO: (2) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 27.823521ms)
Jun 21 22:57:50.972: INFO: (2) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 28.489913ms)
Jun 21 22:57:50.972: INFO: (2) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 28.338781ms)
Jun 21 22:57:50.985: INFO: (2) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 41.087465ms)
Jun 21 22:57:50.972: INFO: (2) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 28.278823ms)
Jun 21 22:57:50.980: INFO: (2) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 36.716073ms)
Jun 21 22:57:50.995: INFO: (2) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 51.235692ms)
Jun 21 22:57:50.995: INFO: (2) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 51.939217ms)
Jun 21 22:57:50.996: INFO: (2) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 51.909426ms)
Jun 21 22:57:50.996: INFO: (2) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 52.071732ms)
Jun 21 22:57:51.017: INFO: (3) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 21.169586ms)
Jun 21 22:57:51.024: INFO: (3) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 25.961525ms)
Jun 21 22:57:51.025: INFO: (3) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 27.158333ms)
Jun 21 22:57:51.025: INFO: (3) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 27.872969ms)
Jun 21 22:57:51.025: INFO: (3) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 27.956559ms)
Jun 21 22:57:51.025: INFO: (3) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 28.816668ms)
Jun 21 22:57:51.025: INFO: (3) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 27.90808ms)
Jun 21 22:57:51.026: INFO: (3) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.082958ms)
Jun 21 22:57:51.026: INFO: (3) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.027873ms)
Jun 21 22:57:51.026: INFO: (3) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 29.500838ms)
Jun 21 22:57:51.032: INFO: (3) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 35.452515ms)
Jun 21 22:57:51.033: INFO: (3) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 37.109156ms)
Jun 21 22:57:51.033: INFO: (3) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 35.585699ms)
Jun 21 22:57:51.033: INFO: (3) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 35.760879ms)
Jun 21 22:57:51.035: INFO: (3) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 37.270611ms)
Jun 21 22:57:51.035: INFO: (3) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 36.911083ms)
Jun 21 22:57:51.054: INFO: (4) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 18.824045ms)
Jun 21 22:57:51.056: INFO: (4) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 20.540406ms)
Jun 21 22:57:51.056: INFO: (4) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 21.000746ms)
Jun 21 22:57:51.062: INFO: (4) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 26.192135ms)
Jun 21 22:57:51.062: INFO: (4) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 26.956154ms)
Jun 21 22:57:51.063: INFO: (4) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 27.431568ms)
Jun 21 22:57:51.063: INFO: (4) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 27.126292ms)
Jun 21 22:57:51.064: INFO: (4) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 28.231636ms)
Jun 21 22:57:51.064: INFO: (4) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 28.596397ms)
Jun 21 22:57:51.071: INFO: (4) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 35.235119ms)
Jun 21 22:57:51.071: INFO: (4) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 35.870435ms)
Jun 21 22:57:51.075: INFO: (4) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 39.765344ms)
Jun 21 22:57:51.076: INFO: (4) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 40.715629ms)
Jun 21 22:57:51.076: INFO: (4) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 40.56735ms)
Jun 21 22:57:51.076: INFO: (4) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 40.996015ms)
Jun 21 22:57:51.081: INFO: (4) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 45.342868ms)
Jun 21 22:57:51.111: INFO: (5) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 28.838557ms)
Jun 21 22:57:51.111: INFO: (5) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 27.96807ms)
Jun 21 22:57:51.111: INFO: (5) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.659815ms)
Jun 21 22:57:51.112: INFO: (5) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 30.203867ms)
Jun 21 22:57:51.112: INFO: (5) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 30.670081ms)
Jun 21 22:57:51.112: INFO: (5) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 30.999127ms)
Jun 21 22:57:51.112: INFO: (5) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 29.962631ms)
Jun 21 22:57:51.112: INFO: (5) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 30.998016ms)
Jun 21 22:57:51.113: INFO: (5) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 31.666256ms)
Jun 21 22:57:51.114: INFO: (5) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 32.557776ms)
Jun 21 22:57:51.114: INFO: (5) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 31.467171ms)
Jun 21 22:57:51.123: INFO: (5) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 40.648109ms)
Jun 21 22:57:51.123: INFO: (5) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 40.396051ms)
Jun 21 22:57:51.123: INFO: (5) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 41.754267ms)
Jun 21 22:57:51.124: INFO: (5) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 41.056757ms)
Jun 21 22:57:51.124: INFO: (5) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 41.693818ms)
Jun 21 22:57:51.155: INFO: (6) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.829808ms)
Jun 21 22:57:51.155: INFO: (6) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 29.053529ms)
Jun 21 22:57:51.155: INFO: (6) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 31.133573ms)
Jun 21 22:57:51.156: INFO: (6) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 31.273447ms)
Jun 21 22:57:51.158: INFO: (6) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.876959ms)
Jun 21 22:57:51.158: INFO: (6) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 31.962871ms)
Jun 21 22:57:51.158: INFO: (6) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 32.746662ms)
Jun 21 22:57:51.158: INFO: (6) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 33.84213ms)
Jun 21 22:57:51.159: INFO: (6) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 30.147545ms)
Jun 21 22:57:51.159: INFO: (6) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 32.205269ms)
Jun 21 22:57:51.159: INFO: (6) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 30.53396ms)
Jun 21 22:57:51.164: INFO: (6) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 39.034566ms)
Jun 21 22:57:51.171: INFO: (6) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 46.613351ms)
Jun 21 22:57:51.171: INFO: (6) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 43.786579ms)
Jun 21 22:57:51.176: INFO: (6) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 48.589336ms)
Jun 21 22:57:51.177: INFO: (6) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 49.272308ms)
Jun 21 22:57:51.221: INFO: (7) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 43.948349ms)
Jun 21 22:57:51.227: INFO: (7) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 48.318215ms)
Jun 21 22:57:51.228: INFO: (7) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 49.280688ms)
Jun 21 22:57:51.228: INFO: (7) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 49.289674ms)
Jun 21 22:57:51.229: INFO: (7) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 49.976312ms)
Jun 21 22:57:51.229: INFO: (7) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 50.158478ms)
Jun 21 22:57:51.229: INFO: (7) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 51.865482ms)
Jun 21 22:57:51.229: INFO: (7) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 50.256243ms)
Jun 21 22:57:51.229: INFO: (7) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 50.78488ms)
Jun 21 22:57:51.229: INFO: (7) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 51.331856ms)
Jun 21 22:57:51.236: INFO: (7) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 57.62238ms)
Jun 21 22:57:51.247: INFO: (7) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 68.209851ms)
Jun 21 22:57:51.248: INFO: (7) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 68.917429ms)
Jun 21 22:57:51.248: INFO: (7) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 69.968573ms)
Jun 21 22:57:51.248: INFO: (7) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 70.420378ms)
Jun 21 22:57:51.248: INFO: (7) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 70.455493ms)
Jun 21 22:57:51.276: INFO: (8) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 27.386043ms)
Jun 21 22:57:51.277: INFO: (8) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 26.750805ms)
Jun 21 22:57:51.277: INFO: (8) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 26.551998ms)
Jun 21 22:57:51.277: INFO: (8) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 26.885811ms)
Jun 21 22:57:51.277: INFO: (8) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 26.475933ms)
Jun 21 22:57:51.279: INFO: (8) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 28.981211ms)
Jun 21 22:57:51.280: INFO: (8) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 29.940546ms)
Jun 21 22:57:51.280: INFO: (8) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 30.035799ms)
Jun 21 22:57:51.283: INFO: (8) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 32.41481ms)
Jun 21 22:57:51.283: INFO: (8) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 32.546886ms)
Jun 21 22:57:51.283: INFO: (8) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 32.90065ms)
Jun 21 22:57:51.284: INFO: (8) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 34.167923ms)
Jun 21 22:57:51.284: INFO: (8) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 34.710498ms)
Jun 21 22:57:51.288: INFO: (8) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 37.897769ms)
Jun 21 22:57:51.301: INFO: (8) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 50.638686ms)
Jun 21 22:57:51.301: INFO: (8) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 50.511074ms)
Jun 21 22:57:51.334: INFO: (9) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 32.928466ms)
Jun 21 22:57:51.334: INFO: (9) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 32.092984ms)
Jun 21 22:57:51.334: INFO: (9) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 32.500655ms)
Jun 21 22:57:51.334: INFO: (9) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 32.355794ms)
Jun 21 22:57:51.334: INFO: (9) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 32.34597ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 37.405565ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 37.451529ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 37.526508ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 37.533687ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 37.456042ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 37.498945ms)
Jun 21 22:57:51.340: INFO: (9) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 38.589991ms)
Jun 21 22:57:51.343: INFO: (9) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 40.365855ms)
Jun 21 22:57:51.342: INFO: (9) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 40.228019ms)
Jun 21 22:57:51.344: INFO: (9) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 41.82818ms)
Jun 21 22:57:51.344: INFO: (9) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 42.28452ms)
Jun 21 22:57:51.372: INFO: (10) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 27.305423ms)
Jun 21 22:57:51.379: INFO: (10) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 31.558969ms)
Jun 21 22:57:51.379: INFO: (10) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 31.51274ms)
Jun 21 22:57:51.379: INFO: (10) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 32.88481ms)
Jun 21 22:57:51.379: INFO: (10) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 32.386604ms)
Jun 21 22:57:51.379: INFO: (10) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 33.865655ms)
Jun 21 22:57:51.381: INFO: (10) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 34.551113ms)
Jun 21 22:57:51.381: INFO: (10) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 34.776241ms)
Jun 21 22:57:51.381: INFO: (10) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 36.584813ms)
Jun 21 22:57:51.382: INFO: (10) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 34.448686ms)
Jun 21 22:57:51.385: INFO: (10) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 39.258631ms)
Jun 21 22:57:51.392: INFO: (10) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 45.448592ms)
Jun 21 22:57:51.392: INFO: (10) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 47.039024ms)
Jun 21 22:57:51.393: INFO: (10) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 47.278505ms)
Jun 21 22:57:51.393: INFO: (10) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 45.767194ms)
Jun 21 22:57:51.393: INFO: (10) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 46.404101ms)
Jun 21 22:57:51.414: INFO: (11) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 21.035282ms)
Jun 21 22:57:51.417: INFO: (11) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 23.356123ms)
Jun 21 22:57:51.418: INFO: (11) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 24.862482ms)
Jun 21 22:57:51.419: INFO: (11) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 25.423519ms)
Jun 21 22:57:51.419: INFO: (11) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 25.452685ms)
Jun 21 22:57:51.420: INFO: (11) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 25.895136ms)
Jun 21 22:57:51.420: INFO: (11) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 26.397769ms)
Jun 21 22:57:51.420: INFO: (11) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 26.052606ms)
Jun 21 22:57:51.420: INFO: (11) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 26.359112ms)
Jun 21 22:57:51.421: INFO: (11) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 27.463023ms)
Jun 21 22:57:51.425: INFO: (11) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 31.775957ms)
Jun 21 22:57:51.442: INFO: (11) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 48.774765ms)
Jun 21 22:57:51.443: INFO: (11) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 49.188758ms)
Jun 21 22:57:51.443: INFO: (11) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 49.372477ms)
Jun 21 22:57:51.443: INFO: (11) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 49.530663ms)
Jun 21 22:57:51.443: INFO: (11) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 49.701557ms)
Jun 21 22:57:51.466: INFO: (12) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 22.568914ms)
Jun 21 22:57:51.471: INFO: (12) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 26.248888ms)
Jun 21 22:57:51.471: INFO: (12) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 26.044297ms)
Jun 21 22:57:51.473: INFO: (12) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 28.387523ms)
Jun 21 22:57:51.473: INFO: (12) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 27.689639ms)
Jun 21 22:57:51.473: INFO: (12) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.683622ms)
Jun 21 22:57:51.473: INFO: (12) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.641274ms)
Jun 21 22:57:51.473: INFO: (12) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 28.520949ms)
Jun 21 22:57:51.475: INFO: (12) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 30.071771ms)
Jun 21 22:57:51.479: INFO: (12) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 35.122134ms)
Jun 21 22:57:51.479: INFO: (12) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 34.230149ms)
Jun 21 22:57:51.479: INFO: (12) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 34.190162ms)
Jun 21 22:57:51.482: INFO: (12) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 37.178651ms)
Jun 21 22:57:51.483: INFO: (12) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 37.546743ms)
Jun 21 22:57:51.483: INFO: (12) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 38.334818ms)
Jun 21 22:57:51.484: INFO: (12) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 38.749282ms)
Jun 21 22:57:51.505: INFO: (13) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 20.683976ms)
Jun 21 22:57:51.511: INFO: (13) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 25.688384ms)
Jun 21 22:57:51.512: INFO: (13) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 26.015572ms)
Jun 21 22:57:51.512: INFO: (13) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 26.30667ms)
Jun 21 22:57:51.512: INFO: (13) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 25.769667ms)
Jun 21 22:57:51.513: INFO: (13) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 28.020435ms)
Jun 21 22:57:51.513: INFO: (13) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 27.899311ms)
Jun 21 22:57:51.513: INFO: (13) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 28.653625ms)
Jun 21 22:57:51.513: INFO: (13) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 28.809299ms)
Jun 21 22:57:51.513: INFO: (13) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 28.157318ms)
Jun 21 22:57:51.513: INFO: (13) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 28.061509ms)
Jun 21 22:57:51.521: INFO: (13) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 34.654425ms)
Jun 21 22:57:51.526: INFO: (13) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 40.138478ms)
Jun 21 22:57:51.528: INFO: (13) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 41.525987ms)
Jun 21 22:57:51.528: INFO: (13) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 42.801882ms)
Jun 21 22:57:51.528: INFO: (13) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 43.925989ms)
Jun 21 22:57:51.560: INFO: (14) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 30.137052ms)
Jun 21 22:57:51.563: INFO: (14) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 32.314906ms)
Jun 21 22:57:51.563: INFO: (14) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 32.418869ms)
Jun 21 22:57:51.564: INFO: (14) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 33.376838ms)
Jun 21 22:57:51.564: INFO: (14) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 33.772136ms)
Jun 21 22:57:51.564: INFO: (14) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 34.55048ms)
Jun 21 22:57:51.564: INFO: (14) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 35.911645ms)
Jun 21 22:57:51.564: INFO: (14) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 34.364753ms)
Jun 21 22:57:51.565: INFO: (14) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 34.57694ms)
Jun 21 22:57:51.565: INFO: (14) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 36.338749ms)
Jun 21 22:57:51.573: INFO: (14) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 43.981946ms)
Jun 21 22:57:51.573: INFO: (14) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 41.889917ms)
Jun 21 22:57:51.573: INFO: (14) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 42.625887ms)
Jun 21 22:57:51.573: INFO: (14) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 42.882042ms)
Jun 21 22:57:51.575: INFO: (14) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 45.91628ms)
Jun 21 22:57:51.575: INFO: (14) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 44.385889ms)
Jun 21 22:57:51.597: INFO: (15) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 20.587415ms)
Jun 21 22:57:51.598: INFO: (15) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 22.365918ms)
Jun 21 22:57:51.603: INFO: (15) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 25.216094ms)
Jun 21 22:57:51.605: INFO: (15) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 27.862589ms)
Jun 21 22:57:51.609: INFO: (15) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 31.728386ms)
Jun 21 22:57:51.610: INFO: (15) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 32.518691ms)
Jun 21 22:57:51.611: INFO: (15) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 32.711481ms)
Jun 21 22:57:51.611: INFO: (15) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 33.986751ms)
Jun 21 22:57:51.611: INFO: (15) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 32.899404ms)
Jun 21 22:57:51.611: INFO: (15) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 34.35895ms)
Jun 21 22:57:51.611: INFO: (15) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 33.321984ms)
Jun 21 22:57:51.618: INFO: (15) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 40.768013ms)
Jun 21 22:57:51.619: INFO: (15) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 42.415555ms)
Jun 21 22:57:51.620: INFO: (15) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 42.29489ms)
Jun 21 22:57:51.621: INFO: (15) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 42.666917ms)
Jun 21 22:57:51.621: INFO: (15) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 44.342177ms)
Jun 21 22:57:51.643: INFO: (16) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 21.537184ms)
Jun 21 22:57:51.645: INFO: (16) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 22.282283ms)
Jun 21 22:57:51.645: INFO: (16) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 22.551252ms)
Jun 21 22:57:51.645: INFO: (16) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 22.604282ms)
Jun 21 22:57:51.646: INFO: (16) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 23.951537ms)
Jun 21 22:57:51.648: INFO: (16) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 25.835525ms)
Jun 21 22:57:51.656: INFO: (16) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 33.810949ms)
Jun 21 22:57:51.656: INFO: (16) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 33.622298ms)
Jun 21 22:57:51.656: INFO: (16) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 32.861494ms)
Jun 21 22:57:51.656: INFO: (16) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 33.740222ms)
Jun 21 22:57:51.657: INFO: (16) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 34.805385ms)
Jun 21 22:57:51.657: INFO: (16) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 34.611468ms)
Jun 21 22:57:51.662: INFO: (16) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 39.215464ms)
Jun 21 22:57:51.662: INFO: (16) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 39.983602ms)
Jun 21 22:57:51.664: INFO: (16) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 41.040243ms)
Jun 21 22:57:51.664: INFO: (16) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 41.016693ms)
Jun 21 22:57:51.684: INFO: (17) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 20.031156ms)
Jun 21 22:57:51.705: INFO: (17) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 40.822621ms)
Jun 21 22:57:51.710: INFO: (17) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 44.33641ms)
Jun 21 22:57:51.710: INFO: (17) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 44.90268ms)
Jun 21 22:57:51.711: INFO: (17) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 46.872243ms)
Jun 21 22:57:51.711: INFO: (17) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 46.994785ms)
Jun 21 22:57:51.711: INFO: (17) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 46.4975ms)
Jun 21 22:57:51.711: INFO: (17) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 46.849784ms)
Jun 21 22:57:51.711: INFO: (17) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 46.668947ms)
Jun 21 22:57:51.711: INFO: (17) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 45.796487ms)
Jun 21 22:57:51.720: INFO: (17) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 55.055986ms)
Jun 21 22:57:51.720: INFO: (17) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 55.545556ms)
Jun 21 22:57:51.723: INFO: (17) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 58.08763ms)
Jun 21 22:57:51.723: INFO: (17) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 58.160489ms)
Jun 21 22:57:51.724: INFO: (17) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 58.182936ms)
Jun 21 22:57:51.724: INFO: (17) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 59.18165ms)
Jun 21 22:57:51.744: INFO: (18) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 19.665533ms)
Jun 21 22:57:51.748: INFO: (18) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 22.808229ms)
Jun 21 22:57:51.749: INFO: (18) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 23.510942ms)
Jun 21 22:57:51.768: INFO: (18) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 41.46738ms)
Jun 21 22:57:51.768: INFO: (18) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 42.665781ms)
Jun 21 22:57:51.770: INFO: (18) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 44.450987ms)
Jun 21 22:57:51.771: INFO: (18) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 44.792378ms)
Jun 21 22:57:51.771: INFO: (18) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 45.611006ms)
Jun 21 22:57:51.771: INFO: (18) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 45.066347ms)
Jun 21 22:57:51.772: INFO: (18) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 46.113993ms)
Jun 21 22:57:51.772: INFO: (18) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 45.328279ms)
Jun 21 22:57:51.772: INFO: (18) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 46.757002ms)
Jun 21 22:57:51.772: INFO: (18) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 45.907029ms)
Jun 21 22:57:51.775: INFO: (18) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 49.131119ms)
Jun 21 22:57:51.775: INFO: (18) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 48.779593ms)
Jun 21 22:57:51.775: INFO: (18) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 49.577469ms)
Jun 21 22:57:51.801: INFO: (19) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj/proxy/rewriteme">test</a> (200; 23.948647ms)
Jun 21 22:57:51.801: INFO: (19) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:160/proxy/: foo (200; 25.450401ms)
Jun 21 22:57:51.803: INFO: (19) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:1080/proxy/rewriteme">... (200; 26.553892ms)
Jun 21 22:57:51.804: INFO: (19) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:462/proxy/: tls qux (200; 27.598056ms)
Jun 21 22:57:51.806: INFO: (19) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:1080/proxy/rewriteme">test<... (200; 28.602523ms)
Jun 21 22:57:51.806: INFO: (19) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/: <a href="/api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:443/proxy/tlsrewritem... (200; 29.252578ms)
Jun 21 22:57:51.808: INFO: (19) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:162/proxy/: bar (200; 31.303956ms)
Jun 21 22:57:51.808: INFO: (19) /api/v1/namespaces/proxy-8083/pods/proxy-service-k224b-n7rbj:160/proxy/: foo (200; 31.4889ms)
Jun 21 22:57:51.809: INFO: (19) /api/v1/namespaces/proxy-8083/pods/https:proxy-service-k224b-n7rbj:460/proxy/: tls baz (200; 32.178254ms)
Jun 21 22:57:51.809: INFO: (19) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname2/proxy/: tls qux (200; 34.145904ms)
Jun 21 22:57:51.810: INFO: (19) /api/v1/namespaces/proxy-8083/pods/http:proxy-service-k224b-n7rbj:162/proxy/: bar (200; 32.853338ms)
Jun 21 22:57:51.810: INFO: (19) /api/v1/namespaces/proxy-8083/services/https:proxy-service-k224b:tlsportname1/proxy/: tls baz (200; 33.735776ms)
Jun 21 22:57:51.815: INFO: (19) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname1/proxy/: foo (200; 37.876165ms)
Jun 21 22:57:51.818: INFO: (19) /api/v1/namespaces/proxy-8083/services/http:proxy-service-k224b:portname2/proxy/: bar (200; 40.367126ms)
Jun 21 22:57:51.818: INFO: (19) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname2/proxy/: bar (200; 41.220887ms)
Jun 21 22:57:51.818: INFO: (19) /api/v1/namespaces/proxy-8083/services/proxy-service-k224b:portname1/proxy/: foo (200; 40.595932ms)
STEP: deleting ReplicationController proxy-service-k224b in namespace proxy-8083, will wait for the garbage collector to delete the pods
Jun 21 22:57:51.914: INFO: Deleting ReplicationController proxy-service-k224b took: 28.846478ms
Jun 21 22:57:52.015: INFO: Terminating ReplicationController proxy-service-k224b pods took: 100.643501ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:57:55.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8083" for this suite.

• [SLOW TEST:8.513 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":55,"skipped":822,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:57:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun 21 22:57:56.077: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:58:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8233" for this suite.

• [SLOW TEST:7.019 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":56,"skipped":830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:58:02.903: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:58:07.332: INFO: Deleting pod "var-expansion-6a9052f0-c45e-4210-94df-7d88704c4c80" in namespace "var-expansion-8267"
Jun 21 22:58:07.359: INFO: Wait up to 5m0s for pod "var-expansion-6a9052f0-c45e-4210-94df-7d88704c4c80" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:58:11.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8267" for this suite.

• [SLOW TEST:8.551 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":57,"skipped":897,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:58:11.460: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 21 22:58:11.904: INFO: Number of nodes with available pods: 0
Jun 21 22:58:11.905: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 22:58:12.944: INFO: Number of nodes with available pods: 0
Jun 21 22:58:12.944: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 22:58:13.945: INFO: Number of nodes with available pods: 0
Jun 21 22:58:13.945: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 22:58:14.943: INFO: Number of nodes with available pods: 2
Jun 21 22:58:14.943: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 22:58:15.943: INFO: Number of nodes with available pods: 3
Jun 21 22:58:15.943: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Jun 21 22:58:16.002: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jun 21 22:58:16.048: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jun 21 22:58:16.053: INFO: Observed &DaemonSet event: ADDED
Jun 21 22:58:16.053: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.053: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.053: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.054: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.054: INFO: Found daemon set daemon-set in namespace daemonsets-6985 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 21 22:58:16.054: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jun 21 22:58:16.084: INFO: Observed &DaemonSet event: ADDED
Jun 21 22:58:16.084: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.085: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.085: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.085: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.085: INFO: Observed daemon set daemon-set in namespace daemonsets-6985 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 21 22:58:16.086: INFO: Observed &DaemonSet event: MODIFIED
Jun 21 22:58:16.086: INFO: Found daemon set daemon-set in namespace daemonsets-6985 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun 21 22:58:16.086: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6985, will wait for the garbage collector to delete the pods
Jun 21 22:58:16.184: INFO: Deleting DaemonSet.extensions daemon-set took: 21.633799ms
Jun 21 22:58:16.285: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.733926ms
Jun 21 22:58:19.216: INFO: Number of nodes with available pods: 0
Jun 21 22:58:19.216: INFO: Number of running nodes: 0, number of available pods: 0
Jun 21 22:58:19.228: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"63906"},"items":null}

Jun 21 22:58:19.250: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"63906"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:58:19.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6985" for this suite.

• [SLOW TEST:7.938 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":58,"skipped":933,"failed":0}
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:58:19.399: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:58:19.782: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a" in namespace "security-context-test-911" to be "Succeeded or Failed"
Jun 21 22:58:19.805: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.678512ms
Jun 21 22:58:21.829: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046568722s
Jun 21 22:58:23.847: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064874453s
Jun 21 22:58:25.876: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09363511s
Jun 21 22:58:27.895: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113219865s
Jun 21 22:58:29.912: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.130156304s
Jun 21 22:58:31.930: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.148219574s
Jun 21 22:58:33.964: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.18232676s
Jun 21 22:58:33.964: INFO: Pod "alpine-nnp-false-4886d125-3c86-4f52-8642-247234b3281a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:58:34.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-911" for this suite.

• [SLOW TEST:14.769 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:58:34.170: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1353 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1353;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1353 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1353;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1353.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1353.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1353.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1353.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1353.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1353.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1353.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1353.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1353.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 68.80.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.80.68_udp@PTR;check="$$(dig +tcp +noall +answer +search 68.80.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.80.68_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1353 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1353;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1353 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1353;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1353.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1353.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1353.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1353.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1353.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1353.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1353.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1353.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1353.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1353.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 68.80.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.80.68_udp@PTR;check="$$(dig +tcp +noall +answer +search 68.80.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.80.68_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 21 22:58:38.770: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.792: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.846: INFO: Unable to read wheezy_udp@dns-test-service.dns-1353 from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.871: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1353 from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.894: INFO: Unable to read wheezy_udp@dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.934: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:38.970: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.140: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.159: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.178: INFO: Unable to read jessie_udp@dns-test-service.dns-1353 from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.197: INFO: Unable to read jessie_tcp@dns-test-service.dns-1353 from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.220: INFO: Unable to read jessie_udp@dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.242: INFO: Unable to read jessie_tcp@dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.262: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1353.svc from pod dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109: the server could not find the requested resource (get pods dns-test-0915896f-a10c-4c96-8309-cc631f767109)
Jun 21 22:58:39.428: INFO: Lookups using dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1353 wheezy_tcp@dns-test-service.dns-1353 wheezy_udp@dns-test-service.dns-1353.svc wheezy_tcp@dns-test-service.dns-1353.svc wheezy_udp@_http._tcp.dns-test-service.dns-1353.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1353.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1353 jessie_tcp@dns-test-service.dns-1353 jessie_udp@dns-test-service.dns-1353.svc jessie_tcp@dns-test-service.dns-1353.svc jessie_udp@_http._tcp.dns-test-service.dns-1353.svc]

Jun 21 22:58:45.129: INFO: DNS probes using dns-1353/dns-test-0915896f-a10c-4c96-8309-cc631f767109 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:58:45.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1353" for this suite.

• [SLOW TEST:11.286 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":60,"skipped":978,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:58:45.456: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 21 22:58:55.918: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0621 22:58:55.918436      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:58:55.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8756" for this suite.

• [SLOW TEST:10.522 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":61,"skipped":979,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:58:55.979: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 21 22:58:56.429: INFO: Waiting up to 5m0s for pod "pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30" in namespace "emptydir-367" to be "Succeeded or Failed"
Jun 21 22:58:56.442: INFO: Pod "pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30": Phase="Pending", Reason="", readiness=false. Elapsed: 13.328936ms
Jun 21 22:58:58.458: INFO: Pod "pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029274883s
Jun 21 22:59:00.473: INFO: Pod "pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044537861s
Jun 21 22:59:02.490: INFO: Pod "pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061609688s
STEP: Saw pod success
Jun 21 22:59:02.490: INFO: Pod "pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30" satisfied condition "Succeeded or Failed"
Jun 21 22:59:02.503: INFO: Trying to get logs from node 10.10.24.214 pod pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30 container test-container: <nil>
STEP: delete the pod
Jun 21 22:59:02.652: INFO: Waiting for pod pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30 to disappear
Jun 21 22:59:02.664: INFO: Pod pod-ba3c79e7-b291-41ee-95fd-9df91bf37a30 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:59:02.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-367" for this suite.

• [SLOW TEST:6.733 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:59:02.714: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:59:02.943: INFO: Creating ReplicaSet my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2
Jun 21 22:59:03.013: INFO: Pod name my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2: Found 0 pods out of 1
Jun 21 22:59:08.030: INFO: Pod name my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2: Found 1 pods out of 1
Jun 21 22:59:08.030: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2" is running
Jun 21 22:59:08.044: INFO: Pod "my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2-h7v5q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 22:59:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 22:59:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 22:59:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 22:59:03 +0000 UTC Reason: Message:}])
Jun 21 22:59:08.044: INFO: Trying to dial the pod
Jun 21 22:59:13.101: INFO: Controller my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2: Got expected result from replica 1 [my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2-h7v5q]: "my-hostname-basic-6d2cbaac-9a29-4b8d-b122-3853159f0ec2-h7v5q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:59:13.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9903" for this suite.

• [SLOW TEST:10.444 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":63,"skipped":1026,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:59:13.160: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 21 22:59:14.201: INFO: starting watch
STEP: patching
STEP: updating
Jun 21 22:59:14.252: INFO: waiting for watch events with expected annotations
Jun 21 22:59:14.252: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:59:14.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5769" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":64,"skipped":1027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:59:14.539: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun 21 22:59:14.848: INFO: Waiting up to 5m0s for pod "security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6" in namespace "security-context-858" to be "Succeeded or Failed"
Jun 21 22:59:14.869: INFO: Pod "security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.165197ms
Jun 21 22:59:16.884: INFO: Pod "security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034943608s
Jun 21 22:59:18.901: INFO: Pod "security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052284211s
Jun 21 22:59:20.915: INFO: Pod "security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066378638s
STEP: Saw pod success
Jun 21 22:59:20.915: INFO: Pod "security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6" satisfied condition "Succeeded or Failed"
Jun 21 22:59:20.928: INFO: Trying to get logs from node 10.10.24.208 pod security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6 container test-container: <nil>
STEP: delete the pod
Jun 21 22:59:21.028: INFO: Waiting for pod security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6 to disappear
Jun 21 22:59:21.041: INFO: Pod security-context-7e08edb7-e754-44e8-bc3d-852db0b18bd6 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:59:21.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-858" for this suite.

• [SLOW TEST:6.561 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":65,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:59:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 21 22:59:21.391: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-46  d0e4e4e4-7c2c-423a-b999-c2ebcd978bcf 65066 0 2022-06-21 22:59:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-21 22:59:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 21 22:59:21.391: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-46  d0e4e4e4-7c2c-423a-b999-c2ebcd978bcf 65069 0 2022-06-21 22:59:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-21 22:59:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 21 22:59:21.453: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-46  d0e4e4e4-7c2c-423a-b999-c2ebcd978bcf 65072 0 2022-06-21 22:59:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-21 22:59:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 21 22:59:21.454: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-46  d0e4e4e4-7c2c-423a-b999-c2ebcd978bcf 65073 0 2022-06-21 22:59:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-21 22:59:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:59:21.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-46" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":66,"skipped":1130,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:59:21.529: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:59:21.865: INFO: The status of Pod pod-secrets-6a699e26-ef4d-4afa-a120-24195c9853f0 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:59:23.889: INFO: The status of Pod pod-secrets-6a699e26-ef4d-4afa-a120-24195c9853f0 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 22:59:25.886: INFO: The status of Pod pod-secrets-6a699e26-ef4d-4afa-a120-24195c9853f0 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 22:59:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3783" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":67,"skipped":1141,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 22:59:26.054: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 22:59:26.427: INFO: created pod
Jun 21 22:59:26.427: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2799" to be "Succeeded or Failed"
Jun 21 22:59:26.458: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 31.077366ms
Jun 21 22:59:28.470: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04330926s
Jun 21 22:59:30.488: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061883859s
Jun 21 22:59:32.508: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081362091s
STEP: Saw pod success
Jun 21 22:59:32.508: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun 21 23:00:02.514: INFO: polling logs
Jun 21 23:00:02.565: INFO: Pod logs: 
2022/06/21 22:59:28 OK: Got token
2022/06/21 22:59:28 validating with in-cluster discovery
2022/06/21 22:59:28 OK: got issuer https://kubernetes.default.svc
2022/06/21 22:59:28 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2799:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1655852966, NotBefore:1655852366, IssuedAt:1655852366, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2799", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b243c91f-7fc0-4c3c-876f-c3dbc2c692d0"}}}
2022/06/21 22:59:28 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2022/06/21 22:59:28 OK: Validated signature on JWT
2022/06/21 22:59:28 OK: Got valid claims from token!
2022/06/21 22:59:28 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2799:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1655852966, NotBefore:1655852366, IssuedAt:1655852366, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2799", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b243c91f-7fc0-4c3c-876f-c3dbc2c692d0"}}}

Jun 21 23:00:02.565: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:00:02.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2799" for this suite.

• [SLOW TEST:36.620 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":68,"skipped":1154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:00:02.678: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun 21 23:00:03.008: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:00:10.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6912" for this suite.

• [SLOW TEST:8.103 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":69,"skipped":1178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:00:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1227.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1227.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1227.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1227.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1227.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1227.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 21 23:00:15.340: INFO: DNS probes using dns-1227/dns-test-36847647-f334-4fdf-9f7a-afef54332513 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:00:15.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1227" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":70,"skipped":1202,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:00:15.488: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:00:15.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2439" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":71,"skipped":1218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:00:16.003: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jun 21 23:00:16.338: INFO: Waiting up to 5m0s for pod "var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1" in namespace "var-expansion-5919" to be "Succeeded or Failed"
Jun 21 23:00:16.351: INFO: Pod "var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.876866ms
Jun 21 23:00:18.369: INFO: Pod "var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030820645s
Jun 21 23:00:20.383: INFO: Pod "var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044366291s
Jun 21 23:00:22.403: INFO: Pod "var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06428312s
STEP: Saw pod success
Jun 21 23:00:22.403: INFO: Pod "var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1" satisfied condition "Succeeded or Failed"
Jun 21 23:00:22.425: INFO: Trying to get logs from node 10.10.24.208 pod var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1 container dapi-container: <nil>
STEP: delete the pod
Jun 21 23:00:22.504: INFO: Waiting for pod var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1 to disappear
Jun 21 23:00:22.517: INFO: Pod var-expansion-121001da-e250-4632-9f62-ee62d9acb5a1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:00:22.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5919" for this suite.

• [SLOW TEST:6.578 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1287,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:00:22.582: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-e68d859b-a822-4e08-aecb-eb5e99380af8
STEP: Creating a pod to test consume configMaps
Jun 21 23:00:22.965: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9" in namespace "projected-5964" to be "Succeeded or Failed"
Jun 21 23:00:22.978: INFO: Pod "pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.515688ms
Jun 21 23:00:24.993: INFO: Pod "pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027342563s
Jun 21 23:00:27.014: INFO: Pod "pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048348273s
Jun 21 23:00:29.028: INFO: Pod "pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062911117s
STEP: Saw pod success
Jun 21 23:00:29.028: INFO: Pod "pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9" satisfied condition "Succeeded or Failed"
Jun 21 23:00:29.040: INFO: Trying to get logs from node 10.10.24.214 pod pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:00:29.160: INFO: Waiting for pod pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9 to disappear
Jun 21 23:00:29.171: INFO: Pod pod-projected-configmaps-7a220663-7be2-4c87-af50-d01feb8b87f9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:00:29.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5964" for this suite.

• [SLOW TEST:6.656 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1290,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:00:29.240: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-c39435e5-79a5-40e5-9b52-1685cac14e96 in namespace container-probe-3035
Jun 21 23:00:33.676: INFO: Started pod busybox-c39435e5-79a5-40e5-9b52-1685cac14e96 in namespace container-probe-3035
STEP: checking the pod's current state and verifying that restartCount is present
Jun 21 23:00:33.689: INFO: Initial restart count of pod busybox-c39435e5-79a5-40e5-9b52-1685cac14e96 is 0
Jun 21 23:01:22.171: INFO: Restart count of pod container-probe-3035/busybox-c39435e5-79a5-40e5-9b52-1685cac14e96 is now 1 (48.482016573s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:01:22.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3035" for this suite.

• [SLOW TEST:53.051 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1305,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:01:22.292: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-aba5a964-bcbc-48e2-a41b-e04fa9e84423
STEP: Creating a pod to test consume secrets
Jun 21 23:01:22.650: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28" in namespace "projected-2258" to be "Succeeded or Failed"
Jun 21 23:01:22.678: INFO: Pod "pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28": Phase="Pending", Reason="", readiness=false. Elapsed: 27.66582ms
Jun 21 23:01:24.694: INFO: Pod "pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043322436s
Jun 21 23:01:26.762: INFO: Pod "pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11134782s
Jun 21 23:01:28.780: INFO: Pod "pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.129224104s
STEP: Saw pod success
Jun 21 23:01:28.780: INFO: Pod "pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28" satisfied condition "Succeeded or Failed"
Jun 21 23:01:28.793: INFO: Trying to get logs from node 10.10.24.214 pod pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:01:28.902: INFO: Waiting for pod pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28 to disappear
Jun 21 23:01:28.925: INFO: Pod pod-projected-secrets-530b198f-21cf-4066-8595-8bf3c4ff9f28 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:01:28.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2258" for this suite.

• [SLOW TEST:6.681 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1314,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:01:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Jun 21 23:01:29.295: INFO: Creating simple deployment test-deployment-5xhdm
Jun 21 23:01:29.378: INFO: deployment "test-deployment-5xhdm" doesn't have the required revision set
Jun 21 23:01:31.433: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449289, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449289, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449289, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449289, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-5xhdm-794dd694d8\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Jun 21 23:01:33.510: INFO: Deployment test-deployment-5xhdm has Conditions: [{Available True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xhdm-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Jun 21 23:01:33.553: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449292, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449292, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449292, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449289, loc:(*time.Location)(0xa0acfa0)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5xhdm-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jun 21 23:01:33.578: INFO: Observed &Deployment event: ADDED
Jun 21 23:01:33.578: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xhdm-794dd694d8"}
Jun 21 23:01:33.578: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.579: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xhdm-794dd694d8"}
Jun 21 23:01:33.579: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 21 23:01:33.579: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.579: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 21 23:01:33.579: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5xhdm-794dd694d8" is progressing.}
Jun 21 23:01:33.580: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.580: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 21 23:01:33.580: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xhdm-794dd694d8" has successfully progressed.}
Jun 21 23:01:33.580: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.580: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 21 23:01:33.580: INFO: Observed Deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xhdm-794dd694d8" has successfully progressed.}
Jun 21 23:01:33.580: INFO: Found Deployment test-deployment-5xhdm in namespace deployment-6438 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 21 23:01:33.580: INFO: Deployment test-deployment-5xhdm has an updated status
STEP: patching the Statefulset Status
Jun 21 23:01:33.581: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 21 23:01:33.602: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jun 21 23:01:33.611: INFO: Observed &Deployment event: ADDED
Jun 21 23:01:33.611: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xhdm-794dd694d8"}
Jun 21 23:01:33.612: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.612: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5xhdm-794dd694d8"}
Jun 21 23:01:33.612: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 21 23:01:33.612: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.613: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 21 23:01:33.613: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:29 +0000 UTC 2022-06-21 23:01:29 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5xhdm-794dd694d8" is progressing.}
Jun 21 23:01:33.613: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.614: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 21 23:01:33.614: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xhdm-794dd694d8" has successfully progressed.}
Jun 21 23:01:33.614: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.614: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 21 23:01:33.615: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-21 23:01:32 +0000 UTC 2022-06-21 23:01:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5xhdm-794dd694d8" has successfully progressed.}
Jun 21 23:01:33.615: INFO: Observed deployment test-deployment-5xhdm in namespace deployment-6438 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 21 23:01:33.615: INFO: Observed &Deployment event: MODIFIED
Jun 21 23:01:33.615: INFO: Found deployment test-deployment-5xhdm in namespace deployment-6438 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun 21 23:01:33.615: INFO: Deployment test-deployment-5xhdm has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 21 23:01:33.629: INFO: Deployment "test-deployment-5xhdm":
&Deployment{ObjectMeta:{test-deployment-5xhdm  deployment-6438  75881dd6-a970-42af-8c3e-4f361c7f22b3 66686 1 2022-06-21 23:01:29 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-06-21 23:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-06-21 23:01:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-06-21 23:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a8bb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5xhdm-794dd694d8",LastUpdateTime:2022-06-21 23:01:33 +0000 UTC,LastTransitionTime:2022-06-21 23:01:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 21 23:01:33.643: INFO: New ReplicaSet "test-deployment-5xhdm-794dd694d8" of Deployment "test-deployment-5xhdm":
&ReplicaSet{ObjectMeta:{test-deployment-5xhdm-794dd694d8  deployment-6438  d53f9455-21b4-445b-b19d-fa4d0b1366ff 66672 1 2022-06-21 23:01:29 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5xhdm 75881dd6-a970-42af-8c3e-4f361c7f22b3 0xc005a8bf60 0xc005a8bf61}] []  [{kube-controller-manager Update apps/v1 2022-06-21 23:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75881dd6-a970-42af-8c3e-4f361c7f22b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:01:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003072008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 21 23:01:33.659: INFO: Pod "test-deployment-5xhdm-794dd694d8-dlgk8" is available:
&Pod{ObjectMeta:{test-deployment-5xhdm-794dd694d8-dlgk8 test-deployment-5xhdm-794dd694d8- deployment-6438  1b27477c-74ec-4679-94a3-e86563654d2d 66671 0 2022-06-21 23:01:29 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:d45010691120d7f7e33f5d5fdbd268996059566d63f6c56a814db5cbdd66e91e cni.projectcalico.org/podIP:172.30.47.244/32 cni.projectcalico.org/podIPs:172.30.47.244/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.244"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.244"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-5xhdm-794dd694d8 d53f9455-21b4-445b-b19d-fa4d0b1366ff 0xc003072407 0xc003072408}] []  [{kube-controller-manager Update v1 2022-06-21 23:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d53f9455-21b4-445b-b19d-fa4d0b1366ff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 23:01:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 23:01:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 23:01:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zm47n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zm47n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c39,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2bb7t,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:01:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:01:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:01:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.244,StartTime:2022-06-21 23:01:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 23:01:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://9d410235648a08d14efe84d8cbeb858044b87be88753449576696b1c507b9dfb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:01:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6438" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":76,"skipped":1319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:01:33.719: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jun 21 23:01:54.490: INFO: EndpointSlice for Service endpointslice-8240/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:02:04.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8240" for this suite.

• [SLOW TEST:30.949 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":77,"skipped":1360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:02:04.673: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:02:05.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6182" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":78,"skipped":1386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:02:05.337: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 21 23:02:05.610: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 21 23:02:36.884: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:02:45.368: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:20.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3643" for this suite.

• [SLOW TEST:75.291 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":79,"skipped":1412,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun 21 23:03:20.989: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:20.989: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:21.012: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:21.012: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:21.043: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:21.043: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:21.170: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:21.170: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 21 23:03:23.931: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 21 23:03:23.931: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 21 23:03:24.096: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun 21 23:03:24.138: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun 21 23:03:24.144: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.144: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.144: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.144: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 0
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.145: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.173: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.173: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.228: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.228: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:24.250: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:24.250: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:24.341: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:24.341: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:26.927: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:26.927: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:26.984: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
STEP: listing Deployments
Jun 21 23:03:27.018: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun 21 23:03:27.071: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun 21 23:03:27.102: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:27.129: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:27.173: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:27.214: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:27.233: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:27.293: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:30.155: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:30.252: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:30.348: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:30.470: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 21 23:03:32.994: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun 21 23:03:33.135: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:33.135: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:33.136: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:33.137: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:33.137: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:33.137: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 1
Jun 21 23:03:33.137: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:33.138: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 3
Jun 21 23:03:33.138: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:33.139: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 2
Jun 21 23:03:33.139: INFO: observed Deployment test-deployment in namespace deployment-5063 with ReadyReplicas 3
STEP: deleting the Deployment
Jun 21 23:03:33.177: INFO: observed event type MODIFIED
Jun 21 23:03:33.177: INFO: observed event type MODIFIED
Jun 21 23:03:33.178: INFO: observed event type MODIFIED
Jun 21 23:03:33.180: INFO: observed event type MODIFIED
Jun 21 23:03:33.180: INFO: observed event type MODIFIED
Jun 21 23:03:33.180: INFO: observed event type MODIFIED
Jun 21 23:03:33.180: INFO: observed event type MODIFIED
Jun 21 23:03:33.181: INFO: observed event type MODIFIED
Jun 21 23:03:33.181: INFO: observed event type MODIFIED
Jun 21 23:03:33.181: INFO: observed event type MODIFIED
Jun 21 23:03:33.182: INFO: observed event type MODIFIED
Jun 21 23:03:33.182: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 21 23:03:33.193: INFO: Log out all the ReplicaSets if there is no deployment created
Jun 21 23:03:33.213: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-5063  a4611584-9d23-44c8-a981-8fc391505d01 67970 4 2022-06-21 23:03:24 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment f033ec63-77e1-4be4-8581-6551b73af431 0xc00a1950d7 0xc00a1950d8}] []  [{kube-controller-manager Update apps/v1 2022-06-21 23:03:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f033ec63-77e1-4be4-8581-6551b73af431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:03:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a195170 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 21 23:03:33.225: INFO: pod: "test-deployment-56c98d85f9-87n2s":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-87n2s test-deployment-56c98d85f9- deployment-5063  411f6184-1f86-4b4d-91e4-ff464d9781ab 67954 0 2022-06-21 23:03:27 +0000 UTC 2022-06-21 23:03:31 +0000 UTC 0xc00a195668 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:403d924219fc11199b42ffa1e043907460e3e202bc550d1ce20df733d5587594 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.246"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.246"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 a4611584-9d23-44c8-a981-8fc391505d01 0xc00a1956a7 0xc00a1956a8}] []  [{kube-controller-manager Update v1 2022-06-21 23:03:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4611584-9d23-44c8-a981-8fc391505d01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 23:03:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 23:03:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 23:03:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4cdh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4cdh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nbl8w,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.246,StartTime:2022-06-21 23:03:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 23:03:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:cri-o://c1a1bc4f1d3c12e76b3c38d95f721701c1a31dd76947590380931461ec78804f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 21 23:03:33.226: INFO: pod: "test-deployment-56c98d85f9-zrm7v":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-zrm7v test-deployment-56c98d85f9- deployment-5063  4e324ebe-679a-46b1-879d-5b46b08ce2ad 67965 0 2022-06-21 23:03:24 +0000 UTC 2022-06-21 23:03:33 +0000 UTC 0xc00a195900 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:af2e2d6d0ca6a9cba51bdd03931794aaadd89acb3311901bde188d6664d2011c cni.projectcalico.org/podIP:172.30.224.61/32 cni.projectcalico.org/podIPs:172.30.224.61/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.61"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.61"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 a4611584-9d23-44c8-a981-8fc391505d01 0xc00a195967 0xc00a195968}] []  [{kube-controller-manager Update v1 2022-06-21 23:03:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4611584-9d23-44c8-a981-8fc391505d01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 23:03:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 23:03:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 23:03:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k86h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k86h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nbl8w,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.61,StartTime:2022-06-21 23:03:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 23:03:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:cri-o://1cb94c9528e4f189ea40fb70cb209f9b7f2c5ecc537bed314a7dd2393b35a6c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 21 23:03:33.227: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-5063  45f1996a-b4e4-4caa-a0a2-66b4c59d2cc9 67813 3 2022-06-21 23:03:20 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment f033ec63-77e1-4be4-8581-6551b73af431 0xc00a1951d7 0xc00a1951d8}] []  [{kube-controller-manager Update apps/v1 2022-06-21 23:03:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f033ec63-77e1-4be4-8581-6551b73af431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:03:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a195280 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 21 23:03:33.241: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-5063  abb2c626-bdc7-49e3-b61d-9b1bc1bb4ae2 67960 2 2022-06-21 23:03:27 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment f033ec63-77e1-4be4-8581-6551b73af431 0xc00a1952f7 0xc00a1952f8}] []  [{kube-controller-manager Update apps/v1 2022-06-21 23:03:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f033ec63-77e1-4be4-8581-6551b73af431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:03:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a195390 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun 21 23:03:33.255: INFO: pod: "test-deployment-d4dfddfbf-mh9lk":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-mh9lk test-deployment-d4dfddfbf- deployment-5063  9b40d3d6-92fc-4e16-9261-12a12965d245 67915 0 2022-06-21 23:03:27 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:16a256870111c9cd2f6253a272718e0d0c5148efdfb80cc4f52f3f5b000d7c36 cni.projectcalico.org/podIP:172.30.47.247/32 cni.projectcalico.org/podIPs:172.30.47.247/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.247"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.247"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf abb2c626-bdc7-49e3-b61d-9b1bc1bb4ae2 0xc00a076ed7 0xc00a076ed8}] []  [{kube-controller-manager Update v1 2022-06-21 23:03:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abb2c626-bdc7-49e3-b61d-9b1bc1bb4ae2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 23:03:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 23:03:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 23:03:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfk85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfk85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nbl8w,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.247,StartTime:2022-06-21 23:03:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 23:03:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://6b131c21c153ad0a89054705d253532213b9b1ea6e226194b3f3f17711462027,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 21 23:03:33.256: INFO: pod: "test-deployment-d4dfddfbf-pcsw8":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-pcsw8 test-deployment-d4dfddfbf- deployment-5063  dbf1717e-1fee-48c3-a636-3ee4a846e2ec 67959 0 2022-06-21 23:03:30 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:eef718a1e801a331366d27ea71edf74d7b92040d7f5d6e1b18e1f2c964d9de1a cni.projectcalico.org/podIP:172.30.96.34/32 cni.projectcalico.org/podIPs:172.30.96.34/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.34"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.34"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf abb2c626-bdc7-49e3-b61d-9b1bc1bb4ae2 0xc00a0771f7 0xc00a0771f8}] []  [{kube-controller-manager Update v1 2022-06-21 23:03:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abb2c626-bdc7-49e3-b61d-9b1bc1bb4ae2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-21 23:03:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-21 23:03:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-21 23:03:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.96.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kd2s6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kd2s6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nbl8w,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:03:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:172.30.96.34,StartTime:2022-06-21 23:03:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-21 23:03:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://f24aaf3cbe01ce98c9572fd9bcc35ee06fd3f24138aca5d5a635a200a4bdfb04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.96.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:33.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5063" for this suite.

• [SLOW TEST:12.691 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":80,"skipped":1432,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:33.326: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:33.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1916" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":81,"skipped":1442,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:33.723: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:03:34.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:03:36.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449414, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449414, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449415, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449414, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:03:40.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:40.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5952" for this suite.
STEP: Destroying namespace "webhook-5952-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.843 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":82,"skipped":1446,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:40.566: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:03:41.397: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:03:43.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449421, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449421, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449421, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449421, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:03:46.528: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:46.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5814" for this suite.
STEP: Destroying namespace "webhook-5814-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.575 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":83,"skipped":1457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jun 21 23:03:47.490: INFO: Waiting up to 5m0s for pod "client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1" in namespace "containers-705" to be "Succeeded or Failed"
Jun 21 23:03:47.520: INFO: Pod "client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1": Phase="Pending", Reason="", readiness=false. Elapsed: 29.867827ms
Jun 21 23:03:49.545: INFO: Pod "client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055705915s
Jun 21 23:03:51.559: INFO: Pod "client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069583823s
Jun 21 23:03:53.579: INFO: Pod "client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089448604s
STEP: Saw pod success
Jun 21 23:03:53.579: INFO: Pod "client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1" satisfied condition "Succeeded or Failed"
Jun 21 23:03:53.592: INFO: Trying to get logs from node 10.10.24.214 pod client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:03:53.743: INFO: Waiting for pod client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1 to disappear
Jun 21 23:03:53.758: INFO: Pod client-containers-06dc3826-78f6-46b7-bbe0-252237d524f1 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:53.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-705" for this suite.

• [SLOW TEST:6.692 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1486,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:53.836: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:03:54.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5017" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":85,"skipped":1507,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:03:54.812: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 21 23:03:55.336: INFO: Waiting up to 5m0s for pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34" in namespace "emptydir-1506" to be "Succeeded or Failed"
Jun 21 23:03:55.356: INFO: Pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34": Phase="Pending", Reason="", readiness=false. Elapsed: 19.460618ms
Jun 21 23:03:57.383: INFO: Pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046648869s
Jun 21 23:03:59.409: INFO: Pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072959249s
Jun 21 23:04:01.437: INFO: Pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100385596s
Jun 21 23:04:03.463: INFO: Pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.126963541s
STEP: Saw pod success
Jun 21 23:04:03.464: INFO: Pod "pod-cf47242e-648c-48b1-817b-b55c831dcd34" satisfied condition "Succeeded or Failed"
Jun 21 23:04:03.478: INFO: Trying to get logs from node 10.10.24.214 pod pod-cf47242e-648c-48b1-817b-b55c831dcd34 container test-container: <nil>
STEP: delete the pod
Jun 21 23:04:03.571: INFO: Waiting for pod pod-cf47242e-648c-48b1-817b-b55c831dcd34 to disappear
Jun 21 23:04:03.589: INFO: Pod pod-cf47242e-648c-48b1-817b-b55c831dcd34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:04:03.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1506" for this suite.

• [SLOW TEST:8.823 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:04:03.648: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jun 21 23:04:06.170: INFO: running pods: 0 < 3
Jun 21 23:04:08.197: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:04:10.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8687" for this suite.

• [SLOW TEST:6.599 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":87,"skipped":1581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:04:10.248: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jun 21 23:04:10.634: INFO: created test-podtemplate-1
Jun 21 23:04:10.663: INFO: created test-podtemplate-2
Jun 21 23:04:10.689: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 21 23:04:10.722: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 21 23:04:10.822: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:04:10.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7224" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":88,"skipped":1616,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:04:10.898: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 21 23:04:11.267: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 21 23:05:11.593: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:05:11.620: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 21 23:05:16.079: INFO: found a healthy node: 10.10.24.208
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:05:30.515: INFO: pods created so far: [1 1 1]
Jun 21 23:05:30.515: INFO: length of pods created so far: 3
Jun 21 23:05:34.592: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:05:41.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9485" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:05:41.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9077" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:91.234 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":89,"skipped":1636,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:05:42.144: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-7ca3e9d6-523e-4336-ac41-9d8fe48e80b4
STEP: Creating a pod to test consume secrets
Jun 21 23:05:42.475: INFO: Waiting up to 5m0s for pod "pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593" in namespace "secrets-5843" to be "Succeeded or Failed"
Jun 21 23:05:42.488: INFO: Pod "pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593": Phase="Pending", Reason="", readiness=false. Elapsed: 12.327702ms
Jun 21 23:05:44.524: INFO: Pod "pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047475044s
Jun 21 23:05:46.547: INFO: Pod "pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070433898s
Jun 21 23:05:48.575: INFO: Pod "pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098453587s
STEP: Saw pod success
Jun 21 23:05:48.575: INFO: Pod "pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593" satisfied condition "Succeeded or Failed"
Jun 21 23:05:48.589: INFO: Trying to get logs from node 10.10.24.214 pod pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593 container secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:05:48.698: INFO: Waiting for pod pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593 to disappear
Jun 21 23:05:48.710: INFO: Pod pod-secrets-f829335b-3e61-45cf-8dec-28ecf3568593 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:05:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5843" for this suite.

• [SLOW TEST:6.633 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":90,"skipped":1640,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:05:48.778: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 21 23:05:49.258: INFO: Number of nodes with available pods: 0
Jun 21 23:05:49.258: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:05:50.301: INFO: Number of nodes with available pods: 0
Jun 21 23:05:50.301: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:05:51.304: INFO: Number of nodes with available pods: 0
Jun 21 23:05:51.304: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:05:52.307: INFO: Number of nodes with available pods: 3
Jun 21 23:05:52.308: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Jun 21 23:05:52.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70339"},"items":null}

Jun 21 23:05:52.491: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70339"},"items":[{"metadata":{"name":"daemon-set-2gh4n","generateName":"daemon-set-","namespace":"daemonsets-1488","uid":"6a60923c-dfe1-4f39-a058-5c283a260955","resourceVersion":"70338","creationTimestamp":"2022-06-21T23:05:49Z","deletionTimestamp":"2022-06-21T23:06:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"85c65f039e59d20995bd79296e0dd556b88b942ed211b11599daf89fd0f0dd0e","cni.projectcalico.org/podIP":"172.30.47.255/32","cni.projectcalico.org/podIPs":"172.30.47.255/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.47.255\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.47.255\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1631df0a-2c5d-45f9-be7f-795fc6d7d649","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1631df0a-2c5d-45f9-be7f-795fc6d7d649\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d2c2x","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d2c2x","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.24.214","securityContext":{"seLinuxOptions":{"level":"s0:c42,c9"}},"imagePullSecrets":[{"name":"default-dockercfg-tdcls"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.24.214"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:52Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:52Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:49Z"}],"hostIP":"10.10.24.214","podIP":"172.30.47.255","podIPs":[{"ip":"172.30.47.255"}],"startTime":"2022-06-21T23:05:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-21T23:05:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://d0c6697da9972f0538b556833ec7c5c7e4a28a368fada182b8ec125e12d946f6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fvsd7","generateName":"daemon-set-","namespace":"daemonsets-1488","uid":"ec41f322-4305-43a4-b12f-3ef96621cb4b","resourceVersion":"70339","creationTimestamp":"2022-06-21T23:05:49Z","deletionTimestamp":"2022-06-21T23:06:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"252d072a10e9369e030ce92afbfd44751797bf725922069e67cffc880ef45afb","cni.projectcalico.org/podIP":"172.30.96.35/32","cni.projectcalico.org/podIPs":"172.30.96.35/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.96.35\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.96.35\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1631df0a-2c5d-45f9-be7f-795fc6d7d649","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1631df0a-2c5d-45f9-be7f-795fc6d7d649\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.96.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-j48kj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-j48kj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.24.206","securityContext":{"seLinuxOptions":{"level":"s0:c42,c9"}},"imagePullSecrets":[{"name":"default-dockercfg-tdcls"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.24.206"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:49Z"}],"hostIP":"10.10.24.206","podIP":"172.30.96.35","podIPs":[{"ip":"172.30.96.35"}],"startTime":"2022-06-21T23:05:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-21T23:05:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://3c37dfe3d0861eef293cb0cdfb84a3dd1177ca50309d8c6ab4e38425fec58366","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jbjl2","generateName":"daemon-set-","namespace":"daemonsets-1488","uid":"36e62567-a2c3-4a4e-baa7-ef5042e69658","resourceVersion":"70337","creationTimestamp":"2022-06-21T23:05:49Z","deletionTimestamp":"2022-06-21T23:06:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"af0421b4adda29e8a8312ef992826b35654efea7f2aae38af5ca214e8e157b90","cni.projectcalico.org/podIP":"172.30.224.42/32","cni.projectcalico.org/podIPs":"172.30.224.42/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.224.42\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.224.42\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1631df0a-2c5d-45f9-be7f-795fc6d7d649","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1631df0a-2c5d-45f9-be7f-795fc6d7d649\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-21T23:05:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kqk6p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kqk6p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.10.24.208","securityContext":{"seLinuxOptions":{"level":"s0:c42,c9"}},"imagePullSecrets":[{"name":"default-dockercfg-tdcls"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.10.24.208"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:49Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:52Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:52Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-21T23:05:49Z"}],"hostIP":"10.10.24.208","podIP":"172.30.224.42","podIPs":[{"ip":"172.30.224.42"}],"startTime":"2022-06-21T23:05:49Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-21T23:05:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://9dcf61f4a92fdd27702a7dbfc4ec4dcef37735897ba5cf9297e20eed09938f0d","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:05:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1488" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":91,"skipped":1655,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:05:52.623: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:05:53.016: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-b4cbbca2-b86f-4a00-8069-8d2fe8d82f6a
STEP: Creating secret with name s-test-opt-upd-4567b34c-4f3f-4f2c-bfda-69144869540f
STEP: Creating the pod
Jun 21 23:05:53.137: INFO: The status of Pod pod-secrets-05c0c1e9-4372-4e96-9b08-33840cb28e6d is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:05:55.157: INFO: The status of Pod pod-secrets-05c0c1e9-4372-4e96-9b08-33840cb28e6d is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:05:57.166: INFO: The status of Pod pod-secrets-05c0c1e9-4372-4e96-9b08-33840cb28e6d is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-b4cbbca2-b86f-4a00-8069-8d2fe8d82f6a
STEP: Updating secret s-test-opt-upd-4567b34c-4f3f-4f2c-bfda-69144869540f
STEP: Creating secret with name s-test-opt-create-50266e75-a350-41a6-8880-2f298d047655
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:07:29.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4770" for this suite.

• [SLOW TEST:97.119 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1705,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:07:29.745: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:07:30.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:07:32.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449650, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449650, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449651, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449650, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:07:36.047: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:07:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6091-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:07:39.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2534" for this suite.
STEP: Destroying namespace "webhook-2534-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.009 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":93,"skipped":1714,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:07:39.754: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 21 23:07:46.388: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:07:46.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2966" for this suite.

• [SLOW TEST:6.751 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:07:46.514: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5f45e885-d332-45e3-9f73-6b155eb3f980
STEP: Creating a pod to test consume secrets
Jun 21 23:07:47.019: INFO: Waiting up to 5m0s for pod "pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e" in namespace "secrets-5714" to be "Succeeded or Failed"
Jun 21 23:07:47.034: INFO: Pod "pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.317243ms
Jun 21 23:07:49.059: INFO: Pod "pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e": Phase="Running", Reason="", readiness=true. Elapsed: 2.039530481s
Jun 21 23:07:51.082: INFO: Pod "pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e": Phase="Running", Reason="", readiness=false. Elapsed: 4.062317646s
Jun 21 23:07:53.097: INFO: Pod "pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077576491s
STEP: Saw pod success
Jun 21 23:07:53.097: INFO: Pod "pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e" satisfied condition "Succeeded or Failed"
Jun 21 23:07:53.109: INFO: Trying to get logs from node 10.10.24.214 pod pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e container secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:07:53.176: INFO: Waiting for pod pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e to disappear
Jun 21 23:07:53.188: INFO: Pod pod-secrets-65f243ab-69f4-4174-bcca-228244fbc83e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:07:53.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5714" for this suite.
STEP: Destroying namespace "secret-namespace-9802" for this suite.

• [SLOW TEST:6.742 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1742,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:07:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Jun 21 23:07:53.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 create -f -'
Jun 21 23:07:56.523: INFO: stderr: ""
Jun 21 23:07:56.523: INFO: stdout: "pod/pause created\n"
Jun 21 23:07:56.523: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 21 23:07:56.523: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-477" to be "running and ready"
Jun 21 23:07:56.535: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.302858ms
Jun 21 23:07:58.557: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033782094s
Jun 21 23:08:00.581: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.057789446s
Jun 21 23:08:00.581: INFO: Pod "pause" satisfied condition "running and ready"
Jun 21 23:08:00.581: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 21 23:08:00.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 label pods pause testing-label=testing-label-value'
Jun 21 23:08:00.840: INFO: stderr: ""
Jun 21 23:08:00.841: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 21 23:08:00.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 get pod pause -L testing-label'
Jun 21 23:08:00.981: INFO: stderr: ""
Jun 21 23:08:00.981: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 21 23:08:00.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 label pods pause testing-label-'
Jun 21 23:08:01.210: INFO: stderr: ""
Jun 21 23:08:01.210: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 21 23:08:01.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 get pod pause -L testing-label'
Jun 21 23:08:01.350: INFO: stderr: ""
Jun 21 23:08:01.350: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Jun 21 23:08:01.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 delete --grace-period=0 --force -f -'
Jun 21 23:08:01.541: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:08:01.541: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 21 23:08:01.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 get rc,svc -l name=pause --no-headers'
Jun 21 23:08:01.692: INFO: stderr: "No resources found in kubectl-477 namespace.\n"
Jun 21 23:08:01.693: INFO: stdout: ""
Jun 21 23:08:01.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-477 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 21 23:08:01.853: INFO: stderr: ""
Jun 21 23:08:01.853: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:08:01.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-477" for this suite.

• [SLOW TEST:8.647 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1316
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":96,"skipped":1753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:08:01.907: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 21 23:08:02.181: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:08:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:08:45.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3500" for this suite.

• [SLOW TEST:43.623 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":97,"skipped":1812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:08:45.532: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 21 23:08:45.916: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:08:46.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7035" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":98,"skipped":1841,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:08:46.100: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-b6dw
STEP: Creating a pod to test atomic-volume-subpath
Jun 21 23:08:46.457: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-b6dw" in namespace "subpath-521" to be "Succeeded or Failed"
Jun 21 23:08:46.475: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Pending", Reason="", readiness=false. Elapsed: 18.472244ms
Jun 21 23:08:48.495: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038195001s
Jun 21 23:08:50.513: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 4.056323015s
Jun 21 23:08:52.530: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 6.073486591s
Jun 21 23:08:54.548: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 8.091285154s
Jun 21 23:08:56.571: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 10.114295999s
Jun 21 23:08:58.590: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 12.133325409s
Jun 21 23:09:00.619: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 14.161785547s
Jun 21 23:09:02.639: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 16.182000828s
Jun 21 23:09:04.660: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 18.203138964s
Jun 21 23:09:06.681: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=true. Elapsed: 20.224196322s
Jun 21 23:09:08.698: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=false. Elapsed: 22.240983562s
Jun 21 23:09:10.715: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Running", Reason="", readiness=false. Elapsed: 24.257602182s
Jun 21 23:09:12.736: INFO: Pod "pod-subpath-test-secret-b6dw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.279111602s
STEP: Saw pod success
Jun 21 23:09:12.736: INFO: Pod "pod-subpath-test-secret-b6dw" satisfied condition "Succeeded or Failed"
Jun 21 23:09:12.756: INFO: Trying to get logs from node 10.10.24.214 pod pod-subpath-test-secret-b6dw container test-container-subpath-secret-b6dw: <nil>
STEP: delete the pod
Jun 21 23:09:12.944: INFO: Waiting for pod pod-subpath-test-secret-b6dw to disappear
Jun 21 23:09:12.967: INFO: Pod pod-subpath-test-secret-b6dw no longer exists
STEP: Deleting pod pod-subpath-test-secret-b6dw
Jun 21 23:09:12.967: INFO: Deleting pod "pod-subpath-test-secret-b6dw" in namespace "subpath-521"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:09:12.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-521" for this suite.

• [SLOW TEST:26.933 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":99,"skipped":1845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:09:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun 21 23:09:13.538: INFO: Waiting up to 5m0s for pod "downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56" in namespace "downward-api-2295" to be "Succeeded or Failed"
Jun 21 23:09:13.570: INFO: Pod "downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56": Phase="Pending", Reason="", readiness=false. Elapsed: 31.865789ms
Jun 21 23:09:15.613: INFO: Pod "downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074422289s
Jun 21 23:09:17.633: INFO: Pod "downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095064203s
Jun 21 23:09:19.649: INFO: Pod "downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.11051897s
STEP: Saw pod success
Jun 21 23:09:19.649: INFO: Pod "downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56" satisfied condition "Succeeded or Failed"
Jun 21 23:09:19.669: INFO: Trying to get logs from node 10.10.24.208 pod downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56 container dapi-container: <nil>
STEP: delete the pod
Jun 21 23:09:19.826: INFO: Waiting for pod downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56 to disappear
Jun 21 23:09:19.843: INFO: Pod downward-api-7dd43aef-0736-4f6e-b55a-132e537bcd56 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:09:19.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2295" for this suite.

• [SLOW TEST:6.854 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1880,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:09:19.891: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2128
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 21 23:09:20.104: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 21 23:09:20.409: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:09:22.441: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:24.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:26.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:28.434: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:30.429: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:32.426: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:34.430: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:36.427: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:38.425: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:09:40.428: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 21 23:09:40.467: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 21 23:09:40.538: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 21 23:09:44.927: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 21 23:09:44.927: INFO: Going to poll 172.30.96.37 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 21 23:09:44.941: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.96.37 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:09:44.941: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:09:46.272: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 21 23:09:46.272: INFO: Going to poll 172.30.224.44 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 21 23:09:46.289: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.224.44 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:09:46.289: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:09:47.583: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 21 23:09:47.583: INFO: Going to poll 172.30.47.211 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 21 23:09:47.601: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.47.211 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2128 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:09:47.601: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:09:48.947: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:09:48.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2128" for this suite.

• [SLOW TEST:29.106 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1882,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:09:48.999: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:09:55.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2774" for this suite.

• [SLOW TEST:6.633 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":102,"skipped":1885,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:09:55.642: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jun 21 23:09:55.966: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 21 23:09:55.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 create -f -'
Jun 21 23:09:57.582: INFO: stderr: ""
Jun 21 23:09:57.582: INFO: stdout: "service/agnhost-replica created\n"
Jun 21 23:09:57.582: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 21 23:09:57.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 create -f -'
Jun 21 23:09:59.633: INFO: stderr: ""
Jun 21 23:09:59.634: INFO: stdout: "service/agnhost-primary created\n"
Jun 21 23:09:59.634: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 21 23:09:59.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 create -f -'
Jun 21 23:10:01.651: INFO: stderr: ""
Jun 21 23:10:01.651: INFO: stdout: "service/frontend created\n"
Jun 21 23:10:01.651: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 21 23:10:01.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 create -f -'
Jun 21 23:10:02.240: INFO: stderr: ""
Jun 21 23:10:02.240: INFO: stdout: "deployment.apps/frontend created\n"
Jun 21 23:10:02.240: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 21 23:10:02.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 create -f -'
Jun 21 23:10:02.867: INFO: stderr: ""
Jun 21 23:10:02.867: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 21 23:10:02.867: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 21 23:10:02.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 create -f -'
Jun 21 23:10:03.443: INFO: stderr: ""
Jun 21 23:10:03.443: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 21 23:10:03.443: INFO: Waiting for all frontend pods to be Running.
Jun 21 23:10:08.495: INFO: Waiting for frontend to serve content.
Jun 21 23:10:08.539: INFO: Trying to add a new entry to the guestbook.
Jun 21 23:10:08.584: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 21 23:10:08.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 delete --grace-period=0 --force -f -'
Jun 21 23:10:08.875: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:10:08.875: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 21 23:10:08.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 delete --grace-period=0 --force -f -'
Jun 21 23:10:09.093: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:10:09.093: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 21 23:10:09.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 delete --grace-period=0 --force -f -'
Jun 21 23:10:09.284: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:10:09.284: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 21 23:10:09.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 delete --grace-period=0 --force -f -'
Jun 21 23:10:09.442: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:10:09.442: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 21 23:10:09.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 delete --grace-period=0 --force -f -'
Jun 21 23:10:09.620: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:10:09.620: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 21 23:10:09.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2259 delete --grace-period=0 --force -f -'
Jun 21 23:10:09.793: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 21 23:10:09.793: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:10:09.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2259" for this suite.

• [SLOW TEST:14.230 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":103,"skipped":1901,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:10:09.872: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1260
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jun 21 23:10:10.208: INFO: Found 0 stateful pods, waiting for 3
Jun 21 23:10:20.231: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:10:20.231: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:10:20.231: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:10:20.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1260 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:10:20.946: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:10:20.946: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:10:20.946: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jun 21 23:10:31.089: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 21 23:10:41.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1260 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:10:41.673: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:10:41.673: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:10:41.674: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:10:51.815: INFO: Waiting for StatefulSet statefulset-1260/ss2 to complete update
Jun 21 23:10:51.815: INFO: Waiting for Pod statefulset-1260/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun 21 23:10:51.815: INFO: Waiting for Pod statefulset-1260/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun 21 23:11:01.875: INFO: Waiting for StatefulSet statefulset-1260/ss2 to complete update
Jun 21 23:11:01.876: INFO: Waiting for Pod statefulset-1260/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun 21 23:11:01.876: INFO: Waiting for Pod statefulset-1260/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun 21 23:11:11.860: INFO: Waiting for StatefulSet statefulset-1260/ss2 to complete update
Jun 21 23:11:11.860: INFO: Waiting for Pod statefulset-1260/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun 21 23:11:21.880: INFO: Waiting for StatefulSet statefulset-1260/ss2 to complete update
Jun 21 23:11:31.849: INFO: Waiting for StatefulSet statefulset-1260/ss2 to complete update
STEP: Rolling back to a previous revision
Jun 21 23:11:41.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1260 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:11:42.325: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:11:42.325: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:11:42.325: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:11:52.465: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 21 23:12:02.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1260 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:12:03.060: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:12:03.060: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:12:03.060: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 23:12:23.172: INFO: Deleting all statefulset in ns statefulset-1260
Jun 21 23:12:23.187: INFO: Scaling statefulset ss2 to 0
Jun 21 23:12:33.280: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:12:33.297: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:12:33.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1260" for this suite.

• [SLOW TEST:143.543 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":104,"skipped":1911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:12:33.415: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:12:34.256: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:12:36.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449954, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449954, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449954, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791449954, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:12:39.357: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:12:39.428: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3826-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:12:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6573" for this suite.
STEP: Destroying namespace "webhook-6573-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":105,"skipped":1939,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:12:43.466: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun 21 23:12:43.832: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 21 23:13:44.100: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:13:44.126: INFO: Starting informer...
STEP: Starting pods...
Jun 21 23:13:44.425: INFO: Pod1 is running on 10.10.24.208. Tainting Node
Jun 21 23:13:46.741: INFO: Pod2 is running on 10.10.24.208. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 21 23:14:01.184: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 21 23:14:13.500: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:14:13.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7133" for this suite.

• [SLOW TEST:90.162 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":106,"skipped":1950,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:14:13.646: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:14:14.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9" in namespace "downward-api-8115" to be "Succeeded or Failed"
Jun 21 23:14:14.084: INFO: Pod "downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 59.564367ms
Jun 21 23:14:16.115: INFO: Pod "downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090992438s
Jun 21 23:14:18.133: INFO: Pod "downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108719949s
Jun 21 23:14:20.156: INFO: Pod "downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.131808974s
STEP: Saw pod success
Jun 21 23:14:20.156: INFO: Pod "downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9" satisfied condition "Succeeded or Failed"
Jun 21 23:14:20.176: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9 container client-container: <nil>
STEP: delete the pod
Jun 21 23:14:20.352: INFO: Waiting for pod downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9 to disappear
Jun 21 23:14:20.368: INFO: Pod downwardapi-volume-fbf402fb-f23a-481f-9fa9-01780a052bb9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:14:20.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8115" for this suite.

• [SLOW TEST:6.775 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":1960,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:14:20.417: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:14:21.426: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 21 23:14:21.433: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 21 23:14:21.433: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun 21 23:14:21.433: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 21 23:14:21.433: INFO: Checking APIGroup: apps
Jun 21 23:14:21.441: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 21 23:14:21.441: INFO: Versions found [{apps/v1 v1}]
Jun 21 23:14:21.441: INFO: apps/v1 matches apps/v1
Jun 21 23:14:21.441: INFO: Checking APIGroup: events.k8s.io
Jun 21 23:14:21.449: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 21 23:14:21.449: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.449: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 21 23:14:21.449: INFO: Checking APIGroup: authentication.k8s.io
Jun 21 23:14:21.455: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 21 23:14:21.455: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun 21 23:14:21.455: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 21 23:14:21.455: INFO: Checking APIGroup: authorization.k8s.io
Jun 21 23:14:21.460: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 21 23:14:21.460: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun 21 23:14:21.460: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 21 23:14:21.460: INFO: Checking APIGroup: autoscaling
Jun 21 23:14:21.471: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun 21 23:14:21.471: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 21 23:14:21.471: INFO: autoscaling/v1 matches autoscaling/v1
Jun 21 23:14:21.471: INFO: Checking APIGroup: batch
Jun 21 23:14:21.476: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 21 23:14:21.476: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 21 23:14:21.476: INFO: batch/v1 matches batch/v1
Jun 21 23:14:21.476: INFO: Checking APIGroup: certificates.k8s.io
Jun 21 23:14:21.480: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 21 23:14:21.480: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun 21 23:14:21.480: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 21 23:14:21.480: INFO: Checking APIGroup: networking.k8s.io
Jun 21 23:14:21.492: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 21 23:14:21.492: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun 21 23:14:21.493: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 21 23:14:21.493: INFO: Checking APIGroup: policy
Jun 21 23:14:21.497: INFO: PreferredVersion.GroupVersion: policy/v1
Jun 21 23:14:21.497: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jun 21 23:14:21.498: INFO: policy/v1 matches policy/v1
Jun 21 23:14:21.498: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 21 23:14:21.502: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 21 23:14:21.503: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun 21 23:14:21.503: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 21 23:14:21.503: INFO: Checking APIGroup: storage.k8s.io
Jun 21 23:14:21.509: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 21 23:14:21.509: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.509: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 21 23:14:21.509: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 21 23:14:21.517: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 21 23:14:21.523: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun 21 23:14:21.523: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 21 23:14:21.524: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 21 23:14:21.530: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 21 23:14:21.530: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun 21 23:14:21.530: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 21 23:14:21.530: INFO: Checking APIGroup: scheduling.k8s.io
Jun 21 23:14:21.536: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 21 23:14:21.536: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun 21 23:14:21.536: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 21 23:14:21.536: INFO: Checking APIGroup: coordination.k8s.io
Jun 21 23:14:21.552: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 21 23:14:21.552: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun 21 23:14:21.552: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 21 23:14:21.552: INFO: Checking APIGroup: node.k8s.io
Jun 21 23:14:21.559: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 21 23:14:21.559: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.559: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 21 23:14:21.559: INFO: Checking APIGroup: discovery.k8s.io
Jun 21 23:14:21.567: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun 21 23:14:21.567: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.567: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun 21 23:14:21.567: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 21 23:14:21.576: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jun 21 23:14:21.576: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.576: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jun 21 23:14:21.576: INFO: Checking APIGroup: apps.openshift.io
Jun 21 23:14:21.582: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jun 21 23:14:21.582: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jun 21 23:14:21.582: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jun 21 23:14:21.582: INFO: Checking APIGroup: authorization.openshift.io
Jun 21 23:14:21.590: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jun 21 23:14:21.590: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jun 21 23:14:21.590: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jun 21 23:14:21.590: INFO: Checking APIGroup: build.openshift.io
Jun 21 23:14:21.595: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jun 21 23:14:21.595: INFO: Versions found [{build.openshift.io/v1 v1}]
Jun 21 23:14:21.595: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jun 21 23:14:21.595: INFO: Checking APIGroup: image.openshift.io
Jun 21 23:14:21.601: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jun 21 23:14:21.601: INFO: Versions found [{image.openshift.io/v1 v1}]
Jun 21 23:14:21.601: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jun 21 23:14:21.601: INFO: Checking APIGroup: oauth.openshift.io
Jun 21 23:14:21.605: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jun 21 23:14:21.605: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jun 21 23:14:21.605: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jun 21 23:14:21.605: INFO: Checking APIGroup: project.openshift.io
Jun 21 23:14:21.609: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jun 21 23:14:21.609: INFO: Versions found [{project.openshift.io/v1 v1}]
Jun 21 23:14:21.609: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jun 21 23:14:21.609: INFO: Checking APIGroup: quota.openshift.io
Jun 21 23:14:21.613: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jun 21 23:14:21.613: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jun 21 23:14:21.613: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jun 21 23:14:21.613: INFO: Checking APIGroup: route.openshift.io
Jun 21 23:14:21.616: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jun 21 23:14:21.616: INFO: Versions found [{route.openshift.io/v1 v1}]
Jun 21 23:14:21.616: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jun 21 23:14:21.617: INFO: Checking APIGroup: security.openshift.io
Jun 21 23:14:21.622: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jun 21 23:14:21.622: INFO: Versions found [{security.openshift.io/v1 v1}]
Jun 21 23:14:21.622: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jun 21 23:14:21.622: INFO: Checking APIGroup: template.openshift.io
Jun 21 23:14:21.628: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jun 21 23:14:21.628: INFO: Versions found [{template.openshift.io/v1 v1}]
Jun 21 23:14:21.628: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jun 21 23:14:21.628: INFO: Checking APIGroup: user.openshift.io
Jun 21 23:14:21.633: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jun 21 23:14:21.633: INFO: Versions found [{user.openshift.io/v1 v1}]
Jun 21 23:14:21.633: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jun 21 23:14:21.633: INFO: Checking APIGroup: packages.operators.coreos.com
Jun 21 23:14:21.638: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jun 21 23:14:21.650: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jun 21 23:14:21.650: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jun 21 23:14:21.650: INFO: Checking APIGroup: config.openshift.io
Jun 21 23:14:21.667: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jun 21 23:14:21.667: INFO: Versions found [{config.openshift.io/v1 v1}]
Jun 21 23:14:21.667: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jun 21 23:14:21.667: INFO: Checking APIGroup: operator.openshift.io
Jun 21 23:14:21.672: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jun 21 23:14:21.672: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jun 21 23:14:21.672: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jun 21 23:14:21.672: INFO: Checking APIGroup: apiserver.openshift.io
Jun 21 23:14:21.679: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Jun 21 23:14:21.679: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Jun 21 23:14:21.679: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Jun 21 23:14:21.679: INFO: Checking APIGroup: cloudcredential.openshift.io
Jun 21 23:14:21.683: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jun 21 23:14:21.683: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jun 21 23:14:21.683: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jun 21 23:14:21.683: INFO: Checking APIGroup: console.openshift.io
Jun 21 23:14:21.690: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jun 21 23:14:21.690: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jun 21 23:14:21.690: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jun 21 23:14:21.690: INFO: Checking APIGroup: crd.projectcalico.org
Jun 21 23:14:21.698: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 21 23:14:21.698: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 21 23:14:21.698: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 21 23:14:21.698: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jun 21 23:14:21.704: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jun 21 23:14:21.704: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jun 21 23:14:21.704: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jun 21 23:14:21.704: INFO: Checking APIGroup: ingress.operator.openshift.io
Jun 21 23:14:21.712: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jun 21 23:14:21.712: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jun 21 23:14:21.712: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jun 21 23:14:21.712: INFO: Checking APIGroup: k8s.cni.cncf.io
Jun 21 23:14:21.717: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jun 21 23:14:21.717: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jun 21 23:14:21.718: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jun 21 23:14:21.718: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jun 21 23:14:21.722: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jun 21 23:14:21.722: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jun 21 23:14:21.722: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jun 21 23:14:21.722: INFO: Checking APIGroup: monitoring.coreos.com
Jun 21 23:14:21.734: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jun 21 23:14:21.739: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jun 21 23:14:21.739: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jun 21 23:14:21.739: INFO: Checking APIGroup: network.operator.openshift.io
Jun 21 23:14:21.748: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jun 21 23:14:21.748: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jun 21 23:14:21.749: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jun 21 23:14:21.749: INFO: Checking APIGroup: operator.tigera.io
Jun 21 23:14:21.754: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 21 23:14:21.754: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 21 23:14:21.754: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun 21 23:14:21.754: INFO: Checking APIGroup: operators.coreos.com
Jun 21 23:14:21.760: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Jun 21 23:14:21.760: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jun 21 23:14:21.760: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Jun 21 23:14:21.760: INFO: Checking APIGroup: samples.operator.openshift.io
Jun 21 23:14:21.768: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jun 21 23:14:21.768: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jun 21 23:14:21.768: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jun 21 23:14:21.768: INFO: Checking APIGroup: security.internal.openshift.io
Jun 21 23:14:21.803: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jun 21 23:14:21.803: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jun 21 23:14:21.803: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jun 21 23:14:21.803: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun 21 23:14:21.828: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jun 21 23:14:21.828: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.828: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jun 21 23:14:21.828: INFO: Checking APIGroup: tuned.openshift.io
Jun 21 23:14:21.835: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jun 21 23:14:21.835: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jun 21 23:14:21.835: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jun 21 23:14:21.835: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jun 21 23:14:21.847: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jun 21 23:14:21.858: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jun 21 23:14:21.864: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jun 21 23:14:21.864: INFO: Checking APIGroup: ibm.com
Jun 21 23:14:21.881: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jun 21 23:14:21.881: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jun 21 23:14:21.881: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jun 21 23:14:21.881: INFO: Checking APIGroup: migration.k8s.io
Jun 21 23:14:21.885: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jun 21 23:14:21.885: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jun 21 23:14:21.886: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jun 21 23:14:21.886: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jun 21 23:14:21.892: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jun 21 23:14:21.892: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jun 21 23:14:21.892: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jun 21 23:14:21.893: INFO: Checking APIGroup: helm.openshift.io
Jun 21 23:14:21.899: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jun 21 23:14:21.899: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jun 21 23:14:21.899: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jun 21 23:14:21.899: INFO: Checking APIGroup: metrics.k8s.io
Jun 21 23:14:21.904: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 21 23:14:21.904: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 21 23:14:21.904: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:14:21.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8070" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":108,"skipped":1969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:14:21.988: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8892.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8892.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8892.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8892.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8892.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8892.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 21 23:14:40.660: INFO: DNS probes using dns-8892/dns-test-133d50f7-c912-460e-ad5d-8ccd1d07a83c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:14:40.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8892" for this suite.

• [SLOW TEST:18.805 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":109,"skipped":2015,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:14:40.799: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-8599
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:14:41.182: INFO: Found 0 stateful pods, waiting for 1
Jun 21 23:14:51.202: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jun 21 23:14:51.328: INFO: Found 1 stateful pods, waiting for 2
Jun 21 23:15:01.401: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:15:01.401: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 23:15:01.550: INFO: Deleting all statefulset in ns statefulset-8599
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:15:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8599" for this suite.

• [SLOW TEST:20.926 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":110,"skipped":2034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:15:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:15:18.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3744" for this suite.

• [SLOW TEST:16.905 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":111,"skipped":2074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:15:18.665: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:15:18.966: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab" in namespace "projected-55" to be "Succeeded or Failed"
Jun 21 23:15:19.005: INFO: Pod "downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 39.080863ms
Jun 21 23:15:21.030: INFO: Pod "downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064186501s
Jun 21 23:15:23.046: INFO: Pod "downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080409624s
Jun 21 23:15:25.062: INFO: Pod "downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096550714s
STEP: Saw pod success
Jun 21 23:15:25.062: INFO: Pod "downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab" satisfied condition "Succeeded or Failed"
Jun 21 23:15:25.103: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab container client-container: <nil>
STEP: delete the pod
Jun 21 23:15:25.202: INFO: Waiting for pod downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab to disappear
Jun 21 23:15:25.248: INFO: Pod downwardapi-volume-cbea3e1f-75a8-4043-85d4-1a5c6548f2ab no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:15:25.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-55" for this suite.

• [SLOW TEST:6.641 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2125,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:15:25.306: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 21 23:15:25.588: INFO: Waiting up to 5m0s for pod "pod-3d61087f-7123-4817-9776-5853b0c83f20" in namespace "emptydir-6724" to be "Succeeded or Failed"
Jun 21 23:15:25.623: INFO: Pod "pod-3d61087f-7123-4817-9776-5853b0c83f20": Phase="Pending", Reason="", readiness=false. Elapsed: 35.112627ms
Jun 21 23:15:27.643: INFO: Pod "pod-3d61087f-7123-4817-9776-5853b0c83f20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055366926s
Jun 21 23:15:29.664: INFO: Pod "pod-3d61087f-7123-4817-9776-5853b0c83f20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075762193s
Jun 21 23:15:31.690: INFO: Pod "pod-3d61087f-7123-4817-9776-5853b0c83f20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102343574s
STEP: Saw pod success
Jun 21 23:15:31.691: INFO: Pod "pod-3d61087f-7123-4817-9776-5853b0c83f20" satisfied condition "Succeeded or Failed"
Jun 21 23:15:31.708: INFO: Trying to get logs from node 10.10.24.208 pod pod-3d61087f-7123-4817-9776-5853b0c83f20 container test-container: <nil>
STEP: delete the pod
Jun 21 23:15:31.814: INFO: Waiting for pod pod-3d61087f-7123-4817-9776-5853b0c83f20 to disappear
Jun 21 23:15:31.832: INFO: Pod pod-3d61087f-7123-4817-9776-5853b0c83f20 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:15:31.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6724" for this suite.

• [SLOW TEST:6.603 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2133,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:15:31.910: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9367
Jun 21 23:15:32.233: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:15:34.248: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun 21 23:15:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 21 23:15:34.751: INFO: rc: 7
Jun 21 23:15:34.807: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 21 23:15:34.823: INFO: Pod kube-proxy-mode-detector no longer exists
Jun 21 23:15:34.823: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-9367
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9367
I0621 23:15:34.923551      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9367, replica count: 3
I0621 23:15:37.974530      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 23:15:38.021: INFO: Creating new exec pod
Jun 21 23:15:43.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jun 21 23:15:43.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 21 23:15:43.598: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:15:43.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.97.90 80'
Jun 21 23:15:44.156: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.97.90 80\nConnection to 172.21.97.90 80 port [tcp/http] succeeded!\n"
Jun 21 23:15:44.156: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:15:44.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.214 30336'
Jun 21 23:15:44.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.214 30336\nConnection to 10.10.24.214 30336 port [tcp/*] succeeded!\n"
Jun 21 23:15:44.552: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:15:44.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 30336'
Jun 21 23:15:45.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 30336\nConnection to 10.10.24.206 30336 port [tcp/*] succeeded!\n"
Jun 21 23:15:45.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:15:45.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.10.24.206:30336/ ; done'
Jun 21 23:15:45.691: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n"
Jun 21 23:15:45.691: INFO: stdout: "\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl\naffinity-nodeport-timeout-cl8kl"
Jun 21 23:15:45.691: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.691: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.691: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Received response from host: affinity-nodeport-timeout-cl8kl
Jun 21 23:15:45.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.24.206:30336/'
Jun 21 23:15:46.141: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n"
Jun 21 23:15:46.141: INFO: stdout: "affinity-nodeport-timeout-cl8kl"
Jun 21 23:16:06.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-9367 exec execpod-affinityn5zfm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.10.24.206:30336/'
Jun 21 23:16:06.652: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.10.24.206:30336/\n"
Jun 21 23:16:06.652: INFO: stdout: "affinity-nodeport-timeout-nlvdc"
Jun 21 23:16:06.652: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9367, will wait for the garbage collector to delete the pods
Jun 21 23:16:06.823: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 30.001769ms
Jun 21 23:16:07.025: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 201.177697ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:16:11.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9367" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:39.564 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":114,"skipped":2145,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:16:11.492: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:16:12.861: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:16:14.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791450172, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791450172, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791450172, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791450172, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:16:17.983: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:16:30.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5889" for this suite.
STEP: Destroying namespace "webhook-5889-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.439 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":115,"skipped":2151,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:16:30.924: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-2839a2c3-3dc3-40c3-89cc-337fd826b027
STEP: Creating a pod to test consume configMaps
Jun 21 23:16:31.329: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e" in namespace "configmap-7245" to be "Succeeded or Failed"
Jun 21 23:16:31.349: INFO: Pod "pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.303412ms
Jun 21 23:16:33.366: INFO: Pod "pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037181609s
Jun 21 23:16:35.385: INFO: Pod "pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056074046s
Jun 21 23:16:37.401: INFO: Pod "pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071603261s
STEP: Saw pod success
Jun 21 23:16:37.401: INFO: Pod "pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e" satisfied condition "Succeeded or Failed"
Jun 21 23:16:37.420: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:16:37.529: INFO: Waiting for pod pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e to disappear
Jun 21 23:16:37.572: INFO: Pod pod-configmaps-c1bcc8cc-6180-4a9f-bfa3-1aeea8b3cf3e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:16:37.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7245" for this suite.

• [SLOW TEST:6.713 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:16:37.638: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 21 23:16:37.962: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2620  e3dda6b7-c1f2-4974-ae35-60b37a18446f 79352 0 2022-06-21 23:16:37 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-06-21 23:16:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jx7fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jx7fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-pw2vg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 21 23:16:37.994: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:16:40.024: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:16:42.012: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 21 23:16:42.012: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2620 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:16:42.012: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Verifying customized DNS server is configured on pod...
Jun 21 23:16:42.299: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2620 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:16:42.299: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:16:42.638: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:16:42.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2620" for this suite.

• [SLOW TEST:5.127 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":117,"skipped":2202,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:16:42.766: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:16:43.049: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88" in namespace "projected-5757" to be "Succeeded or Failed"
Jun 21 23:16:43.065: INFO: Pod "downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88": Phase="Pending", Reason="", readiness=false. Elapsed: 16.585452ms
Jun 21 23:16:45.103: INFO: Pod "downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053895392s
Jun 21 23:16:47.118: INFO: Pod "downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069647885s
Jun 21 23:16:49.144: INFO: Pod "downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095722692s
STEP: Saw pod success
Jun 21 23:16:49.145: INFO: Pod "downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88" satisfied condition "Succeeded or Failed"
Jun 21 23:16:49.158: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88 container client-container: <nil>
STEP: delete the pod
Jun 21 23:16:49.236: INFO: Waiting for pod downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88 to disappear
Jun 21 23:16:49.255: INFO: Pod downwardapi-volume-7b10caca-ef43-4479-acc8-d85aed63cf88 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:16:49.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5757" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2206,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:16:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:16:49.634: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 21 23:16:58.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 create -f -'
Jun 21 23:17:01.143: INFO: stderr: ""
Jun 21 23:17:01.143: INFO: stdout: "e2e-test-crd-publish-openapi-4503-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 21 23:17:01.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 delete e2e-test-crd-publish-openapi-4503-crds test-foo'
Jun 21 23:17:01.323: INFO: stderr: ""
Jun 21 23:17:01.323: INFO: stdout: "e2e-test-crd-publish-openapi-4503-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 21 23:17:01.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 apply -f -'
Jun 21 23:17:03.125: INFO: stderr: ""
Jun 21 23:17:03.125: INFO: stdout: "e2e-test-crd-publish-openapi-4503-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 21 23:17:03.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 delete e2e-test-crd-publish-openapi-4503-crds test-foo'
Jun 21 23:17:03.355: INFO: stderr: ""
Jun 21 23:17:03.355: INFO: stdout: "e2e-test-crd-publish-openapi-4503-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 21 23:17:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 create -f -'
Jun 21 23:17:03.842: INFO: rc: 1
Jun 21 23:17:03.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 apply -f -'
Jun 21 23:17:04.323: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 21 23:17:04.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 create -f -'
Jun 21 23:17:05.960: INFO: rc: 1
Jun 21 23:17:05.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 --namespace=crd-publish-openapi-7844 apply -f -'
Jun 21 23:17:06.491: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 21 23:17:06.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 explain e2e-test-crd-publish-openapi-4503-crds'
Jun 21 23:17:07.032: INFO: stderr: ""
Jun 21 23:17:07.032: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4503-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 21 23:17:07.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 explain e2e-test-crd-publish-openapi-4503-crds.metadata'
Jun 21 23:17:07.585: INFO: stderr: ""
Jun 21 23:17:07.585: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4503-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 21 23:17:07.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 explain e2e-test-crd-publish-openapi-4503-crds.spec'
Jun 21 23:17:08.099: INFO: stderr: ""
Jun 21 23:17:08.099: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4503-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 21 23:17:08.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 explain e2e-test-crd-publish-openapi-4503-crds.spec.bars'
Jun 21 23:17:08.625: INFO: stderr: ""
Jun 21 23:17:08.625: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4503-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 21 23:17:08.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7844 explain e2e-test-crd-publish-openapi-4503-crds.spec.bars2'
Jun 21 23:17:09.143: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:17:18.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7844" for this suite.

• [SLOW TEST:28.885 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":119,"skipped":2222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:17:18.221: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jun 21 23:17:18.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-8951 create -f -'
Jun 21 23:17:20.501: INFO: stderr: ""
Jun 21 23:17:20.501: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 21 23:17:21.517: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:17:21.517: INFO: Found 0 / 1
Jun 21 23:17:22.515: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:17:22.515: INFO: Found 0 / 1
Jun 21 23:17:23.519: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:17:23.519: INFO: Found 1 / 1
Jun 21 23:17:23.519: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 21 23:17:23.539: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:17:23.539: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 21 23:17:23.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-8951 patch pod agnhost-primary-nvz8m -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 21 23:17:23.726: INFO: stderr: ""
Jun 21 23:17:23.726: INFO: stdout: "pod/agnhost-primary-nvz8m patched\n"
STEP: checking annotations
Jun 21 23:17:23.745: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:17:23.745: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:17:23.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8951" for this suite.

• [SLOW TEST:5.577 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
    should add annotations for pods in rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":120,"skipped":2268,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:17:23.799: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 21 23:17:24.050: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:17:33.366: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:18:07.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7751" for this suite.

• [SLOW TEST:44.052 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":121,"skipped":2271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:18:07.865: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jun 21 23:18:08.477: INFO: Waiting up to 5m0s for pod "client-containers-da818e44-59ef-4ada-be88-e309b22f2173" in namespace "containers-7428" to be "Succeeded or Failed"
Jun 21 23:18:08.490: INFO: Pod "client-containers-da818e44-59ef-4ada-be88-e309b22f2173": Phase="Pending", Reason="", readiness=false. Elapsed: 12.969764ms
Jun 21 23:18:10.519: INFO: Pod "client-containers-da818e44-59ef-4ada-be88-e309b22f2173": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041562788s
Jun 21 23:18:12.536: INFO: Pod "client-containers-da818e44-59ef-4ada-be88-e309b22f2173": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058329856s
Jun 21 23:18:14.555: INFO: Pod "client-containers-da818e44-59ef-4ada-be88-e309b22f2173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077362009s
STEP: Saw pod success
Jun 21 23:18:14.555: INFO: Pod "client-containers-da818e44-59ef-4ada-be88-e309b22f2173" satisfied condition "Succeeded or Failed"
Jun 21 23:18:14.569: INFO: Trying to get logs from node 10.10.24.208 pod client-containers-da818e44-59ef-4ada-be88-e309b22f2173 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:18:14.687: INFO: Waiting for pod client-containers-da818e44-59ef-4ada-be88-e309b22f2173 to disappear
Jun 21 23:18:14.700: INFO: Pod client-containers-da818e44-59ef-4ada-be88-e309b22f2173 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:18:14.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7428" for this suite.

• [SLOW TEST:6.889 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2298,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:18:14.754: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:18:15.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9011" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":123,"skipped":2299,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:18:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1249
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1249
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1249
Jun 21 23:18:15.758: INFO: Found 0 stateful pods, waiting for 1
Jun 21 23:18:25.774: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 21 23:18:25.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:18:26.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:18:26.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:18:26.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:18:26.240: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 21 23:18:36.267: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:18:36.267: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:18:36.363: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999994982s
Jun 21 23:18:37.380: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.984695882s
Jun 21 23:18:38.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.967373651s
Jun 21 23:18:39.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.952924344s
Jun 21 23:18:40.426: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.937685476s
Jun 21 23:18:41.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.921377174s
Jun 21 23:18:42.472: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.891059989s
Jun 21 23:18:43.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.874703127s
Jun 21 23:18:44.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.855776301s
Jun 21 23:18:45.527: INFO: Verifying statefulset ss doesn't scale past 1 for another 838.717072ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1249
Jun 21 23:18:46.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:18:46.985: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:18:46.985: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:18:46.985: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:18:47.002: INFO: Found 1 stateful pods, waiting for 3
Jun 21 23:18:57.024: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:18:57.024: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:18:57.024: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 21 23:18:57.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:18:57.540: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:18:57.540: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:18:57.540: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:18:57.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:18:58.062: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:18:58.062: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:18:58.062: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:18:58.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:18:58.665: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:18:58.665: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:18:58.665: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:18:58.665: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:18:58.680: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 21 23:19:08.729: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:19:08.729: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:19:08.729: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:19:08.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994203s
Jun 21 23:19:09.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976610589s
Jun 21 23:19:10.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.955400277s
Jun 21 23:19:11.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.935092487s
Jun 21 23:19:12.897: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.903824482s
Jun 21 23:19:14.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.883568797s
Jun 21 23:19:15.111: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.69061976s
Jun 21 23:19:16.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.66975352s
Jun 21 23:19:17.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.6539209s
Jun 21 23:19:18.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 632.130188ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1249
Jun 21 23:19:19.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:19:19.701: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:19:19.701: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:19:19.701: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:19:19.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:19:20.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:19:20.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:19:20.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:19:20.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-1249 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:19:20.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:19:20.622: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:19:20.622: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:19:20.622: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 23:19:30.716: INFO: Deleting all statefulset in ns statefulset-1249
Jun 21 23:19:30.731: INFO: Scaling statefulset ss to 0
Jun 21 23:19:30.782: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:19:30.796: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:19:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1249" for this suite.

• [SLOW TEST:75.562 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":124,"skipped":2304,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:19:30.911: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:19:31.196: INFO: The status of Pod busybox-readonly-fs50ec2a47-8d07-4f76-950b-05791ed5c141 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:19:33.215: INFO: The status of Pod busybox-readonly-fs50ec2a47-8d07-4f76-950b-05791ed5c141 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:19:35.212: INFO: The status of Pod busybox-readonly-fs50ec2a47-8d07-4f76-950b-05791ed5c141 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:19:35.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9558" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2320,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:19:35.377: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3246
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3246
I0621 23:19:35.807222      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3246, replica count: 2
Jun 21 23:19:38.860: INFO: Creating new exec pod
I0621 23:19:38.860463      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 23:19:43.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 21 23:19:44.502: INFO: stderr: "+ echo hostName+ \nnc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 21 23:19:44.502: INFO: stdout: "externalname-service-2v4pl"
Jun 21 23:19:44.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.203.197 80'
Jun 21 23:19:44.964: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.203.197 80\nConnection to 172.21.203.197 80 port [tcp/http] succeeded!\n"
Jun 21 23:19:44.964: INFO: stdout: ""
Jun 21 23:19:45.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.203.197 80'
Jun 21 23:19:46.391: INFO: stderr: "+ nc -v -t -w 2 172.21.203.197 80\n+ echo hostName\nConnection to 172.21.203.197 80 port [tcp/http] succeeded!\n"
Jun 21 23:19:46.391: INFO: stdout: ""
Jun 21 23:19:46.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.203.197 80'
Jun 21 23:19:47.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.203.197 80\nConnection to 172.21.203.197 80 port [tcp/http] succeeded!\n"
Jun 21 23:19:47.428: INFO: stdout: ""
Jun 21 23:19:47.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.203.197 80'
Jun 21 23:19:48.435: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.203.197 80\nConnection to 172.21.203.197 80 port [tcp/http] succeeded!\n"
Jun 21 23:19:48.435: INFO: stdout: ""
Jun 21 23:19:48.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.203.197 80'
Jun 21 23:19:49.432: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.203.197 80\nConnection to 172.21.203.197 80 port [tcp/http] succeeded!\n"
Jun 21 23:19:49.432: INFO: stdout: "externalname-service-2v4pl"
Jun 21 23:19:49.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 30029'
Jun 21 23:19:50.082: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 30029\nConnection to 10.10.24.206 30029 port [tcp/*] succeeded!\n"
Jun 21 23:19:50.082: INFO: stdout: ""
Jun 21 23:19:51.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 30029'
Jun 21 23:19:51.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 30029\nConnection to 10.10.24.206 30029 port [tcp/*] succeeded!\n"
Jun 21 23:19:51.611: INFO: stdout: "externalname-service-2v4pl"
Jun 21 23:19:51.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.208 30029'
Jun 21 23:19:52.059: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.208 30029\nConnection to 10.10.24.208 30029 port [tcp/*] succeeded!\n"
Jun 21 23:19:52.059: INFO: stdout: ""
Jun 21 23:19:53.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3246 exec execpodjmtxm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.208 30029'
Jun 21 23:19:53.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.208 30029\nConnection to 10.10.24.208 30029 port [tcp/*] succeeded!\n"
Jun 21 23:19:53.604: INFO: stdout: "externalname-service-xmpgb"
Jun 21 23:19:53.604: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:19:53.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3246" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:18.389 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":126,"skipped":2333,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:19:53.767: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun 21 23:19:54.196: INFO: The status of Pod labelsupdate0d38efc4-5d77-4ec1-863f-2bea2fef3264 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:19:56.226: INFO: The status of Pod labelsupdate0d38efc4-5d77-4ec1-863f-2bea2fef3264 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:19:58.214: INFO: The status of Pod labelsupdate0d38efc4-5d77-4ec1-863f-2bea2fef3264 is Running (Ready = true)
Jun 21 23:19:58.844: INFO: Successfully updated pod "labelsupdate0d38efc4-5d77-4ec1-863f-2bea2fef3264"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:20:00.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6233" for this suite.

• [SLOW TEST:7.200 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2348,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:20:00.969: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:20:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1480" for this suite.

• [SLOW TEST:28.528 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":128,"skipped":2348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:20:29.498: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855
Jun 21 23:20:29.859: INFO: Pod name my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855: Found 0 pods out of 1
Jun 21 23:20:34.877: INFO: Pod name my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855: Found 1 pods out of 1
Jun 21 23:20:34.877: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855" are running
Jun 21 23:20:34.892: INFO: Pod "my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855-j78g4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 23:20:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 23:20:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 23:20:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-21 23:20:29 +0000 UTC Reason: Message:}])
Jun 21 23:20:34.893: INFO: Trying to dial the pod
Jun 21 23:20:39.958: INFO: Controller my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855: Got expected result from replica 1 [my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855-j78g4]: "my-hostname-basic-12a05cae-b2e5-49bc-8141-f47779b38855-j78g4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:20:39.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7604" for this suite.

• [SLOW TEST:10.510 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":129,"skipped":2370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:20:40.009: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jun 21 23:20:40.303: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jun 21 23:20:42.366: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jun 21 23:20:44.446: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:20:46.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1715" for this suite.

• [SLOW TEST:6.511 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":130,"skipped":2393,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:20:46.520: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 21 23:20:49.413: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8876 pod-service-account-e33d923a-eefc-4f60-87a8-3957473b3178 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 21 23:20:49.874: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8876 pod-service-account-e33d923a-eefc-4f60-87a8-3957473b3178 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 21 23:20:50.322: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8876 pod-service-account-e33d923a-eefc-4f60-87a8-3957473b3178 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:20:50.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8876" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":131,"skipped":2398,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:20:50.864: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:21:12.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7531" for this suite.
STEP: Destroying namespace "nsdeletetest-763" for this suite.
Jun 21 23:21:12.849: INFO: Namespace nsdeletetest-763 was already deleted
STEP: Destroying namespace "nsdeletetest-8767" for this suite.

• [SLOW TEST:22.015 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":132,"skipped":2404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:21:12.881: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:21:13.105: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Creating first CR 
Jun 21 23:21:15.794: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-21T23:21:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-21T23:21:15Z]] name:name1 resourceVersion:82599 uid:1e28742d-b8b1-4f53-9267-7c1cd38c73e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 21 23:21:25.828: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-21T23:21:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-21T23:21:25Z]] name:name2 resourceVersion:82700 uid:328ee5db-b343-41ac-a205-917251b7ab41] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 21 23:21:35.865: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-21T23:21:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-21T23:21:35Z]] name:name1 resourceVersion:82754 uid:1e28742d-b8b1-4f53-9267-7c1cd38c73e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 21 23:21:45.896: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-21T23:21:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-21T23:21:45Z]] name:name2 resourceVersion:82809 uid:328ee5db-b343-41ac-a205-917251b7ab41] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 21 23:21:55.927: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-21T23:21:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-21T23:21:35Z]] name:name1 resourceVersion:82867 uid:1e28742d-b8b1-4f53-9267-7c1cd38c73e5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 21 23:22:05.959: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-21T23:21:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-21T23:21:45Z]] name:name2 resourceVersion:82918 uid:328ee5db-b343-41ac-a205-917251b7ab41] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:22:16.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7772" for this suite.

• [SLOW TEST:63.695 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":133,"skipped":2474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:22:16.577: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:27:16.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6850" for this suite.

• [SLOW TEST:300.446 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":134,"skipped":2505,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:27:17.024: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jun 21 23:27:17.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6703 create -f -'
Jun 21 23:27:18.143: INFO: stderr: ""
Jun 21 23:27:18.143: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 21 23:27:18.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6703 diff -f -'
Jun 21 23:27:18.735: INFO: rc: 1
Jun 21 23:27:18.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6703 delete -f -'
Jun 21 23:27:18.931: INFO: stderr: ""
Jun 21 23:27:18.931: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:27:18.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6703" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":135,"skipped":2508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:27:19.020: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jun 21 23:27:19.455: INFO: Found Service test-service-4fzjc in namespace services-668 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun 21 23:27:19.455: INFO: Service test-service-4fzjc created
STEP: Getting /status
Jun 21 23:27:19.480: INFO: Service test-service-4fzjc has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jun 21 23:27:19.507: INFO: observed Service test-service-4fzjc in namespace services-668 with annotations: map[] & LoadBalancer: {[]}
Jun 21 23:27:19.508: INFO: Found Service test-service-4fzjc in namespace services-668 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun 21 23:27:19.508: INFO: Service test-service-4fzjc has service status patched
STEP: updating the ServiceStatus
Jun 21 23:27:19.540: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jun 21 23:27:19.546: INFO: Observed Service test-service-4fzjc in namespace services-668 with annotations: map[] & Conditions: {[]}
Jun 21 23:27:19.547: INFO: Observed event: &Service{ObjectMeta:{test-service-4fzjc  services-668  8dca1681-9bbd-4501-99ba-b0a2443aab41 84851 0 2022-06-21 23:27:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-06-21 23:27:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-06-21 23:27:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.13.70,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.13.70],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun 21 23:27:19.547: INFO: Found Service test-service-4fzjc in namespace services-668 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 21 23:27:19.547: INFO: Service test-service-4fzjc has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jun 21 23:27:19.585: INFO: observed Service test-service-4fzjc in namespace services-668 with labels: map[test-service-static:true]
Jun 21 23:27:19.585: INFO: observed Service test-service-4fzjc in namespace services-668 with labels: map[test-service-static:true]
Jun 21 23:27:19.586: INFO: observed Service test-service-4fzjc in namespace services-668 with labels: map[test-service-static:true]
Jun 21 23:27:19.586: INFO: Found Service test-service-4fzjc in namespace services-668 with labels: map[test-service:patched test-service-static:true]
Jun 21 23:27:19.586: INFO: Service test-service-4fzjc patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jun 21 23:27:19.650: INFO: Observed event: ADDED
Jun 21 23:27:19.650: INFO: Observed event: MODIFIED
Jun 21 23:27:19.651: INFO: Observed event: MODIFIED
Jun 21 23:27:19.651: INFO: Observed event: MODIFIED
Jun 21 23:27:19.651: INFO: Found Service test-service-4fzjc in namespace services-668 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun 21 23:27:19.660: INFO: Service test-service-4fzjc deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:27:19.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-668" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":136,"skipped":2588,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:27:19.719: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 21 23:27:26.199: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:27:26.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-646" for this suite.

• [SLOW TEST:6.599 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2589,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:27:26.319: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-1e31bee0-8fa1-4c3d-9a9f-670263f3e9f5 in namespace container-probe-4990
Jun 21 23:27:30.696: INFO: Started pod liveness-1e31bee0-8fa1-4c3d-9a9f-670263f3e9f5 in namespace container-probe-4990
STEP: checking the pod's current state and verifying that restartCount is present
Jun 21 23:27:30.714: INFO: Initial restart count of pod liveness-1e31bee0-8fa1-4c3d-9a9f-670263f3e9f5 is 0
Jun 21 23:27:48.918: INFO: Restart count of pod container-probe-4990/liveness-1e31bee0-8fa1-4c3d-9a9f-670263f3e9f5 is now 1 (18.204219187s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:27:48.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4990" for this suite.

• [SLOW TEST:22.704 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2590,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:27:49.023: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:06.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5657" for this suite.

• [SLOW TEST:17.639 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":139,"skipped":2601,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:06.666: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jun 21 23:28:07.011: INFO: created test-pod-1
Jun 21 23:28:07.108: INFO: created test-pod-2
Jun 21 23:28:07.179: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Jun 21 23:28:07.279: INFO: Pod quantity 3 is different from expected quantity 0
Jun 21 23:28:08.296: INFO: Pod quantity 3 is different from expected quantity 0
Jun 21 23:28:09.309: INFO: Pod quantity 3 is different from expected quantity 0
Jun 21 23:28:10.296: INFO: Pod quantity 3 is different from expected quantity 0
Jun 21 23:28:11.295: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:12.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7192" for this suite.

• [SLOW TEST:5.683 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":140,"skipped":2618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:12.350: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:12.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1082" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":141,"skipped":2651,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:12.665: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-cc2e0aff-8c96-4753-86aa-3ad9eead5661
STEP: Creating a pod to test consume secrets
Jun 21 23:28:13.062: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6" in namespace "projected-204" to be "Succeeded or Failed"
Jun 21 23:28:13.088: INFO: Pod "pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.412839ms
Jun 21 23:28:15.107: INFO: Pod "pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044831138s
Jun 21 23:28:17.120: INFO: Pod "pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058378734s
Jun 21 23:28:19.136: INFO: Pod "pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074309322s
STEP: Saw pod success
Jun 21 23:28:19.137: INFO: Pod "pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6" satisfied condition "Succeeded or Failed"
Jun 21 23:28:19.148: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:28:19.277: INFO: Waiting for pod pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6 to disappear
Jun 21 23:28:19.291: INFO: Pod pod-projected-secrets-e9d26290-1b85-45be-95db-a7e83d265bb6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:19.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-204" for this suite.

• [SLOW TEST:6.679 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2657,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:19.345: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-98e5e0a4-0c6f-47a6-b799-a0e3b2384fac
STEP: Creating a pod to test consume configMaps
Jun 21 23:28:19.760: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb" in namespace "projected-4593" to be "Succeeded or Failed"
Jun 21 23:28:19.776: INFO: Pod "pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.439531ms
Jun 21 23:28:21.798: INFO: Pod "pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038488731s
Jun 21 23:28:23.815: INFO: Pod "pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055581209s
Jun 21 23:28:25.833: INFO: Pod "pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073405443s
STEP: Saw pod success
Jun 21 23:28:25.833: INFO: Pod "pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb" satisfied condition "Succeeded or Failed"
Jun 21 23:28:25.846: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:28:25.924: INFO: Waiting for pod pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb to disappear
Jun 21 23:28:25.952: INFO: Pod pod-projected-configmaps-c49ce2cd-58c3-4170-9898-ef21b34a60fb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:25.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4593" for this suite.

• [SLOW TEST:6.652 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2663,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:25.998: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-42bd5e44-23f7-436a-bc32-fc8e60c1913c
STEP: Creating a pod to test consume secrets
Jun 21 23:28:26.491: INFO: Waiting up to 5m0s for pod "pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399" in namespace "secrets-3402" to be "Succeeded or Failed"
Jun 21 23:28:26.506: INFO: Pod "pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399": Phase="Pending", Reason="", readiness=false. Elapsed: 15.105049ms
Jun 21 23:28:28.521: INFO: Pod "pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030267988s
Jun 21 23:28:30.541: INFO: Pod "pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050100138s
Jun 21 23:28:32.561: INFO: Pod "pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069336654s
STEP: Saw pod success
Jun 21 23:28:32.561: INFO: Pod "pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399" satisfied condition "Succeeded or Failed"
Jun 21 23:28:32.572: INFO: Trying to get logs from node 10.10.24.208 pod pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399 container secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:28:32.640: INFO: Waiting for pod pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399 to disappear
Jun 21 23:28:32.653: INFO: Pod pod-secrets-09083554-f335-4368-86f0-0a6aeadaf399 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:32.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3402" for this suite.

• [SLOW TEST:6.728 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:32.736: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:28:32.963: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 21 23:28:33.040: INFO: The status of Pod pod-logs-websocket-b98c18a0-9cfd-4af5-a62b-337e058ddd02 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:28:35.056: INFO: The status of Pod pod-logs-websocket-b98c18a0-9cfd-4af5-a62b-337e058ddd02 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:28:37.207: INFO: The status of Pod pod-logs-websocket-b98c18a0-9cfd-4af5-a62b-337e058ddd02 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:28:37.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8317" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:28:37.360: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:09.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5280" for this suite.

• [SLOW TEST:32.596 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2745,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:09.959: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:10.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4624" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":147,"skipped":2747,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:10.317: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:10.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5378" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":148,"skipped":2755,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:10.645: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:11.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2940" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":149,"skipped":2763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:11.317: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:29:11.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612" in namespace "projected-5864" to be "Succeeded or Failed"
Jun 21 23:29:11.692: INFO: Pod "downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612": Phase="Pending", Reason="", readiness=false. Elapsed: 15.035267ms
Jun 21 23:29:13.711: INFO: Pod "downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033761689s
Jun 21 23:29:15.731: INFO: Pod "downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053466285s
Jun 21 23:29:17.751: INFO: Pod "downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073315888s
STEP: Saw pod success
Jun 21 23:29:17.751: INFO: Pod "downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612" satisfied condition "Succeeded or Failed"
Jun 21 23:29:17.771: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612 container client-container: <nil>
STEP: delete the pod
Jun 21 23:29:17.849: INFO: Waiting for pod downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612 to disappear
Jun 21 23:29:17.869: INFO: Pod downwardapi-volume-6a9476d8-330b-48f7-a53a-b04e60230612 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:17.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5864" for this suite.

• [SLOW TEST:6.603 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2806,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:17.920: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:29:18.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda" in namespace "projected-6263" to be "Succeeded or Failed"
Jun 21 23:29:18.223: INFO: Pod "downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda": Phase="Pending", Reason="", readiness=false. Elapsed: 13.505856ms
Jun 21 23:29:20.242: INFO: Pod "downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032003999s
Jun 21 23:29:22.264: INFO: Pod "downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054044184s
Jun 21 23:29:24.279: INFO: Pod "downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069699529s
STEP: Saw pod success
Jun 21 23:29:24.280: INFO: Pod "downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda" satisfied condition "Succeeded or Failed"
Jun 21 23:29:24.294: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda container client-container: <nil>
STEP: delete the pod
Jun 21 23:29:24.370: INFO: Waiting for pod downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda to disappear
Jun 21 23:29:24.388: INFO: Pod downwardapi-volume-821bab10-1f91-4792-a052-64dba14bfeda no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:24.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6263" for this suite.

• [SLOW TEST:6.519 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2808,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jun 21 23:29:24.784: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 21 23:29:29.803: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jun 21 23:29:29.821: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jun 21 23:29:29.870: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jun 21 23:29:29.878: INFO: Observed &ReplicaSet event: ADDED
Jun 21 23:29:29.879: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.879: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.880: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.880: INFO: Found replicaset test-rs in namespace replicaset-9389 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 21 23:29:29.880: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jun 21 23:29:29.880: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 21 23:29:29.912: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jun 21 23:29:29.919: INFO: Observed &ReplicaSet event: ADDED
Jun 21 23:29:29.920: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.920: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.921: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.921: INFO: Observed replicaset test-rs in namespace replicaset-9389 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 21 23:29:29.921: INFO: Observed &ReplicaSet event: MODIFIED
Jun 21 23:29:29.921: INFO: Found replicaset test-rs in namespace replicaset-9389 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun 21 23:29:29.921: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:29:29.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9389" for this suite.

• [SLOW TEST:5.543 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":152,"skipped":2825,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:29:29.984: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jun 21 23:29:30.402: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:30:19.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8154" for this suite.

• [SLOW TEST:49.528 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":153,"skipped":2825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:30:19.513: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:30:19.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a" in namespace "downward-api-4496" to be "Succeeded or Failed"
Jun 21 23:30:19.909: INFO: Pod "downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.649101ms
Jun 21 23:30:21.927: INFO: Pod "downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033647146s
Jun 21 23:30:23.943: INFO: Pod "downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050137488s
Jun 21 23:30:25.972: INFO: Pod "downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07894564s
STEP: Saw pod success
Jun 21 23:30:25.973: INFO: Pod "downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a" satisfied condition "Succeeded or Failed"
Jun 21 23:30:25.992: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a container client-container: <nil>
STEP: delete the pod
Jun 21 23:30:26.066: INFO: Waiting for pod downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a to disappear
Jun 21 23:30:26.082: INFO: Pod downwardapi-volume-5b8dca05-ae5e-43a4-b65c-c9603fa1578a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:30:26.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4496" for this suite.

• [SLOW TEST:6.640 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":2851,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:30:26.153: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-2407eb72-f4d5-4616-815b-9467c827b7a1
STEP: Creating a pod to test consume secrets
Jun 21 23:30:26.611: INFO: Waiting up to 5m0s for pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05" in namespace "secrets-6497" to be "Succeeded or Failed"
Jun 21 23:30:26.628: INFO: Pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05": Phase="Pending", Reason="", readiness=false. Elapsed: 15.65144ms
Jun 21 23:30:28.647: INFO: Pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034660655s
Jun 21 23:30:30.672: INFO: Pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060285811s
Jun 21 23:30:32.691: INFO: Pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078624503s
Jun 21 23:30:34.712: INFO: Pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.100216269s
STEP: Saw pod success
Jun 21 23:30:34.712: INFO: Pod "pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05" satisfied condition "Succeeded or Failed"
Jun 21 23:30:34.725: INFO: Trying to get logs from node 10.10.24.208 pod pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05 container secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:30:34.810: INFO: Waiting for pod pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05 to disappear
Jun 21 23:30:34.831: INFO: Pod pod-secrets-0196ce3a-69ff-4c8a-97c8-3f599d0e2b05 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:30:34.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6497" for this suite.

• [SLOW TEST:8.734 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2851,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:30:34.888: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:30:35.146: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d7b632c6-58ec-47f6-b7e0-ab115fe5b070
STEP: Creating the pod
Jun 21 23:30:35.242: INFO: The status of Pod pod-projected-configmaps-264d5139-2ec6-4ac8-9d39-60f9798bbf50 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:30:37.263: INFO: The status of Pod pod-projected-configmaps-264d5139-2ec6-4ac8-9d39-60f9798bbf50 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:30:39.303: INFO: The status of Pod pod-projected-configmaps-264d5139-2ec6-4ac8-9d39-60f9798bbf50 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-d7b632c6-58ec-47f6-b7e0-ab115fe5b070
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:31:59.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2852" for this suite.

• [SLOW TEST:84.311 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":2872,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:31:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:32:06.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9160" for this suite.

• [SLOW TEST:6.975 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":157,"skipped":2889,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:32:06.175: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jun 21 23:32:06.531: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:32:51.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8892" for this suite.

• [SLOW TEST:45.573 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":158,"skipped":2894,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:32:51.753: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 21 23:32:52.182: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 21 23:32:52.211: INFO: starting watch
STEP: patching
STEP: updating
Jun 21 23:32:52.291: INFO: waiting for watch events with expected annotations
Jun 21 23:32:52.291: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:32:52.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9671" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":159,"skipped":2905,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:32:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:32:54.936: INFO: Deleting pod "var-expansion-af7fb8ea-2841-431e-a1a4-893c60f57dcf" in namespace "var-expansion-5977"
Jun 21 23:32:54.964: INFO: Wait up to 5m0s for pod "var-expansion-af7fb8ea-2841-431e-a1a4-893c60f57dcf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:32:58.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5977" for this suite.

• [SLOW TEST:6.444 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":160,"skipped":2915,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:32:59.048: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-3065a32e-1442-400d-baa4-116e5904c780
STEP: Creating a pod to test consume configMaps
Jun 21 23:32:59.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a" in namespace "configmap-1863" to be "Succeeded or Failed"
Jun 21 23:32:59.385: INFO: Pod "pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.56864ms
Jun 21 23:33:01.403: INFO: Pod "pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028172949s
Jun 21 23:33:03.420: INFO: Pod "pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045116015s
Jun 21 23:33:05.445: INFO: Pod "pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06942146s
STEP: Saw pod success
Jun 21 23:33:05.445: INFO: Pod "pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a" satisfied condition "Succeeded or Failed"
Jun 21 23:33:05.469: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:33:05.584: INFO: Waiting for pod pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a to disappear
Jun 21 23:33:05.599: INFO: Pod pod-configmaps-08549b11-0b5a-425e-b6c2-f2f798b9a75a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:33:05.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1863" for this suite.

• [SLOW TEST:6.600 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":2915,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:33:05.650: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:33:12.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7662" for this suite.

• [SLOW TEST:7.351 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":162,"skipped":2920,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:33:13.002: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:33:24.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1368" for this suite.

• [SLOW TEST:11.869 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":163,"skipped":2922,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:33:24.871: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun 21 23:33:25.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3575 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Jun 21 23:33:25.349: INFO: stderr: ""
Jun 21 23:33:25.349: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Jun 21 23:33:25.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3575 delete pods e2e-test-httpd-pod'
Jun 21 23:33:30.029: INFO: stderr: ""
Jun 21 23:33:30.029: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:33:30.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3575" for this suite.

• [SLOW TEST:5.200 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":164,"skipped":2928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:33:30.073: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8055
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 21 23:33:30.341: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 21 23:33:30.558: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:33:32.581: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:34.582: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:36.590: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:38.576: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:40.594: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:42.579: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:44.585: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:46.574: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:48.578: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:33:50.583: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 21 23:33:50.650: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 21 23:33:50.702: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 21 23:33:54.828: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 21 23:33:54.829: INFO: Breadth first check of 172.30.96.61 on host 10.10.24.206...
Jun 21 23:33:54.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.224.30:9080/dial?request=hostname&protocol=udp&host=172.30.96.61&port=8081&tries=1'] Namespace:pod-network-test-8055 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:33:54.841: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:33:55.185: INFO: Waiting for responses: map[]
Jun 21 23:33:55.185: INFO: reached 172.30.96.61 after 0/1 tries
Jun 21 23:33:55.186: INFO: Breadth first check of 172.30.224.31 on host 10.10.24.208...
Jun 21 23:33:55.200: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.224.30:9080/dial?request=hostname&protocol=udp&host=172.30.224.31&port=8081&tries=1'] Namespace:pod-network-test-8055 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:33:55.200: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:33:55.475: INFO: Waiting for responses: map[]
Jun 21 23:33:55.475: INFO: reached 172.30.224.31 after 0/1 tries
Jun 21 23:33:55.475: INFO: Breadth first check of 172.30.47.244 on host 10.10.24.214...
Jun 21 23:33:55.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.224.30:9080/dial?request=hostname&protocol=udp&host=172.30.47.244&port=8081&tries=1'] Namespace:pod-network-test-8055 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:33:55.493: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:33:55.754: INFO: Waiting for responses: map[]
Jun 21 23:33:55.754: INFO: reached 172.30.47.244 after 0/1 tries
Jun 21 23:33:55.754: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:33:55.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8055" for this suite.

• [SLOW TEST:25.728 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":2955,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:33:55.807: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-1d31064f-166a-40eb-a11a-02be4107437e
STEP: Creating a pod to test consume secrets
Jun 21 23:33:56.100: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806" in namespace "projected-958" to be "Succeeded or Failed"
Jun 21 23:33:56.135: INFO: Pod "pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806": Phase="Pending", Reason="", readiness=false. Elapsed: 34.146537ms
Jun 21 23:33:58.152: INFO: Pod "pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051514001s
Jun 21 23:34:00.171: INFO: Pod "pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070773525s
Jun 21 23:34:02.200: INFO: Pod "pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.099560331s
STEP: Saw pod success
Jun 21 23:34:02.200: INFO: Pod "pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806" satisfied condition "Succeeded or Failed"
Jun 21 23:34:02.215: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:34:02.325: INFO: Waiting for pod pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806 to disappear
Jun 21 23:34:02.342: INFO: Pod pod-projected-secrets-54dfb4a3-ceed-4d0a-82cf-2e17f0e61806 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:34:02.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-958" for this suite.

• [SLOW TEST:6.597 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":2968,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:34:02.404: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 21 23:34:13.229: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0621 23:34:13.229318      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 21 23:34:13.229: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vknf" in namespace "gc-4334"
Jun 21 23:34:13.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2qt7" in namespace "gc-4334"
Jun 21 23:34:13.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpfhp" in namespace "gc-4334"
Jun 21 23:34:13.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8p4l" in namespace "gc-4334"
Jun 21 23:34:13.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-kd62h" in namespace "gc-4334"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:34:13.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4334" for this suite.

• [SLOW TEST:11.106 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":167,"skipped":2969,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:34:13.510: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-34231cb3-5848-4086-b32b-d09b5398df48
STEP: Creating a pod to test consume secrets
Jun 21 23:34:13.783: INFO: Waiting up to 5m0s for pod "pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23" in namespace "secrets-5349" to be "Succeeded or Failed"
Jun 21 23:34:13.793: INFO: Pod "pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23": Phase="Pending", Reason="", readiness=false. Elapsed: 10.090698ms
Jun 21 23:34:15.810: INFO: Pod "pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026392004s
Jun 21 23:34:17.821: INFO: Pod "pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037541936s
Jun 21 23:34:19.835: INFO: Pod "pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05221701s
STEP: Saw pod success
Jun 21 23:34:19.836: INFO: Pod "pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23" satisfied condition "Succeeded or Failed"
Jun 21 23:34:19.847: INFO: Trying to get logs from node 10.10.24.208 pod pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23 container secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:34:19.938: INFO: Waiting for pod pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23 to disappear
Jun 21 23:34:19.952: INFO: Pod pod-secrets-1910ff90-a7c4-47d9-8873-b9e5a4045d23 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:34:19.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5349" for this suite.

• [SLOW TEST:6.527 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":2974,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:34:20.039: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1884
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jun 21 23:34:20.441: INFO: Found 0 stateful pods, waiting for 3
Jun 21 23:34:30.465: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:34:30.465: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:34:30.465: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jun 21 23:34:30.585: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 21 23:34:40.858: INFO: Updating stateful set ss2
Jun 21 23:34:40.932: INFO: Waiting for Pod statefulset-1884/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Jun 21 23:34:51.129: INFO: Found 1 stateful pods, waiting for 3
Jun 21 23:35:01.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:35:01.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:35:01.170: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 21 23:35:01.278: INFO: Updating stateful set ss2
Jun 21 23:35:01.311: INFO: Waiting for Pod statefulset-1884/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun 21 23:35:11.404: INFO: Updating stateful set ss2
Jun 21 23:35:11.456: INFO: Waiting for StatefulSet statefulset-1884/ss2 to complete update
Jun 21 23:35:11.456: INFO: Waiting for Pod statefulset-1884/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 23:35:21.509: INFO: Deleting all statefulset in ns statefulset-1884
Jun 21 23:35:21.525: INFO: Scaling statefulset ss2 to 0
Jun 21 23:35:31.635: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:35:31.652: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:35:31.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1884" for this suite.

• [SLOW TEST:71.731 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":169,"skipped":2981,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:35:31.770: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-rrtp
STEP: Creating a pod to test atomic-volume-subpath
Jun 21 23:35:32.163: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rrtp" in namespace "subpath-7072" to be "Succeeded or Failed"
Jun 21 23:35:32.178: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Pending", Reason="", readiness=false. Elapsed: 14.650322ms
Jun 21 23:35:34.198: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034650349s
Jun 21 23:35:36.221: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 4.057950176s
Jun 21 23:35:38.238: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 6.074444748s
Jun 21 23:35:40.259: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 8.095881403s
Jun 21 23:35:42.275: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 10.111889737s
Jun 21 23:35:44.298: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 12.134890418s
Jun 21 23:35:46.323: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 14.159841991s
Jun 21 23:35:48.349: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 16.18556784s
Jun 21 23:35:50.375: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 18.212046958s
Jun 21 23:35:52.395: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 20.231298115s
Jun 21 23:35:54.417: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=true. Elapsed: 22.253455994s
Jun 21 23:35:56.444: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Running", Reason="", readiness=false. Elapsed: 24.281091039s
Jun 21 23:35:58.461: INFO: Pod "pod-subpath-test-projected-rrtp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.298154818s
STEP: Saw pod success
Jun 21 23:35:58.462: INFO: Pod "pod-subpath-test-projected-rrtp" satisfied condition "Succeeded or Failed"
Jun 21 23:35:58.479: INFO: Trying to get logs from node 10.10.24.208 pod pod-subpath-test-projected-rrtp container test-container-subpath-projected-rrtp: <nil>
STEP: delete the pod
Jun 21 23:35:58.622: INFO: Waiting for pod pod-subpath-test-projected-rrtp to disappear
Jun 21 23:35:58.639: INFO: Pod pod-subpath-test-projected-rrtp no longer exists
STEP: Deleting pod pod-subpath-test-projected-rrtp
Jun 21 23:35:58.639: INFO: Deleting pod "pod-subpath-test-projected-rrtp" in namespace "subpath-7072"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:35:58.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7072" for this suite.

• [SLOW TEST:26.926 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":170,"skipped":2990,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:35:58.697: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0621 23:36:00.164349      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 21 23:36:00.164: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:36:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1093" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":171,"skipped":3000,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:36:00.212: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:36:00.601: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-67f21c5c-1920-4520-b646-257a6b9f6a36
STEP: Creating the pod
Jun 21 23:36:00.706: INFO: The status of Pod pod-configmaps-67c72ec1-0896-476a-aae1-f8178f342a23 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:36:02.748: INFO: The status of Pod pod-configmaps-67c72ec1-0896-476a-aae1-f8178f342a23 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:36:04.726: INFO: The status of Pod pod-configmaps-67c72ec1-0896-476a-aae1-f8178f342a23 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-67f21c5c-1920-4520-b646-257a6b9f6a36
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:37:36.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4169" for this suite.

• [SLOW TEST:96.423 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3016,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:37:36.636: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 21 23:37:37.088: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5544  f0fc9d42-77e0-44eb-a406-40c50ffd694e 92195 0 2022-06-21 23:37:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-21 23:37:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 21 23:37:37.089: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5544  f0fc9d42-77e0-44eb-a406-40c50ffd694e 92200 0 2022-06-21 23:37:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-21 23:37:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 21 23:37:37.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5544  f0fc9d42-77e0-44eb-a406-40c50ffd694e 92204 0 2022-06-21 23:37:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-21 23:37:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 21 23:37:47.301: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5544  f0fc9d42-77e0-44eb-a406-40c50ffd694e 92284 0 2022-06-21 23:37:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-21 23:37:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 21 23:37:47.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5544  f0fc9d42-77e0-44eb-a406-40c50ffd694e 92285 0 2022-06-21 23:37:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-21 23:37:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 21 23:37:47.302: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5544  f0fc9d42-77e0-44eb-a406-40c50ffd694e 92286 0 2022-06-21 23:37:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-21 23:37:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:37:47.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5544" for this suite.

• [SLOW TEST:10.755 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":173,"skipped":3024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:37:47.392: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jun 21 23:37:47.664: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun 21 23:37:52.687: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jun 21 23:37:52.708: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:37:52.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7939" for this suite.

• [SLOW TEST:5.513 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":174,"skipped":3059,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:37:52.920: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:37:53.294: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 21 23:37:53.367: INFO: Number of nodes with available pods: 0
Jun 21 23:37:53.367: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:37:54.405: INFO: Number of nodes with available pods: 0
Jun 21 23:37:54.405: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:37:55.414: INFO: Number of nodes with available pods: 0
Jun 21 23:37:55.414: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:37:56.424: INFO: Number of nodes with available pods: 3
Jun 21 23:37:56.424: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 21 23:37:56.610: INFO: Wrong image for pod: daemon-set-g6mxr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:56.610: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:56.610: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:57.656: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:57.656: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:58.679: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:58.679: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:59.653: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:37:59.653: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:00.657: INFO: Pod daemon-set-f8ml9 is not available
Jun 21 23:38:00.657: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:00.657: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:01.656: INFO: Pod daemon-set-f8ml9 is not available
Jun 21 23:38:01.656: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:01.656: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:02.656: INFO: Pod daemon-set-f8ml9 is not available
Jun 21 23:38:02.656: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:02.656: INFO: Wrong image for pod: daemon-set-ljjtj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:03.654: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:04.656: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:05.671: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:05.671: INFO: Pod daemon-set-p7s9g is not available
Jun 21 23:38:06.653: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:06.653: INFO: Pod daemon-set-p7s9g is not available
Jun 21 23:38:07.657: INFO: Wrong image for pod: daemon-set-hdkfq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun 21 23:38:07.657: INFO: Pod daemon-set-p7s9g is not available
Jun 21 23:38:10.663: INFO: Pod daemon-set-zm7nt is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 21 23:38:10.722: INFO: Number of nodes with available pods: 2
Jun 21 23:38:10.722: INFO: Node 10.10.24.214 is running more than one daemon pod
Jun 21 23:38:11.763: INFO: Number of nodes with available pods: 2
Jun 21 23:38:11.763: INFO: Node 10.10.24.214 is running more than one daemon pod
Jun 21 23:38:12.769: INFO: Number of nodes with available pods: 2
Jun 21 23:38:12.769: INFO: Node 10.10.24.214 is running more than one daemon pod
Jun 21 23:38:13.814: INFO: Number of nodes with available pods: 3
Jun 21 23:38:13.814: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4945, will wait for the garbage collector to delete the pods
Jun 21 23:38:14.004: INFO: Deleting DaemonSet.extensions daemon-set took: 34.167868ms
Jun 21 23:38:14.104: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.119626ms
Jun 21 23:38:17.248: INFO: Number of nodes with available pods: 0
Jun 21 23:38:17.248: INFO: Number of running nodes: 0, number of available pods: 0
Jun 21 23:38:17.273: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"92811"},"items":null}

Jun 21 23:38:17.287: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"92811"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:17.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4945" for this suite.

• [SLOW TEST:24.541 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":175,"skipped":3076,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:17.459: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:22.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5848" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3133,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:22.107: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:22.376: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-5815
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:26.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9831" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:27.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5815" for this suite.

• [SLOW TEST:5.027 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":177,"skipped":3143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:38:28.190: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:38:30.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451508, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451508, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451508, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451508, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:38:33.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:38:33.391: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7352-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:36.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3111" for this suite.
STEP: Destroying namespace "webhook-3111-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.996 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":178,"skipped":3171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-459406a5-3cb6-4833-9abe-b8006a090bc6
STEP: Creating a pod to test consume secrets
Jun 21 23:38:37.580: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e" in namespace "projected-408" to be "Succeeded or Failed"
Jun 21 23:38:37.593: INFO: Pod "pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.835953ms
Jun 21 23:38:39.619: INFO: Pod "pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039160801s
Jun 21 23:38:41.652: INFO: Pod "pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072693344s
Jun 21 23:38:43.666: INFO: Pod "pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086576282s
STEP: Saw pod success
Jun 21 23:38:43.666: INFO: Pod "pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e" satisfied condition "Succeeded or Failed"
Jun 21 23:38:43.689: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 21 23:38:43.782: INFO: Waiting for pod pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e to disappear
Jun 21 23:38:43.803: INFO: Pod pod-projected-secrets-e427f478-354c-439d-844f-fecfce85e45e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:43.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-408" for this suite.

• [SLOW TEST:6.725 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:43.864: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:38:44.159: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e" in namespace "security-context-test-4672" to be "Succeeded or Failed"
Jun 21 23:38:44.174: INFO: Pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.879019ms
Jun 21 23:38:46.197: INFO: Pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037932559s
Jun 21 23:38:48.219: INFO: Pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060563228s
Jun 21 23:38:50.242: INFO: Pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082837383s
Jun 21 23:38:50.242: INFO: Pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e" satisfied condition "Succeeded or Failed"
Jun 21 23:38:50.272: INFO: Got logs for pod "busybox-privileged-false-232ea753-7d83-4f38-b351-f1694c53db2e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:50.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4672" for this suite.

• [SLOW TEST:6.475 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:232
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":180,"skipped":3246,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:50.340: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 21 23:38:50.689: INFO: Waiting up to 5m0s for pod "pod-02ee3468-289e-436d-ae24-f90e20579f75" in namespace "emptydir-6936" to be "Succeeded or Failed"
Jun 21 23:38:50.707: INFO: Pod "pod-02ee3468-289e-436d-ae24-f90e20579f75": Phase="Pending", Reason="", readiness=false. Elapsed: 17.188998ms
Jun 21 23:38:52.738: INFO: Pod "pod-02ee3468-289e-436d-ae24-f90e20579f75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04806193s
Jun 21 23:38:54.761: INFO: Pod "pod-02ee3468-289e-436d-ae24-f90e20579f75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071492937s
Jun 21 23:38:56.785: INFO: Pod "pod-02ee3468-289e-436d-ae24-f90e20579f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095759094s
STEP: Saw pod success
Jun 21 23:38:56.785: INFO: Pod "pod-02ee3468-289e-436d-ae24-f90e20579f75" satisfied condition "Succeeded or Failed"
Jun 21 23:38:56.795: INFO: Trying to get logs from node 10.10.24.208 pod pod-02ee3468-289e-436d-ae24-f90e20579f75 container test-container: <nil>
STEP: delete the pod
Jun 21 23:38:56.876: INFO: Waiting for pod pod-02ee3468-289e-436d-ae24-f90e20579f75 to disappear
Jun 21 23:38:56.890: INFO: Pod pod-02ee3468-289e-436d-ae24-f90e20579f75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:38:56.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6936" for this suite.

• [SLOW TEST:6.602 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3255,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:38:56.948: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:38:57.263: INFO: The status of Pod busybox-scheduling-fbe89099-484e-43b4-8d37-95aeea31d910 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:38:59.300: INFO: The status of Pod busybox-scheduling-fbe89099-484e-43b4-8d37-95aeea31d910 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:39:01.281: INFO: The status of Pod busybox-scheduling-fbe89099-484e-43b4-8d37-95aeea31d910 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:39:01.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3016" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3264,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:39:01.408: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun 21 23:39:01.763: INFO: Waiting up to 5m0s for pod "downward-api-6ece274a-42c5-4083-a0af-e02018202e55" in namespace "downward-api-2167" to be "Succeeded or Failed"
Jun 21 23:39:01.819: INFO: Pod "downward-api-6ece274a-42c5-4083-a0af-e02018202e55": Phase="Pending", Reason="", readiness=false. Elapsed: 55.458728ms
Jun 21 23:39:03.833: INFO: Pod "downward-api-6ece274a-42c5-4083-a0af-e02018202e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069937761s
Jun 21 23:39:05.849: INFO: Pod "downward-api-6ece274a-42c5-4083-a0af-e02018202e55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085936166s
Jun 21 23:39:07.865: INFO: Pod "downward-api-6ece274a-42c5-4083-a0af-e02018202e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101552974s
STEP: Saw pod success
Jun 21 23:39:07.865: INFO: Pod "downward-api-6ece274a-42c5-4083-a0af-e02018202e55" satisfied condition "Succeeded or Failed"
Jun 21 23:39:07.876: INFO: Trying to get logs from node 10.10.24.208 pod downward-api-6ece274a-42c5-4083-a0af-e02018202e55 container dapi-container: <nil>
STEP: delete the pod
Jun 21 23:39:08.040: INFO: Waiting for pod downward-api-6ece274a-42c5-4083-a0af-e02018202e55 to disappear
Jun 21 23:39:08.053: INFO: Pod downward-api-6ece274a-42c5-4083-a0af-e02018202e55 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:39:08.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2167" for this suite.

• [SLOW TEST:6.692 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:39:08.109: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:39:08.536: INFO: The status of Pod server-envvars-c4f1932b-b83a-458c-95ab-6c51cac072e9 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:39:10.564: INFO: The status of Pod server-envvars-c4f1932b-b83a-458c-95ab-6c51cac072e9 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:39:12.565: INFO: The status of Pod server-envvars-c4f1932b-b83a-458c-95ab-6c51cac072e9 is Running (Ready = true)
Jun 21 23:39:12.736: INFO: Waiting up to 5m0s for pod "client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a" in namespace "pods-282" to be "Succeeded or Failed"
Jun 21 23:39:12.751: INFO: Pod "client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.787004ms
Jun 21 23:39:14.770: INFO: Pod "client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034263999s
Jun 21 23:39:16.796: INFO: Pod "client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060282041s
Jun 21 23:39:18.824: INFO: Pod "client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087715438s
STEP: Saw pod success
Jun 21 23:39:18.824: INFO: Pod "client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a" satisfied condition "Succeeded or Failed"
Jun 21 23:39:18.841: INFO: Trying to get logs from node 10.10.24.208 pod client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a container env3cont: <nil>
STEP: delete the pod
Jun 21 23:39:18.935: INFO: Waiting for pod client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a to disappear
Jun 21 23:39:18.950: INFO: Pod client-envvars-5ed135d5-4bdb-4c16-99e9-a853324a479a no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:39:18.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-282" for this suite.

• [SLOW TEST:10.896 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3300,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:39:19.021: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6566
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-6566
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6566
Jun 21 23:39:19.500: INFO: Found 0 stateful pods, waiting for 1
Jun 21 23:39:29.524: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 21 23:39:29.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:39:30.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:39:30.440: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:39:30.440: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:39:30.456: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 21 23:39:40.503: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:39:40.503: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:39:40.651: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 21 23:39:40.651: INFO: ss-0  10.10.24.208  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  }]
Jun 21 23:39:40.651: INFO: 
Jun 21 23:39:40.651: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 21 23:39:41.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981402356s
Jun 21 23:39:42.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.960228879s
Jun 21 23:39:43.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.944564889s
Jun 21 23:39:44.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.929238895s
Jun 21 23:39:45.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.91577733s
Jun 21 23:39:46.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.883832307s
Jun 21 23:39:47.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.846235475s
Jun 21 23:39:48.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.823429845s
Jun 21 23:39:49.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 807.841636ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6566
Jun 21 23:39:50.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:39:51.297: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 21 23:39:51.297: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:39:51.297: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:39:51.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:39:51.780: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 21 23:39:51.780: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:39:51.780: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:39:51.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 21 23:39:52.292: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 21 23:39:52.292: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 21 23:39:52.292: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 21 23:39:52.315: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:39:52.315: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 21 23:39:52.315: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 21 23:39:52.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:39:52.797: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:39:52.797: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:39:52.797: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:39:52.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:39:53.245: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:39:53.245: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:39:53.245: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:39:53.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=statefulset-6566 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 21 23:39:53.674: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 21 23:39:53.674: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 21 23:39:53.674: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 21 23:39:53.674: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:39:53.693: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 21 23:40:03.747: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:40:03.747: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:40:03.747: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 21 23:40:03.825: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 21 23:40:03.825: INFO: ss-0  10.10.24.208  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  }]
Jun 21 23:40:03.825: INFO: ss-1  10.10.24.206  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  }]
Jun 21 23:40:03.825: INFO: ss-2  10.10.24.214  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  }]
Jun 21 23:40:03.825: INFO: 
Jun 21 23:40:03.825: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 21 23:40:04.878: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 21 23:40:04.878: INFO: ss-0  10.10.24.208  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  }]
Jun 21 23:40:04.878: INFO: ss-1  10.10.24.206  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  }]
Jun 21 23:40:04.878: INFO: ss-2  10.10.24.214  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  }]
Jun 21 23:40:04.878: INFO: 
Jun 21 23:40:04.878: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 21 23:40:05.900: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jun 21 23:40:05.900: INFO: ss-0  10.10.24.208  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:19 +0000 UTC  }]
Jun 21 23:40:05.900: INFO: ss-1  10.10.24.206  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  }]
Jun 21 23:40:05.900: INFO: ss-2  10.10.24.214  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:39:40 +0000 UTC  }]
Jun 21 23:40:05.901: INFO: 
Jun 21 23:40:05.901: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 21 23:40:06.920: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.908579001s
Jun 21 23:40:07.934: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.889512291s
Jun 21 23:40:08.954: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.87603885s
Jun 21 23:40:09.975: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.855600012s
Jun 21 23:40:11.000: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.834759728s
Jun 21 23:40:12.027: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.810528839s
Jun 21 23:40:13.050: INFO: Verifying statefulset ss doesn't scale past 0 for another 782.229857ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6566
Jun 21 23:40:14.066: INFO: Scaling statefulset ss to 0
Jun 21 23:40:14.135: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 23:40:14.152: INFO: Deleting all statefulset in ns statefulset-6566
Jun 21 23:40:14.169: INFO: Scaling statefulset ss to 0
Jun 21 23:40:14.232: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:40:14.251: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:14.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6566" for this suite.

• [SLOW TEST:55.364 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":185,"skipped":3320,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:14.391: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun 21 23:40:14.686: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:16.702: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:18.706: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun 21 23:40:18.795: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:20.810: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:22.834: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 21 23:40:22.906: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 21 23:40:22.919: INFO: Pod pod-with-poststart-http-hook still exists
Jun 21 23:40:24.920: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 21 23:40:24.940: INFO: Pod pod-with-poststart-http-hook still exists
Jun 21 23:40:26.919: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 21 23:40:26.939: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:26.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-239" for this suite.

• [SLOW TEST:12.603 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3341,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:26.999: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6024/configmap-test-3bba5854-05a0-45d9-9e2a-00f574afb02d
STEP: Creating a pod to test consume configMaps
Jun 21 23:40:27.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c" in namespace "configmap-6024" to be "Succeeded or Failed"
Jun 21 23:40:27.418: INFO: Pod "pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.873152ms
Jun 21 23:40:29.437: INFO: Pod "pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030843943s
Jun 21 23:40:31.455: INFO: Pod "pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048464456s
Jun 21 23:40:33.483: INFO: Pod "pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076389777s
STEP: Saw pod success
Jun 21 23:40:33.483: INFO: Pod "pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c" satisfied condition "Succeeded or Failed"
Jun 21 23:40:33.495: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c container env-test: <nil>
STEP: delete the pod
Jun 21 23:40:33.573: INFO: Waiting for pod pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c to disappear
Jun 21 23:40:33.599: INFO: Pod pod-configmaps-2479f6ea-7586-45c9-af7c-e2773db9b70c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:33.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6024" for this suite.

• [SLOW TEST:6.667 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3411,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:33.670: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:40:33.979: INFO: The status of Pod busybox-host-aliases6b4bf52c-35cd-48e8-9aa7-fd004c8ed0a9 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:35.994: INFO: The status of Pod busybox-host-aliases6b4bf52c-35cd-48e8-9aa7-fd004c8ed0a9 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:37.994: INFO: The status of Pod busybox-host-aliases6b4bf52c-35cd-48e8-9aa7-fd004c8ed0a9 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:38.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5147" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3428,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:40:38.386: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a9034664-682d-45f7-b39f-5abae72420ae" in namespace "security-context-test-5085" to be "Succeeded or Failed"
Jun 21 23:40:38.403: INFO: Pod "busybox-user-65534-a9034664-682d-45f7-b39f-5abae72420ae": Phase="Pending", Reason="", readiness=false. Elapsed: 16.313239ms
Jun 21 23:40:40.421: INFO: Pod "busybox-user-65534-a9034664-682d-45f7-b39f-5abae72420ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035128274s
Jun 21 23:40:42.438: INFO: Pod "busybox-user-65534-a9034664-682d-45f7-b39f-5abae72420ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051583785s
Jun 21 23:40:44.464: INFO: Pod "busybox-user-65534-a9034664-682d-45f7-b39f-5abae72420ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077450403s
Jun 21 23:40:44.464: INFO: Pod "busybox-user-65534-a9034664-682d-45f7-b39f-5abae72420ae" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:44.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5085" for this suite.

• [SLOW TEST:6.435 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3448,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:44.522: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jun 21 23:40:44.755: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-7464 proxy --unix-socket=/tmp/kubectl-proxy-unix547806988/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:44.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7464" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":190,"skipped":3458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-aadb7585-d675-408b-9c26-6104d0569f79
STEP: Creating a pod to test consume configMaps
Jun 21 23:40:45.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d" in namespace "projected-2036" to be "Succeeded or Failed"
Jun 21 23:40:45.217: INFO: Pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.147217ms
Jun 21 23:40:47.237: INFO: Pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032585311s
Jun 21 23:40:49.271: INFO: Pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066117149s
Jun 21 23:40:51.291: INFO: Pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086036403s
Jun 21 23:40:53.311: INFO: Pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.106624563s
STEP: Saw pod success
Jun 21 23:40:53.311: INFO: Pod "pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d" satisfied condition "Succeeded or Failed"
Jun 21 23:40:53.326: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:40:53.402: INFO: Waiting for pod pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d to disappear
Jun 21 23:40:53.416: INFO: Pod pod-projected-configmaps-0f3f90cb-ed8a-470f-b503-5d705d7b530d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:40:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2036" for this suite.

• [SLOW TEST:8.570 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:40:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:40:53.819: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:55.838: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:40:57.838: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:40:59.847: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:01.844: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:03.836: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:05.839: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:07.846: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:09.842: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:11.857: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:13.834: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = false)
Jun 21 23:41:15.832: INFO: The status of Pod test-webserver-1e138507-d21e-439c-b108-5fc1543eaa71 is Running (Ready = true)
Jun 21 23:41:15.854: INFO: Container started at 2022-06-21 23:40:55 +0000 UTC, pod became ready at 2022-06-21 23:41:13 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:41:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5907" for this suite.

• [SLOW TEST:22.427 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:41:15.925: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:41:16.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4232" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":193,"skipped":3540,"failed":0}

------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:41:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:00.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6360" for this suite.

• [SLOW TEST:104.529 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":194,"skipped":3540,"failed":0}
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:00.846: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jun 21 23:43:01.329: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:43:03.357: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:43:05.353: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jun 21 23:43:05.434: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:43:07.451: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 21 23:43:07.461: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:07.461: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:07.779: INFO: Exec stderr: ""
Jun 21 23:43:07.779: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:07.779: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:08.005: INFO: Exec stderr: ""
Jun 21 23:43:08.005: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:08.005: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:08.281: INFO: Exec stderr: ""
Jun 21 23:43:08.281: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:08.281: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:08.586: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 21 23:43:08.586: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:08.586: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:08.864: INFO: Exec stderr: ""
Jun 21 23:43:08.864: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:08.864: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:09.139: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 21 23:43:09.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:09.139: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:09.426: INFO: Exec stderr: ""
Jun 21 23:43:09.427: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:09.427: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:09.647: INFO: Exec stderr: ""
Jun 21 23:43:09.648: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:09.648: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:09.904: INFO: Exec stderr: ""
Jun 21 23:43:09.904: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2587 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:43:09.904: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:43:10.183: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:10.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2587" for this suite.

• [SLOW TEST:9.399 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3540,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:10.245: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:43:11.561: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 21 23:43:13.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451791, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451791, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451791, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451791, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:43:16.692: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:17.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8195" for this suite.
STEP: Destroying namespace "webhook-8195-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":196,"skipped":3542,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:17.489: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-6c90b38a-71d1-4a30-9c37-e06f93bcc8e6
STEP: Creating a pod to test consume configMaps
Jun 21 23:43:17.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8" in namespace "configmap-2482" to be "Succeeded or Failed"
Jun 21 23:43:18.005: INFO: Pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8": Phase="Pending", Reason="", readiness=false. Elapsed: 55.027702ms
Jun 21 23:43:20.022: INFO: Pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071945783s
Jun 21 23:43:22.056: INFO: Pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8": Phase="Running", Reason="", readiness=false. Elapsed: 4.105807623s
Jun 21 23:43:24.080: INFO: Pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8": Phase="Running", Reason="", readiness=false. Elapsed: 6.130031112s
Jun 21 23:43:26.096: INFO: Pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.145935415s
STEP: Saw pod success
Jun 21 23:43:26.096: INFO: Pod "pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8" satisfied condition "Succeeded or Failed"
Jun 21 23:43:26.108: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:43:26.208: INFO: Waiting for pod pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8 to disappear
Jun 21 23:43:26.222: INFO: Pod pod-configmaps-e5b7d111-7156-48b4-aea7-f9e18fbf22b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:26.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2482" for this suite.

• [SLOW TEST:8.812 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3551,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:26.307: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:43:26.580: INFO: Creating deployment "test-recreate-deployment"
Jun 21 23:43:26.601: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 21 23:43:26.658: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 21 23:43:28.690: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 21 23:43:28.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451806, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451806, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451806, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451806, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 21 23:43:30.727: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 21 23:43:30.783: INFO: Updating deployment test-recreate-deployment
Jun 21 23:43:30.783: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 21 23:43:31.081: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3929  2bf3b330-a2d3-4728-a872-3a9b8d8e985f 97055 2 2022-06-21 23:43:26 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-21 23:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:43:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000cdf2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-21 23:43:30 +0000 UTC,LastTransitionTime:2022-06-21 23:43:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-06-21 23:43:31 +0000 UTC,LastTransitionTime:2022-06-21 23:43:26 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 21 23:43:31.103: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-3929  88bca0d4-573a-4edd-8975-e5bc8c62588d 97053 1 2022-06-21 23:43:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2bf3b330-a2d3-4728-a872-3a9b8d8e985f 0xc0034d97b0 0xc0034d97b1}] []  [{kube-controller-manager Update apps/v1 2022-06-21 23:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bf3b330-a2d3-4728-a872-3a9b8d8e985f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:43:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034d9848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 21 23:43:31.103: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 21 23:43:31.104: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-3929  d2bf1e7d-b518-44a7-84b7-694f99db1f9b 97042 2 2022-06-21 23:43:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2bf3b330-a2d3-4728-a872-3a9b8d8e985f 0xc0034d9697 0xc0034d9698}] []  [{kube-controller-manager Update apps/v1 2022-06-21 23:43:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bf3b330-a2d3-4728-a872-3a9b8d8e985f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-21 23:43:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034d9748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 21 23:43:31.118: INFO: Pod "test-recreate-deployment-85d47dcb4-jp5kc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-jp5kc test-recreate-deployment-85d47dcb4- deployment-3929  b35dd6c5-ae1c-4b5b-873e-1ec4adf085b1 97054 0 2022-06-21 23:43:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 88bca0d4-573a-4edd-8975-e5bc8c62588d 0xc000cdf6b7 0xc000cdf6b8}] []  [{kube-controller-manager Update v1 2022-06-21 23:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88bca0d4-573a-4edd-8975-e5bc8c62588d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-21 23:43:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qvxg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qvxg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-bq9dz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:43:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:43:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-21 23:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-21 23:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:31.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3929" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":198,"skipped":3562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:31.180: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-9759/configmap-test-01c20c40-6d4e-4cdc-a2cb-4d4df4872948
STEP: Creating a pod to test consume configMaps
Jun 21 23:43:31.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e" in namespace "configmap-9759" to be "Succeeded or Failed"
Jun 21 23:43:31.531: INFO: Pod "pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.688258ms
Jun 21 23:43:33.558: INFO: Pod "pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039483279s
Jun 21 23:43:35.571: INFO: Pod "pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053140236s
Jun 21 23:43:37.589: INFO: Pod "pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071010807s
STEP: Saw pod success
Jun 21 23:43:37.590: INFO: Pod "pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e" satisfied condition "Succeeded or Failed"
Jun 21 23:43:37.608: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e container env-test: <nil>
STEP: delete the pod
Jun 21 23:43:37.811: INFO: Waiting for pod pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e to disappear
Jun 21 23:43:37.825: INFO: Pod pod-configmaps-8b2f5aba-ffc8-492d-8a6b-dcd261e8ad7e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:37.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9759" for this suite.

• [SLOW TEST:6.707 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3584,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 21 23:43:38.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50" in namespace "downward-api-3056" to be "Succeeded or Failed"
Jun 21 23:43:38.260: INFO: Pod "downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50": Phase="Pending", Reason="", readiness=false. Elapsed: 15.317207ms
Jun 21 23:43:40.287: INFO: Pod "downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042496351s
Jun 21 23:43:42.307: INFO: Pod "downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062087134s
Jun 21 23:43:44.333: INFO: Pod "downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088207387s
STEP: Saw pod success
Jun 21 23:43:44.334: INFO: Pod "downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50" satisfied condition "Succeeded or Failed"
Jun 21 23:43:44.345: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50 container client-container: <nil>
STEP: delete the pod
Jun 21 23:43:44.461: INFO: Waiting for pod downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50 to disappear
Jun 21 23:43:44.484: INFO: Pod downwardapi-volume-137ae17e-85d1-4872-9b5e-f63c0a021c50 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:44.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3056" for this suite.

• [SLOW TEST:6.659 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3601,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:44.552: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jun 21 23:43:44.988: INFO: Waiting up to 5m0s for pod "var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff" in namespace "var-expansion-3379" to be "Succeeded or Failed"
Jun 21 23:43:45.002: INFO: Pod "var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.314222ms
Jun 21 23:43:47.020: INFO: Pod "var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031909273s
Jun 21 23:43:49.038: INFO: Pod "var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050060784s
Jun 21 23:43:51.051: INFO: Pod "var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06314763s
STEP: Saw pod success
Jun 21 23:43:51.052: INFO: Pod "var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff" satisfied condition "Succeeded or Failed"
Jun 21 23:43:51.065: INFO: Trying to get logs from node 10.10.24.208 pod var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff container dapi-container: <nil>
STEP: delete the pod
Jun 21 23:43:51.142: INFO: Waiting for pod var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff to disappear
Jun 21 23:43:51.155: INFO: Pod var-expansion-a7e90e20-f002-45ed-8df8-587dbddc16ff no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:43:51.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3379" for this suite.

• [SLOW TEST:6.659 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":201,"skipped":3621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:43:51.221: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8075
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8075
STEP: creating replication controller externalsvc in namespace services-8075
I0621 23:43:51.701530      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8075, replica count: 2
I0621 23:43:54.755533      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 21 23:43:54.873: INFO: Creating new exec pod
Jun 21 23:43:58.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-8075 exec execpod6qjcq -- /bin/sh -x -c nslookup nodeport-service.services-8075.svc.cluster.local'
Jun 21 23:43:59.429: INFO: stderr: "+ nslookup nodeport-service.services-8075.svc.cluster.local\n"
Jun 21 23:43:59.429: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-8075.svc.cluster.local\tcanonical name = externalsvc.services-8075.svc.cluster.local.\nName:\texternalsvc.services-8075.svc.cluster.local\nAddress: 172.21.54.19\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8075, will wait for the garbage collector to delete the pods
Jun 21 23:43:59.535: INFO: Deleting ReplicationController externalsvc took: 33.73493ms
Jun 21 23:43:59.636: INFO: Terminating ReplicationController externalsvc pods took: 101.397639ms
Jun 21 23:44:03.515: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:44:03.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8075" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.400 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":202,"skipped":3668,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:44:03.622: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-3248
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3248 to expose endpoints map[]
Jun 21 23:44:03.940: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 21 23:44:04.996: INFO: successfully validated that service endpoint-test2 in namespace services-3248 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3248
Jun 21 23:44:05.057: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:07.071: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:09.085: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3248 to expose endpoints map[pod1:[80]]
Jun 21 23:44:09.144: INFO: successfully validated that service endpoint-test2 in namespace services-3248 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jun 21 23:44:09.144: INFO: Creating new exec pod
Jun 21 23:44:14.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3248 exec execpodlwrfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 21 23:44:14.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 21 23:44:14.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:44:14.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3248 exec execpodlwrfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.212.44 80'
Jun 21 23:44:15.140: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.212.44 80\nConnection to 172.21.212.44 80 port [tcp/http] succeeded!\n"
Jun 21 23:44:15.140: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3248
Jun 21 23:44:15.206: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:17.221: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:19.272: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3248 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 21 23:44:19.375: INFO: successfully validated that service endpoint-test2 in namespace services-3248 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jun 21 23:44:20.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3248 exec execpodlwrfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 21 23:44:20.919: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 21 23:44:20.919: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:44:20.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3248 exec execpodlwrfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.212.44 80'
Jun 21 23:44:21.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.212.44 80\nConnection to 172.21.212.44 80 port [tcp/http] succeeded!\n"
Jun 21 23:44:21.318: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3248
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3248 to expose endpoints map[pod2:[80]]
Jun 21 23:44:21.418: INFO: successfully validated that service endpoint-test2 in namespace services-3248 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jun 21 23:44:22.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3248 exec execpodlwrfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 21 23:44:22.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 21 23:44:22.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:44:22.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3248 exec execpodlwrfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.212.44 80'
Jun 21 23:44:23.291: INFO: stderr: "+ + echonc -v -t hostName\n -w 2 172.21.212.44 80\nConnection to 172.21.212.44 80 port [tcp/http] succeeded!\n"
Jun 21 23:44:23.291: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3248
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3248 to expose endpoints map[]
Jun 21 23:44:23.416: INFO: successfully validated that service endpoint-test2 in namespace services-3248 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:44:23.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3248" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:19.908 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":203,"skipped":3672,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:44:23.530: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jun 21 23:44:23.822: INFO: Waiting up to 5m0s for pod "test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989" in namespace "svcaccounts-967" to be "Succeeded or Failed"
Jun 21 23:44:23.842: INFO: Pod "test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989": Phase="Pending", Reason="", readiness=false. Elapsed: 13.139289ms
Jun 21 23:44:25.863: INFO: Pod "test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03410831s
Jun 21 23:44:27.883: INFO: Pod "test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054327659s
Jun 21 23:44:29.915: INFO: Pod "test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.086347732s
STEP: Saw pod success
Jun 21 23:44:29.915: INFO: Pod "test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989" satisfied condition "Succeeded or Failed"
Jun 21 23:44:29.926: INFO: Trying to get logs from node 10.10.24.208 pod test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:44:30.029: INFO: Waiting for pod test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989 to disappear
Jun 21 23:44:30.058: INFO: Pod test-pod-a8e4e8fd-e0e0-4899-9594-585d4679e989 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:44:30.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-967" for this suite.

• [SLOW TEST:6.581 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":204,"skipped":3682,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:44:30.115: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1316
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-1316
Jun 21 23:44:30.426: INFO: Found 0 stateful pods, waiting for 1
Jun 21 23:44:40.452: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 21 23:44:40.659: INFO: Deleting all statefulset in ns statefulset-1316
Jun 21 23:44:40.682: INFO: Scaling statefulset ss to 0
Jun 21 23:44:50.830: INFO: Waiting for statefulset status.replicas updated to 0
Jun 21 23:44:50.855: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:44:50.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1316" for this suite.

• [SLOW TEST:20.876 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":205,"skipped":3683,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:44:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-3445
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3445 to expose endpoints map[]
Jun 21 23:44:51.640: INFO: successfully validated that service multi-endpoint-test in namespace services-3445 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3445
Jun 21 23:44:51.726: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:53.741: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:55.753: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3445 to expose endpoints map[pod1:[100]]
Jun 21 23:44:55.817: INFO: successfully validated that service multi-endpoint-test in namespace services-3445 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3445
Jun 21 23:44:55.880: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:57.894: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:44:59.911: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3445 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 21 23:44:59.986: INFO: successfully validated that service multi-endpoint-test in namespace services-3445 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jun 21 23:44:59.987: INFO: Creating new exec pod
Jun 21 23:45:05.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3445 exec execpodk2qtp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun 21 23:45:05.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun 21 23:45:05.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:45:05.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3445 exec execpodk2qtp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.96.86 80'
Jun 21 23:45:06.126: INFO: stderr: "+ + nc -v -t -w 2 172.21.96.86 80\necho hostName\nConnection to 172.21.96.86 80 port [tcp/http] succeeded!\n"
Jun 21 23:45:06.126: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:45:06.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3445 exec execpodk2qtp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun 21 23:45:06.779: INFO: stderr: "+ echo hostName+ nc -v -t -w 2 multi-endpoint-test 81\n\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun 21 23:45:06.780: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:45:06.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-3445 exec execpodk2qtp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.96.86 81'
Jun 21 23:45:07.441: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.96.86 81\nConnection to 172.21.96.86 81 port [tcp/*] succeeded!\n"
Jun 21 23:45:07.441: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3445
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3445 to expose endpoints map[pod2:[101]]
Jun 21 23:45:07.558: INFO: successfully validated that service multi-endpoint-test in namespace services-3445 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3445
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3445 to expose endpoints map[]
Jun 21 23:45:08.716: INFO: successfully validated that service multi-endpoint-test in namespace services-3445 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:08.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3445" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:17.894 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":206,"skipped":3692,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun 21 23:45:09.227: INFO: Waiting up to 5m0s for pod "downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f" in namespace "downward-api-2071" to be "Succeeded or Failed"
Jun 21 23:45:09.237: INFO: Pod "downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085467ms
Jun 21 23:45:11.256: INFO: Pod "downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028822782s
Jun 21 23:45:13.273: INFO: Pod "downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046561259s
Jun 21 23:45:15.298: INFO: Pod "downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070782926s
STEP: Saw pod success
Jun 21 23:45:15.298: INFO: Pod "downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f" satisfied condition "Succeeded or Failed"
Jun 21 23:45:15.308: INFO: Trying to get logs from node 10.10.24.208 pod downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f container dapi-container: <nil>
STEP: delete the pod
Jun 21 23:45:15.404: INFO: Waiting for pod downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f to disappear
Jun 21 23:45:15.416: INFO: Pod downward-api-fa9c5d0d-a53f-4bc8-b304-576a47c81c5f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2071" for this suite.

• [SLOW TEST:6.584 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":207,"skipped":3699,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:15.470: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:21.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4168" for this suite.

• [SLOW TEST:6.568 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":208,"skipped":3706,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:22.041: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:45:22.664: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ec8ea3b3-959b-4cb8-bdf0-f690b2d520c4" in namespace "security-context-test-7604" to be "Succeeded or Failed"
Jun 21 23:45:22.676: INFO: Pod "busybox-readonly-false-ec8ea3b3-959b-4cb8-bdf0-f690b2d520c4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.606462ms
Jun 21 23:45:24.690: INFO: Pod "busybox-readonly-false-ec8ea3b3-959b-4cb8-bdf0-f690b2d520c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025434732s
Jun 21 23:45:26.709: INFO: Pod "busybox-readonly-false-ec8ea3b3-959b-4cb8-bdf0-f690b2d520c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04436945s
Jun 21 23:45:28.728: INFO: Pod "busybox-readonly-false-ec8ea3b3-959b-4cb8-bdf0-f690b2d520c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064107875s
Jun 21 23:45:28.729: INFO: Pod "busybox-readonly-false-ec8ea3b3-959b-4cb8-bdf0-f690b2d520c4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:28.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7604" for this suite.

• [SLOW TEST:6.742 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:171
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3715,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:28.788: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:45:29.057: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-f51db358-c2fe-44cc-a69b-a31c2ce0345d
STEP: Creating configMap with name cm-test-opt-upd-f348c7b2-d6fe-477e-a958-2483cd5c0601
STEP: Creating the pod
Jun 21 23:45:29.188: INFO: The status of Pod pod-projected-configmaps-a4519a0e-d8b4-469d-8c04-57f99aa9f296 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:45:31.205: INFO: The status of Pod pod-projected-configmaps-a4519a0e-d8b4-469d-8c04-57f99aa9f296 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:45:33.209: INFO: The status of Pod pod-projected-configmaps-a4519a0e-d8b4-469d-8c04-57f99aa9f296 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-f51db358-c2fe-44cc-a69b-a31c2ce0345d
STEP: Updating configmap cm-test-opt-upd-f348c7b2-d6fe-477e-a958-2483cd5c0601
STEP: Creating configMap with name cm-test-opt-create-db28dae6-acd3-4998-9ab4-d4839796804d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:35.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6805" for this suite.

• [SLOW TEST:6.843 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":3735,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:35.631: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:45:36.651: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:45:38.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451936, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451936, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451936, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451936, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:45:41.809: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:42.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6932" for this suite.
STEP: Destroying namespace "webhook-6932-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.357 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":211,"skipped":3751,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:42.989: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 21 23:45:43.269: INFO: Waiting up to 5m0s for pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3" in namespace "emptydir-8751" to be "Succeeded or Failed"
Jun 21 23:45:43.284: INFO: Pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.636676ms
Jun 21 23:45:45.301: INFO: Pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031738011s
Jun 21 23:45:47.322: INFO: Pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3": Phase="Running", Reason="", readiness=false. Elapsed: 4.052013017s
Jun 21 23:45:49.342: INFO: Pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3": Phase="Running", Reason="", readiness=false. Elapsed: 6.072111829s
Jun 21 23:45:51.353: INFO: Pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.08394752s
STEP: Saw pod success
Jun 21 23:45:51.354: INFO: Pod "pod-8d839f43-75a5-4b5d-929c-c3469edf72b3" satisfied condition "Succeeded or Failed"
Jun 21 23:45:51.364: INFO: Trying to get logs from node 10.10.24.208 pod pod-8d839f43-75a5-4b5d-929c-c3469edf72b3 container test-container: <nil>
STEP: delete the pod
Jun 21 23:45:51.443: INFO: Waiting for pod pod-8d839f43-75a5-4b5d-929c-c3469edf72b3 to disappear
Jun 21 23:45:51.457: INFO: Pod pod-8d839f43-75a5-4b5d-929c-c3469edf72b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:51.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8751" for this suite.

• [SLOW TEST:8.520 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":212,"skipped":3765,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:51.511: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 21 23:45:51.785: INFO: Waiting up to 5m0s for pod "pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d" in namespace "emptydir-8778" to be "Succeeded or Failed"
Jun 21 23:45:51.811: INFO: Pod "pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d": Phase="Pending", Reason="", readiness=false. Elapsed: 25.124315ms
Jun 21 23:45:53.827: INFO: Pod "pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041517156s
Jun 21 23:45:55.869: INFO: Pod "pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083621877s
Jun 21 23:45:57.898: INFO: Pod "pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.11272982s
STEP: Saw pod success
Jun 21 23:45:57.898: INFO: Pod "pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d" satisfied condition "Succeeded or Failed"
Jun 21 23:45:57.917: INFO: Trying to get logs from node 10.10.24.208 pod pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d container test-container: <nil>
STEP: delete the pod
Jun 21 23:45:58.007: INFO: Waiting for pod pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d to disappear
Jun 21 23:45:58.018: INFO: Pod pod-d4308bc2-7c33-45ce-9b26-a3c4098e944d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:45:58.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8778" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":3769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:45:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun 21 23:45:58.290: INFO: PodSpec: initContainers in spec.initContainers
Jun 21 23:46:51.964: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d64ec28c-5232-4160-937a-3fe3ac6ae50b", GenerateName:"", Namespace:"init-container-1218", SelfLink:"", UID:"95537277-fc73-4366-acaf-6991306295d1", ResourceVersion:"100125", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63791451958, loc:(*time.Location)(0xa0acfa0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"290877040"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"cece531924a6ba2dc79ee4296498ff0f0358e1d536264e32cfa597be50064cac", "cni.projectcalico.org/podIP":"172.30.224.61/32", "cni.projectcalico.org/podIPs":"172.30.224.61/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.224.61\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.224.61\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c84348), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c84360), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c84378), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c84390), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c843a8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c843c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003c843d8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003c843f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-88bdh", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0028481a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88bdh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002882720), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88bdh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002882780), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-88bdh", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0028826c0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0051e4870), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.10.24.208", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003112310), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0051e4930)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0051e4950)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0051e496c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0051e4970), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f844f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451958, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451958, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451958, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791451958, loc:(*time.Location)(0xa0acfa0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.24.208", PodIP:"172.30.224.61", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.224.61"}}, StartTime:(*v1.Time)(0xc003c84420), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031123f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003112460)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:244bdbdf4b8d368b5836e9d2c7808a280a73ad72ae321d644e9f220da503218f", ContainerID:"cri-o://65489f0b4628e708c4059449e219fa2a591ca9a334a91d181acf4ad34f42cac3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002848220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002848200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc0051e49ef)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:46:51.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1218" for this suite.

• [SLOW TEST:53.947 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":214,"skipped":3803,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:46:52.027: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6330
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6330
I0621 23:46:52.399144      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6330, replica count: 2
I0621 23:46:55.450525      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 23:46:55.450: INFO: Creating new exec pod
Jun 21 23:47:00.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-6330 exec execpodvbsm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 21 23:47:01.000: INFO: stderr: "+ echo hostName+ nc -v\n -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 21 23:47:01.000: INFO: stdout: "externalname-service-b28sc"
Jun 21 23:47:01.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-6330 exec execpodvbsm9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.226.56 80'
Jun 21 23:47:01.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.226.56 80\nConnection to 172.21.226.56 80 port [tcp/http] succeeded!\n"
Jun 21 23:47:01.444: INFO: stdout: "externalname-service-b28sc"
Jun 21 23:47:01.444: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:01.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6330" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.549 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":215,"skipped":3817,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun 21 23:47:01.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3451 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 21 23:47:02.085: INFO: stderr: ""
Jun 21 23:47:02.085: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 21 23:47:07.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3451 get pod e2e-test-httpd-pod -o json'
Jun 21 23:47:07.403: INFO: stderr: ""
Jun 21 23:47:07.404: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"df3495267022c46b9e89ce8c5c9d34d1dbd18432ed7b258490d8995747ec8397\",\n            \"cni.projectcalico.org/podIP\": \"172.30.224.63/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.224.63/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.224.63\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.224.63\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2022-06-21T23:47:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3451\",\n        \"resourceVersion\": \"100367\",\n        \"uid\": \"6ea8a4a3-67f2-4ff5-b26a-95601c588418\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-j5wxq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-gmkbs\"\n            }\n        ],\n        \"nodeName\": \"10.10.24.208\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c56,c5\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-j5wxq\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-21T23:47:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-21T23:47:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-21T23:47:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-21T23:47:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://df2eb5b3a815e496472a3cd92549f0fbedd2ba3c56927f559cb3875e41b22921\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-06-21T23:47:04Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.24.208\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.224.63\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.224.63\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-06-21T23:47:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 21 23:47:07.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3451 replace -f -'
Jun 21 23:47:08.054: INFO: stderr: ""
Jun 21 23:47:08.054: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Jun 21 23:47:08.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3451 delete pods e2e-test-httpd-pod'
Jun 21 23:47:11.165: INFO: stderr: ""
Jun 21 23:47:11.165: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:11.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3451" for this suite.

• [SLOW TEST:9.645 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":216,"skipped":3825,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:11.223: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun 21 23:47:11.578: INFO: Waiting up to 5m0s for pod "security-context-1a02022d-926a-4421-9fa6-2321b5848e8c" in namespace "security-context-3337" to be "Succeeded or Failed"
Jun 21 23:47:11.601: INFO: Pod "security-context-1a02022d-926a-4421-9fa6-2321b5848e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.594643ms
Jun 21 23:47:13.618: INFO: Pod "security-context-1a02022d-926a-4421-9fa6-2321b5848e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040056057s
Jun 21 23:47:15.632: INFO: Pod "security-context-1a02022d-926a-4421-9fa6-2321b5848e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054051128s
Jun 21 23:47:17.648: INFO: Pod "security-context-1a02022d-926a-4421-9fa6-2321b5848e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070049651s
STEP: Saw pod success
Jun 21 23:47:17.648: INFO: Pod "security-context-1a02022d-926a-4421-9fa6-2321b5848e8c" satisfied condition "Succeeded or Failed"
Jun 21 23:47:17.662: INFO: Trying to get logs from node 10.10.24.208 pod security-context-1a02022d-926a-4421-9fa6-2321b5848e8c container test-container: <nil>
STEP: delete the pod
Jun 21 23:47:17.732: INFO: Waiting for pod security-context-1a02022d-926a-4421-9fa6-2321b5848e8c to disappear
Jun 21 23:47:17.749: INFO: Pod security-context-1a02022d-926a-4421-9fa6-2321b5848e8c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:17.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3337" for this suite.

• [SLOW TEST:6.579 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":217,"skipped":3833,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jun 21 23:47:18.164: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:47:20.198: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:21.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7063" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":218,"skipped":3834,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:21.331: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2604
STEP: creating service affinity-clusterip in namespace services-2604
STEP: creating replication controller affinity-clusterip in namespace services-2604
I0621 23:47:21.707601      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2604, replica count: 3
I0621 23:47:24.759033      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 21 23:47:24.800: INFO: Creating new exec pod
Jun 21 23:47:29.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-2604 exec execpod-affinityvtbr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun 21 23:47:30.346: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 21 23:47:30.346: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:47:30.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-2604 exec execpod-affinityvtbr5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.96.122 80'
Jun 21 23:47:30.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.96.122 80\nConnection to 172.21.96.122 80 port [tcp/http] succeeded!\n"
Jun 21 23:47:30.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 21 23:47:30.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-2604 exec execpod-affinityvtbr5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.96.122:80/ ; done'
Jun 21 23:47:31.358: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.96.122:80/\n"
Jun 21 23:47:31.358: INFO: stdout: "\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj\naffinity-clusterip-fzgcj"
Jun 21 23:47:31.358: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.358: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.358: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.358: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Received response from host: affinity-clusterip-fzgcj
Jun 21 23:47:31.359: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2604, will wait for the garbage collector to delete the pods
Jun 21 23:47:31.520: INFO: Deleting ReplicationController affinity-clusterip took: 44.93812ms
Jun 21 23:47:31.625: INFO: Terminating ReplicationController affinity-clusterip pods took: 105.053579ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:35.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2604" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.160 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":219,"skipped":3835,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:35.493: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jun 21 23:47:35.842: INFO: running pods: 0 < 1
Jun 21 23:47:37.860: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:39.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3783" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":220,"skipped":3846,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jun 21 23:47:40.351: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-8791 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:40.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8791" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":221,"skipped":3848,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:40.622: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:41.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9019" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":222,"skipped":3868,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun 21 23:47:41.877: INFO: observed Pod pod-test in namespace pods-1705 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun 21 23:47:41.887: INFO: observed Pod pod-test in namespace pods-1705 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  }]
Jun 21 23:47:41.941: INFO: observed Pod pod-test in namespace pods-1705 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  }]
Jun 21 23:47:42.969: INFO: observed Pod pod-test in namespace pods-1705 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  }]
Jun 21 23:47:43.072: INFO: observed Pod pod-test in namespace pods-1705 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  }]
Jun 21 23:47:44.370: INFO: Found Pod pod-test in namespace pods-1705 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-21 23:47:41 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jun 21 23:47:44.433: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun 21 23:47:44.522: INFO: observed event type ADDED
Jun 21 23:47:44.523: INFO: observed event type MODIFIED
Jun 21 23:47:44.525: INFO: observed event type MODIFIED
Jun 21 23:47:44.525: INFO: observed event type MODIFIED
Jun 21 23:47:44.525: INFO: observed event type MODIFIED
Jun 21 23:47:44.525: INFO: observed event type MODIFIED
Jun 21 23:47:44.526: INFO: observed event type MODIFIED
Jun 21 23:47:44.528: INFO: observed event type MODIFIED
Jun 21 23:47:44.528: INFO: observed event type MODIFIED
Jun 21 23:47:46.385: INFO: observed event type MODIFIED
Jun 21 23:47:46.970: INFO: observed event type MODIFIED
Jun 21 23:47:48.411: INFO: observed event type MODIFIED
Jun 21 23:47:48.437: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1705" for this suite.

• [SLOW TEST:7.035 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":223,"skipped":3877,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:48.535: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 21 23:47:54.930: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:47:54.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0621 23:47:54.930404      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-7076" for this suite.

• [SLOW TEST:6.456 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":224,"skipped":3890,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:47:54.997: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 21 23:47:55.284: INFO: Waiting up to 5m0s for pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69" in namespace "emptydir-2084" to be "Succeeded or Failed"
Jun 21 23:47:55.301: INFO: Pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69": Phase="Pending", Reason="", readiness=false. Elapsed: 17.144808ms
Jun 21 23:47:57.325: INFO: Pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041022227s
Jun 21 23:47:59.347: INFO: Pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063026014s
Jun 21 23:48:01.377: INFO: Pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093029831s
Jun 21 23:48:03.409: INFO: Pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.125408138s
STEP: Saw pod success
Jun 21 23:48:03.409: INFO: Pod "pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69" satisfied condition "Succeeded or Failed"
Jun 21 23:48:03.423: INFO: Trying to get logs from node 10.10.24.208 pod pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69 container test-container: <nil>
STEP: delete the pod
Jun 21 23:48:03.494: INFO: Waiting for pod pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69 to disappear
Jun 21 23:48:03.511: INFO: Pod pod-fd82d5a4-9e48-4ffa-95e4-1b08d9cf1a69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:03.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2084" for this suite.

• [SLOW TEST:8.565 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":3902,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:03.564: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:48:03.779: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 21 23:48:03.859: INFO: The status of Pod pod-exec-websocket-f801c405-ba71-45bd-b1c9-cd1aa5a4dddb is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:48:05.881: INFO: The status of Pod pod-exec-websocket-f801c405-ba71-45bd-b1c9-cd1aa5a4dddb is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:48:07.883: INFO: The status of Pod pod-exec-websocket-f801c405-ba71-45bd-b1c9-cd1aa5a4dddb is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:08.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3110" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":226,"skipped":3921,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:08.185: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:48:08.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 create -f -'
Jun 21 23:48:09.006: INFO: stderr: ""
Jun 21 23:48:09.006: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 21 23:48:09.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 create -f -'
Jun 21 23:48:09.570: INFO: stderr: ""
Jun 21 23:48:09.570: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 21 23:48:10.594: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:48:10.595: INFO: Found 0 / 1
Jun 21 23:48:11.590: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:48:11.590: INFO: Found 0 / 1
Jun 21 23:48:12.589: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:48:12.589: INFO: Found 1 / 1
Jun 21 23:48:12.589: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 21 23:48:12.601: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 21 23:48:12.601: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 21 23:48:12.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 describe pod agnhost-primary-x7cwg'
Jun 21 23:48:12.802: INFO: stderr: ""
Jun 21 23:48:12.802: INFO: stdout: "Name:         agnhost-primary-x7cwg\nNamespace:    kubectl-5219\nPriority:     0\nNode:         10.10.24.208/10.10.24.208\nStart Time:   Tue, 21 Jun 2022 23:48:09 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 2dbfa2946d60449119d7e473de16b1aec0eed261fd6e99cd3641939aaf64714f\n              cni.projectcalico.org/podIP: 172.30.224.34/32\n              cni.projectcalico.org/podIPs: 172.30.224.34/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.224.34\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.224.34\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.224.34\nIPs:\n  IP:           172.30.224.34\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://7da3b4aef1d006a019d14818c2e3f3993860e23a49f51c3e5637b7c4c3354e66\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 21 Jun 2022 23:48:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-trkjs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-trkjs:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-5219/agnhost-primary-x7cwg to 10.10.24.208\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.224.34/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jun 21 23:48:12.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 describe rc agnhost-primary'
Jun 21 23:48:13.039: INFO: stderr: ""
Jun 21 23:48:13.039: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5219\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-x7cwg\n"
Jun 21 23:48:13.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 describe service agnhost-primary'
Jun 21 23:48:13.255: INFO: stderr: ""
Jun 21 23:48:13.255: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5219\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.74.193\nIPs:               172.21.74.193\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.224.34:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 21 23:48:13.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 describe node 10.10.24.206'
Jun 21 23:48:13.903: INFO: stderr: ""
Jun 21 23:48:13.903: INFO: stdout: "Name:               10.10.24.206\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-osa\n                    failure-domain.beta.kubernetes.io/zone=osa23\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.73.71.139\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.10.24.206\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=jp-osa\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cap2u74o0egdplkgdo2g-kubee2epvg1-default-000001f0\n                    ibm-cloud.kubernetes.io/worker-pool-id=cap2u74o0egdplkgdo2g-7e00d41\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.9.38_1543_openshift\n                    ibm-cloud.kubernetes.io/zone=osa23\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.10.24.206\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2981446\n                    publicVLAN=2981452\n                    topology.kubernetes.io/region=jp-osa\n                    topology.kubernetes.io/zone=osa23\nAnnotations:        projectcalico.org/IPv4Address: 10.10.24.206/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.96.0\nCreationTimestamp:  Tue, 21 Jun 2022 21:08:41 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.10.24.206\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 21 Jun 2022 23:48:08 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 21 Jun 2022 21:12:04 +0000   Tue, 21 Jun 2022 21:12:04 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 21 Jun 2022 23:45:46 +0000   Tue, 21 Jun 2022 21:08:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 21 Jun 2022 23:45:46 +0000   Tue, 21 Jun 2022 21:08:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 21 Jun 2022 23:45:46 +0000   Tue, 21 Jun 2022 21:08:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 21 Jun 2022 23:45:46 +0000   Tue, 21 Jun 2022 21:11:41 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.24.206\n  ExternalIP:  163.73.71.139\n  Hostname:    10.10.24.206\nCapacity:\n  cpu:                4\n  ephemeral-storage:  103078840Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16253380Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  94369515442\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13477316Ki\n  pods:               110\nSystem Info:\n  Machine ID:                              ca191c683a7c4f0ebb2f4a037cdeeaa7\n  System UUID:                             AA507EE5-7B8C-60D4-5F0F-D3703D4FDDF4\n  Boot ID:                                 6b17b84b-d19c-4746-87d6-cb5c203180f2\n  Kernel Version:                          3.10.0-1160.66.1.el7.x86_64\n  OS Image:                                Red Hat\n  Operating System:                        linux\n  Architecture:                            amd64\n  Container Runtime Version:               cri-o://1.22.5-3.rhaos4.9.gitb6d3a87.el7\n  Kubelet Version:                         v1.22.8+f34b40c\n  Kube-Proxy Version:                      v1.22.8+f34b40c\nPodCIDR:                                   172.30.1.0/24\nPodCIDRs:                                  172.30.1.0/24\nProviderID:                                ibm://fee034388aa6435883a1f720010ab3a2///cap2u74o0egdplkgdo2g/kube-cap2u74o0egdplkgdo2g-kubee2epvg1-default-000001f0\nNon-terminated Pods:                       (52 in total)\n  Namespace                                Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                            calico-node-24dvg                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         157m\n  calico-system                            calico-typha-65cc4575bc-f88zd                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         157m\n  default                                  test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  ibm-odf-validation-webhook               managed-storage-validation-webhooks-695b4c95d9-wsqbq       10m (0%)      100m (2%)   10Mi (0%)        100Mi (0%)     34m\n  ibm-system                               ibm-cloud-provider-ip-163-73-69-51-775496d795-hd8s7        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         148m\n  kube-system                              ibm-keepalived-watcher-rpq7r                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         159m\n  kube-system                              ibm-master-proxy-static-10.10.24.206                       26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      158m\n  kube-system                              ibmcloud-block-storage-driver-l2pxj                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     159m\n  kube-system                              ibmcloud-block-storage-plugin-5469669bb7-f7t75             50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     34m\n  kube-system                              vpn-69b96645d-qtgxb                                        5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         156m\n  openshift-cluster-node-tuning-operator   cluster-node-tuning-operator-5f78c6cfc9-6nlpv              10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         34m\n  openshift-cluster-node-tuning-operator   tuned-pc9t5                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         152m\n  openshift-cluster-storage-operator       csi-snapshot-controller-69d5f9c777-p6845                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         155m\n  openshift-cluster-storage-operator       csi-snapshot-webhook-55d4797dc4-2c7hn                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         155m\n  openshift-console                        console-67bfbdb4dc-mtf8v                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         151m\n  openshift-console                        downloads-988dbf8f4-wqhz7                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         154m\n  openshift-dns                            dns-default-wqq2m                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         152m\n  openshift-dns                            node-resolver-xjddx                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         152m\n  openshift-image-registry                 node-ca-wf6f5                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         152m\n  openshift-ingress-canary                 ingress-canary-mcgps                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         152m\n  openshift-ingress                        router-default-bb6fc54c5-hbbwg                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         152m\n  openshift-kube-proxy                     openshift-kube-proxy-g5jv6                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         157m\n  openshift-kube-storage-version-migrator  migrator-8467484867-zz9sp                                  10m (0%)      0 (0%)      200Mi (1%)       0 (0%)         155m\n  openshift-marketplace                    certified-operators-tkfzk                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34m\n  openshift-marketplace                    community-operators-jwwzz                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34m\n  openshift-marketplace                    marketplace-operator-757fc48f95-zzxsj                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34m\n  openshift-marketplace                    redhat-marketplace-4hlf8                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34m\n  openshift-marketplace                    redhat-operators-2krtf                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34m\n  openshift-monitoring                     alertmanager-main-0                                        8m (0%)       0 (0%)      105Mi (0%)       0 (0%)         152m\n  openshift-monitoring                     alertmanager-main-1                                        8m (0%)       0 (0%)      105Mi (0%)       0 (0%)         34m\n  openshift-monitoring                     grafana-6db9675f77-rv2zg                                   5m (0%)       0 (0%)      84Mi (0%)        0 (0%)         34m\n  openshift-monitoring                     kube-state-metrics-5f9b9688bc-jrnb4                        4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         154m\n  openshift-monitoring                     node-exporter-xvzpx                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         154m\n  openshift-monitoring                     openshift-state-metrics-744d546498-5g75r                   3m (0%)       0 (0%)      72Mi (0%)        0 (0%)         154m\n  openshift-monitoring                     prometheus-adapter-5c564c9546-wz2lv                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         149m\n  openshift-monitoring                     prometheus-k8s-0                                           100m (2%)     0 (0%)      1119Mi (8%)      0 (0%)         151m\n  openshift-monitoring                     prometheus-operator-bf9ff66c-t6r25                         6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         155m\n  openshift-monitoring                     telemeter-client-8dfb6c944-8k9b8                           3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         154m\n  openshift-monitoring                     thanos-querier-75f4c69596-kmkqk                            14m (0%)      0 (0%)      77Mi (0%)        0 (0%)         151m\n  openshift-multus                         multus-additional-cni-plugins-qjdfj                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         157m\n  openshift-multus                         multus-admission-controller-sndqs                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         156m\n  openshift-multus                         multus-c4bqm                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         157m\n  openshift-multus                         network-metrics-daemon-l6zp7                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         157m\n  openshift-network-diagnostics            network-check-source-778bbb8ccb-g4vw8                      10m (0%)      0 (0%)      40Mi (0%)        0 (0%)         34m\n  openshift-network-diagnostics            network-check-target-4dx2q                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         157m\n  openshift-operator-lifecycle-manager     catalog-operator-86b47c4c86-vwfqr                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         34m\n  openshift-operator-lifecycle-manager     olm-operator-767cc8584d-5xlcg                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         34m\n  openshift-operator-lifecycle-manager     package-server-manager-bbb676bfb-x766w                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         34m\n  openshift-operator-lifecycle-manager     packageserver-cfb4bb4b4-kzchv                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         154m\n  openshift-service-ca                     service-ca-7f5f4d65b9-gd4rm                                10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         155m\n  sonobuoy                                 sonobuoy-e2e-job-9869bda7abe94c50                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                                 sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-8pkdj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1357m (34%)      1 (25%)\n  memory             4686355Ki (34%)  1216800Ki (9%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From        Message\n  ----    ------                   ----                 ----        -------\n  Normal  Starting                 157m                 kube-proxy  \n  Normal  Starting                 159m                 kubelet     Starting kubelet.\n  Normal  NodeAllocatableEnforced  159m                 kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientPID     159m (x7 over 159m)  kubelet     Node 10.10.24.206 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  159m (x8 over 159m)  kubelet     Node 10.10.24.206 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    159m (x8 over 159m)  kubelet     Node 10.10.24.206 status is now: NodeHasNoDiskPressure\n"
Jun 21 23:48:13.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-5219 describe namespace kubectl-5219'
Jun 21 23:48:14.193: INFO: stderr: ""
Jun 21 23:48:14.193: INFO: stdout: "Name:         kubectl-5219\nLabels:       e2e-framework=kubectl\n              e2e-run=32b5b847-8c64-4d96-bcfe-c7d2136dd517\n              kubernetes.io/metadata.name=kubectl-5219\nAnnotations:  openshift.io/sa.scc.mcs: s0:c57,c4\n              openshift.io/sa.scc.supplemental-groups: 1003200000/10000\n              openshift.io/sa.scc.uid-range: 1003200000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:14.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5219" for this suite.

• [SLOW TEST:6.070 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1094
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":227,"skipped":3923,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:14.255: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-9725
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9725
STEP: Deleting pre-stop pod
Jun 21 23:48:27.768: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:27.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9725" for this suite.

• [SLOW TEST:13.621 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":228,"skipped":3937,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun 21 23:48:28.280: INFO: The status of Pod annotationupdateab855cd9-0acf-49f7-a6c4-f56fb6ef550b is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:48:30.298: INFO: The status of Pod annotationupdateab855cd9-0acf-49f7-a6c4-f56fb6ef550b is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:48:32.297: INFO: The status of Pod annotationupdateab855cd9-0acf-49f7-a6c4-f56fb6ef550b is Running (Ready = true)
Jun 21 23:48:32.916: INFO: Successfully updated pod "annotationupdateab855cd9-0acf-49f7-a6c4-f56fb6ef550b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:35.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-720" for this suite.

• [SLOW TEST:7.180 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":3941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:35.060: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:48:36.049: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:48:38.111: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452116, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452116, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452116, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452116, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:48:41.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:41.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2875" for this suite.
STEP: Destroying namespace "webhook-2875-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.680 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":230,"skipped":3963,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:41.740: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:48:43.051: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:48:45.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452123, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452123, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452123, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452123, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:48:48.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 21 23:48:52.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=webhook-1644 attach --namespace=webhook-1644 to-be-attached-pod -i -c=container1'
Jun 21 23:48:52.742: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:48:52.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1644" for this suite.
STEP: Destroying namespace "webhook-1644-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.396 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":231,"skipped":3976,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:48:53.139: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jun 21 23:48:55.542: INFO: pods: 0 < 3
Jun 21 23:48:57.564: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jun 21 23:49:03.902: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:49:06.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3157" for this suite.

• [SLOW TEST:12.979 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":232,"skipped":3986,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:49:06.118: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 21 23:49:06.514: INFO: Number of nodes with available pods: 0
Jun 21 23:49:06.514: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:49:07.559: INFO: Number of nodes with available pods: 0
Jun 21 23:49:07.559: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:49:08.563: INFO: Number of nodes with available pods: 0
Jun 21 23:49:08.563: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 21 23:49:09.557: INFO: Number of nodes with available pods: 3
Jun 21 23:49:09.558: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 21 23:49:09.679: INFO: Number of nodes with available pods: 2
Jun 21 23:49:09.680: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 23:49:10.800: INFO: Number of nodes with available pods: 2
Jun 21 23:49:10.800: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 23:49:11.738: INFO: Number of nodes with available pods: 2
Jun 21 23:49:11.738: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 21 23:49:12.729: INFO: Number of nodes with available pods: 3
Jun 21 23:49:12.730: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2076, will wait for the garbage collector to delete the pods
Jun 21 23:49:12.855: INFO: Deleting DaemonSet.extensions daemon-set took: 26.842294ms
Jun 21 23:49:12.956: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.464863ms
Jun 21 23:49:16.669: INFO: Number of nodes with available pods: 0
Jun 21 23:49:16.669: INFO: Number of running nodes: 0, number of available pods: 0
Jun 21 23:49:16.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"103432"},"items":null}

Jun 21 23:49:16.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"103432"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:49:16.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2076" for this suite.

• [SLOW TEST:10.755 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":233,"skipped":3987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:49:16.874: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:51:01.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4544" for this suite.

• [SLOW TEST:104.449 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":234,"skipped":4009,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:51:01.325: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jun 21 23:51:01.719: INFO: Waiting up to 5m0s for pod "client-containers-9dab89ad-bd97-4a35-9225-998286d26828" in namespace "containers-4730" to be "Succeeded or Failed"
Jun 21 23:51:01.737: INFO: Pod "client-containers-9dab89ad-bd97-4a35-9225-998286d26828": Phase="Pending", Reason="", readiness=false. Elapsed: 18.250193ms
Jun 21 23:51:03.760: INFO: Pod "client-containers-9dab89ad-bd97-4a35-9225-998286d26828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041051367s
Jun 21 23:51:05.786: INFO: Pod "client-containers-9dab89ad-bd97-4a35-9225-998286d26828": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067277505s
Jun 21 23:51:07.807: INFO: Pod "client-containers-9dab89ad-bd97-4a35-9225-998286d26828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088735376s
STEP: Saw pod success
Jun 21 23:51:07.807: INFO: Pod "client-containers-9dab89ad-bd97-4a35-9225-998286d26828" satisfied condition "Succeeded or Failed"
Jun 21 23:51:07.822: INFO: Trying to get logs from node 10.10.24.208 pod client-containers-9dab89ad-bd97-4a35-9225-998286d26828 container agnhost-container: <nil>
STEP: delete the pod
Jun 21 23:51:08.005: INFO: Waiting for pod client-containers-9dab89ad-bd97-4a35-9225-998286d26828 to disappear
Jun 21 23:51:08.016: INFO: Pod client-containers-9dab89ad-bd97-4a35-9225-998286d26828 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:51:08.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4730" for this suite.

• [SLOW TEST:6.745 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4086,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:51:08.071: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jun 21 23:51:08.529: INFO: The status of Pod pod-hostip-9611ec9d-d860-43f1-9b50-d4b919b0230b is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:51:10.550: INFO: The status of Pod pod-hostip-9611ec9d-d860-43f1-9b50-d4b919b0230b is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:51:12.550: INFO: The status of Pod pod-hostip-9611ec9d-d860-43f1-9b50-d4b919b0230b is Running (Ready = true)
Jun 21 23:51:12.573: INFO: Pod pod-hostip-9611ec9d-d860-43f1-9b50-d4b919b0230b has hostIP: 10.10.24.208
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:51:12.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8769" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4093,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:51:12.627: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-b0d93a26-01b8-400b-b03b-a2fbdcacc4bb in namespace container-probe-9813
Jun 21 23:51:17.005: INFO: Started pod test-webserver-b0d93a26-01b8-400b-b03b-a2fbdcacc4bb in namespace container-probe-9813
STEP: checking the pod's current state and verifying that restartCount is present
Jun 21 23:51:17.019: INFO: Initial restart count of pod test-webserver-b0d93a26-01b8-400b-b03b-a2fbdcacc4bb is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:55:17.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9813" for this suite.

• [SLOW TEST:245.189 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:55:17.820: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun 21 23:55:18.159: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:55:20.177: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:55:22.185: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun 21 23:55:22.245: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:55:24.257: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:55:26.265: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 21 23:55:26.367: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 21 23:55:26.385: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 21 23:55:28.386: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 21 23:55:28.402: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 21 23:55:30.387: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 21 23:55:30.401: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:55:30.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4174" for this suite.

• [SLOW TEST:12.638 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4172,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:55:30.459: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:55:30.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2804" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4175,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:55:30.867: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:55:42.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6335" for this suite.

• [SLOW TEST:11.656 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":240,"skipped":4181,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:55:42.529: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7205
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 21 23:55:42.811: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 21 23:55:43.037: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:55:45.053: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:55:47.054: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:55:49.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:55:51.057: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:55:53.061: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:55:55.051: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:55:57.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:55:59.075: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:56:01.059: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 21 23:56:03.059: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 21 23:56:03.096: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 21 23:56:03.130: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 21 23:56:07.322: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 21 23:56:07.322: INFO: Going to poll 172.30.96.59 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 21 23:56:07.340: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.96.59:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7205 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:56:07.340: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:56:07.671: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 21 23:56:07.671: INFO: Going to poll 172.30.224.19 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 21 23:56:07.687: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.224.19:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7205 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:56:07.687: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:56:07.966: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 21 23:56:07.966: INFO: Going to poll 172.30.47.215 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 21 23:56:07.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.47.215:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7205 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 21 23:56:07.980: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 21 23:56:08.285: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:56:08.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7205" for this suite.

• [SLOW TEST:25.810 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4192,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:56:08.339: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:56:19.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6301" for this suite.

• [SLOW TEST:11.683 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":242,"skipped":4207,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:56:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 21 23:56:20.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 21 23:56:23.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452581, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452581, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452581, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791452580, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 21 23:56:26.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:56:26.136: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:56:29.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2788" for this suite.
STEP: Destroying namespace "webhook-2788-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.896 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":243,"skipped":4216,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:56:29.919: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun 21 23:56:30.462: INFO: The status of Pod labelsupdate1407b19b-6b87-47d6-abfd-087573a5025b is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:56:32.482: INFO: The status of Pod labelsupdate1407b19b-6b87-47d6-abfd-087573a5025b is Pending, waiting for it to be Running (with Ready = true)
Jun 21 23:56:34.482: INFO: The status of Pod labelsupdate1407b19b-6b87-47d6-abfd-087573a5025b is Running (Ready = true)
Jun 21 23:56:35.105: INFO: Successfully updated pod "labelsupdate1407b19b-6b87-47d6-abfd-087573a5025b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:56:37.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8493" for this suite.

• [SLOW TEST:7.337 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":244,"skipped":4235,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:56:37.257: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun 21 23:56:37.454: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 21 23:57:37.684: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 21 23:57:37.707: INFO: Starting informer...
STEP: Starting pod...
Jun 21 23:57:38.038: INFO: Pod is running on 10.10.24.208. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 21 23:57:38.112: INFO: Pod wasn't evicted. Proceeding
Jun 21 23:57:38.112: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 21 23:58:53.212: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:58:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5943" for this suite.

• [SLOW TEST:136.006 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":245,"skipped":4253,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:58:53.265: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:58:53.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5894" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":246,"skipped":4260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:58:54.016: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 21 23:59:54.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8549" for this suite.

• [SLOW TEST:60.375 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4312,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 21 23:59:54.396: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jun 21 23:59:54.741: INFO: Waiting up to 5m0s for pod "var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9" in namespace "var-expansion-9722" to be "Succeeded or Failed"
Jun 21 23:59:54.752: INFO: Pod "var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.644101ms
Jun 21 23:59:56.772: INFO: Pod "var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031169738s
Jun 21 23:59:58.787: INFO: Pod "var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046280925s
Jun 22 00:00:00.805: INFO: Pod "var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063643862s
STEP: Saw pod success
Jun 22 00:00:00.805: INFO: Pod "var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9" satisfied condition "Succeeded or Failed"
Jun 22 00:00:00.816: INFO: Trying to get logs from node 10.10.24.208 pod var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9 container dapi-container: <nil>
STEP: delete the pod
Jun 22 00:00:00.906: INFO: Waiting for pod var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9 to disappear
Jun 22 00:00:00.920: INFO: Pod var-expansion-d7a2f5d6-2a2f-410b-99b7-d35b3987e0b9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:00:00.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9722" for this suite.

• [SLOW TEST:6.589 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4320,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:00:00.993: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-lpkx
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 00:00:01.443: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lpkx" in namespace "subpath-3728" to be "Succeeded or Failed"
Jun 22 00:00:01.456: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.95004ms
Jun 22 00:00:03.475: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031614458s
Jun 22 00:00:05.503: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 4.059238058s
Jun 22 00:00:07.516: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 6.07234812s
Jun 22 00:00:09.534: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 8.091110098s
Jun 22 00:00:11.608: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 10.164465418s
Jun 22 00:00:13.626: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 12.18224145s
Jun 22 00:00:15.647: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 14.204129286s
Jun 22 00:00:17.663: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 16.219998429s
Jun 22 00:00:19.688: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 18.244579854s
Jun 22 00:00:21.711: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 20.267825541s
Jun 22 00:00:23.741: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=true. Elapsed: 22.297578311s
Jun 22 00:00:25.756: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Running", Reason="", readiness=false. Elapsed: 24.312849773s
Jun 22 00:00:27.771: INFO: Pod "pod-subpath-test-configmap-lpkx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.32720825s
STEP: Saw pod success
Jun 22 00:00:27.771: INFO: Pod "pod-subpath-test-configmap-lpkx" satisfied condition "Succeeded or Failed"
Jun 22 00:00:27.782: INFO: Trying to get logs from node 10.10.24.208 pod pod-subpath-test-configmap-lpkx container test-container-subpath-configmap-lpkx: <nil>
STEP: delete the pod
Jun 22 00:00:27.865: INFO: Waiting for pod pod-subpath-test-configmap-lpkx to disappear
Jun 22 00:00:27.875: INFO: Pod pod-subpath-test-configmap-lpkx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lpkx
Jun 22 00:00:27.875: INFO: Deleting pod "pod-subpath-test-configmap-lpkx" in namespace "subpath-3728"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:00:27.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3728" for this suite.

• [SLOW TEST:26.945 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":249,"skipped":4341,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:00:27.940: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-b03916fc-8fef-4686-a184-b3730fc9a3e4 in namespace container-probe-6613
Jun 22 00:00:32.441: INFO: Started pod busybox-b03916fc-8fef-4686-a184-b3730fc9a3e4 in namespace container-probe-6613
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 00:00:32.455: INFO: Initial restart count of pod busybox-b03916fc-8fef-4686-a184-b3730fc9a3e4 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:04:33.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6613" for this suite.

• [SLOW TEST:245.343 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:04:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 22 00:04:34.038: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jun 22 00:04:36.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453074, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453074, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453074, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453074, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 00:04:39.159: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:04:39.182: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:04:42.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8734" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.589 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":251,"skipped":4369,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:04:42.885: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 22 00:04:43.234: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 00:05:43.573: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:05:43.604: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:05:43.953: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun 22 00:05:43.984: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:05:44.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8699" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:05:44.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1665" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.599 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":252,"skipped":4371,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:05:44.484: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Jun 22 00:05:44.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 22 00:05:45.317: INFO: stderr: ""
Jun 22 00:05:45.317: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jun 22 00:05:45.317: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 22 00:05:45.317: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6539" to be "running and ready, or succeeded"
Jun 22 00:05:45.339: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 21.756608ms
Jun 22 00:05:47.357: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039111122s
Jun 22 00:05:49.375: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.057721876s
Jun 22 00:05:49.375: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 22 00:05:49.375: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 22 00:05:49.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 logs logs-generator logs-generator'
Jun 22 00:05:49.730: INFO: stderr: ""
Jun 22 00:05:49.730: INFO: stdout: "I0622 00:05:47.123240       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/rh8 277\nI0622 00:05:47.322710       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/h78 396\nI0622 00:05:47.524200       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/6r7v 273\nI0622 00:05:47.727866       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/9qm 255\nI0622 00:05:47.923337       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rdp 460\nI0622 00:05:48.122785       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/696 223\nI0622 00:05:48.323370       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9bnr 485\nI0622 00:05:48.523069       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/pqh8 321\nI0622 00:05:48.725787       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/kpk 550\nI0622 00:05:48.923402       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9jcf 514\nI0622 00:05:49.122797       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/vzd6 239\nI0622 00:05:49.323117       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/dzwl 538\nI0622 00:05:49.523660       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/qh8 352\n"
STEP: limiting log lines
Jun 22 00:05:49.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 logs logs-generator logs-generator --tail=1'
Jun 22 00:05:49.985: INFO: stderr: ""
Jun 22 00:05:49.985: INFO: stdout: "I0622 00:05:49.923561       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/g2t5 556\n"
Jun 22 00:05:49.985: INFO: got output "I0622 00:05:49.923561       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/g2t5 556\n"
STEP: limiting log bytes
Jun 22 00:05:49.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 logs logs-generator logs-generator --limit-bytes=1'
Jun 22 00:05:50.211: INFO: stderr: ""
Jun 22 00:05:50.211: INFO: stdout: "I"
Jun 22 00:05:50.211: INFO: got output "I"
STEP: exposing timestamps
Jun 22 00:05:50.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 22 00:05:50.446: INFO: stderr: ""
Jun 22 00:05:50.446: INFO: stdout: "2022-06-21T19:05:50.323520795-05:00 I0622 00:05:50.323409       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/589 429\n"
Jun 22 00:05:50.446: INFO: got output "2022-06-21T19:05:50.323520795-05:00 I0622 00:05:50.323409       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/589 429\n"
STEP: restricting to a time range
Jun 22 00:05:52.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 logs logs-generator logs-generator --since=1s'
Jun 22 00:05:53.129: INFO: stderr: ""
Jun 22 00:05:53.129: INFO: stdout: "I0622 00:05:52.122989       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/f2ts 497\nI0622 00:05:52.323543       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/hnx 431\nI0622 00:05:52.523098       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/gvb 215\nI0622 00:05:52.723680       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/wx6 342\nI0622 00:05:52.923240       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/s68 207\n"
Jun 22 00:05:53.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 logs logs-generator logs-generator --since=24h'
Jun 22 00:05:53.347: INFO: stderr: ""
Jun 22 00:05:53.347: INFO: stdout: "I0622 00:05:47.123240       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/rh8 277\nI0622 00:05:47.322710       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/h78 396\nI0622 00:05:47.524200       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/6r7v 273\nI0622 00:05:47.727866       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/9qm 255\nI0622 00:05:47.923337       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rdp 460\nI0622 00:05:48.122785       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/696 223\nI0622 00:05:48.323370       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9bnr 485\nI0622 00:05:48.523069       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/pqh8 321\nI0622 00:05:48.725787       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/kpk 550\nI0622 00:05:48.923402       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9jcf 514\nI0622 00:05:49.122797       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/vzd6 239\nI0622 00:05:49.323117       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/dzwl 538\nI0622 00:05:49.523660       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/qh8 352\nI0622 00:05:49.723038       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/bf5 245\nI0622 00:05:49.923561       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/g2t5 556\nI0622 00:05:50.122856       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/wd5 540\nI0622 00:05:50.323409       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/589 429\nI0622 00:05:50.522808       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/j9n 489\nI0622 00:05:50.723328       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/mcdk 327\nI0622 00:05:50.922727       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/jzc 256\nI0622 00:05:51.123112       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/89q 253\nI0622 00:05:51.323622       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/57gr 345\nI0622 00:05:51.523148       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/nxp 479\nI0622 00:05:51.723705       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/v7m2 427\nI0622 00:05:51.923120       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/22cr 423\nI0622 00:05:52.122989       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/f2ts 497\nI0622 00:05:52.323543       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/hnx 431\nI0622 00:05:52.523098       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/gvb 215\nI0622 00:05:52.723680       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/wx6 342\nI0622 00:05:52.923240       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/s68 207\nI0622 00:05:53.122761       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/6gp 301\nI0622 00:05:53.323245       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/4s7 582\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Jun 22 00:05:53.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-6539 delete pod logs-generator'
Jun 22 00:05:55.833: INFO: stderr: ""
Jun 22 00:05:55.833: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:05:55.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6539" for this suite.

• [SLOW TEST:11.421 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":253,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:05:55.907: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 22 00:05:56.281: INFO: Waiting up to 5m0s for pod "pod-419dd3bc-2b8b-487d-8c92-1437445550d2" in namespace "emptydir-7467" to be "Succeeded or Failed"
Jun 22 00:05:56.295: INFO: Pod "pod-419dd3bc-2b8b-487d-8c92-1437445550d2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.262949ms
Jun 22 00:05:58.311: INFO: Pod "pod-419dd3bc-2b8b-487d-8c92-1437445550d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029296985s
Jun 22 00:06:00.329: INFO: Pod "pod-419dd3bc-2b8b-487d-8c92-1437445550d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047466268s
Jun 22 00:06:02.354: INFO: Pod "pod-419dd3bc-2b8b-487d-8c92-1437445550d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072446124s
STEP: Saw pod success
Jun 22 00:06:02.361: INFO: Pod "pod-419dd3bc-2b8b-487d-8c92-1437445550d2" satisfied condition "Succeeded or Failed"
Jun 22 00:06:02.376: INFO: Trying to get logs from node 10.10.24.208 pod pod-419dd3bc-2b8b-487d-8c92-1437445550d2 container test-container: <nil>
STEP: delete the pod
Jun 22 00:06:02.497: INFO: Waiting for pod pod-419dd3bc-2b8b-487d-8c92-1437445550d2 to disappear
Jun 22 00:06:02.516: INFO: Pod pod-419dd3bc-2b8b-487d-8c92-1437445550d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:06:02.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7467" for this suite.

• [SLOW TEST:6.671 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4402,"failed":0}
SSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:06:02.579: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun 22 00:06:02.920: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun 22 00:06:03.031: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:06:03.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6205" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":255,"skipped":4409,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:06:03.223: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:06:03.460: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3688
I0622 00:06:03.561378      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3688, replica count: 1
I0622 00:06:04.614634      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 00:06:05.614931      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 00:06:06.615464      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 00:06:06.765: INFO: Created: latency-svc-76r9h
Jun 22 00:06:06.790: INFO: Got endpoints: latency-svc-76r9h [74.084559ms]
Jun 22 00:06:06.853: INFO: Created: latency-svc-xbjrw
Jun 22 00:06:06.888: INFO: Got endpoints: latency-svc-xbjrw [96.55954ms]
Jun 22 00:06:06.900: INFO: Created: latency-svc-kd767
Jun 22 00:06:06.923: INFO: Created: latency-svc-sbn7j
Jun 22 00:06:06.929: INFO: Got endpoints: latency-svc-kd767 [138.135151ms]
Jun 22 00:06:06.949: INFO: Got endpoints: latency-svc-sbn7j [157.225205ms]
Jun 22 00:06:06.950: INFO: Created: latency-svc-s479w
Jun 22 00:06:06.967: INFO: Got endpoints: latency-svc-s479w [175.629184ms]
Jun 22 00:06:06.973: INFO: Created: latency-svc-bmx52
Jun 22 00:06:06.996: INFO: Created: latency-svc-4cjwr
Jun 22 00:06:06.998: INFO: Got endpoints: latency-svc-bmx52 [205.775699ms]
Jun 22 00:06:07.020: INFO: Created: latency-svc-5pfvr
Jun 22 00:06:07.028: INFO: Got endpoints: latency-svc-4cjwr [235.691743ms]
Jun 22 00:06:07.041: INFO: Got endpoints: latency-svc-5pfvr [248.267846ms]
Jun 22 00:06:07.046: INFO: Created: latency-svc-wv449
Jun 22 00:06:07.073: INFO: Created: latency-svc-56mbx
Jun 22 00:06:07.073: INFO: Got endpoints: latency-svc-wv449 [280.924929ms]
Jun 22 00:06:07.104: INFO: Got endpoints: latency-svc-56mbx [311.110443ms]
Jun 22 00:06:07.104: INFO: Created: latency-svc-wq2zm
Jun 22 00:06:07.121: INFO: Got endpoints: latency-svc-wq2zm [328.283324ms]
Jun 22 00:06:07.125: INFO: Created: latency-svc-wqb7v
Jun 22 00:06:07.140: INFO: Got endpoints: latency-svc-wqb7v [348.011239ms]
Jun 22 00:06:07.152: INFO: Created: latency-svc-crb7h
Jun 22 00:06:07.174: INFO: Got endpoints: latency-svc-crb7h [381.05792ms]
Jun 22 00:06:07.190: INFO: Created: latency-svc-kskh9
Jun 22 00:06:07.209: INFO: Got endpoints: latency-svc-kskh9 [415.72564ms]
Jun 22 00:06:07.209: INFO: Created: latency-svc-lv8fd
Jun 22 00:06:07.229: INFO: Got endpoints: latency-svc-lv8fd [436.958874ms]
Jun 22 00:06:07.231: INFO: Created: latency-svc-hfpmp
Jun 22 00:06:07.277: INFO: Got endpoints: latency-svc-hfpmp [484.473678ms]
Jun 22 00:06:07.284: INFO: Created: latency-svc-wkwhh
Jun 22 00:06:07.314: INFO: Created: latency-svc-4stwm
Jun 22 00:06:07.316: INFO: Got endpoints: latency-svc-wkwhh [427.782893ms]
Jun 22 00:06:07.340: INFO: Got endpoints: latency-svc-4stwm [410.400785ms]
Jun 22 00:06:07.341: INFO: Created: latency-svc-6nkc9
Jun 22 00:06:07.371: INFO: Got endpoints: latency-svc-6nkc9 [422.028384ms]
Jun 22 00:06:07.396: INFO: Created: latency-svc-hmvks
Jun 22 00:06:07.417: INFO: Got endpoints: latency-svc-hmvks [449.871376ms]
Jun 22 00:06:07.419: INFO: Created: latency-svc-4q6k6
Jun 22 00:06:07.467: INFO: Got endpoints: latency-svc-4q6k6 [468.349217ms]
Jun 22 00:06:07.467: INFO: Created: latency-svc-v6vx4
Jun 22 00:06:07.500: INFO: Got endpoints: latency-svc-v6vx4 [471.877924ms]
Jun 22 00:06:07.503: INFO: Created: latency-svc-ljtj5
Jun 22 00:06:07.526: INFO: Created: latency-svc-8s5j6
Jun 22 00:06:07.557: INFO: Created: latency-svc-s79d2
Jun 22 00:06:07.558: INFO: Got endpoints: latency-svc-8s5j6 [484.91629ms]
Jun 22 00:06:07.559: INFO: Got endpoints: latency-svc-ljtj5 [518.193174ms]
Jun 22 00:06:07.583: INFO: Got endpoints: latency-svc-s79d2 [478.816191ms]
Jun 22 00:06:07.587: INFO: Created: latency-svc-dfn7x
Jun 22 00:06:07.608: INFO: Got endpoints: latency-svc-dfn7x [486.341946ms]
Jun 22 00:06:07.616: INFO: Created: latency-svc-95995
Jun 22 00:06:07.636: INFO: Got endpoints: latency-svc-95995 [495.633469ms]
Jun 22 00:06:07.639: INFO: Created: latency-svc-kh9zj
Jun 22 00:06:07.661: INFO: Got endpoints: latency-svc-kh9zj [486.947646ms]
Jun 22 00:06:07.669: INFO: Created: latency-svc-bgp4m
Jun 22 00:06:07.686: INFO: Got endpoints: latency-svc-bgp4m [477.605301ms]
Jun 22 00:06:07.694: INFO: Created: latency-svc-gs6gv
Jun 22 00:06:07.717: INFO: Got endpoints: latency-svc-gs6gv [487.204201ms]
Jun 22 00:06:07.719: INFO: Created: latency-svc-pk5k4
Jun 22 00:06:07.740: INFO: Created: latency-svc-96qcq
Jun 22 00:06:07.745: INFO: Got endpoints: latency-svc-pk5k4 [467.208169ms]
Jun 22 00:06:07.757: INFO: Got endpoints: latency-svc-96qcq [441.396309ms]
Jun 22 00:06:07.779: INFO: Created: latency-svc-vrgm2
Jun 22 00:06:07.792: INFO: Got endpoints: latency-svc-vrgm2 [451.520268ms]
Jun 22 00:06:07.801: INFO: Created: latency-svc-jw2gs
Jun 22 00:06:07.822: INFO: Got endpoints: latency-svc-jw2gs [451.130153ms]
Jun 22 00:06:07.835: INFO: Created: latency-svc-qvn8j
Jun 22 00:06:07.859: INFO: Got endpoints: latency-svc-qvn8j [442.335682ms]
Jun 22 00:06:07.860: INFO: Created: latency-svc-zl5w9
Jun 22 00:06:07.878: INFO: Got endpoints: latency-svc-zl5w9 [411.265537ms]
Jun 22 00:06:07.881: INFO: Created: latency-svc-dqm58
Jun 22 00:06:07.910: INFO: Created: latency-svc-xnl5z
Jun 22 00:06:07.916: INFO: Got endpoints: latency-svc-dqm58 [415.980472ms]
Jun 22 00:06:07.948: INFO: Got endpoints: latency-svc-xnl5z [389.290332ms]
Jun 22 00:06:07.951: INFO: Created: latency-svc-7x5v8
Jun 22 00:06:07.951: INFO: Got endpoints: latency-svc-7x5v8 [391.941115ms]
Jun 22 00:06:07.956: INFO: Created: latency-svc-9ljwk
Jun 22 00:06:07.979: INFO: Got endpoints: latency-svc-9ljwk [392.38808ms]
Jun 22 00:06:07.990: INFO: Created: latency-svc-btgj4
Jun 22 00:06:08.007: INFO: Got endpoints: latency-svc-btgj4 [399.092558ms]
Jun 22 00:06:08.016: INFO: Created: latency-svc-tq9d7
Jun 22 00:06:08.040: INFO: Got endpoints: latency-svc-tq9d7 [403.978142ms]
Jun 22 00:06:08.053: INFO: Created: latency-svc-d7rcg
Jun 22 00:06:08.081: INFO: Created: latency-svc-npb6k
Jun 22 00:06:08.109: INFO: Got endpoints: latency-svc-d7rcg [447.717956ms]
Jun 22 00:06:08.110: INFO: Created: latency-svc-vz2m2
Jun 22 00:06:08.123: INFO: Got endpoints: latency-svc-npb6k [437.004449ms]
Jun 22 00:06:08.131: INFO: Created: latency-svc-w59qv
Jun 22 00:06:08.139: INFO: Got endpoints: latency-svc-vz2m2 [421.661879ms]
Jun 22 00:06:08.154: INFO: Got endpoints: latency-svc-w59qv [408.710077ms]
Jun 22 00:06:08.162: INFO: Created: latency-svc-clp85
Jun 22 00:06:08.178: INFO: Got endpoints: latency-svc-clp85 [420.070257ms]
Jun 22 00:06:08.187: INFO: Created: latency-svc-glxwc
Jun 22 00:06:08.215: INFO: Got endpoints: latency-svc-glxwc [422.502865ms]
Jun 22 00:06:08.216: INFO: Created: latency-svc-9j69h
Jun 22 00:06:08.246: INFO: Created: latency-svc-rzmpw
Jun 22 00:06:08.254: INFO: Got endpoints: latency-svc-9j69h [431.866419ms]
Jun 22 00:06:08.273: INFO: Created: latency-svc-sfxkz
Jun 22 00:06:08.273: INFO: Got endpoints: latency-svc-rzmpw [414.223831ms]
Jun 22 00:06:08.290: INFO: Got endpoints: latency-svc-sfxkz [412.19475ms]
Jun 22 00:06:08.323: INFO: Created: latency-svc-rfkl9
Jun 22 00:06:08.323: INFO: Got endpoints: latency-svc-rfkl9 [406.809309ms]
Jun 22 00:06:08.324: INFO: Created: latency-svc-pf8k9
Jun 22 00:06:08.352: INFO: Got endpoints: latency-svc-pf8k9 [404.470105ms]
Jun 22 00:06:08.352: INFO: Created: latency-svc-7c8cv
Jun 22 00:06:08.369: INFO: Got endpoints: latency-svc-7c8cv [417.96861ms]
Jun 22 00:06:08.397: INFO: Created: latency-svc-9vmf8
Jun 22 00:06:08.441: INFO: Got endpoints: latency-svc-9vmf8 [461.74926ms]
Jun 22 00:06:08.441: INFO: Created: latency-svc-kkkvl
Jun 22 00:06:08.456: INFO: Got endpoints: latency-svc-kkkvl [448.566406ms]
Jun 22 00:06:08.481: INFO: Created: latency-svc-d8gcn
Jun 22 00:06:08.511: INFO: Created: latency-svc-m654g
Jun 22 00:06:08.513: INFO: Got endpoints: latency-svc-d8gcn [466.825236ms]
Jun 22 00:06:08.520: INFO: Got endpoints: latency-svc-m654g [410.138496ms]
Jun 22 00:06:08.530: INFO: Created: latency-svc-wh6x6
Jun 22 00:06:08.558: INFO: Created: latency-svc-5rz2f
Jun 22 00:06:08.558: INFO: Got endpoints: latency-svc-wh6x6 [434.873948ms]
Jun 22 00:06:08.584: INFO: Got endpoints: latency-svc-5rz2f [444.949155ms]
Jun 22 00:06:08.609: INFO: Created: latency-svc-zdptr
Jun 22 00:06:08.630: INFO: Got endpoints: latency-svc-zdptr [476.202518ms]
Jun 22 00:06:08.643: INFO: Created: latency-svc-9qk64
Jun 22 00:06:08.661: INFO: Got endpoints: latency-svc-9qk64 [483.077848ms]
Jun 22 00:06:08.662: INFO: Created: latency-svc-m4kk5
Jun 22 00:06:08.686: INFO: Created: latency-svc-ddc4s
Jun 22 00:06:08.692: INFO: Got endpoints: latency-svc-m4kk5 [477.627758ms]
Jun 22 00:06:08.702: INFO: Got endpoints: latency-svc-ddc4s [447.712355ms]
Jun 22 00:06:08.709: INFO: Created: latency-svc-q8nvr
Jun 22 00:06:08.736: INFO: Got endpoints: latency-svc-q8nvr [462.375258ms]
Jun 22 00:06:08.739: INFO: Created: latency-svc-hzz8x
Jun 22 00:06:08.766: INFO: Got endpoints: latency-svc-hzz8x [475.539528ms]
Jun 22 00:06:08.773: INFO: Created: latency-svc-mwdmv
Jun 22 00:06:08.790: INFO: Got endpoints: latency-svc-mwdmv [466.793149ms]
Jun 22 00:06:08.805: INFO: Created: latency-svc-6fj98
Jun 22 00:06:08.821: INFO: Got endpoints: latency-svc-6fj98 [468.501654ms]
Jun 22 00:06:08.847: INFO: Created: latency-svc-8t4vq
Jun 22 00:06:08.877: INFO: Got endpoints: latency-svc-8t4vq [508.051525ms]
Jun 22 00:06:08.879: INFO: Created: latency-svc-cg8vl
Jun 22 00:06:08.892: INFO: Got endpoints: latency-svc-cg8vl [450.725422ms]
Jun 22 00:06:08.900: INFO: Created: latency-svc-s49rv
Jun 22 00:06:08.919: INFO: Got endpoints: latency-svc-s49rv [463.838863ms]
Jun 22 00:06:08.924: INFO: Created: latency-svc-zzfj7
Jun 22 00:06:08.951: INFO: Created: latency-svc-pbljz
Jun 22 00:06:08.957: INFO: Got endpoints: latency-svc-zzfj7 [442.835586ms]
Jun 22 00:06:08.985: INFO: Got endpoints: latency-svc-pbljz [465.637734ms]
Jun 22 00:06:08.986: INFO: Created: latency-svc-brfwb
Jun 22 00:06:09.001: INFO: Got endpoints: latency-svc-brfwb [442.051778ms]
Jun 22 00:06:09.036: INFO: Created: latency-svc-pvb2b
Jun 22 00:06:09.054: INFO: Got endpoints: latency-svc-pvb2b [470.07073ms]
Jun 22 00:06:09.063: INFO: Created: latency-svc-7cwz9
Jun 22 00:06:09.093: INFO: Got endpoints: latency-svc-7cwz9 [462.81168ms]
Jun 22 00:06:09.100: INFO: Created: latency-svc-2xgvh
Jun 22 00:06:09.121: INFO: Got endpoints: latency-svc-2xgvh [460.175172ms]
Jun 22 00:06:09.122: INFO: Created: latency-svc-r9tgl
Jun 22 00:06:09.142: INFO: Created: latency-svc-gmsc4
Jun 22 00:06:09.169: INFO: Created: latency-svc-z2wgb
Jun 22 00:06:09.171: INFO: Got endpoints: latency-svc-gmsc4 [70.656788ms]
Jun 22 00:06:09.178: INFO: Got endpoints: latency-svc-r9tgl [485.297338ms]
Jun 22 00:06:09.191: INFO: Got endpoints: latency-svc-z2wgb [489.109342ms]
Jun 22 00:06:09.194: INFO: Created: latency-svc-49nvd
Jun 22 00:06:09.214: INFO: Got endpoints: latency-svc-49nvd [478.43156ms]
Jun 22 00:06:09.222: INFO: Created: latency-svc-zd97r
Jun 22 00:06:09.242: INFO: Got endpoints: latency-svc-zd97r [475.895283ms]
Jun 22 00:06:09.271: INFO: Created: latency-svc-zntrh
Jun 22 00:06:09.290: INFO: Got endpoints: latency-svc-zntrh [499.988167ms]
Jun 22 00:06:09.301: INFO: Created: latency-svc-ttfmn
Jun 22 00:06:09.318: INFO: Got endpoints: latency-svc-ttfmn [497.174659ms]
Jun 22 00:06:09.334: INFO: Created: latency-svc-d5b4v
Jun 22 00:06:09.353: INFO: Got endpoints: latency-svc-d5b4v [474.354944ms]
Jun 22 00:06:09.361: INFO: Created: latency-svc-kj7nr
Jun 22 00:06:09.404: INFO: Got endpoints: latency-svc-kj7nr [511.263658ms]
Jun 22 00:06:09.408: INFO: Created: latency-svc-rpqnk
Jun 22 00:06:09.428: INFO: Got endpoints: latency-svc-rpqnk [508.641168ms]
Jun 22 00:06:09.437: INFO: Created: latency-svc-rwsrl
Jun 22 00:06:09.457: INFO: Got endpoints: latency-svc-rwsrl [500.775032ms]
Jun 22 00:06:09.490: INFO: Created: latency-svc-92j8d
Jun 22 00:06:09.496: INFO: Got endpoints: latency-svc-92j8d [510.828582ms]
Jun 22 00:06:09.522: INFO: Created: latency-svc-gzpft
Jun 22 00:06:09.561: INFO: Got endpoints: latency-svc-gzpft [560.36534ms]
Jun 22 00:06:09.569: INFO: Created: latency-svc-l4sk8
Jun 22 00:06:09.590: INFO: Created: latency-svc-dvcnh
Jun 22 00:06:09.591: INFO: Got endpoints: latency-svc-l4sk8 [536.588965ms]
Jun 22 00:06:09.613: INFO: Got endpoints: latency-svc-dvcnh [491.85982ms]
Jun 22 00:06:09.617: INFO: Created: latency-svc-8mnp4
Jun 22 00:06:09.642: INFO: Got endpoints: latency-svc-8mnp4 [470.410551ms]
Jun 22 00:06:09.645: INFO: Created: latency-svc-hb4g2
Jun 22 00:06:09.665: INFO: Got endpoints: latency-svc-hb4g2 [487.233723ms]
Jun 22 00:06:09.671: INFO: Created: latency-svc-7tvjw
Jun 22 00:06:09.687: INFO: Got endpoints: latency-svc-7tvjw [495.789901ms]
Jun 22 00:06:09.693: INFO: Created: latency-svc-kk256
Jun 22 00:06:09.730: INFO: Got endpoints: latency-svc-kk256 [514.001538ms]
Jun 22 00:06:09.755: INFO: Created: latency-svc-5nwwz
Jun 22 00:06:09.771: INFO: Got endpoints: latency-svc-5nwwz [528.43461ms]
Jun 22 00:06:09.792: INFO: Created: latency-svc-g5zgf
Jun 22 00:06:09.827: INFO: Created: latency-svc-j2hjz
Jun 22 00:06:09.828: INFO: Got endpoints: latency-svc-g5zgf [536.93333ms]
Jun 22 00:06:09.869: INFO: Created: latency-svc-qkrbz
Jun 22 00:06:09.868: INFO: Got endpoints: latency-svc-j2hjz [549.805473ms]
Jun 22 00:06:09.892: INFO: Got endpoints: latency-svc-qkrbz [539.236659ms]
Jun 22 00:06:09.906: INFO: Created: latency-svc-k4ptq
Jun 22 00:06:09.927: INFO: Got endpoints: latency-svc-k4ptq [522.777207ms]
Jun 22 00:06:09.932: INFO: Created: latency-svc-drl7r
Jun 22 00:06:09.963: INFO: Created: latency-svc-w2mrm
Jun 22 00:06:09.963: INFO: Got endpoints: latency-svc-drl7r [534.85819ms]
Jun 22 00:06:09.995: INFO: Created: latency-svc-d9zqr
Jun 22 00:06:09.996: INFO: Got endpoints: latency-svc-w2mrm [538.590538ms]
Jun 22 00:06:10.018: INFO: Created: latency-svc-9d8rt
Jun 22 00:06:10.018: INFO: Got endpoints: latency-svc-d9zqr [520.691332ms]
Jun 22 00:06:10.036: INFO: Got endpoints: latency-svc-9d8rt [475.033603ms]
Jun 22 00:06:10.042: INFO: Created: latency-svc-b6bs5
Jun 22 00:06:10.057: INFO: Got endpoints: latency-svc-b6bs5 [466.623683ms]
Jun 22 00:06:10.077: INFO: Created: latency-svc-p6dxv
Jun 22 00:06:10.117: INFO: Got endpoints: latency-svc-p6dxv [501.795076ms]
Jun 22 00:06:10.119: INFO: Created: latency-svc-fw4cr
Jun 22 00:06:10.145: INFO: Got endpoints: latency-svc-fw4cr [501.445831ms]
Jun 22 00:06:10.148: INFO: Created: latency-svc-brj24
Jun 22 00:06:10.177: INFO: Got endpoints: latency-svc-brj24 [511.389476ms]
Jun 22 00:06:10.182: INFO: Created: latency-svc-rcsgj
Jun 22 00:06:10.211: INFO: Created: latency-svc-xp4qt
Jun 22 00:06:10.224: INFO: Got endpoints: latency-svc-rcsgj [535.684538ms]
Jun 22 00:06:10.232: INFO: Created: latency-svc-w26z9
Jun 22 00:06:10.234: INFO: Got endpoints: latency-svc-xp4qt [504.563255ms]
Jun 22 00:06:10.253: INFO: Got endpoints: latency-svc-w26z9 [481.406474ms]
Jun 22 00:06:10.261: INFO: Created: latency-svc-dt9mf
Jun 22 00:06:10.293: INFO: Created: latency-svc-kdbpf
Jun 22 00:06:10.320: INFO: Created: latency-svc-dkc98
Jun 22 00:06:10.322: INFO: Got endpoints: latency-svc-dt9mf [494.049457ms]
Jun 22 00:06:10.323: INFO: Got endpoints: latency-svc-kdbpf [452.205268ms]
Jun 22 00:06:10.359: INFO: Got endpoints: latency-svc-dkc98 [466.759419ms]
Jun 22 00:06:10.364: INFO: Created: latency-svc-9462f
Jun 22 00:06:10.390: INFO: Created: latency-svc-p9p8j
Jun 22 00:06:10.402: INFO: Got endpoints: latency-svc-9462f [474.790638ms]
Jun 22 00:06:10.414: INFO: Got endpoints: latency-svc-p9p8j [450.08618ms]
Jun 22 00:06:10.421: INFO: Created: latency-svc-gk74p
Jun 22 00:06:10.440: INFO: Got endpoints: latency-svc-gk74p [443.798021ms]
Jun 22 00:06:10.449: INFO: Created: latency-svc-fss72
Jun 22 00:06:10.469: INFO: Got endpoints: latency-svc-fss72 [450.467838ms]
Jun 22 00:06:10.473: INFO: Created: latency-svc-zlngt
Jun 22 00:06:10.491: INFO: Got endpoints: latency-svc-zlngt [455.107423ms]
Jun 22 00:06:10.496: INFO: Created: latency-svc-ln6xb
Jun 22 00:06:10.525: INFO: Got endpoints: latency-svc-ln6xb [467.196563ms]
Jun 22 00:06:10.525: INFO: Created: latency-svc-dl576
Jun 22 00:06:10.555: INFO: Got endpoints: latency-svc-dl576 [438.192608ms]
Jun 22 00:06:10.567: INFO: Created: latency-svc-mcsrj
Jun 22 00:06:10.597: INFO: Created: latency-svc-clx5r
Jun 22 00:06:10.638: INFO: Created: latency-svc-kt4vd
Jun 22 00:06:10.638: INFO: Got endpoints: latency-svc-clx5r [452.037648ms]
Jun 22 00:06:10.638: INFO: Got endpoints: latency-svc-mcsrj [493.046248ms]
Jun 22 00:06:10.670: INFO: Got endpoints: latency-svc-kt4vd [445.91786ms]
Jun 22 00:06:10.687: INFO: Created: latency-svc-dntqx
Jun 22 00:06:10.717: INFO: Got endpoints: latency-svc-dntqx [483.024605ms]
Jun 22 00:06:10.731: INFO: Created: latency-svc-fjhfj
Jun 22 00:06:10.763: INFO: Got endpoints: latency-svc-fjhfj [509.700283ms]
Jun 22 00:06:10.765: INFO: Created: latency-svc-wn4lb
Jun 22 00:06:10.784: INFO: Got endpoints: latency-svc-wn4lb [461.722197ms]
Jun 22 00:06:10.798: INFO: Created: latency-svc-9swvg
Jun 22 00:06:10.832: INFO: Got endpoints: latency-svc-9swvg [509.380501ms]
Jun 22 00:06:10.833: INFO: Created: latency-svc-g2pkz
Jun 22 00:06:10.850: INFO: Got endpoints: latency-svc-g2pkz [490.744118ms]
Jun 22 00:06:10.862: INFO: Created: latency-svc-7xd7t
Jun 22 00:06:10.883: INFO: Got endpoints: latency-svc-7xd7t [481.202827ms]
Jun 22 00:06:10.887: INFO: Created: latency-svc-fqf72
Jun 22 00:06:10.913: INFO: Got endpoints: latency-svc-fqf72 [499.322356ms]
Jun 22 00:06:10.914: INFO: Created: latency-svc-77dzn
Jun 22 00:06:10.946: INFO: Got endpoints: latency-svc-77dzn [505.804313ms]
Jun 22 00:06:10.960: INFO: Created: latency-svc-ldfjq
Jun 22 00:06:10.970: INFO: Got endpoints: latency-svc-ldfjq [501.208692ms]
Jun 22 00:06:10.985: INFO: Created: latency-svc-lm8cz
Jun 22 00:06:11.007: INFO: Created: latency-svc-78m4w
Jun 22 00:06:11.036: INFO: Got endpoints: latency-svc-78m4w [511.108409ms]
Jun 22 00:06:11.036: INFO: Got endpoints: latency-svc-lm8cz [544.887225ms]
Jun 22 00:06:11.070: INFO: Created: latency-svc-s5chz
Jun 22 00:06:11.096: INFO: Got endpoints: latency-svc-s5chz [540.355179ms]
Jun 22 00:06:11.122: INFO: Created: latency-svc-wkx7f
Jun 22 00:06:11.142: INFO: Got endpoints: latency-svc-wkx7f [500.595035ms]
Jun 22 00:06:11.178: INFO: Created: latency-svc-4h8kx
Jun 22 00:06:11.187: INFO: Got endpoints: latency-svc-4h8kx [544.211282ms]
Jun 22 00:06:11.204: INFO: Created: latency-svc-qsrl9
Jun 22 00:06:11.249: INFO: Created: latency-svc-6zfwk
Jun 22 00:06:11.261: INFO: Got endpoints: latency-svc-qsrl9 [590.91255ms]
Jun 22 00:06:11.290: INFO: Created: latency-svc-jbjvm
Jun 22 00:06:11.302: INFO: Got endpoints: latency-svc-6zfwk [584.005637ms]
Jun 22 00:06:11.304: INFO: Got endpoints: latency-svc-jbjvm [541.629485ms]
Jun 22 00:06:11.325: INFO: Created: latency-svc-c8s97
Jun 22 00:06:11.359: INFO: Got endpoints: latency-svc-c8s97 [575.063153ms]
Jun 22 00:06:11.366: INFO: Created: latency-svc-b62gj
Jun 22 00:06:11.397: INFO: Created: latency-svc-vqjnw
Jun 22 00:06:11.400: INFO: Got endpoints: latency-svc-b62gj [567.802655ms]
Jun 22 00:06:11.418: INFO: Got endpoints: latency-svc-vqjnw [567.64506ms]
Jun 22 00:06:11.433: INFO: Created: latency-svc-lt5jp
Jun 22 00:06:11.449: INFO: Got endpoints: latency-svc-lt5jp [565.450734ms]
Jun 22 00:06:11.459: INFO: Created: latency-svc-x4j6w
Jun 22 00:06:11.478: INFO: Got endpoints: latency-svc-x4j6w [564.208569ms]
Jun 22 00:06:11.508: INFO: Created: latency-svc-lrssl
Jun 22 00:06:11.542: INFO: Got endpoints: latency-svc-lrssl [594.772888ms]
Jun 22 00:06:11.544: INFO: Created: latency-svc-pdplf
Jun 22 00:06:11.559: INFO: Created: latency-svc-7t8nh
Jun 22 00:06:11.580: INFO: Got endpoints: latency-svc-pdplf [609.676904ms]
Jun 22 00:06:11.608: INFO: Got endpoints: latency-svc-7t8nh [567.790539ms]
Jun 22 00:06:11.624: INFO: Created: latency-svc-tkv8m
Jun 22 00:06:11.642: INFO: Got endpoints: latency-svc-tkv8m [602.064242ms]
Jun 22 00:06:11.661: INFO: Created: latency-svc-fplts
Jun 22 00:06:11.685: INFO: Got endpoints: latency-svc-fplts [581.271213ms]
Jun 22 00:06:11.695: INFO: Created: latency-svc-6bnpb
Jun 22 00:06:11.720: INFO: Created: latency-svc-x4gjg
Jun 22 00:06:11.758: INFO: Got endpoints: latency-svc-6bnpb [615.54597ms]
Jun 22 00:06:11.759: INFO: Got endpoints: latency-svc-x4gjg [572.594609ms]
Jun 22 00:06:11.786: INFO: Created: latency-svc-f7fdv
Jun 22 00:06:11.811: INFO: Got endpoints: latency-svc-f7fdv [549.646614ms]
Jun 22 00:06:11.828: INFO: Created: latency-svc-jg6jk
Jun 22 00:06:11.865: INFO: Got endpoints: latency-svc-jg6jk [563.568848ms]
Jun 22 00:06:11.889: INFO: Created: latency-svc-kvhp9
Jun 22 00:06:11.925: INFO: Got endpoints: latency-svc-kvhp9 [620.976934ms]
Jun 22 00:06:11.934: INFO: Created: latency-svc-64nd5
Jun 22 00:06:11.968: INFO: Got endpoints: latency-svc-64nd5 [608.974865ms]
Jun 22 00:06:11.980: INFO: Created: latency-svc-v4mmf
Jun 22 00:06:12.034: INFO: Created: latency-svc-l7vnl
Jun 22 00:06:12.034: INFO: Got endpoints: latency-svc-v4mmf [633.685866ms]
Jun 22 00:06:12.089: INFO: Got endpoints: latency-svc-l7vnl [670.240657ms]
Jun 22 00:06:12.090: INFO: Created: latency-svc-64zzp
Jun 22 00:06:12.151: INFO: Got endpoints: latency-svc-64zzp [702.378683ms]
Jun 22 00:06:12.158: INFO: Created: latency-svc-gsvlt
Jun 22 00:06:12.183: INFO: Got endpoints: latency-svc-gsvlt [704.835759ms]
Jun 22 00:06:12.187: INFO: Created: latency-svc-jq8lk
Jun 22 00:06:12.241: INFO: Got endpoints: latency-svc-jq8lk [698.103975ms]
Jun 22 00:06:12.245: INFO: Created: latency-svc-gkdcv
Jun 22 00:06:12.264: INFO: Got endpoints: latency-svc-gkdcv [683.263581ms]
Jun 22 00:06:12.305: INFO: Created: latency-svc-d4f2z
Jun 22 00:06:12.324: INFO: Got endpoints: latency-svc-d4f2z [716.432857ms]
Jun 22 00:06:12.342: INFO: Created: latency-svc-zrwvv
Jun 22 00:06:12.364: INFO: Got endpoints: latency-svc-zrwvv [721.449809ms]
Jun 22 00:06:12.374: INFO: Created: latency-svc-jwnhb
Jun 22 00:06:12.394: INFO: Got endpoints: latency-svc-jwnhb [707.974115ms]
Jun 22 00:06:12.394: INFO: Created: latency-svc-69l2n
Jun 22 00:06:12.413: INFO: Created: latency-svc-dkwjq
Jun 22 00:06:12.414: INFO: Got endpoints: latency-svc-69l2n [655.102314ms]
Jun 22 00:06:12.436: INFO: Got endpoints: latency-svc-dkwjq [676.223633ms]
Jun 22 00:06:12.447: INFO: Created: latency-svc-ss8qs
Jun 22 00:06:12.472: INFO: Got endpoints: latency-svc-ss8qs [658.407332ms]
Jun 22 00:06:12.478: INFO: Created: latency-svc-7zhfp
Jun 22 00:06:12.494: INFO: Created: latency-svc-gpgcr
Jun 22 00:06:12.527: INFO: Created: latency-svc-6g8b2
Jun 22 00:06:12.537: INFO: Got endpoints: latency-svc-gpgcr [611.241375ms]
Jun 22 00:06:12.544: INFO: Got endpoints: latency-svc-7zhfp [678.305899ms]
Jun 22 00:06:12.544: INFO: Got endpoints: latency-svc-6g8b2 [576.109436ms]
Jun 22 00:06:12.546: INFO: Created: latency-svc-j8d47
Jun 22 00:06:12.572: INFO: Got endpoints: latency-svc-j8d47 [537.406135ms]
Jun 22 00:06:12.574: INFO: Created: latency-svc-9jg5p
Jun 22 00:06:12.599: INFO: Got endpoints: latency-svc-9jg5p [509.739382ms]
Jun 22 00:06:12.605: INFO: Created: latency-svc-vrww4
Jun 22 00:06:12.630: INFO: Got endpoints: latency-svc-vrww4 [478.388636ms]
Jun 22 00:06:12.631: INFO: Created: latency-svc-q7hz2
Jun 22 00:06:12.659: INFO: Got endpoints: latency-svc-q7hz2 [475.941376ms]
Jun 22 00:06:12.666: INFO: Created: latency-svc-nnqqp
Jun 22 00:06:12.689: INFO: Created: latency-svc-cnxd9
Jun 22 00:06:12.689: INFO: Got endpoints: latency-svc-nnqqp [442.53535ms]
Jun 22 00:06:12.716: INFO: Created: latency-svc-6blxp
Jun 22 00:06:12.721: INFO: Got endpoints: latency-svc-cnxd9 [457.448752ms]
Jun 22 00:06:12.740: INFO: Got endpoints: latency-svc-6blxp [415.439609ms]
Jun 22 00:06:12.746: INFO: Created: latency-svc-h7nqt
Jun 22 00:06:12.770: INFO: Got endpoints: latency-svc-h7nqt [405.774714ms]
Jun 22 00:06:12.772: INFO: Created: latency-svc-xkczv
Jun 22 00:06:12.797: INFO: Got endpoints: latency-svc-xkczv [403.52766ms]
Jun 22 00:06:12.805: INFO: Created: latency-svc-6rlgw
Jun 22 00:06:12.829: INFO: Got endpoints: latency-svc-6rlgw [415.48711ms]
Jun 22 00:06:12.869: INFO: Created: latency-svc-4w754
Jun 22 00:06:12.884: INFO: Got endpoints: latency-svc-4w754 [447.823161ms]
Jun 22 00:06:12.897: INFO: Created: latency-svc-rx6fr
Jun 22 00:06:12.917: INFO: Got endpoints: latency-svc-rx6fr [445.426979ms]
Jun 22 00:06:12.945: INFO: Created: latency-svc-5n9x6
Jun 22 00:06:12.964: INFO: Got endpoints: latency-svc-5n9x6 [427.529685ms]
Jun 22 00:06:12.965: INFO: Created: latency-svc-8fzf2
Jun 22 00:06:12.982: INFO: Got endpoints: latency-svc-8fzf2 [438.494245ms]
Jun 22 00:06:12.989: INFO: Created: latency-svc-zqqb5
Jun 22 00:06:13.014: INFO: Created: latency-svc-n7wtk
Jun 22 00:06:13.046: INFO: Created: latency-svc-jlzp5
Jun 22 00:06:13.063: INFO: Created: latency-svc-r9nrv
Jun 22 00:06:13.098: INFO: Got endpoints: latency-svc-jlzp5 [498.621839ms]
Jun 22 00:06:13.098: INFO: Got endpoints: latency-svc-zqqb5 [553.381202ms]
Jun 22 00:06:13.098: INFO: Got endpoints: latency-svc-n7wtk [526.033326ms]
Jun 22 00:06:13.098: INFO: Created: latency-svc-g77fv
Jun 22 00:06:13.102: INFO: Got endpoints: latency-svc-r9nrv [472.195259ms]
Jun 22 00:06:13.134: INFO: Got endpoints: latency-svc-g77fv [475.085795ms]
Jun 22 00:06:13.135: INFO: Created: latency-svc-brdr5
Jun 22 00:06:13.157: INFO: Got endpoints: latency-svc-brdr5 [467.814808ms]
Jun 22 00:06:13.158: INFO: Created: latency-svc-vllvb
Jun 22 00:06:13.178: INFO: Created: latency-svc-6xs4p
Jun 22 00:06:13.182: INFO: Got endpoints: latency-svc-vllvb [460.167935ms]
Jun 22 00:06:13.205: INFO: Created: latency-svc-j7g87
Jun 22 00:06:13.207: INFO: Got endpoints: latency-svc-6xs4p [466.657687ms]
Jun 22 00:06:13.234: INFO: Got endpoints: latency-svc-j7g87 [464.426334ms]
Jun 22 00:06:13.235: INFO: Created: latency-svc-8c4fd
Jun 22 00:06:13.251: INFO: Got endpoints: latency-svc-8c4fd [454.0936ms]
Jun 22 00:06:13.257: INFO: Created: latency-svc-sxsq9
Jun 22 00:06:13.282: INFO: Got endpoints: latency-svc-sxsq9 [452.248669ms]
Jun 22 00:06:13.285: INFO: Created: latency-svc-xgldn
Jun 22 00:06:13.315: INFO: Got endpoints: latency-svc-xgldn [430.523194ms]
Jun 22 00:06:13.316: INFO: Created: latency-svc-mfckm
Jun 22 00:06:13.345: INFO: Got endpoints: latency-svc-mfckm [427.10993ms]
Jun 22 00:06:13.345: INFO: Created: latency-svc-5lrhv
Jun 22 00:06:13.364: INFO: Got endpoints: latency-svc-5lrhv [399.248771ms]
Jun 22 00:06:13.364: INFO: Latencies: [70.656788ms 96.55954ms 138.135151ms 157.225205ms 175.629184ms 205.775699ms 235.691743ms 248.267846ms 280.924929ms 311.110443ms 328.283324ms 348.011239ms 381.05792ms 389.290332ms 391.941115ms 392.38808ms 399.092558ms 399.248771ms 403.52766ms 403.978142ms 404.470105ms 405.774714ms 406.809309ms 408.710077ms 410.138496ms 410.400785ms 411.265537ms 412.19475ms 414.223831ms 415.439609ms 415.48711ms 415.72564ms 415.980472ms 417.96861ms 420.070257ms 421.661879ms 422.028384ms 422.502865ms 427.10993ms 427.529685ms 427.782893ms 430.523194ms 431.866419ms 434.873948ms 436.958874ms 437.004449ms 438.192608ms 438.494245ms 441.396309ms 442.051778ms 442.335682ms 442.53535ms 442.835586ms 443.798021ms 444.949155ms 445.426979ms 445.91786ms 447.712355ms 447.717956ms 447.823161ms 448.566406ms 449.871376ms 450.08618ms 450.467838ms 450.725422ms 451.130153ms 451.520268ms 452.037648ms 452.205268ms 452.248669ms 454.0936ms 455.107423ms 457.448752ms 460.167935ms 460.175172ms 461.722197ms 461.74926ms 462.375258ms 462.81168ms 463.838863ms 464.426334ms 465.637734ms 466.623683ms 466.657687ms 466.759419ms 466.793149ms 466.825236ms 467.196563ms 467.208169ms 467.814808ms 468.349217ms 468.501654ms 470.07073ms 470.410551ms 471.877924ms 472.195259ms 474.354944ms 474.790638ms 475.033603ms 475.085795ms 475.539528ms 475.895283ms 475.941376ms 476.202518ms 477.605301ms 477.627758ms 478.388636ms 478.43156ms 478.816191ms 481.202827ms 481.406474ms 483.024605ms 483.077848ms 484.473678ms 484.91629ms 485.297338ms 486.341946ms 486.947646ms 487.204201ms 487.233723ms 489.109342ms 490.744118ms 491.85982ms 493.046248ms 494.049457ms 495.633469ms 495.789901ms 497.174659ms 498.621839ms 499.322356ms 499.988167ms 500.595035ms 500.775032ms 501.208692ms 501.445831ms 501.795076ms 504.563255ms 505.804313ms 508.051525ms 508.641168ms 509.380501ms 509.700283ms 509.739382ms 510.828582ms 511.108409ms 511.263658ms 511.389476ms 514.001538ms 518.193174ms 520.691332ms 522.777207ms 526.033326ms 528.43461ms 534.85819ms 535.684538ms 536.588965ms 536.93333ms 537.406135ms 538.590538ms 539.236659ms 540.355179ms 541.629485ms 544.211282ms 544.887225ms 549.646614ms 549.805473ms 553.381202ms 560.36534ms 563.568848ms 564.208569ms 565.450734ms 567.64506ms 567.790539ms 567.802655ms 572.594609ms 575.063153ms 576.109436ms 581.271213ms 584.005637ms 590.91255ms 594.772888ms 602.064242ms 608.974865ms 609.676904ms 611.241375ms 615.54597ms 620.976934ms 633.685866ms 655.102314ms 658.407332ms 670.240657ms 676.223633ms 678.305899ms 683.263581ms 698.103975ms 702.378683ms 704.835759ms 707.974115ms 716.432857ms 721.449809ms]
Jun 22 00:06:13.365: INFO: 50 %ile: 475.539528ms
Jun 22 00:06:13.365: INFO: 90 %ile: 594.772888ms
Jun 22 00:06:13.365: INFO: 99 %ile: 716.432857ms
Jun 22 00:06:13.365: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:06:13.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3688" for this suite.

• [SLOW TEST:10.198 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":256,"skipped":4417,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:06:13.422: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-b59308a8-8004-4b47-b71d-8238908d449a
STEP: Creating a pod to test consume secrets
Jun 22 00:06:13.779: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb" in namespace "projected-3280" to be "Succeeded or Failed"
Jun 22 00:06:13.788: INFO: Pod "pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.27847ms
Jun 22 00:06:15.819: INFO: Pod "pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039800002s
Jun 22 00:06:17.837: INFO: Pod "pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05835349s
Jun 22 00:06:19.863: INFO: Pod "pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.084314949s
STEP: Saw pod success
Jun 22 00:06:19.863: INFO: Pod "pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb" satisfied condition "Succeeded or Failed"
Jun 22 00:06:19.877: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 00:06:20.006: INFO: Waiting for pod pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb to disappear
Jun 22 00:06:20.042: INFO: Pod pod-projected-secrets-443693c6-e580-4a7a-9716-53edde2161cb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:06:20.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3280" for this suite.

• [SLOW TEST:6.686 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:06:20.110: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-6588b784-e236-4d99-ab46-1acd8b273dd3
STEP: Creating a pod to test consume configMaps
Jun 22 00:06:20.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e" in namespace "projected-9134" to be "Succeeded or Failed"
Jun 22 00:06:20.517: INFO: Pod "pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.344821ms
Jun 22 00:06:22.539: INFO: Pod "pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035503382s
Jun 22 00:06:24.564: INFO: Pod "pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061125889s
Jun 22 00:06:26.578: INFO: Pod "pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075421233s
STEP: Saw pod success
Jun 22 00:06:26.579: INFO: Pod "pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e" satisfied condition "Succeeded or Failed"
Jun 22 00:06:26.592: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e container agnhost-container: <nil>
STEP: delete the pod
Jun 22 00:06:26.670: INFO: Waiting for pod pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e to disappear
Jun 22 00:06:26.684: INFO: Pod pod-projected-configmaps-ba74996d-ee0d-4c25-ba32-2e003af4d29e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:06:26.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9134" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4443,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:06:26.737: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 22 00:06:27.106: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 00:07:27.482: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jun 22 00:07:27.612: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 22 00:07:27.655: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 22 00:07:27.755: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 22 00:07:27.803: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 22 00:07:27.896: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 22 00:07:27.939: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:07:42.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5509" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:75.710 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":259,"skipped":4458,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:07:42.447: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-466.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-466.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-466.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-466.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 00:07:46.932: INFO: DNS probes using dns-test-c76e1753-82a8-4e9e-9956-cd0fa0d41d11 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-466.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-466.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-466.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-466.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 00:07:51.208: INFO: DNS probes using dns-test-29701152-543c-4717-87e2-96b2f77a783d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-466.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-466.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-466.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-466.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 00:07:55.482: INFO: DNS probes using dns-test-9768fdcb-b41e-4489-aefd-2848c2b395aa succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:07:55.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-466" for this suite.

• [SLOW TEST:13.182 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":260,"skipped":4476,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:07:55.630: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 00:07:56.001: INFO: Waiting up to 5m0s for pod "pod-65b4c340-9c31-4cc7-97a7-794d51829105" in namespace "emptydir-4237" to be "Succeeded or Failed"
Jun 22 00:07:56.019: INFO: Pod "pod-65b4c340-9c31-4cc7-97a7-794d51829105": Phase="Pending", Reason="", readiness=false. Elapsed: 17.085878ms
Jun 22 00:07:58.049: INFO: Pod "pod-65b4c340-9c31-4cc7-97a7-794d51829105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048030488s
Jun 22 00:08:00.063: INFO: Pod "pod-65b4c340-9c31-4cc7-97a7-794d51829105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061955274s
Jun 22 00:08:02.077: INFO: Pod "pod-65b4c340-9c31-4cc7-97a7-794d51829105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075938773s
STEP: Saw pod success
Jun 22 00:08:02.078: INFO: Pod "pod-65b4c340-9c31-4cc7-97a7-794d51829105" satisfied condition "Succeeded or Failed"
Jun 22 00:08:02.089: INFO: Trying to get logs from node 10.10.24.208 pod pod-65b4c340-9c31-4cc7-97a7-794d51829105 container test-container: <nil>
STEP: delete the pod
Jun 22 00:08:02.194: INFO: Waiting for pod pod-65b4c340-9c31-4cc7-97a7-794d51829105 to disappear
Jun 22 00:08:02.206: INFO: Pod pod-65b4c340-9c31-4cc7-97a7-794d51829105 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:02.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4237" for this suite.

• [SLOW TEST:6.636 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4490,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:02.266: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 00:08:03.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 00:08:05.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453283, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453283, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453283, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453283, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 00:08:08.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 22 00:08:08.453: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:08.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5891" for this suite.
STEP: Destroying namespace "webhook-5891-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":262,"skipped":4498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:08.765: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:08:09.028: INFO: Creating deployment "webserver-deployment"
Jun 22 00:08:09.052: INFO: Waiting for observed generation 1
Jun 22 00:08:11.095: INFO: Waiting for all required pods to come up
Jun 22 00:08:11.125: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 22 00:08:13.172: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 22 00:08:13.205: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 22 00:08:13.332: INFO: Updating deployment webserver-deployment
Jun 22 00:08:13.332: INFO: Waiting for observed generation 2
Jun 22 00:08:15.365: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 00:08:15.383: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 00:08:15.410: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 00:08:15.477: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 00:08:15.477: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 00:08:15.502: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 00:08:15.541: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 22 00:08:15.541: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 22 00:08:15.577: INFO: Updating deployment webserver-deployment
Jun 22 00:08:15.577: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 22 00:08:15.651: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 00:08:17.713: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 22 00:08:17.773: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7307  41236ee1-343e-4501-b023-c3c8f7a4b163 115411 3 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00472ac98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-22 00:08:15 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-06-22 00:08:16 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 22 00:08:17.794: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7307  cedeece2-745a-46d7-bf15-8e5cde832a62 115406 3 2022-06-22 00:08:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 41236ee1-343e-4501-b023-c3c8f7a4b163 0xc004912fd7 0xc004912fd8}] []  [{kube-controller-manager Update apps/v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41236ee1-343e-4501-b023-c3c8f7a4b163\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004913078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 00:08:17.794: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 22 00:08:17.794: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-7307  2b805a70-6a69-418e-a688-0736d49039ac 115404 3 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 41236ee1-343e-4501-b023-c3c8f7a4b163 0xc0049130d7 0xc0049130d8}] []  [{kube-controller-manager Update apps/v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41236ee1-343e-4501-b023-c3c8f7a4b163\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004913168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 22 00:08:17.839: INFO: Pod "webserver-deployment-795d758f88-26hkx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-26hkx webserver-deployment-795d758f88- deployment-7307  d8738efa-c402-43ef-8dee-2c25ae893c3b 115302 0 2022-06-22 00:08:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:8bdf39d4b1b81d00cbce891c324c454d89d8b81a203e8de47ed35abdcf17819a cni.projectcalico.org/podIP:172.30.224.53/32 cni.projectcalico.org/podIPs:172.30.224.53/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.53"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.53"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc004913627 0xc004913628}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5b92v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5b92v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.845: INFO: Pod "webserver-deployment-795d758f88-64qc9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-64qc9 webserver-deployment-795d758f88- deployment-7307  36597956-f38a-4e85-923e-cc96826f76cf 115264 0 2022-06-22 00:08:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:10877c3c715e109b81ff7fdf7783faa85599b9f92c8be9855bf88816c14dc68d cni.projectcalico.org/podIP:172.30.47.220/32 cni.projectcalico.org/podIPs:172.30.47.220/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.220"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.220"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc0049138b7 0xc0049138b8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t75kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t75kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.845: INFO: Pod "webserver-deployment-795d758f88-7dlw4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7dlw4 webserver-deployment-795d758f88- deployment-7307  ee9d29e7-7946-43d6-be8f-08c72e04c333 115481 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:0868aee9c126928eee6d2ec5a7435adc94119ffcdeab1e70bb269bf720fa4bc8 cni.projectcalico.org/podIP:172.30.224.52/32 cni.projectcalico.org/podIPs:172.30.224.52/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.52"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.52"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc004913b47 0xc004913b48}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlmnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlmnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.846: INFO: Pod "webserver-deployment-795d758f88-7dmf2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7dmf2 webserver-deployment-795d758f88- deployment-7307  6d6463de-1133-46d3-8cc7-d8a7c000c3dc 115423 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc004913db7 0xc004913db8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbx7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbx7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.847: INFO: Pod "webserver-deployment-795d758f88-7xv25" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7xv25 webserver-deployment-795d758f88- deployment-7307  d08297f0-5752-4e16-919b-1490570ec1c3 115495 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:1769f46d210f9d781784b7749ccdf46d29c4c5ba359aa0655e97aa9bddabe1f3 cni.projectcalico.org/podIP:172.30.96.27/32 cni.projectcalico.org/podIPs:172.30.96.27/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda8007 0xc00bda8008}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsqzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsqzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.849: INFO: Pod "webserver-deployment-795d758f88-8hc7s" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8hc7s webserver-deployment-795d758f88- deployment-7307  14b4aeab-8c4d-49e6-8100-1845000614f1 115282 0 2022-06-22 00:08:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:ad150fae2a6bc11d419fb62d49e3ca7c1b2d639dc906b312a8b9151384373fa9 cni.projectcalico.org/podIP:172.30.224.34/32 cni.projectcalico.org/podIPs:172.30.224.34/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.34"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.34"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda8277 0xc00bda8278}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78dgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78dgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.851: INFO: Pod "webserver-deployment-795d758f88-92sjn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-92sjn webserver-deployment-795d758f88- deployment-7307  bc48f04d-741d-4506-99b1-018db6e87474 115251 0 2022-06-22 00:08:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:1bb8ba741ec148ce1c3dada96c7337bbea578b994c2b490fc04deb9d2586188d cni.projectcalico.org/podIP:172.30.224.49/32 cni.projectcalico.org/podIPs:172.30.224.49/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.49"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.49"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda8507 0xc00bda8508}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txpfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txpfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.851: INFO: Pod "webserver-deployment-795d758f88-cdwp7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cdwp7 webserver-deployment-795d758f88- deployment-7307  34e8f0c2-0270-4179-aff2-846055716cc1 115352 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda8777 0xc00bda8778}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q847m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q847m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.851: INFO: Pod "webserver-deployment-795d758f88-kvtzb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kvtzb webserver-deployment-795d758f88- deployment-7307  cf671c6e-0333-4684-9b3c-93261f275c98 115461 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:c5017ff22a3e891da0e733ecf8462a8648fd7da3bac2f94dbe71a06df851c659 cni.projectcalico.org/podIP:172.30.47.221/32 cni.projectcalico.org/podIPs:172.30.47.221/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.221"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.221"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda8f17 0xc00bda8f18}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6zmf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6zmf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.857: INFO: Pod "webserver-deployment-795d758f88-pqmml" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pqmml webserver-deployment-795d758f88- deployment-7307  3c75b5b8-791f-466a-9853-dab9966420d6 115496 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:8c86d5ca359100bebb52104ab15c22987594a04b6e53a01d3bf367ce6a27b85c cni.projectcalico.org/podIP:172.30.224.10/32 cni.projectcalico.org/podIPs:172.30.224.10/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.10"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.10"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda9487 0xc00bda9488}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqs9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqs9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.858: INFO: Pod "webserver-deployment-795d758f88-r62sc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r62sc webserver-deployment-795d758f88- deployment-7307  23f8eda7-073f-4ae4-b849-8d430c701712 115415 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda9717 0xc00bda9718}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s969x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s969x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.858: INFO: Pod "webserver-deployment-795d758f88-rd6th" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rd6th webserver-deployment-795d758f88- deployment-7307  d6459e27-8cd7-402e-bd7c-a653cdc76f5d 115483 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:2e8227350c1a00efc6f034637f4fc743e77657d0baad2af21d9effa03bdc9f49 cni.projectcalico.org/podIP:172.30.47.223/32 cni.projectcalico.org/podIPs:172.30.47.223/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.223"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.223"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda9997 0xc00bda9998}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfc4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfc4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.858: INFO: Pod "webserver-deployment-795d758f88-zqh8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zqh8z webserver-deployment-795d758f88- deployment-7307  e57aa477-4cc7-425f-838b-db63de4d7799 115287 0 2022-06-22 00:08:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:f9cca7daeb3df530013f84e8151eadba5a7351398e520742292149c290f4c468 cni.projectcalico.org/podIP:172.30.96.26/32 cni.projectcalico.org/podIPs:172.30.96.26/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.26"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.26"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cedeece2-745a-46d7-bf15-8e5cde832a62 0xc00bda9c27 0xc00bda9c28}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cedeece2-745a-46d7-bf15-8e5cde832a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tfrn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tfrn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.858: INFO: Pod "webserver-deployment-847dcfb7fb-5vscf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5vscf webserver-deployment-847dcfb7fb- deployment-7307  1ec9262c-e324-4cbc-b41c-940635fdfaf2 115414 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc00bda9e97 0xc00bda9e98}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2z58c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z58c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.859: INFO: Pod "webserver-deployment-847dcfb7fb-6rcvw" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6rcvw webserver-deployment-847dcfb7fb- deployment-7307  6bc51e83-51e3-4d29-ba1e-4e95752e8ae2 115154 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:999f747831cb21f9bad355a116d54ff8e344ce38afa5b3bd953046ce9d6a6663 cni.projectcalico.org/podIP:172.30.47.218/32 cni.projectcalico.org/podIPs:172.30.47.218/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.218"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.218"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cc0c7 0xc0047cc0c8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clvbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clvbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.218,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://b617595ca2d27494d02d217f90060fa890aa36ada1240978423146226f4939e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.861: INFO: Pod "webserver-deployment-847dcfb7fb-76z8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-76z8r webserver-deployment-847dcfb7fb- deployment-7307  8af45d8a-2aee-4db7-88b9-097a5541fbbf 115484 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:652dcd778bb34ed2a82178fbb935259c1933bbb483ee8e2dac08ea5d99429c94 cni.projectcalico.org/podIP:172.30.96.30/32 cni.projectcalico.org/podIPs:172.30.96.30/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.30"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.30"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cc357 0xc0047cc358}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2rzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2rzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.861: INFO: Pod "webserver-deployment-847dcfb7fb-8454n" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8454n webserver-deployment-847dcfb7fb- deployment-7307  192f4759-b0b5-4b59-8cf2-1e9e813a270b 115422 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cc5a7 0xc0047cc5a8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8v4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8v4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.862: INFO: Pod "webserver-deployment-847dcfb7fb-ghrfk" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ghrfk webserver-deployment-847dcfb7fb- deployment-7307  7e18036c-7d28-4551-9d01-181c6e74752f 115493 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:d45179862a8d87470644c173b17e9641c91a3cbcded229ebac342462c4f93d88 cni.projectcalico.org/podIP:172.30.47.241/32 cni.projectcalico.org/podIPs:172.30.47.241/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cc7d7 0xc0047cc7d8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98w9x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98w9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.862: INFO: Pod "webserver-deployment-847dcfb7fb-hhb6c" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hhb6c webserver-deployment-847dcfb7fb- deployment-7307  ae35322b-f897-4ce2-a5a1-0a89094943c0 115167 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:d2a9f2db4c4e8570964d12ac8bf043c5df15b1f32734cbba6e92a0276c233194 cni.projectcalico.org/podIP:172.30.96.63/32 cni.projectcalico.org/podIPs:172.30.96.63/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.63"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.63"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cca27 0xc0047cca28}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.96.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf2xg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf2xg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:172.30.96.63,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://b3d1b022078aca1c768a00ad377a6d765f5c0e5659b451960b883672ac524cca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.96.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.862: INFO: Pod "webserver-deployment-847dcfb7fb-hmc92" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hmc92 webserver-deployment-847dcfb7fb- deployment-7307  65f02359-b80c-4a7d-92a9-4844ef3c2041 115421 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047ccca7 0xc0047ccca8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgv2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgv2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.862: INFO: Pod "webserver-deployment-847dcfb7fb-ngvnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ngvnq webserver-deployment-847dcfb7fb- deployment-7307  c9fcca5c-e1e4-4a9b-9a4e-3095ad1ad8a0 115416 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cceb7 0xc0047cceb8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9q78j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9q78j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.864: INFO: Pod "webserver-deployment-847dcfb7fb-nktg2" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nktg2 webserver-deployment-847dcfb7fb- deployment-7307  7bb26073-884d-4f18-8bc5-22910bc63fbf 115447 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:d4b269925e56788a1b50a8012560deaa0af75fbbe6d24dd9613e99998133a244 cni.projectcalico.org/podIP:172.30.224.50/32 cni.projectcalico.org/podIPs:172.30.224.50/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.50"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.50"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cd107 0xc0047cd108}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhcwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhcwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.864: INFO: Pod "webserver-deployment-847dcfb7fb-pgk76" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-pgk76 webserver-deployment-847dcfb7fb- deployment-7307  e0abc581-f4aa-4a2d-b956-fe4d9a27e90a 115164 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:7f8c49477db07f9db8ffb50feb8e6a9463da49eedb8a6f4359801cc0e92ab44c cni.projectcalico.org/podIP:172.30.96.16/32 cni.projectcalico.org/podIPs:172.30.96.16/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.16"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.16"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cd387 0xc0047cd388}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.96.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2z2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2z2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:172.30.96.16,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://3688ca23e0cbe588cd199f6402fd7869fb3dcb6e1e2489ebb726e1441b637333,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.96.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.865: INFO: Pod "webserver-deployment-847dcfb7fb-qkf9h" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qkf9h webserver-deployment-847dcfb7fb- deployment-7307  d93dc9e6-4ace-4ee7-bdea-b943ce5e6674 115351 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cd607 0xc0047cd608}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hvcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hvcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.866: INFO: Pod "webserver-deployment-847dcfb7fb-rb2gc" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rb2gc webserver-deployment-847dcfb7fb- deployment-7307  f72ffb80-f6eb-4b3d-a8cc-d7c79dec3428 115379 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cd817 0xc0047cd818}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7rh2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7rh2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.875: INFO: Pod "webserver-deployment-847dcfb7fb-ssxtt" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ssxtt webserver-deployment-847dcfb7fb- deployment-7307  7403686e-63a0-4f2a-86d0-7d565211a340 115143 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:9ea423e8090c65acc011b289f96d66d1be8332213c24c285a2efe064b0d3eb16 cni.projectcalico.org/podIP:172.30.224.48/32 cni.projectcalico.org/podIPs:172.30.224.48/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.48"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.48"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cda47 0xc0047cda48}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsq9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsq9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.48,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://885e4dc9641dd3e442306733beb4d09fa8b6397e30a32c002c749d2337316a13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.875: INFO: Pod "webserver-deployment-847dcfb7fb-svv6s" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-svv6s webserver-deployment-847dcfb7fb- deployment-7307  8dab92ac-e33a-4807-9b44-cd497e06341d 115161 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:d22d9b800298fec92cb3f4f925f3086360c84b011bb3c7216b08ebb8a5fa6b55 cni.projectcalico.org/podIP:172.30.47.219/32 cni.projectcalico.org/podIPs:172.30.47.219/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.219"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.219"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cdcd7 0xc0047cdcd8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrwqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrwqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.219,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://0e21015eded41d61d2cecf9066de5b23321add9f6c52abc62f55bf482909d4a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.876: INFO: Pod "webserver-deployment-847dcfb7fb-t4bxx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-t4bxx webserver-deployment-847dcfb7fb- deployment-7307  586a04f4-ff61-440b-bb6c-806a26c6ad4c 115405 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc0047cdf47 0xc0047cdf48}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xx2gt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xx2gt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.877: INFO: Pod "webserver-deployment-847dcfb7fb-tfzfx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tfzfx webserver-deployment-847dcfb7fb- deployment-7307  9e5a471d-03a2-47b0-ab5c-d2318d53a7f0 115158 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:3685346afe4ed8bbc7d8fd11ae954ee69c423e0b7f912442ff6a09b310e9bc3e cni.projectcalico.org/podIP:172.30.47.216/32 cni.projectcalico.org/podIPs:172.30.47.216/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.216"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.47.216"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc007b081a7 0xc007b081a8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.47.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrg98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrg98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.214,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.214,PodIP:172.30.47.216,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://a51fe68a0679cffc235c90382501cadecef808f8c167493c1fecb3ea984f930c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.47.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.878: INFO: Pod "webserver-deployment-847dcfb7fb-tj5lg" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tj5lg webserver-deployment-847dcfb7fb- deployment-7307  7c0d8a88-5e0e-4187-8182-76cc88f41d9a 115170 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:8988304ff064e8a84fc78095b46852ef5f5a1ef453a3d75b58ada0133a6a21ab cni.projectcalico.org/podIP:172.30.96.20/32 cni.projectcalico.org/podIPs:172.30.96.20/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.20"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.20"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc007b08437 0xc007b08438}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.96.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvrwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvrwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:172.30.96.20,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://e159b4f027551c4cfa3ff29fcc0632888db531be3aaec8d6de93e89efdbe5963,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.96.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.878: INFO: Pod "webserver-deployment-847dcfb7fb-wdp5r" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wdp5r webserver-deployment-847dcfb7fb- deployment-7307  2edf1cbf-458c-43d0-af7f-b84882aab6b2 115149 0 2022-06-22 00:08:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:fc442e0908e35f82e5c2779d6ae1e8a702a1a0703734f724a046e10d1abd69b8 cni.projectcalico.org/podIP:172.30.224.47/32 cni.projectcalico.org/podIPs:172.30.224.47/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.47"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.47"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc007b086c7 0xc007b086c8}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g54ws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g54ws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.47,StartTime:2022-06-22 00:08:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://b1431e76243f5c65a62a83f67da412ab014b57e29f99094e755f0e4ab3daaea9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.878: INFO: Pod "webserver-deployment-847dcfb7fb-z2sc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-z2sc7 webserver-deployment-847dcfb7fb- deployment-7307  857c4029-0893-4a78-a1a8-5e2ef3d480fd 115467 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:38db93ef66cb88705df51c4e10c6ba2b3e415652c954cef92f6d449e5b9f0027 cni.projectcalico.org/podIP:172.30.96.28/32 cni.projectcalico.org/podIPs:172.30.96.28/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.28"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.96.28"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc007b08957 0xc007b08958}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8z6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8z6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.206,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.206,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 00:08:17.879: INFO: Pod "webserver-deployment-847dcfb7fb-zl5wp" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zl5wp webserver-deployment-847dcfb7fb- deployment-7307  6be3d268-2ab9-43f3-b80e-42ab1ace7248 115462 0 2022-06-22 00:08:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:284159a404b4102e2aaf93c3a75bd5425c388002226a23b29efd02df0e1305b8 cni.projectcalico.org/podIP:172.30.224.54/32 cni.projectcalico.org/podIPs:172.30.224.54/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.54"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.54"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2b805a70-6a69-418e-a688-0736d49039ac 0xc007b08f77 0xc007b08f78}] []  [{kube-controller-manager Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b805a70-6a69-418e-a688-0736d49039ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-22 00:08:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvlzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvlzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rb4m5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:08:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:,StartTime:2022-06-22 00:08:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7307" for this suite.

• [SLOW TEST:9.191 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":263,"skipped":4533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:17.966: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 22 00:08:18.327: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 00:08:23.371: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:24.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8479" for this suite.

• [SLOW TEST:6.568 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":264,"skipped":4618,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-d8f3f1fc-752d-420b-9158-ceafca087896
STEP: Creating a pod to test consume configMaps
Jun 22 00:08:24.971: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace" in namespace "projected-6449" to be "Succeeded or Failed"
Jun 22 00:08:24.985: INFO: Pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684655ms
Jun 22 00:08:27.001: INFO: Pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029610387s
Jun 22 00:08:29.026: INFO: Pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054865211s
Jun 22 00:08:31.046: INFO: Pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074333856s
Jun 22 00:08:33.064: INFO: Pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.092849822s
STEP: Saw pod success
Jun 22 00:08:33.065: INFO: Pod "pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace" satisfied condition "Succeeded or Failed"
Jun 22 00:08:33.079: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace container agnhost-container: <nil>
STEP: delete the pod
Jun 22 00:08:33.143: INFO: Waiting for pod pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace to disappear
Jun 22 00:08:33.155: INFO: Pod pod-projected-configmaps-71716f5d-3cba-4fdd-8df9-77321df56ace no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:33.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6449" for this suite.

• [SLOW TEST:8.656 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4618,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 22 00:08:33.538: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:39.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-27" for this suite.

• [SLOW TEST:6.555 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":4638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:39.769: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 00:08:40.025: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 00:08:40.078: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 00:08:40.108: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.206 before test
Jun 22 00:08:40.197: INFO: calico-node-24dvg from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 00:08:40.197: INFO: calico-typha-65cc4575bc-f88zd from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container calico-typha ready: true, restart count 0
Jun 22 00:08:40.197: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-21 21:14:34 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 22 00:08:40.197: INFO: managed-storage-validation-webhooks-695b4c95d9-wsqbq from ibm-odf-validation-webhook started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 22 00:08:40.197: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-hd8s7 from ibm-system started at 2022-06-21 21:19:53 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 22 00:08:40.197: INFO: ibm-keepalived-watcher-rpq7r from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 00:08:40.197: INFO: ibm-master-proxy-static-10.10.24.206 from kube-system started at 2022-06-21 21:08:38 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container pause ready: true, restart count 0
Jun 22 00:08:40.197: INFO: ibmcloud-block-storage-driver-l2pxj from kube-system started at 2022-06-21 21:08:46 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 22 00:08:40.197: INFO: ibmcloud-block-storage-plugin-5469669bb7-f7t75 from kube-system started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 22 00:08:40.197: INFO: vpn-69b96645d-qtgxb from kube-system started at 2022-06-21 21:11:45 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container vpn ready: true, restart count 1
Jun 22 00:08:40.197: INFO: cluster-node-tuning-operator-5f78c6cfc9-6nlpv from openshift-cluster-node-tuning-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 22 00:08:40.197: INFO: tuned-pc9t5 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container tuned ready: true, restart count 0
Jun 22 00:08:40.197: INFO: csi-snapshot-controller-69d5f9c777-p6845 from openshift-cluster-storage-operator started at 2022-06-21 21:13:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 22 00:08:40.197: INFO: csi-snapshot-webhook-55d4797dc4-2c7hn from openshift-cluster-storage-operator started at 2022-06-21 21:12:56 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container webhook ready: true, restart count 0
Jun 22 00:08:40.197: INFO: console-67bfbdb4dc-mtf8v from openshift-console started at 2022-06-21 21:17:04 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container console ready: true, restart count 0
Jun 22 00:08:40.197: INFO: downloads-988dbf8f4-wqhz7 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container download-server ready: true, restart count 0
Jun 22 00:08:40.197: INFO: dns-default-wqq2m from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container dns ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: node-resolver-xjddx from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 22 00:08:40.197: INFO: node-ca-wf6f5 from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container node-ca ready: true, restart count 0
Jun 22 00:08:40.197: INFO: ingress-canary-mcgps from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 22 00:08:40.197: INFO: router-default-bb6fc54c5-hbbwg from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container router ready: true, restart count 0
Jun 22 00:08:40.197: INFO: openshift-kube-proxy-g5jv6 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: migrator-8467484867-zz9sp from openshift-kube-storage-version-migrator started at 2022-06-21 21:12:46 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container migrator ready: true, restart count 0
Jun 22 00:08:40.197: INFO: certified-operators-tkfzk from openshift-marketplace started at 2022-06-21 23:14:01 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:08:40.197: INFO: community-operators-jwwzz from openshift-marketplace started at 2022-06-21 23:14:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:08:40.197: INFO: marketplace-operator-757fc48f95-zzxsj from openshift-marketplace started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 22 00:08:40.197: INFO: redhat-marketplace-4hlf8 from openshift-marketplace started at 2022-06-21 23:14:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:08:40.197: INFO: redhat-operators-2krtf from openshift-marketplace started at 2022-06-21 23:14:01 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:08:40.197: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-06-21 21:16:13 +0000 UTC (5 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container alertmanager ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-06-21 23:14:01 +0000 UTC (5 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container alertmanager ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: grafana-6db9675f77-rv2zg from openshift-monitoring started at 2022-06-21 23:13:47 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container grafana ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: kube-state-metrics-5f9b9688bc-jrnb4 from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 22 00:08:40.197: INFO: node-exporter-xvzpx from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 00:08:40.197: INFO: openshift-state-metrics-744d546498-5g75r from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 22 00:08:40.197: INFO: prometheus-adapter-5c564c9546-wz2lv from openshift-monitoring started at 2022-06-21 21:18:18 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 22 00:08:40.197: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-06-21 21:16:32 +0000 UTC (7 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container prometheus ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 22 00:08:40.197: INFO: prometheus-operator-bf9ff66c-t6r25 from openshift-monitoring started at 2022-06-21 21:12:54 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jun 22 00:08:40.197: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 22 00:08:40.197: INFO: telemeter-client-8dfb6c944-8k9b8 from openshift-monitoring started at 2022-06-21 21:13:35 +0000 UTC (3 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container reload ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 22 00:08:40.197: INFO: thanos-querier-75f4c69596-kmkqk from openshift-monitoring started at 2022-06-21 21:16:16 +0000 UTC (5 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container thanos-query ready: true, restart count 0
Jun 22 00:08:40.197: INFO: multus-additional-cni-plugins-qjdfj from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 22 00:08:40.197: INFO: multus-admission-controller-sndqs from openshift-multus started at 2022-06-21 21:11:41 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 22 00:08:40.197: INFO: multus-c4bqm from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-multus ready: true, restart count 0
Jun 22 00:08:40.197: INFO: network-metrics-daemon-l6zp7 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.197: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 22 00:08:40.197: INFO: network-check-source-778bbb8ccb-g4vw8 from openshift-network-diagnostics started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 22 00:08:40.197: INFO: network-check-target-4dx2q from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 22 00:08:40.197: INFO: catalog-operator-86b47c4c86-vwfqr from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 00:08:40.197: INFO: olm-operator-767cc8584d-5xlcg from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 00:08:40.197: INFO: package-server-manager-bbb676bfb-x766w from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 22 00:08:40.197: INFO: packageserver-cfb4bb4b4-kzchv from openshift-operator-lifecycle-manager started at 2022-06-21 21:13:57 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container packageserver ready: true, restart count 0
Jun 22 00:08:40.197: INFO: service-ca-7f5f4d65b9-gd4rm from openshift-service-ca started at 2022-06-21 21:12:50 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 22 00:08:40.197: INFO: sonobuoy-e2e-job-9869bda7abe94c50 from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.197: INFO: 	Container e2e ready: true, restart count 0
Jun 22 00:08:40.198: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:08:40.198: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-8pkdj from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.198: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:08:40.198: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 00:08:40.198: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.208 before test
Jun 22 00:08:40.249: INFO: calico-node-fjc52 from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 00:08:40.249: INFO: ibm-keepalived-watcher-p6mn9 from kube-system started at 2022-06-21 21:08:40 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 00:08:40.249: INFO: ibm-master-proxy-static-10.10.24.208 from kube-system started at 2022-06-21 21:08:36 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 00:08:40.249: INFO: 	Container pause ready: true, restart count 0
Jun 22 00:08:40.249: INFO: ibmcloud-block-storage-driver-7wzfz from kube-system started at 2022-06-21 21:08:45 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 22 00:08:40.249: INFO: tuned-gxv8t from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container tuned ready: true, restart count 0
Jun 22 00:08:40.249: INFO: dns-default-rlfd7 from openshift-dns started at 2022-06-21 23:57:59 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container dns ready: true, restart count 0
Jun 22 00:08:40.249: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.249: INFO: node-resolver-2fjfv from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.249: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 22 00:08:40.250: INFO: image-pruner-27597600--1-wvhrf from openshift-image-registry started at 2022-06-22 00:00:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container image-pruner ready: false, restart count 0
Jun 22 00:08:40.250: INFO: node-ca-pw5vl from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container node-ca ready: true, restart count 0
Jun 22 00:08:40.250: INFO: registry-pvc-permissions--1-q7rcf from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 22 00:08:40.250: INFO: ingress-canary-rrmfl from openshift-ingress-canary started at 2022-06-21 23:57:40 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 22 00:08:40.250: INFO: openshift-kube-proxy-8gv94 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 00:08:40.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.250: INFO: node-exporter-6th4k from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.250: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 00:08:40.250: INFO: multus-additional-cni-plugins-6pjp8 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 22 00:08:40.250: INFO: multus-admission-controller-x5qvv from openshift-multus started at 2022-06-21 23:58:10 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.250: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 22 00:08:40.250: INFO: multus-bkbkg from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-multus ready: true, restart count 0
Jun 22 00:08:40.250: INFO: network-metrics-daemon-8bt6v from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.250: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 22 00:08:40.250: INFO: network-check-target-smj6m from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 22 00:08:40.250: INFO: collect-profiles-27597600--1-q58vg from openshift-operator-lifecycle-manager started at 2022-06-22 00:00:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 22 00:08:40.250: INFO: sonobuoy from sonobuoy started at 2022-06-21 22:43:11 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 00:08:40.250: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-lrsbr from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:08:40.250: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 00:08:40.250: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.214 before test
Jun 22 00:08:40.326: INFO: calico-kube-controllers-fbfbb867b-m9m79 from calico-system started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 00:08:40.326: INFO: calico-node-lrfbs from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 00:08:40.326: INFO: calico-typha-65cc4575bc-rjddl from calico-system started at 2022-06-21 21:11:06 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container calico-typha ready: true, restart count 0
Jun 22 00:08:40.326: INFO: managed-storage-validation-webhooks-695b4c95d9-xh79f from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 22 00:08:40.326: INFO: managed-storage-validation-webhooks-695b4c95d9-xmrb6 from ibm-odf-validation-webhook started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 22 00:08:40.326: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-pmvzt from ibm-system started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 22 00:08:40.326: INFO: ibm-file-plugin-58d6b698d9-2ghc9 from kube-system started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 00:08:40.326: INFO: ibm-keepalived-watcher-pdlrz from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 00:08:40.326: INFO: ibm-master-proxy-static-10.10.24.214 from kube-system started at 2022-06-21 21:08:39 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.326: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 00:08:40.326: INFO: 	Container pause ready: true, restart count 0
Jun 22 00:08:40.326: INFO: ibm-storage-metrics-agent-5474c86f6c-mkwv2 from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 22 00:08:40.327: INFO: ibm-storage-watcher-79dd544684-s5lkx from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 00:08:40.327: INFO: ibmcloud-block-storage-driver-8w9mb from kube-system started at 2022-06-21 21:08:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 22 00:08:40.327: INFO: tuned-btmj7 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container tuned ready: true, restart count 0
Jun 22 00:08:40.327: INFO: cluster-samples-operator-665576d565-mmcww from openshift-cluster-samples-operator started at 2022-06-21 23:13:47 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 22 00:08:40.327: INFO: cluster-storage-operator-7bf9d85d5d-57gft from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: csi-snapshot-controller-69d5f9c777-c4lj2 from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 22 00:08:40.327: INFO: csi-snapshot-controller-operator-6b5cb9d8d8-bqsbp from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: csi-snapshot-webhook-55d4797dc4-4zxdg from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container webhook ready: true, restart count 0
Jun 22 00:08:40.327: INFO: console-operator-76fb594985-ghsjf from openshift-console-operator started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container console-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: console-67bfbdb4dc-hf9m7 from openshift-console started at 2022-06-21 21:16:16 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container console ready: true, restart count 0
Jun 22 00:08:40.327: INFO: downloads-988dbf8f4-czbb8 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container download-server ready: true, restart count 0
Jun 22 00:08:40.327: INFO: dns-operator-5d76fb554c-mcw6n from openshift-dns-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container dns-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: dns-default-tdvc6 from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container dns ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: node-resolver-qlhv7 from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 22 00:08:40.327: INFO: cluster-image-registry-operator-bf8fb9fdd-m94j9 from openshift-image-registry started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: image-registry-74b8f796c6-gq2tp from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container registry ready: true, restart count 0
Jun 22 00:08:40.327: INFO: node-ca-26tbk from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container node-ca ready: true, restart count 0
Jun 22 00:08:40.327: INFO: ingress-canary-d82cq from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 22 00:08:40.327: INFO: ingress-operator-5cdfdf4d7c-nhksh from openshift-ingress-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: router-default-bb6fc54c5-92jm4 from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container router ready: true, restart count 0
Jun 22 00:08:40.327: INFO: openshift-kube-proxy-brk2v from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: kube-storage-version-migrator-operator-5b447564b8-ss6lx from openshift-kube-storage-version-migrator-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Jun 22 00:08:40.327: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (5 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container alertmanager ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.327: INFO: cluster-monitoring-operator-747998f9d8-tmcgs from openshift-monitoring started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.327: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 22 00:08:40.328: INFO: node-exporter-pk7wt from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 00:08:40.328: INFO: prometheus-adapter-5c564c9546-r9n6b from openshift-monitoring started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 22 00:08:40.328: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-06-21 23:13:59 +0000 UTC (7 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container prometheus ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 22 00:08:40.328: INFO: thanos-querier-75f4c69596-zg5j2 from openshift-monitoring started at 2022-06-21 23:13:47 +0000 UTC (5 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container thanos-query ready: true, restart count 0
Jun 22 00:08:40.328: INFO: multus-additional-cni-plugins-zbkbh from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 22 00:08:40.328: INFO: multus-admission-controller-xxd4n from openshift-multus started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 22 00:08:40.328: INFO: multus-n2tzz from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container kube-multus ready: true, restart count 0
Jun 22 00:08:40.328: INFO: network-metrics-daemon-wvhtq from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 22 00:08:40.328: INFO: network-check-target-4np7s from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 22 00:08:40.328: INFO: network-operator-54477c9fc6-knql5 from openshift-network-operator started at 2022-06-21 21:09:52 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container network-operator ready: true, restart count 0
Jun 22 00:08:40.328: INFO: packageserver-cfb4bb4b4-6xvr5 from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container packageserver ready: true, restart count 0
Jun 22 00:08:40.328: INFO: metrics-6699958ffb-qcvqb from openshift-roks-metrics started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container metrics ready: false, restart count 0
Jun 22 00:08:40.328: INFO: push-gateway-77d69cd6c6-4xbg9 from openshift-roks-metrics started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container push-gateway ready: false, restart count 0
Jun 22 00:08:40.328: INFO: service-ca-operator-6d9bdb775b-ggrqz from openshift-service-ca-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 22 00:08:40.328: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-52nfv from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:08:40.328: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 00:08:40.328: INFO: tigera-operator-844dcd89f8-scnmx from tigera-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:08:40.328: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e4579064-9874-4e2b-be28-31d96f151342 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e4579064-9874-4e2b-be28-31d96f151342 off the node 10.10.24.208
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e4579064-9874-4e2b-be28-31d96f151342
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:08:48.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7858" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:9.021 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":267,"skipped":4666,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:08:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jun 22 00:08:49.010: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jun 22 00:08:49.570: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 00:08:51.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:08:53.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:08:55.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:08:57.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:08:59.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:01.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:03.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:05.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:07.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:09.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:11.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453329, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:09:15.947: INFO: Waited 2.08189819s for the sample-apiserver to be ready to handle requests.
I0622 00:09:17.109790      22 request.go:665] Waited for 1.022487673s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/scheduling.k8s.io/v1?timeout=32s
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jun 22 00:09:17.671: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:09:18.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-591" for this suite.

• [SLOW TEST:29.684 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":268,"skipped":4672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:09:18.481: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:09:23.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4994" for this suite.

• [SLOW TEST:5.530 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":269,"skipped":4698,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:09:24.014: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-0f8b3cb6-3308-49f8-bd20-d2a81927ee9a
STEP: Creating a pod to test consume configMaps
Jun 22 00:09:24.322: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0" in namespace "projected-724" to be "Succeeded or Failed"
Jun 22 00:09:24.344: INFO: Pod "pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.506546ms
Jun 22 00:09:26.367: INFO: Pod "pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044639183s
Jun 22 00:09:28.395: INFO: Pod "pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072973899s
Jun 22 00:09:30.411: INFO: Pod "pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088829633s
STEP: Saw pod success
Jun 22 00:09:30.411: INFO: Pod "pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0" satisfied condition "Succeeded or Failed"
Jun 22 00:09:30.423: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0 container agnhost-container: <nil>
STEP: delete the pod
Jun 22 00:09:30.499: INFO: Waiting for pod pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0 to disappear
Jun 22 00:09:30.519: INFO: Pod pod-projected-configmaps-1ce5ebea-6ca4-451c-af61-2682589eb5c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:09:30.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-724" for this suite.

• [SLOW TEST:6.563 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4712,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:09:30.577: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-gzvc
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 00:09:30.917: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gzvc" in namespace "subpath-1268" to be "Succeeded or Failed"
Jun 22 00:09:30.932: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.175645ms
Jun 22 00:09:32.949: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031501086s
Jun 22 00:09:34.970: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 4.052600847s
Jun 22 00:09:36.992: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 6.074801883s
Jun 22 00:09:39.004: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 8.085864069s
Jun 22 00:09:41.019: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 10.101759113s
Jun 22 00:09:43.038: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 12.120334049s
Jun 22 00:09:45.063: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 14.145723787s
Jun 22 00:09:47.089: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 16.171172972s
Jun 22 00:09:49.107: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 18.189108055s
Jun 22 00:09:51.135: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 20.217581755s
Jun 22 00:09:53.152: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=true. Elapsed: 22.234599344s
Jun 22 00:09:55.169: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Running", Reason="", readiness=false. Elapsed: 24.251663656s
Jun 22 00:09:57.190: INFO: Pod "pod-subpath-test-downwardapi-gzvc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.272424807s
STEP: Saw pod success
Jun 22 00:09:57.190: INFO: Pod "pod-subpath-test-downwardapi-gzvc" satisfied condition "Succeeded or Failed"
Jun 22 00:09:57.212: INFO: Trying to get logs from node 10.10.24.208 pod pod-subpath-test-downwardapi-gzvc container test-container-subpath-downwardapi-gzvc: <nil>
STEP: delete the pod
Jun 22 00:09:57.307: INFO: Waiting for pod pod-subpath-test-downwardapi-gzvc to disappear
Jun 22 00:09:57.322: INFO: Pod pod-subpath-test-downwardapi-gzvc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gzvc
Jun 22 00:09:57.323: INFO: Deleting pod "pod-subpath-test-downwardapi-gzvc" in namespace "subpath-1268"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:09:57.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1268" for this suite.

• [SLOW TEST:26.806 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":271,"skipped":4713,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:09:57.388: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-9f86c18a-62ea-49a5-bff2-b551ce03f0ff
STEP: Creating a pod to test consume configMaps
Jun 22 00:09:57.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9" in namespace "configmap-6848" to be "Succeeded or Failed"
Jun 22 00:09:57.801: INFO: Pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.461374ms
Jun 22 00:09:59.814: INFO: Pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022815183s
Jun 22 00:10:01.859: INFO: Pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067467702s
Jun 22 00:10:03.880: INFO: Pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088015142s
Jun 22 00:10:05.907: INFO: Pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.115606858s
STEP: Saw pod success
Jun 22 00:10:05.908: INFO: Pod "pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9" satisfied condition "Succeeded or Failed"
Jun 22 00:10:05.925: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9 container agnhost-container: <nil>
STEP: delete the pod
Jun 22 00:10:06.005: INFO: Waiting for pod pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9 to disappear
Jun 22 00:10:06.017: INFO: Pod pod-configmaps-a87454e3-7531-45c1-8712-16679abc0fb9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:06.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6848" for this suite.

• [SLOW TEST:8.685 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":4729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:06.073: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jun 22 00:10:06.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 create -f -'
Jun 22 00:10:09.047: INFO: stderr: ""
Jun 22 00:10:09.047: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 00:10:09.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:09.237: INFO: stderr: ""
Jun 22 00:10:09.237: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-scjk8 "
Jun 22 00:10:09.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:09.411: INFO: stderr: ""
Jun 22 00:10:09.411: INFO: stdout: ""
Jun 22 00:10:09.411: INFO: update-demo-nautilus-gk7cc is created but not running
Jun 22 00:10:14.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:14.597: INFO: stderr: ""
Jun 22 00:10:14.597: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-scjk8 "
Jun 22 00:10:14.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:14.741: INFO: stderr: ""
Jun 22 00:10:14.741: INFO: stdout: "true"
Jun 22 00:10:14.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:14.938: INFO: stderr: ""
Jun 22 00:10:14.938: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:14.938: INFO: validating pod update-demo-nautilus-gk7cc
Jun 22 00:10:14.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:14.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:14.968: INFO: update-demo-nautilus-gk7cc is verified up and running
Jun 22 00:10:14.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-scjk8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:15.109: INFO: stderr: ""
Jun 22 00:10:15.109: INFO: stdout: ""
Jun 22 00:10:15.109: INFO: update-demo-nautilus-scjk8 is created but not running
Jun 22 00:10:20.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:20.350: INFO: stderr: ""
Jun 22 00:10:20.350: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-scjk8 "
Jun 22 00:10:20.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:20.515: INFO: stderr: ""
Jun 22 00:10:20.515: INFO: stdout: "true"
Jun 22 00:10:20.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:20.720: INFO: stderr: ""
Jun 22 00:10:20.720: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:20.720: INFO: validating pod update-demo-nautilus-gk7cc
Jun 22 00:10:20.741: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:20.742: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:20.742: INFO: update-demo-nautilus-gk7cc is verified up and running
Jun 22 00:10:20.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-scjk8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:20.958: INFO: stderr: ""
Jun 22 00:10:20.958: INFO: stdout: ""
Jun 22 00:10:20.958: INFO: update-demo-nautilus-scjk8 is created but not running
Jun 22 00:10:25.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:26.120: INFO: stderr: ""
Jun 22 00:10:26.120: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-scjk8 "
Jun 22 00:10:26.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:26.291: INFO: stderr: ""
Jun 22 00:10:26.291: INFO: stdout: "true"
Jun 22 00:10:26.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:26.442: INFO: stderr: ""
Jun 22 00:10:26.443: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:26.443: INFO: validating pod update-demo-nautilus-gk7cc
Jun 22 00:10:26.465: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:26.465: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:26.465: INFO: update-demo-nautilus-gk7cc is verified up and running
Jun 22 00:10:26.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-scjk8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:26.673: INFO: stderr: ""
Jun 22 00:10:26.673: INFO: stdout: "true"
Jun 22 00:10:26.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-scjk8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:26.791: INFO: stderr: ""
Jun 22 00:10:26.791: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:26.791: INFO: validating pod update-demo-nautilus-scjk8
Jun 22 00:10:26.817: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:26.817: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:26.817: INFO: update-demo-nautilus-scjk8 is verified up and running
STEP: scaling down the replication controller
Jun 22 00:10:26.829: INFO: scanned /root for discovery docs: <nil>
Jun 22 00:10:26.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 22 00:10:28.135: INFO: stderr: ""
Jun 22 00:10:28.135: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 00:10:28.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:28.390: INFO: stderr: ""
Jun 22 00:10:28.390: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-scjk8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 00:10:33.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:33.561: INFO: stderr: ""
Jun 22 00:10:33.561: INFO: stdout: "update-demo-nautilus-gk7cc "
Jun 22 00:10:33.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:33.691: INFO: stderr: ""
Jun 22 00:10:33.691: INFO: stdout: "true"
Jun 22 00:10:33.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:33.828: INFO: stderr: ""
Jun 22 00:10:33.828: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:33.829: INFO: validating pod update-demo-nautilus-gk7cc
Jun 22 00:10:33.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:33.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:33.854: INFO: update-demo-nautilus-gk7cc is verified up and running
STEP: scaling up the replication controller
Jun 22 00:10:33.859: INFO: scanned /root for discovery docs: <nil>
Jun 22 00:10:33.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 22 00:10:35.146: INFO: stderr: ""
Jun 22 00:10:35.146: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 00:10:35.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:35.320: INFO: stderr: ""
Jun 22 00:10:35.320: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-z565d "
Jun 22 00:10:35.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:35.506: INFO: stderr: ""
Jun 22 00:10:35.506: INFO: stdout: "true"
Jun 22 00:10:35.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:35.650: INFO: stderr: ""
Jun 22 00:10:35.650: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:35.650: INFO: validating pod update-demo-nautilus-gk7cc
Jun 22 00:10:35.667: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:35.667: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:35.667: INFO: update-demo-nautilus-gk7cc is verified up and running
Jun 22 00:10:35.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-z565d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:35.839: INFO: stderr: ""
Jun 22 00:10:35.839: INFO: stdout: ""
Jun 22 00:10:35.839: INFO: update-demo-nautilus-z565d is created but not running
Jun 22 00:10:40.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 00:10:41.021: INFO: stderr: ""
Jun 22 00:10:41.021: INFO: stdout: "update-demo-nautilus-gk7cc update-demo-nautilus-z565d "
Jun 22 00:10:41.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:41.157: INFO: stderr: ""
Jun 22 00:10:41.157: INFO: stdout: "true"
Jun 22 00:10:41.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-gk7cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:41.319: INFO: stderr: ""
Jun 22 00:10:41.319: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:41.319: INFO: validating pod update-demo-nautilus-gk7cc
Jun 22 00:10:41.344: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:41.344: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:41.344: INFO: update-demo-nautilus-gk7cc is verified up and running
Jun 22 00:10:41.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-z565d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 00:10:41.498: INFO: stderr: ""
Jun 22 00:10:41.498: INFO: stdout: "true"
Jun 22 00:10:41.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods update-demo-nautilus-z565d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 00:10:41.690: INFO: stderr: ""
Jun 22 00:10:41.690: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun 22 00:10:41.690: INFO: validating pod update-demo-nautilus-z565d
Jun 22 00:10:41.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 00:10:41.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 00:10:41.752: INFO: update-demo-nautilus-z565d is verified up and running
STEP: using delete to clean up resources
Jun 22 00:10:41.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 delete --grace-period=0 --force -f -'
Jun 22 00:10:41.922: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 00:10:41.922: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 00:10:41.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get rc,svc -l name=update-demo --no-headers'
Jun 22 00:10:42.094: INFO: stderr: "No resources found in kubectl-2045 namespace.\n"
Jun 22 00:10:42.094: INFO: stdout: ""
Jun 22 00:10:42.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-2045 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 00:10:42.327: INFO: stderr: ""
Jun 22 00:10:42.327: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:42.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2045" for this suite.

• [SLOW TEST:36.314 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":273,"skipped":4760,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:42.389: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:10:42.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e" in namespace "downward-api-6994" to be "Succeeded or Failed"
Jun 22 00:10:42.707: INFO: Pod "downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.827053ms
Jun 22 00:10:44.729: INFO: Pod "downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039161093s
Jun 22 00:10:46.749: INFO: Pod "downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059819466s
Jun 22 00:10:48.780: INFO: Pod "downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090523388s
STEP: Saw pod success
Jun 22 00:10:48.780: INFO: Pod "downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e" satisfied condition "Succeeded or Failed"
Jun 22 00:10:48.794: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e container client-container: <nil>
STEP: delete the pod
Jun 22 00:10:48.872: INFO: Waiting for pod downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e to disappear
Jun 22 00:10:48.884: INFO: Pod downwardapi-volume-2af7e1ce-40e2-4a77-9ba5-ee42d47f585e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:48.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6994" for this suite.

• [SLOW TEST:6.569 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":274,"skipped":4767,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 22 00:10:49.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8670  345412ff-909f-45e1-8ca2-50ceaa0d32f9 118037 0 2022-06-22 00:10:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-22 00:10:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:10:49.371: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8670  345412ff-909f-45e1-8ca2-50ceaa0d32f9 118040 0 2022-06-22 00:10:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-22 00:10:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:49.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8670" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":275,"skipped":4784,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:49.430: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-409681a1-1bc5-4644-b9fe-b52edc6e47cc
STEP: Creating a pod to test consume secrets
Jun 22 00:10:49.797: INFO: Waiting up to 5m0s for pod "pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8" in namespace "secrets-4013" to be "Succeeded or Failed"
Jun 22 00:10:49.839: INFO: Pod "pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8": Phase="Pending", Reason="", readiness=false. Elapsed: 41.837032ms
Jun 22 00:10:51.889: INFO: Pod "pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091757987s
Jun 22 00:10:53.906: INFO: Pod "pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108914646s
Jun 22 00:10:55.936: INFO: Pod "pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.139027673s
STEP: Saw pod success
Jun 22 00:10:55.937: INFO: Pod "pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8" satisfied condition "Succeeded or Failed"
Jun 22 00:10:55.951: INFO: Trying to get logs from node 10.10.24.208 pod pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 00:10:56.076: INFO: Waiting for pod pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8 to disappear
Jun 22 00:10:56.093: INFO: Pod pod-secrets-efb638c3-3e96-4918-9e98-38a5f4a932e8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:56.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4013" for this suite.

• [SLOW TEST:6.711 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:56.143: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:56.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6287" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":277,"skipped":4859,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:56.583: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 22 00:10:57.020: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 22 00:10:57.050: INFO: starting watch
STEP: patching
STEP: updating
Jun 22 00:10:57.198: INFO: waiting for watch events with expected annotations
Jun 22 00:10:57.206: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:10:57.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2015" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":278,"skipped":4870,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:10:57.541: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 00:10:58.558: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 00:11:00.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453458, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453458, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453458, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453458, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 00:11:03.706: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:11:04.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7087" for this suite.
STEP: Destroying namespace "webhook-7087-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.857 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":279,"skipped":4872,"failed":0}
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:11:04.402: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 22 00:11:08.818: INFO: &Pod{ObjectMeta:{send-events-449583c9-3e94-40ba-b8c9-050a4880ab01  events-290  67756c70-3d9d-41b5-99c1-37fbaafa148f 118597 0 2022-06-22 00:11:04 +0000 UTC <nil> <nil> map[name:foo time:676952690] map[cni.projectcalico.org/containerID:7ffc143de15939adb00c9a80bd5dd8375c000f9706eee0d9864565ab84a63c33 cni.projectcalico.org/podIP:172.30.224.25/32 cni.projectcalico.org/podIPs:172.30.224.25/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.25"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.25"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2022-06-22 00:11:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:11:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:11:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:11:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8dxzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8dxzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c62,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:11:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:11:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:11:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:11:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.25,StartTime:2022-06-22 00:11:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:11:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://ce642999d8885fdae2d8d4de9f39f5ddb8427fd27208ca4ec50de382f1d739e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 22 00:11:10.856: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 22 00:11:12.876: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:11:12.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-290" for this suite.

• [SLOW TEST:8.598 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":280,"skipped":4872,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:11:13.000: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun 22 00:11:13.316: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 00:11:13.421: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 00:11:13.467: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.206 before test
Jun 22 00:11:13.534: INFO: calico-node-24dvg from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.535: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 00:11:13.535: INFO: calico-typha-65cc4575bc-f88zd from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.535: INFO: 	Container calico-typha ready: true, restart count 0
Jun 22 00:11:13.535: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-21 21:14:34 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.535: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 22 00:11:13.535: INFO: managed-storage-validation-webhooks-695b4c95d9-wsqbq from ibm-odf-validation-webhook started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.535: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 22 00:11:13.535: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-hd8s7 from ibm-system started at 2022-06-21 21:19:53 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.536: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 22 00:11:13.536: INFO: ibm-keepalived-watcher-rpq7r from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.536: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 00:11:13.536: INFO: ibm-master-proxy-static-10.10.24.206 from kube-system started at 2022-06-21 21:08:38 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.536: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 00:11:13.536: INFO: 	Container pause ready: true, restart count 0
Jun 22 00:11:13.536: INFO: ibmcloud-block-storage-driver-l2pxj from kube-system started at 2022-06-21 21:08:46 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.536: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 22 00:11:13.537: INFO: ibmcloud-block-storage-plugin-5469669bb7-f7t75 from kube-system started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.537: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 22 00:11:13.537: INFO: vpn-69b96645d-qtgxb from kube-system started at 2022-06-21 21:11:45 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.537: INFO: 	Container vpn ready: true, restart count 1
Jun 22 00:11:13.537: INFO: cluster-node-tuning-operator-5f78c6cfc9-6nlpv from openshift-cluster-node-tuning-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.537: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 22 00:11:13.537: INFO: tuned-pc9t5 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.537: INFO: 	Container tuned ready: true, restart count 0
Jun 22 00:11:13.538: INFO: csi-snapshot-controller-69d5f9c777-p6845 from openshift-cluster-storage-operator started at 2022-06-21 21:13:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.538: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 22 00:11:13.538: INFO: csi-snapshot-webhook-55d4797dc4-2c7hn from openshift-cluster-storage-operator started at 2022-06-21 21:12:56 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.538: INFO: 	Container webhook ready: true, restart count 0
Jun 22 00:11:13.538: INFO: console-67bfbdb4dc-mtf8v from openshift-console started at 2022-06-21 21:17:04 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.538: INFO: 	Container console ready: true, restart count 0
Jun 22 00:11:13.538: INFO: downloads-988dbf8f4-wqhz7 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.538: INFO: 	Container download-server ready: true, restart count 0
Jun 22 00:11:13.538: INFO: dns-default-wqq2m from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.539: INFO: 	Container dns ready: true, restart count 0
Jun 22 00:11:13.539: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.539: INFO: node-resolver-xjddx from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.539: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 22 00:11:13.539: INFO: node-ca-wf6f5 from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.539: INFO: 	Container node-ca ready: true, restart count 0
Jun 22 00:11:13.539: INFO: ingress-canary-mcgps from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.539: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 22 00:11:13.539: INFO: router-default-bb6fc54c5-hbbwg from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.540: INFO: 	Container router ready: true, restart count 0
Jun 22 00:11:13.540: INFO: openshift-kube-proxy-g5jv6 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.540: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 00:11:13.540: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.540: INFO: migrator-8467484867-zz9sp from openshift-kube-storage-version-migrator started at 2022-06-21 21:12:46 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.540: INFO: 	Container migrator ready: true, restart count 0
Jun 22 00:11:13.540: INFO: certified-operators-tkfzk from openshift-marketplace started at 2022-06-21 23:14:01 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.540: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:11:13.541: INFO: community-operators-jwwzz from openshift-marketplace started at 2022-06-21 23:14:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.541: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:11:13.541: INFO: marketplace-operator-757fc48f95-zzxsj from openshift-marketplace started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.541: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 22 00:11:13.541: INFO: redhat-marketplace-4hlf8 from openshift-marketplace started at 2022-06-21 23:14:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.541: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:11:13.541: INFO: redhat-operators-2krtf from openshift-marketplace started at 2022-06-21 23:14:01 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.541: INFO: 	Container registry-server ready: true, restart count 0
Jun 22 00:11:13.541: INFO: alertmanager-main-0 from openshift-monitoring started at 2022-06-21 21:16:13 +0000 UTC (5 container statuses recorded)
Jun 22 00:11:13.542: INFO: 	Container alertmanager ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: alertmanager-main-1 from openshift-monitoring started at 2022-06-21 23:14:01 +0000 UTC (5 container statuses recorded)
Jun 22 00:11:13.542: INFO: 	Container alertmanager ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: grafana-6db9675f77-rv2zg from openshift-monitoring started at 2022-06-21 23:13:47 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.542: INFO: 	Container grafana ready: true, restart count 0
Jun 22 00:11:13.542: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun 22 00:11:13.542: INFO: kube-state-metrics-5f9b9688bc-jrnb4 from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 22 00:11:13.543: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 22 00:11:13.543: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 22 00:11:13.543: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 22 00:11:13.543: INFO: node-exporter-xvzpx from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.543: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.543: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 00:11:13.543: INFO: openshift-state-metrics-744d546498-5g75r from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (3 container statuses recorded)
Jun 22 00:11:13.543: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 22 00:11:13.543: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 22 00:11:13.543: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 22 00:11:13.543: INFO: prometheus-adapter-5c564c9546-wz2lv from openshift-monitoring started at 2022-06-21 21:18:18 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.543: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 22 00:11:13.543: INFO: prometheus-k8s-0 from openshift-monitoring started at 2022-06-21 21:16:32 +0000 UTC (7 container statuses recorded)
Jun 22 00:11:13.544: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container prometheus ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 22 00:11:13.544: INFO: prometheus-operator-bf9ff66c-t6r25 from openshift-monitoring started at 2022-06-21 21:12:54 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jun 22 00:11:13.544: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 22 00:11:13.544: INFO: telemeter-client-8dfb6c944-8k9b8 from openshift-monitoring started at 2022-06-21 21:13:35 +0000 UTC (3 container statuses recorded)
Jun 22 00:11:13.544: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container reload ready: true, restart count 0
Jun 22 00:11:13.544: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 22 00:11:13.545: INFO: thanos-querier-75f4c69596-kmkqk from openshift-monitoring started at 2022-06-21 21:16:16 +0000 UTC (5 container statuses recorded)
Jun 22 00:11:13.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.545: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 22 00:11:13.545: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 22 00:11:13.545: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.545: INFO: 	Container thanos-query ready: true, restart count 0
Jun 22 00:11:13.545: INFO: multus-additional-cni-plugins-qjdfj from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.545: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 22 00:11:13.545: INFO: multus-admission-controller-sndqs from openshift-multus started at 2022-06-21 21:11:41 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.545: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 22 00:11:13.545: INFO: multus-c4bqm from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.545: INFO: 	Container kube-multus ready: true, restart count 0
Jun 22 00:11:13.545: INFO: network-metrics-daemon-l6zp7 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.545: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.546: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 22 00:11:13.546: INFO: network-check-source-778bbb8ccb-g4vw8 from openshift-network-diagnostics started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 22 00:11:13.546: INFO: network-check-target-4dx2q from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 22 00:11:13.546: INFO: catalog-operator-86b47c4c86-vwfqr from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 22 00:11:13.546: INFO: olm-operator-767cc8584d-5xlcg from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container olm-operator ready: true, restart count 0
Jun 22 00:11:13.546: INFO: package-server-manager-bbb676bfb-x766w from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 22 00:11:13.546: INFO: packageserver-cfb4bb4b4-kzchv from openshift-operator-lifecycle-manager started at 2022-06-21 21:13:57 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container packageserver ready: true, restart count 0
Jun 22 00:11:13.546: INFO: service-ca-7f5f4d65b9-gd4rm from openshift-service-ca started at 2022-06-21 21:12:50 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.546: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 22 00:11:13.547: INFO: sonobuoy-e2e-job-9869bda7abe94c50 from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.547: INFO: 	Container e2e ready: true, restart count 0
Jun 22 00:11:13.547: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:11:13.547: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-8pkdj from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.547: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:11:13.547: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 00:11:13.547: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.208 before test
Jun 22 00:11:13.606: INFO: calico-node-fjc52 from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 00:11:13.606: INFO: send-events-449583c9-3e94-40ba-b8c9-050a4880ab01 from events-290 started at 2022-06-22 00:11:04 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container p ready: true, restart count 0
Jun 22 00:11:13.606: INFO: ibm-keepalived-watcher-p6mn9 from kube-system started at 2022-06-21 21:08:40 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 00:11:13.606: INFO: ibm-master-proxy-static-10.10.24.208 from kube-system started at 2022-06-21 21:08:36 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container pause ready: true, restart count 0
Jun 22 00:11:13.606: INFO: ibmcloud-block-storage-driver-7wzfz from kube-system started at 2022-06-21 21:08:45 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 22 00:11:13.606: INFO: tuned-gxv8t from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container tuned ready: true, restart count 0
Jun 22 00:11:13.606: INFO: dns-default-rlfd7 from openshift-dns started at 2022-06-21 23:57:59 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container dns ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: node-resolver-2fjfv from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 22 00:11:13.606: INFO: image-pruner-27597600--1-wvhrf from openshift-image-registry started at 2022-06-22 00:00:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container image-pruner ready: false, restart count 0
Jun 22 00:11:13.606: INFO: node-ca-pw5vl from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container node-ca ready: true, restart count 0
Jun 22 00:11:13.606: INFO: registry-pvc-permissions--1-q7rcf from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 22 00:11:13.606: INFO: ingress-canary-rrmfl from openshift-ingress-canary started at 2022-06-21 23:57:40 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 22 00:11:13.606: INFO: openshift-kube-proxy-8gv94 from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: node-exporter-6th4k from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 00:11:13.606: INFO: multus-additional-cni-plugins-6pjp8 from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 22 00:11:13.606: INFO: multus-admission-controller-x5qvv from openshift-multus started at 2022-06-21 23:58:10 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 22 00:11:13.606: INFO: multus-bkbkg from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-multus ready: true, restart count 0
Jun 22 00:11:13.606: INFO: network-metrics-daemon-8bt6v from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 22 00:11:13.606: INFO: network-check-target-smj6m from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 22 00:11:13.606: INFO: collect-profiles-27597600--1-q58vg from openshift-operator-lifecycle-manager started at 2022-06-22 00:00:00 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 22 00:11:13.606: INFO: sonobuoy from sonobuoy started at 2022-06-21 22:43:11 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 00:11:13.606: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-lrsbr from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.606: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 00:11:13.606: INFO: 
Logging pods the apiserver thinks is on node 10.10.24.214 before test
Jun 22 00:11:13.681: INFO: calico-kube-controllers-fbfbb867b-m9m79 from calico-system started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 00:11:13.681: INFO: calico-node-lrfbs from calico-system started at 2022-06-21 21:10:58 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 00:11:13.681: INFO: calico-typha-65cc4575bc-rjddl from calico-system started at 2022-06-21 21:11:06 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container calico-typha ready: true, restart count 0
Jun 22 00:11:13.681: INFO: managed-storage-validation-webhooks-695b4c95d9-xh79f from ibm-odf-validation-webhook started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 22 00:11:13.681: INFO: managed-storage-validation-webhooks-695b4c95d9-xmrb6 from ibm-odf-validation-webhook started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibm-cloud-provider-ip-163-73-69-51-775496d795-pmvzt from ibm-system started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ibm-cloud-provider-ip-163-73-69-51 ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibm-file-plugin-58d6b698d9-2ghc9 from kube-system started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibm-keepalived-watcher-pdlrz from kube-system started at 2022-06-21 21:08:42 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibm-master-proxy-static-10.10.24.214 from kube-system started at 2022-06-21 21:08:39 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 22 00:11:13.681: INFO: 	Container pause ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibm-storage-metrics-agent-5474c86f6c-mkwv2 from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibm-storage-watcher-79dd544684-s5lkx from kube-system started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ibmcloud-block-storage-driver-8w9mb from kube-system started at 2022-06-21 21:08:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 22 00:11:13.681: INFO: tuned-btmj7 from openshift-cluster-node-tuning-operator started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container tuned ready: true, restart count 0
Jun 22 00:11:13.681: INFO: cluster-samples-operator-665576d565-mmcww from openshift-cluster-samples-operator started at 2022-06-21 23:13:47 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 22 00:11:13.681: INFO: cluster-storage-operator-7bf9d85d5d-57gft from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: csi-snapshot-controller-69d5f9c777-c4lj2 from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 22 00:11:13.681: INFO: csi-snapshot-controller-operator-6b5cb9d8d8-bqsbp from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: csi-snapshot-webhook-55d4797dc4-4zxdg from openshift-cluster-storage-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container webhook ready: true, restart count 0
Jun 22 00:11:13.681: INFO: console-operator-76fb594985-ghsjf from openshift-console-operator started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container console-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: console-67bfbdb4dc-hf9m7 from openshift-console started at 2022-06-21 21:16:16 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container console ready: true, restart count 0
Jun 22 00:11:13.681: INFO: downloads-988dbf8f4-czbb8 from openshift-console started at 2022-06-21 21:13:34 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container download-server ready: true, restart count 0
Jun 22 00:11:13.681: INFO: dns-operator-5d76fb554c-mcw6n from openshift-dns-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container dns-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.681: INFO: dns-default-tdvc6 from openshift-dns started at 2022-06-21 21:15:29 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container dns ready: true, restart count 0
Jun 22 00:11:13.681: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.681: INFO: node-resolver-qlhv7 from openshift-dns started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 22 00:11:13.681: INFO: cluster-image-registry-operator-bf8fb9fdd-m94j9 from openshift-image-registry started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: image-registry-74b8f796c6-gq2tp from openshift-image-registry started at 2022-06-21 21:18:07 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container registry ready: true, restart count 0
Jun 22 00:11:13.681: INFO: node-ca-26tbk from openshift-image-registry started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container node-ca ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ingress-canary-d82cq from openshift-ingress-canary started at 2022-06-21 21:15:30 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 22 00:11:13.681: INFO: ingress-operator-5cdfdf4d7c-nhksh from openshift-ingress-operator started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 22 00:11:13.681: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.681: INFO: router-default-bb6fc54c5-92jm4 from openshift-ingress started at 2022-06-21 21:15:29 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container router ready: true, restart count 0
Jun 22 00:11:13.681: INFO: openshift-kube-proxy-brk2v from openshift-kube-proxy started at 2022-06-21 21:10:16 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.681: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 00:11:13.681: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: kube-storage-version-migrator-operator-5b447564b8-ss6lx from openshift-kube-storage-version-migrator-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 0
Jun 22 00:11:13.682: INFO: alertmanager-main-2 from openshift-monitoring started at 2022-06-21 21:16:14 +0000 UTC (5 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container alertmanager ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: cluster-monitoring-operator-747998f9d8-tmcgs from openshift-monitoring started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 3
Jun 22 00:11:13.682: INFO: node-exporter-pk7wt from openshift-monitoring started at 2022-06-21 21:13:30 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 00:11:13.682: INFO: prometheus-adapter-5c564c9546-r9n6b from openshift-monitoring started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 22 00:11:13.682: INFO: prometheus-k8s-1 from openshift-monitoring started at 2022-06-21 23:13:59 +0000 UTC (7 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container config-reloader ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container prometheus ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 22 00:11:13.682: INFO: thanos-querier-75f4c69596-zg5j2 from openshift-monitoring started at 2022-06-21 23:13:47 +0000 UTC (5 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container thanos-query ready: true, restart count 0
Jun 22 00:11:13.682: INFO: multus-additional-cni-plugins-zbkbh from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 22 00:11:13.682: INFO: multus-admission-controller-xxd4n from openshift-multus started at 2022-06-21 21:11:33 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 22 00:11:13.682: INFO: multus-n2tzz from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-multus ready: true, restart count 0
Jun 22 00:11:13.682: INFO: network-metrics-daemon-wvhtq from openshift-multus started at 2022-06-21 21:10:15 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 22 00:11:13.682: INFO: network-check-target-4np7s from openshift-network-diagnostics started at 2022-06-21 21:10:17 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 22 00:11:13.682: INFO: network-operator-54477c9fc6-knql5 from openshift-network-operator started at 2022-06-21 21:09:52 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container network-operator ready: true, restart count 0
Jun 22 00:11:13.682: INFO: packageserver-cfb4bb4b4-6xvr5 from openshift-operator-lifecycle-manager started at 2022-06-21 23:13:48 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container packageserver ready: true, restart count 0
Jun 22 00:11:13.682: INFO: metrics-6699958ffb-qcvqb from openshift-roks-metrics started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container metrics ready: false, restart count 0
Jun 22 00:11:13.682: INFO: push-gateway-77d69cd6c6-4xbg9 from openshift-roks-metrics started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container push-gateway ready: false, restart count 0
Jun 22 00:11:13.682: INFO: service-ca-operator-6d9bdb775b-ggrqz from openshift-service-ca-operator started at 2022-06-21 21:11:33 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 22 00:11:13.682: INFO: sonobuoy-systemd-logs-daemon-set-31bc6cee67334458-52nfv from sonobuoy started at 2022-06-21 22:43:23 +0000 UTC (2 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 00:11:13.682: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 00:11:13.682: INFO: tigera-operator-844dcd89f8-scnmx from tigera-operator started at 2022-06-21 23:13:47 +0000 UTC (1 container statuses recorded)
Jun 22 00:11:13.682: INFO: 	Container tigera-operator ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e303c98c-0d61-43ce-bc84-f2f7fad1d679 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.10.24.208 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-e303c98c-0d61-43ce-bc84-f2f7fad1d679 off the node 10.10.24.208
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e303c98c-0d61-43ce-bc84-f2f7fad1d679
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:16:22.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5195" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:309.289 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":281,"skipped":4877,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:16:22.299: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jun 22 00:16:23.253: INFO: created pod pod-service-account-defaultsa
Jun 22 00:16:23.253: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 22 00:16:23.297: INFO: created pod pod-service-account-mountsa
Jun 22 00:16:23.297: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 22 00:16:23.341: INFO: created pod pod-service-account-nomountsa
Jun 22 00:16:23.342: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 22 00:16:23.389: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 00:16:23.389: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 22 00:16:23.424: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 00:16:23.425: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 22 00:16:23.483: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 00:16:23.486: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 22 00:16:23.529: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 00:16:23.529: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 22 00:16:23.579: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 00:16:23.579: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 22 00:16:23.626: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 00:16:23.628: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:16:23.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4381" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":282,"skipped":4880,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:16:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 00:16:24.616: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 00:16:26.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:16:28.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453784, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 00:16:31.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:16:32.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1682" for this suite.
STEP: Destroying namespace "webhook-1682-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.215 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":283,"skipped":4887,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:16:32.908: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6485
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-6485
Jun 22 00:16:33.246: INFO: Found 0 stateful pods, waiting for 1
Jun 22 00:16:43.270: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jun 22 00:16:43.368: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jun 22 00:16:43.441: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jun 22 00:16:43.449: INFO: Observed &StatefulSet event: ADDED
Jun 22 00:16:43.449: INFO: Found Statefulset ss in namespace statefulset-6485 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 00:16:43.449: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jun 22 00:16:43.449: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 22 00:16:43.494: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jun 22 00:16:43.503: INFO: Observed &StatefulSet event: ADDED
Jun 22 00:16:43.503: INFO: Observed Statefulset ss in namespace statefulset-6485 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 00:16:43.503: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun 22 00:16:43.504: INFO: Deleting all statefulset in ns statefulset-6485
Jun 22 00:16:43.521: INFO: Scaling statefulset ss to 0
Jun 22 00:16:53.616: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 00:16:53.672: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:16:53.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6485" for this suite.

• [SLOW TEST:20.919 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":284,"skipped":4894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:16:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 00:16:54.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 22 00:16:56.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453814, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453814, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453814, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791453814, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 00:16:59.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:17:10.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5450" for this suite.
STEP: Destroying namespace "webhook-5450-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.985 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":285,"skipped":4922,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:17:10.814: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:17:11.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf" in namespace "downward-api-8452" to be "Succeeded or Failed"
Jun 22 00:17:11.230: INFO: Pod "downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.000317ms
Jun 22 00:17:13.246: INFO: Pod "downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031362224s
Jun 22 00:17:15.258: INFO: Pod "downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043821318s
Jun 22 00:17:17.275: INFO: Pod "downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060314354s
STEP: Saw pod success
Jun 22 00:17:17.275: INFO: Pod "downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf" satisfied condition "Succeeded or Failed"
Jun 22 00:17:17.296: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf container client-container: <nil>
STEP: delete the pod
Jun 22 00:17:17.450: INFO: Waiting for pod downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf to disappear
Jun 22 00:17:17.461: INFO: Pod downwardapi-volume-a0796069-7ee2-4eae-bb10-af11236004cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:17:17.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8452" for this suite.

• [SLOW TEST:6.711 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":4926,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:17:17.531: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-4e995ab8-9b9c-485f-8b78-3e5ac5eda7d6
STEP: Creating a pod to test consume secrets
Jun 22 00:17:18.300: INFO: Waiting up to 5m0s for pod "pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0" in namespace "secrets-6538" to be "Succeeded or Failed"
Jun 22 00:17:18.312: INFO: Pod "pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.604541ms
Jun 22 00:17:20.325: INFO: Pod "pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025371379s
Jun 22 00:17:22.390: INFO: Pod "pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089640957s
Jun 22 00:17:24.411: INFO: Pod "pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.111208502s
STEP: Saw pod success
Jun 22 00:17:24.411: INFO: Pod "pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0" satisfied condition "Succeeded or Failed"
Jun 22 00:17:24.424: INFO: Trying to get logs from node 10.10.24.208 pod pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 00:17:24.486: INFO: Waiting for pod pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0 to disappear
Jun 22 00:17:24.498: INFO: Pod pod-secrets-f973cb7d-c1bd-4a21-be92-e1a4c807ebe0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:17:24.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6538" for this suite.

• [SLOW TEST:7.025 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":4929,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:17:24.560: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 00:17:24.972: INFO: Waiting up to 5m0s for pod "pod-334694a4-acba-4606-9462-2fdd4b047f2c" in namespace "emptydir-5163" to be "Succeeded or Failed"
Jun 22 00:17:25.008: INFO: Pod "pod-334694a4-acba-4606-9462-2fdd4b047f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 36.431974ms
Jun 22 00:17:27.028: INFO: Pod "pod-334694a4-acba-4606-9462-2fdd4b047f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055816325s
Jun 22 00:17:29.047: INFO: Pod "pod-334694a4-acba-4606-9462-2fdd4b047f2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07529174s
Jun 22 00:17:31.080: INFO: Pod "pod-334694a4-acba-4606-9462-2fdd4b047f2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108064602s
STEP: Saw pod success
Jun 22 00:17:31.080: INFO: Pod "pod-334694a4-acba-4606-9462-2fdd4b047f2c" satisfied condition "Succeeded or Failed"
Jun 22 00:17:31.095: INFO: Trying to get logs from node 10.10.24.208 pod pod-334694a4-acba-4606-9462-2fdd4b047f2c container test-container: <nil>
STEP: delete the pod
Jun 22 00:17:31.163: INFO: Waiting for pod pod-334694a4-acba-4606-9462-2fdd4b047f2c to disappear
Jun 22 00:17:31.179: INFO: Pod pod-334694a4-acba-4606-9462-2fdd4b047f2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:17:31.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5163" for this suite.

• [SLOW TEST:6.686 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":4937,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:17:31.248: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:17:31.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635" in namespace "downward-api-8806" to be "Succeeded or Failed"
Jun 22 00:17:31.600: INFO: Pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635": Phase="Pending", Reason="", readiness=false. Elapsed: 14.286026ms
Jun 22 00:17:33.616: INFO: Pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030744166s
Jun 22 00:17:35.639: INFO: Pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052884351s
Jun 22 00:17:37.656: INFO: Pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070752082s
Jun 22 00:17:39.675: INFO: Pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.089419464s
STEP: Saw pod success
Jun 22 00:17:39.675: INFO: Pod "downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635" satisfied condition "Succeeded or Failed"
Jun 22 00:17:39.686: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635 container client-container: <nil>
STEP: delete the pod
Jun 22 00:17:39.759: INFO: Waiting for pod downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635 to disappear
Jun 22 00:17:39.780: INFO: Pod downwardapi-volume-cf3feb71-9944-4c95-a5a3-331aa500e635 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:17:39.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8806" for this suite.

• [SLOW TEST:8.594 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":4943,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:17:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1343
Jun 22 00:17:40.277: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:17:42.302: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun 22 00:17:42.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 22 00:17:43.006: INFO: rc: 7
Jun 22 00:17:43.055: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 22 00:17:43.070: INFO: Pod kube-proxy-mode-detector no longer exists
Jun 22 00:17:43.070: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-1343
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1343
I0622 00:17:43.152658      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1343, replica count: 3
I0622 00:17:46.209573      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 00:17:49.212430      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 00:17:49.243: INFO: Creating new exec pod
Jun 22 00:17:52.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec execpod-affinitydxkvp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jun 22 00:17:52.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 22 00:17:52.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 22 00:17:52.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec execpod-affinitydxkvp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.212.238 80'
Jun 22 00:17:53.296: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.212.238 80\nConnection to 172.21.212.238 80 port [tcp/http] succeeded!\n"
Jun 22 00:17:53.297: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 22 00:17:53.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec execpod-affinitydxkvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.212.238:80/ ; done'
Jun 22 00:17:53.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n"
Jun 22 00:17:53.894: INFO: stdout: "\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z\naffinity-clusterip-timeout-pnm2z"
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Received response from host: affinity-clusterip-timeout-pnm2z
Jun 22 00:17:53.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec execpod-affinitydxkvp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.212.238:80/'
Jun 22 00:17:54.317: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n"
Jun 22 00:17:54.317: INFO: stdout: "affinity-clusterip-timeout-pnm2z"
Jun 22 00:18:14.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-1343 exec execpod-affinitydxkvp -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.212.238:80/'
Jun 22 00:18:14.721: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.212.238:80/\n"
Jun 22 00:18:14.721: INFO: stdout: "affinity-clusterip-timeout-4pt8w"
Jun 22 00:18:14.721: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1343, will wait for the garbage collector to delete the pods
Jun 22 00:18:14.875: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 31.240534ms
Jun 22 00:18:15.076: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 201.294279ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:18:18.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1343" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:38.244 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":290,"skipped":4954,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:18:18.090: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun 22 00:18:18.463: INFO: The status of Pod annotationupdate411fad6c-a6df-40bf-9bec-420b563f83a9 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:18:20.491: INFO: The status of Pod annotationupdate411fad6c-a6df-40bf-9bec-420b563f83a9 is Running (Ready = true)
Jun 22 00:18:21.141: INFO: Successfully updated pod "annotationupdate411fad6c-a6df-40bf-9bec-420b563f83a9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:18:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4460" for this suite.

• [SLOW TEST:5.193 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":4972,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:18:23.294: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:18:23.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-569 version'
Jun 22 00:18:23.750: INFO: stderr: ""
Jun 22 00:18:23.750: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.8\", GitCommit:\"7061dbbf75f9f82e8ab21f9be7e8ffcaae8e0d44\", GitTreeState:\"clean\", BuildDate:\"2022-03-16T14:10:06Z\", GoVersion:\"go1.16.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.8+f34b40c\", GitCommit:\"496de02aacae03fbbd977e802b08b71ca76f390a\", GitTreeState:\"clean\", BuildDate:\"2022-05-18T18:53:42Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:18:23.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-569" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":292,"skipped":4983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:18:23.812: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:18:24.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7502" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":293,"skipped":5049,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:18:24.174: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:18:24.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7818" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":294,"skipped":5057,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:18:24.648: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7623
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7623
STEP: creating replication controller externalsvc in namespace services-7623
I0622 00:18:25.257168      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7623, replica count: 2
I0622 00:18:28.308596      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 22 00:18:28.410: INFO: Creating new exec pod
Jun 22 00:18:32.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-7623 exec execpodcwhm7 -- /bin/sh -x -c nslookup clusterip-service.services-7623.svc.cluster.local'
Jun 22 00:18:32.946: INFO: stderr: "+ nslookup clusterip-service.services-7623.svc.cluster.local\n"
Jun 22 00:18:32.946: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7623.svc.cluster.local\tcanonical name = externalsvc.services-7623.svc.cluster.local.\nName:\texternalsvc.services-7623.svc.cluster.local\nAddress: 172.21.210.106\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7623, will wait for the garbage collector to delete the pods
Jun 22 00:18:33.043: INFO: Deleting ReplicationController externalsvc took: 28.656339ms
Jun 22 00:18:33.150: INFO: Terminating ReplicationController externalsvc pods took: 107.127793ms
Jun 22 00:18:36.370: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:18:36.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7623" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.860 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":295,"skipped":5075,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:18:36.510: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 22 00:18:36.826: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123123 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:18:36.827: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123123 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 22 00:18:46.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123240 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:18:46.879: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123240 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 22 00:18:56.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123298 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:18:56.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123298 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 22 00:19:06.970: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123356 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:19:06.970: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9806  95d60738-e8fe-4910-b264-1a1f347f3caa 123356 0 2022-06-22 00:18:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-22 00:18:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 22 00:19:17.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9806  82970bb0-4f40-4f12-bf5e-8763adcf77a3 123405 0 2022-06-22 00:19:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-22 00:19:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:19:17.024: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9806  82970bb0-4f40-4f12-bf5e-8763adcf77a3 123405 0 2022-06-22 00:19:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-22 00:19:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 22 00:19:27.088: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9806  82970bb0-4f40-4f12-bf5e-8763adcf77a3 123457 0 2022-06-22 00:19:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-22 00:19:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 00:19:27.088: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9806  82970bb0-4f40-4f12-bf5e-8763adcf77a3 123457 0 2022-06-22 00:19:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-22 00:19:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:19:37.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9806" for this suite.

• [SLOW TEST:60.647 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":296,"skipped":5075,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:19:37.158: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jun 22 00:19:37.402: INFO: namespace kubectl-7387
Jun 22 00:19:37.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-7387 create -f -'
Jun 22 00:19:37.973: INFO: stderr: ""
Jun 22 00:19:37.973: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 22 00:19:38.989: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 00:19:38.989: INFO: Found 0 / 1
Jun 22 00:19:39.992: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 00:19:39.992: INFO: Found 0 / 1
Jun 22 00:19:41.003: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 00:19:41.003: INFO: Found 1 / 1
Jun 22 00:19:41.003: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 00:19:41.015: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 00:19:41.015: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 00:19:41.015: INFO: wait on agnhost-primary startup in kubectl-7387 
Jun 22 00:19:41.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-7387 logs agnhost-primary-5nfw5 agnhost-primary'
Jun 22 00:19:41.261: INFO: stderr: ""
Jun 22 00:19:41.262: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 22 00:19:41.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-7387 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 22 00:19:41.508: INFO: stderr: ""
Jun 22 00:19:41.508: INFO: stdout: "service/rm2 exposed\n"
Jun 22 00:19:41.525: INFO: Service rm2 in namespace kubectl-7387 found.
STEP: exposing service
Jun 22 00:19:43.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-7387 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 22 00:19:43.873: INFO: stderr: ""
Jun 22 00:19:43.874: INFO: stdout: "service/rm3 exposed\n"
Jun 22 00:19:43.895: INFO: Service rm3 in namespace kubectl-7387 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:19:45.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7387" for this suite.

• [SLOW TEST:8.830 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":297,"skipped":5077,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:19:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5887
STEP: creating service affinity-clusterip-transition in namespace services-5887
STEP: creating replication controller affinity-clusterip-transition in namespace services-5887
I0622 00:19:46.413696      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5887, replica count: 3
I0622 00:19:49.467852      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 00:19:52.468741      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 00:19:52.505: INFO: Creating new exec pod
Jun 22 00:19:57.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-5887 exec execpod-affinity79jfr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun 22 00:19:58.095: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 22 00:19:58.095: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 22 00:19:58.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-5887 exec execpod-affinity79jfr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.58.200 80'
Jun 22 00:19:58.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.58.200 80\nConnection to 172.21.58.200 80 port [tcp/http] succeeded!\n"
Jun 22 00:19:58.531: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 22 00:19:58.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-5887 exec execpod-affinity79jfr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.58.200:80/ ; done'
Jun 22 00:19:59.197: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n"
Jun 22 00:19:59.197: INFO: stdout: "\naffinity-clusterip-transition-rdmxs\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-442s2\naffinity-clusterip-transition-442s2\naffinity-clusterip-transition-442s2\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-rdmxs\naffinity-clusterip-transition-rdmxs\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-442s2\naffinity-clusterip-transition-rdmxs\naffinity-clusterip-transition-rdmxs\naffinity-clusterip-transition-442s2\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt"
Jun 22 00:19:59.197: INFO: Received response from host: affinity-clusterip-transition-rdmxs
Jun 22 00:19:59.197: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.197: INFO: Received response from host: affinity-clusterip-transition-442s2
Jun 22 00:19:59.197: INFO: Received response from host: affinity-clusterip-transition-442s2
Jun 22 00:19:59.197: INFO: Received response from host: affinity-clusterip-transition-442s2
Jun 22 00:19:59.197: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-rdmxs
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-rdmxs
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-442s2
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-rdmxs
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-rdmxs
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-442s2
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.198: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-5887 exec execpod-affinity79jfr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.58.200:80/ ; done'
Jun 22 00:19:59.814: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.58.200:80/\n"
Jun 22 00:19:59.814: INFO: stdout: "\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt\naffinity-clusterip-transition-fh4qt"
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Received response from host: affinity-clusterip-transition-fh4qt
Jun 22 00:19:59.814: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5887, will wait for the garbage collector to delete the pods
Jun 22 00:19:59.957: INFO: Deleting ReplicationController affinity-clusterip-transition took: 31.178503ms
Jun 22 00:20:00.057: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.881366ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:20:03.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5887" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:18.015 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":298,"skipped":5095,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:20:04.003: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9173
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 00:20:04.281: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 22 00:20:04.542: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:20:06.558: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:20:08.592: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:10.579: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:12.565: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:14.570: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:16.557: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:18.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:20.566: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:22.567: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 22 00:20:24.564: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 22 00:20:24.595: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 22 00:20:24.623: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 22 00:20:26.639: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun 22 00:20:30.730: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 22 00:20:30.730: INFO: Breadth first check of 172.30.96.38 on host 10.10.24.206...
Jun 22 00:20:30.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.224.7:9080/dial?request=hostname&protocol=http&host=172.30.96.38&port=8083&tries=1'] Namespace:pod-network-test-9173 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:20:30.743: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 22 00:20:31.027: INFO: Waiting for responses: map[]
Jun 22 00:20:31.027: INFO: reached 172.30.96.38 after 0/1 tries
Jun 22 00:20:31.028: INFO: Breadth first check of 172.30.224.34 on host 10.10.24.208...
Jun 22 00:20:31.045: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.224.7:9080/dial?request=hostname&protocol=http&host=172.30.224.34&port=8083&tries=1'] Namespace:pod-network-test-9173 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:20:31.046: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 22 00:20:31.310: INFO: Waiting for responses: map[]
Jun 22 00:20:31.310: INFO: reached 172.30.224.34 after 0/1 tries
Jun 22 00:20:31.310: INFO: Breadth first check of 172.30.47.240 on host 10.10.24.214...
Jun 22 00:20:31.326: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.224.7:9080/dial?request=hostname&protocol=http&host=172.30.47.240&port=8083&tries=1'] Namespace:pod-network-test-9173 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:20:31.326: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 22 00:20:31.593: INFO: Waiting for responses: map[]
Jun 22 00:20:31.593: INFO: reached 172.30.47.240 after 0/1 tries
Jun 22 00:20:31.593: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:20:31.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9173" for this suite.

• [SLOW TEST:27.643 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5109,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:20:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:20:32.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1068" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":300,"skipped":5119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:20:32.658: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:20:37.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5044" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5142,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:20:37.117: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jun 22 00:20:37.368: INFO: Major version: 1
STEP: Confirm minor version
Jun 22 00:20:37.368: INFO: cleanMinorVersion: 22
Jun 22 00:20:37.368: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:20:37.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-728" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":302,"skipped":5160,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:20:37.421: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:20:37.742: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 22 00:20:48.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7746 --namespace=crd-publish-openapi-7746 create -f -'
Jun 22 00:20:50.553: INFO: stderr: ""
Jun 22 00:20:50.553: INFO: stdout: "e2e-test-crd-publish-openapi-7952-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 00:20:50.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7746 --namespace=crd-publish-openapi-7746 delete e2e-test-crd-publish-openapi-7952-crds test-cr'
Jun 22 00:20:50.830: INFO: stderr: ""
Jun 22 00:20:50.830: INFO: stdout: "e2e-test-crd-publish-openapi-7952-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 22 00:20:50.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7746 --namespace=crd-publish-openapi-7746 apply -f -'
Jun 22 00:20:51.468: INFO: stderr: ""
Jun 22 00:20:51.468: INFO: stdout: "e2e-test-crd-publish-openapi-7952-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 00:20:51.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7746 --namespace=crd-publish-openapi-7746 delete e2e-test-crd-publish-openapi-7952-crds test-cr'
Jun 22 00:20:51.718: INFO: stderr: ""
Jun 22 00:20:51.718: INFO: stdout: "e2e-test-crd-publish-openapi-7952-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 22 00:20:51.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=crd-publish-openapi-7746 explain e2e-test-crd-publish-openapi-7952-crds'
Jun 22 00:20:52.333: INFO: stderr: ""
Jun 22 00:20:52.333: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7952-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:21:01.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7746" for this suite.

• [SLOW TEST:23.989 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":303,"skipped":5171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:21:01.427: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:21:01.649: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 22 00:21:01.700: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 00:21:06.727: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 00:21:06.727: INFO: Creating deployment "test-rolling-update-deployment"
Jun 22 00:21:06.753: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 00:21:06.785: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 00:21:08.825: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 00:21:08.839: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454066, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454066, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454066, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454066, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 00:21:10.860: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 22 00:21:10.902: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-166  91af26ec-e848-43c6-81a6-37e2cd80c306 124998 1 2022-06-22 00:21:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-06-22 00:21:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:21:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00628e6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-22 00:21:06 +0000 UTC,LastTransitionTime:2022-06-22 00:21:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-06-22 00:21:09 +0000 UTC,LastTransitionTime:2022-06-22 00:21:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 00:21:10.923: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-166  0bffef8d-7e95-46f0-9781-469613d9f01a 124987 1 2022-06-22 00:21:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 91af26ec-e848-43c6-81a6-37e2cd80c306 0xc0062b3aa7 0xc0062b3aa8}] []  [{kube-controller-manager Update apps/v1 2022-06-22 00:21:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91af26ec-e848-43c6-81a6-37e2cd80c306\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:21:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0062b3b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 00:21:10.923: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 00:21:10.923: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-166  772f6ef8-dc82-499a-957d-0ebd9a2199f6 124997 2 2022-06-22 00:21:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 91af26ec-e848-43c6-81a6-37e2cd80c306 0xc0062b3887 0xc0062b3888}] []  [{e2e.test Update apps/v1 2022-06-22 00:21:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:21:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91af26ec-e848-43c6-81a6-37e2cd80c306\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:21:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0062b3998 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 00:21:10.957: INFO: Pod "test-rolling-update-deployment-585b757574-klqrp" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-klqrp test-rolling-update-deployment-585b757574- deployment-166  fbbb83a2-a344-468d-b02f-c180a2988a4b 124986 0 2022-06-22 00:21:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:ad423705762cb893a553ac100c1baf2f2c60048f2846617391c6c1c9b6533fcb cni.projectcalico.org/podIP:172.30.224.54/32 cni.projectcalico.org/podIPs:172.30.224.54/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.54"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.54"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 0bffef8d-7e95-46f0-9781-469613d9f01a 0xc00628ec07 0xc00628ec08}] []  [{kube-controller-manager Update v1 2022-06-22 00:21:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bffef8d-7e95-46f0-9781-469613d9f01a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:21:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:21:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:21:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7gjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7gjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gk47t,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:21:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:21:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:21:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:21:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.54,StartTime:2022-06-22 00:21:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:21:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://8c4cf8c23af43eee6499880f5880d3046c7bd48c8b4dc4324b7c0cd0b8726200,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:21:10.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-166" for this suite.

• [SLOW TEST:9.665 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":304,"skipped":5259,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:21:11.092: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:27:01.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8302" for this suite.

• [SLOW TEST:350.606 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":305,"skipped":5268,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:27:01.699: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:27:02.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69" in namespace "projected-240" to be "Succeeded or Failed"
Jun 22 00:27:02.108: INFO: Pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69": Phase="Pending", Reason="", readiness=false. Elapsed: 35.451312ms
Jun 22 00:27:04.126: INFO: Pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053034317s
Jun 22 00:27:06.144: INFO: Pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070984482s
Jun 22 00:27:08.164: INFO: Pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.091414976s
Jun 22 00:27:10.180: INFO: Pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.107034252s
STEP: Saw pod success
Jun 22 00:27:10.180: INFO: Pod "downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69" satisfied condition "Succeeded or Failed"
Jun 22 00:27:10.195: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69 container client-container: <nil>
STEP: delete the pod
Jun 22 00:27:10.332: INFO: Waiting for pod downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69 to disappear
Jun 22 00:27:10.345: INFO: Pod downwardapi-volume-3243204a-aa51-445a-8587-7cee6bd7dd69 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:27:10.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-240" for this suite.

• [SLOW TEST:8.695 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:27:10.397: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 22 00:27:12.129: INFO: Pod name wrapped-volume-race-59b83845-67a0-4273-ae91-62f3ce2c5d2f: Found 0 pods out of 5
Jun 22 00:27:17.164: INFO: Pod name wrapped-volume-race-59b83845-67a0-4273-ae91-62f3ce2c5d2f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-59b83845-67a0-4273-ae91-62f3ce2c5d2f in namespace emptydir-wrapper-4910, will wait for the garbage collector to delete the pods
Jun 22 00:27:17.399: INFO: Deleting ReplicationController wrapped-volume-race-59b83845-67a0-4273-ae91-62f3ce2c5d2f took: 39.789283ms
Jun 22 00:27:17.702: INFO: Terminating ReplicationController wrapped-volume-race-59b83845-67a0-4273-ae91-62f3ce2c5d2f pods took: 303.563367ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 00:27:21.299: INFO: Pod name wrapped-volume-race-eba099e9-4cec-48e4-bc9f-eadfff8d7020: Found 0 pods out of 5
Jun 22 00:27:26.345: INFO: Pod name wrapped-volume-race-eba099e9-4cec-48e4-bc9f-eadfff8d7020: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-eba099e9-4cec-48e4-bc9f-eadfff8d7020 in namespace emptydir-wrapper-4910, will wait for the garbage collector to delete the pods
Jun 22 00:27:28.624: INFO: Deleting ReplicationController wrapped-volume-race-eba099e9-4cec-48e4-bc9f-eadfff8d7020 took: 41.955712ms
Jun 22 00:27:28.726: INFO: Terminating ReplicationController wrapped-volume-race-eba099e9-4cec-48e4-bc9f-eadfff8d7020 pods took: 101.084135ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 00:27:32.707: INFO: Pod name wrapped-volume-race-cd77d9d5-a379-45d7-a467-8049cde974c5: Found 0 pods out of 5
Jun 22 00:27:37.736: INFO: Pod name wrapped-volume-race-cd77d9d5-a379-45d7-a467-8049cde974c5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cd77d9d5-a379-45d7-a467-8049cde974c5 in namespace emptydir-wrapper-4910, will wait for the garbage collector to delete the pods
Jun 22 00:27:37.954: INFO: Deleting ReplicationController wrapped-volume-race-cd77d9d5-a379-45d7-a467-8049cde974c5 took: 35.081612ms
Jun 22 00:27:38.155: INFO: Terminating ReplicationController wrapped-volume-race-cd77d9d5-a379-45d7-a467-8049cde974c5 pods took: 200.739063ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:27:44.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4910" for this suite.

• [SLOW TEST:34.582 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":307,"skipped":5303,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:27:44.979: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 22 00:27:49.381: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6066 PodName:var-expansion-fc8d581f-2a82-493f-96c4-99f71d35fb56 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:27:49.381: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: test for file in mounted path
Jun 22 00:27:49.714: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6066 PodName:var-expansion-fc8d581f-2a82-493f-96c4-99f71d35fb56 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:27:49.714: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: updating the annotation value
Jun 22 00:27:50.570: INFO: Successfully updated pod "var-expansion-fc8d581f-2a82-493f-96c4-99f71d35fb56"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 22 00:27:50.586: INFO: Deleting pod "var-expansion-fc8d581f-2a82-493f-96c4-99f71d35fb56" in namespace "var-expansion-6066"
Jun 22 00:27:50.613: INFO: Wait up to 5m0s for pod "var-expansion-fc8d581f-2a82-493f-96c4-99f71d35fb56" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:28:24.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6066" for this suite.

• [SLOW TEST:39.722 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":308,"skipped":5312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:28:24.705: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jun 22 00:28:25.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3117 cluster-info'
Jun 22 00:28:25.256: INFO: stderr: ""
Jun 22 00:28:25.256: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:28:25.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3117" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":309,"skipped":5353,"failed":0}

------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:28:25.312: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:28:25.566: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-a311a996-e5ea-4faa-98ad-159baefac331
STEP: Creating secret with name s-test-opt-upd-8b26ceb6-cd36-4ee1-bf73-81b3829a375a
STEP: Creating the pod
Jun 22 00:28:25.692: INFO: The status of Pod pod-projected-secrets-3a624612-76bd-474a-a5cd-7cc59eee963b is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:28:27.735: INFO: The status of Pod pod-projected-secrets-3a624612-76bd-474a-a5cd-7cc59eee963b is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:28:29.720: INFO: The status of Pod pod-projected-secrets-3a624612-76bd-474a-a5cd-7cc59eee963b is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-a311a996-e5ea-4faa-98ad-159baefac331
STEP: Updating secret s-test-opt-upd-8b26ceb6-cd36-4ee1-bf73-81b3829a375a
STEP: Creating secret with name s-test-opt-create-2fa6ab18-7a6c-484a-bf60-7ae4e5ede17a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:29:53.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5557" for this suite.

• [SLOW TEST:88.453 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:29:53.767: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:29:54.046: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 00:29:59.073: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 00:29:59.074: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun 22 00:30:03.196: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2632  cbd95490-62d0-42fc-bac2-f0b2387a7f8a 129214 1 2022-06-22 00:29:59 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-06-22 00:29:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00644a938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-22 00:29:59 +0000 UTC,LastTransitionTime:2022-06-22 00:29:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2022-06-22 00:30:02 +0000 UTC,LastTransitionTime:2022-06-22 00:29:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 00:30:03.214: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-2632  18f63528-6238-45df-9296-3e742d485829 129203 1 2022-06-22 00:29:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cbd95490-62d0-42fc-bac2-f0b2387a7f8a 0xc00644ad07 0xc00644ad08}] []  [{kube-controller-manager Update apps/v1 2022-06-22 00:29:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd95490-62d0-42fc-bac2-f0b2387a7f8a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-22 00:30:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00644adb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 00:30:03.233: INFO: Pod "test-cleanup-deployment-5b4d99b59b-bjnql" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-bjnql test-cleanup-deployment-5b4d99b59b- deployment-2632  122810ec-df41-4c0e-8db0-d9df99fcfdd7 129202 0 2022-06-22 00:29:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[cni.projectcalico.org/containerID:56883d096ec4fb89b9a76ae99962123b6e3e240732052b9c6ebeed2a3c4a09a3 cni.projectcalico.org/podIP:172.30.224.32/32 cni.projectcalico.org/podIPs:172.30.224.32/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.32"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.32"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 18f63528-6238-45df-9296-3e742d485829 0xc006428037 0xc006428038}] []  [{kube-controller-manager Update v1 2022-06-22 00:29:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18f63528-6238-45df-9296-3e742d485829\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-22 00:30:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2022-06-22 00:30:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2022-06-22 00:30:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thk8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thk8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.10.24.208,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6qzgc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:29:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:30:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:30:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-22 00:29:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.10.24.208,PodIP:172.30.224.32,StartTime:2022-06-22 00:29:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-22 00:30:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://4f228169ca1e1addd796e53ba1827eb9f764c82bce5bdf19d804c2bb551e9bc7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:03.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2632" for this suite.

• [SLOW TEST:9.521 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":311,"skipped":5387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:03.290: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun 22 00:30:07.792: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1793 PodName:pod-sharedvolume-bfa6d636-af07-43de-a2a2-50e02b07f141 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:30:07.792: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
Jun 22 00:30:08.085: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:08.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1793" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":312,"skipped":5415,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:08.167: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:21.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4868" for this suite.

• [SLOW TEST:13.656 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":313,"skipped":5420,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:21.824: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:30:22.052: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:22.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7015" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":314,"skipped":5436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:22.826: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:30:23.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0" in namespace "downward-api-8322" to be "Succeeded or Failed"
Jun 22 00:30:23.219: INFO: Pod "downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.801178ms
Jun 22 00:30:25.243: INFO: Pod "downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0": Phase="Running", Reason="", readiness=true. Elapsed: 2.035006464s
Jun 22 00:30:27.258: INFO: Pod "downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0": Phase="Running", Reason="", readiness=false. Elapsed: 4.049457118s
Jun 22 00:30:29.272: INFO: Pod "downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063734478s
STEP: Saw pod success
Jun 22 00:30:29.272: INFO: Pod "downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0" satisfied condition "Succeeded or Failed"
Jun 22 00:30:29.286: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0 container client-container: <nil>
STEP: delete the pod
Jun 22 00:30:29.363: INFO: Waiting for pod downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0 to disappear
Jun 22 00:30:29.377: INFO: Pod downwardapi-volume-b99078c9-39ce-4124-85ad-f0a5dc8c22d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:29.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8322" for this suite.

• [SLOW TEST:6.611 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":5458,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:29.441: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:30:29.681: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:33.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8823" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":316,"skipped":5464,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:33.156: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun 22 00:30:33.540: INFO: Waiting up to 5m0s for pod "downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af" in namespace "downward-api-2597" to be "Succeeded or Failed"
Jun 22 00:30:33.551: INFO: Pod "downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af": Phase="Pending", Reason="", readiness=false. Elapsed: 10.796183ms
Jun 22 00:30:35.594: INFO: Pod "downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053564194s
Jun 22 00:30:37.610: INFO: Pod "downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070002328s
Jun 22 00:30:39.628: INFO: Pod "downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087500564s
STEP: Saw pod success
Jun 22 00:30:39.628: INFO: Pod "downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af" satisfied condition "Succeeded or Failed"
Jun 22 00:30:39.651: INFO: Trying to get logs from node 10.10.24.208 pod downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af container dapi-container: <nil>
STEP: delete the pod
Jun 22 00:30:39.744: INFO: Waiting for pod downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af to disappear
Jun 22 00:30:39.758: INFO: Pod downward-api-32916de6-24c5-4692-88ea-2ffa6fb214af no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:39.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2597" for this suite.

• [SLOW TEST:6.654 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":5474,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:39.810: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:40.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8779" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":318,"skipped":5478,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:40.123: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-b7e64eb9-6efa-4b21-87d4-61aeb9cf466b
STEP: Creating a pod to test consume secrets
Jun 22 00:30:40.462: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e" in namespace "projected-1471" to be "Succeeded or Failed"
Jun 22 00:30:40.478: INFO: Pod "pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.221739ms
Jun 22 00:30:42.508: INFO: Pod "pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044921961s
Jun 22 00:30:44.530: INFO: Pod "pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0668911s
Jun 22 00:30:46.543: INFO: Pod "pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080011494s
STEP: Saw pod success
Jun 22 00:30:46.543: INFO: Pod "pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e" satisfied condition "Succeeded or Failed"
Jun 22 00:30:46.553: INFO: Trying to get logs from node 10.10.24.208 pod pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 00:30:46.644: INFO: Waiting for pod pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e to disappear
Jun 22 00:30:46.678: INFO: Pod pod-projected-secrets-63e0ef52-cbba-46db-be7d-f25922699c1e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:30:46.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1471" for this suite.

• [SLOW TEST:6.611 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":5482,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:30:46.734: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:30:47.124: INFO: Create a RollingUpdate DaemonSet
Jun 22 00:30:47.160: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 00:30:47.210: INFO: Number of nodes with available pods: 0
Jun 22 00:30:47.210: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:30:48.262: INFO: Number of nodes with available pods: 0
Jun 22 00:30:48.262: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:30:49.259: INFO: Number of nodes with available pods: 0
Jun 22 00:30:49.259: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:30:50.265: INFO: Number of nodes with available pods: 3
Jun 22 00:30:50.266: INFO: Number of running nodes: 3, number of available pods: 3
Jun 22 00:30:50.266: INFO: Update the DaemonSet to trigger a rollout
Jun 22 00:30:50.386: INFO: Updating DaemonSet daemon-set
Jun 22 00:30:55.456: INFO: Roll back the DaemonSet before rollout is complete
Jun 22 00:30:55.505: INFO: Updating DaemonSet daemon-set
Jun 22 00:30:55.506: INFO: Make sure DaemonSet rollback is complete
Jun 22 00:30:55.524: INFO: Wrong image for pod: daemon-set-w2jfv. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Jun 22 00:30:55.524: INFO: Pod daemon-set-w2jfv is not available
Jun 22 00:31:02.575: INFO: Pod daemon-set-vtkl9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3791, will wait for the garbage collector to delete the pods
Jun 22 00:31:02.796: INFO: Deleting DaemonSet.extensions daemon-set took: 30.752794ms
Jun 22 00:31:02.898: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.272174ms
Jun 22 00:31:06.518: INFO: Number of nodes with available pods: 0
Jun 22 00:31:06.518: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 00:31:06.535: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"130431"},"items":null}

Jun 22 00:31:06.554: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"130431"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:31:06.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3791" for this suite.

• [SLOW TEST:19.960 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":320,"skipped":5484,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:31:06.696: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:31:06.977: INFO: Got root ca configmap in namespace "svcaccounts-6887"
Jun 22 00:31:07.026: INFO: Deleted root ca configmap in namespace "svcaccounts-6887"
STEP: waiting for a new root ca configmap created
Jun 22 00:31:07.546: INFO: Recreated root ca configmap in namespace "svcaccounts-6887"
Jun 22 00:31:07.571: INFO: Updated root ca configmap in namespace "svcaccounts-6887"
STEP: waiting for the root ca configmap reconciled
Jun 22 00:31:08.098: INFO: Reconciled root ca configmap in namespace "svcaccounts-6887"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:31:08.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6887" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":321,"skipped":5500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:31:08.161: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-5f18772c-aaf9-4f03-a6e8-c0b825077b59 in namespace container-probe-4397
Jun 22 00:31:10.512: INFO: Started pod liveness-5f18772c-aaf9-4f03-a6e8-c0b825077b59 in namespace container-probe-4397
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 00:31:10.524: INFO: Initial restart count of pod liveness-5f18772c-aaf9-4f03-a6e8-c0b825077b59 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:11.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4397" for this suite.

• [SLOW TEST:243.185 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":5531,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:11.347: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:35:11.686: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:19.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2364" for this suite.

• [SLOW TEST:7.910 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":323,"skipped":5544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:19.276: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:35:19.622: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-74937400-234e-4812-a1ec-bb88d0eb9b24
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:23.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1849" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":324,"skipped":5601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:24.027: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 22 00:35:25.131: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 22 00:35:27.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454925, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454925, loc:(*time.Location)(0xa0acfa0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454925, loc:(*time.Location)(0xa0acfa0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63791454925, loc:(*time.Location)(0xa0acfa0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 22 00:35:30.266: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:30.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9295" for this suite.
STEP: Destroying namespace "webhook-9295-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":325,"skipped":5683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:30.872: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:40.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9000" for this suite.
STEP: Destroying namespace "nsdeletetest-3729" for this suite.
Jun 22 00:35:40.783: INFO: Namespace nsdeletetest-3729 was already deleted
STEP: Destroying namespace "nsdeletetest-7230" for this suite.

• [SLOW TEST:9.961 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":326,"skipped":5750,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:40.837: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun 22 00:35:41.164: INFO: The status of Pod pod-update-be282d11-55d5-434d-97ad-4b0c281a565a is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:35:43.197: INFO: The status of Pod pod-update-be282d11-55d5-434d-97ad-4b0c281a565a is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:35:45.196: INFO: The status of Pod pod-update-be282d11-55d5-434d-97ad-4b0c281a565a is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 00:35:45.794: INFO: Successfully updated pod "pod-update-be282d11-55d5-434d-97ad-4b0c281a565a"
STEP: verifying the updated pod is in kubernetes
Jun 22 00:35:45.816: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:45.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7189" for this suite.

• [SLOW TEST:5.024 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5782,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:45.863: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-b7f528d8-a0b7-4655-9216-a7a6b5823ffa
STEP: Creating secret with name secret-projected-all-test-volume-e644b41b-a195-4f03-9ea7-83c98a70d771
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 22 00:35:46.302: INFO: Waiting up to 5m0s for pod "projected-volume-3008ee70-9061-4356-a8e9-856beab536fe" in namespace "projected-7669" to be "Succeeded or Failed"
Jun 22 00:35:46.330: INFO: Pod "projected-volume-3008ee70-9061-4356-a8e9-856beab536fe": Phase="Pending", Reason="", readiness=false. Elapsed: 28.527434ms
Jun 22 00:35:48.346: INFO: Pod "projected-volume-3008ee70-9061-4356-a8e9-856beab536fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044101137s
Jun 22 00:35:50.361: INFO: Pod "projected-volume-3008ee70-9061-4356-a8e9-856beab536fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058882645s
Jun 22 00:35:52.385: INFO: Pod "projected-volume-3008ee70-9061-4356-a8e9-856beab536fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.083334879s
STEP: Saw pod success
Jun 22 00:35:52.385: INFO: Pod "projected-volume-3008ee70-9061-4356-a8e9-856beab536fe" satisfied condition "Succeeded or Failed"
Jun 22 00:35:52.397: INFO: Trying to get logs from node 10.10.24.208 pod projected-volume-3008ee70-9061-4356-a8e9-856beab536fe container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 22 00:35:52.467: INFO: Waiting for pod projected-volume-3008ee70-9061-4356-a8e9-856beab536fe to disappear
Jun 22 00:35:52.478: INFO: Pod projected-volume-3008ee70-9061-4356-a8e9-856beab536fe no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:52.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7669" for this suite.

• [SLOW TEST:6.670 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":328,"skipped":5791,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:52.534: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jun 22 00:35:52.843: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:35:54.866: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:35:56.863: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 22 00:35:57.960: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:35:59.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2936" for this suite.

• [SLOW TEST:6.593 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":329,"skipped":5797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:35:59.129: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3612.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3612.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3612.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3612.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3612.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3612.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3612.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 00:36:03.600: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.641: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.670: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.690: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.752: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.776: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.809: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.832: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3612.svc.cluster.local from pod dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f: the server could not find the requested resource (get pods dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f)
Jun 22 00:36:03.872: INFO: Lookups using dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3612.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3612.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3612.svc.cluster.local jessie_udp@dns-test-service-2.dns-3612.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3612.svc.cluster.local]

Jun 22 00:36:09.119: INFO: DNS probes using dns-3612/dns-test-d89384c8-53a6-46ee-8d0e-c07c9105e59f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:09.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3612" for this suite.

• [SLOW TEST:10.125 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":330,"skipped":5841,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:09.262: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:36:09.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30" in namespace "projected-8738" to be "Succeeded or Failed"
Jun 22 00:36:09.588: INFO: Pod "downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30": Phase="Pending", Reason="", readiness=false. Elapsed: 12.213385ms
Jun 22 00:36:11.614: INFO: Pod "downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038525623s
Jun 22 00:36:13.643: INFO: Pod "downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066640742s
Jun 22 00:36:15.658: INFO: Pod "downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08233254s
STEP: Saw pod success
Jun 22 00:36:15.658: INFO: Pod "downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30" satisfied condition "Succeeded or Failed"
Jun 22 00:36:15.677: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30 container client-container: <nil>
STEP: delete the pod
Jun 22 00:36:15.786: INFO: Waiting for pod downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30 to disappear
Jun 22 00:36:15.804: INFO: Pod downwardapi-volume-9ca8c741-b504-437e-994e-311b3a980e30 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:15.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8738" for this suite.

• [SLOW TEST:6.608 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":5847,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:15.872: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 00:36:21.326: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:21.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-17" for this suite.

• [SLOW TEST:5.569 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":5858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:21.443: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jun 22 00:36:21.776: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:23.796: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:25.800: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.10.24.206 on the node which pod1 resides and expect scheduled
Jun 22 00:36:25.859: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:27.879: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:29.877: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.10.24.206 but use UDP protocol on the node which pod2 resides
Jun 22 00:36:29.961: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:32.005: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:33.984: INFO: The status of Pod pod3 is Running (Ready = true)
Jun 22 00:36:34.041: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:36.064: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:36:38.062: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jun 22 00:36:38.077: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.10.24.206 http://127.0.0.1:54323/hostname] Namespace:hostport-1917 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:36:38.077: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.24.206, port: 54323
Jun 22 00:36:38.482: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.10.24.206:54323/hostname] Namespace:hostport-1917 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:36:38.482: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.10.24.206, port: 54323 UDP
Jun 22 00:36:38.771: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.10.24.206 54323] Namespace:hostport-1917 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 00:36:38.771: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:44.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1917" for this suite.

• [SLOW TEST:22.738 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":333,"skipped":5882,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:44.182: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jun 22 00:36:44.567: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 00:36:49.599: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:49.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8474" for this suite.

• [SLOW TEST:5.609 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":334,"skipped":5903,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:49.793: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jun 22 00:36:50.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=kubectl-3502 api-versions'
Jun 22 00:36:50.255: INFO: stderr: ""
Jun 22 00:36:50.256: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:50.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3502" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":335,"skipped":5917,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:50.310: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6759" for this suite.
STEP: Destroying namespace "nspatchtest-e87dff4e-f6ff-4543-8950-c3df32eed43a-1597" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":336,"skipped":5919,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:50.978: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-a68f750d-b96e-42ab-bfe5-08c814461651
STEP: Creating a pod to test consume configMaps
Jun 22 00:36:51.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb" in namespace "configmap-6707" to be "Succeeded or Failed"
Jun 22 00:36:51.410: INFO: Pod "pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.435086ms
Jun 22 00:36:53.425: INFO: Pod "pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03131637s
Jun 22 00:36:55.451: INFO: Pod "pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057420497s
Jun 22 00:36:57.481: INFO: Pod "pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087141458s
STEP: Saw pod success
Jun 22 00:36:57.481: INFO: Pod "pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb" satisfied condition "Succeeded or Failed"
Jun 22 00:36:57.496: INFO: Trying to get logs from node 10.10.24.208 pod pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 00:36:57.601: INFO: Waiting for pod pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb to disappear
Jun 22 00:36:57.612: INFO: Pod pod-configmaps-80e59b73-7a42-4896-b747-8968cf5f37cb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:36:57.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6707" for this suite.

• [SLOW TEST:6.689 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":337,"skipped":5925,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:36:57.666: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 00:36:58.151: INFO: Number of nodes with available pods: 0
Jun 22 00:36:58.151: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:36:59.261: INFO: Number of nodes with available pods: 0
Jun 22 00:36:59.261: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:37:00.208: INFO: Number of nodes with available pods: 0
Jun 22 00:37:00.208: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:37:01.200: INFO: Number of nodes with available pods: 2
Jun 22 00:37:01.200: INFO: Node 10.10.24.206 is running more than one daemon pod
Jun 22 00:37:02.254: INFO: Number of nodes with available pods: 3
Jun 22 00:37:02.254: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 22 00:37:02.353: INFO: Number of nodes with available pods: 2
Jun 22 00:37:02.353: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 22 00:37:03.399: INFO: Number of nodes with available pods: 2
Jun 22 00:37:03.399: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 22 00:37:04.420: INFO: Number of nodes with available pods: 2
Jun 22 00:37:04.420: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 22 00:37:05.400: INFO: Number of nodes with available pods: 2
Jun 22 00:37:05.400: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 22 00:37:06.390: INFO: Number of nodes with available pods: 2
Jun 22 00:37:06.390: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 22 00:37:07.389: INFO: Number of nodes with available pods: 2
Jun 22 00:37:07.389: INFO: Node 10.10.24.208 is running more than one daemon pod
Jun 22 00:37:08.398: INFO: Number of nodes with available pods: 3
Jun 22 00:37:08.398: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1012, will wait for the garbage collector to delete the pods
Jun 22 00:37:08.521: INFO: Deleting DaemonSet.extensions daemon-set took: 41.373692ms
Jun 22 00:37:08.635: INFO: Terminating DaemonSet.extensions daemon-set pods took: 113.657054ms
Jun 22 00:37:11.691: INFO: Number of nodes with available pods: 0
Jun 22 00:37:11.691: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 00:37:11.708: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"134427"},"items":null}

Jun 22 00:37:11.720: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"134427"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:37:11.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1012" for this suite.

• [SLOW TEST:14.179 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":338,"skipped":5926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:37:11.857: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:37:26.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7609" for this suite.

• [SLOW TEST:14.486 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":339,"skipped":5995,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:37:26.344: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun 22 00:37:26.654: INFO: Waiting up to 5m0s for pod "downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662" in namespace "downward-api-195" to be "Succeeded or Failed"
Jun 22 00:37:26.667: INFO: Pod "downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662": Phase="Pending", Reason="", readiness=false. Elapsed: 12.7758ms
Jun 22 00:37:28.693: INFO: Pod "downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039197694s
Jun 22 00:37:30.708: INFO: Pod "downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054402483s
Jun 22 00:37:32.732: INFO: Pod "downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078398283s
STEP: Saw pod success
Jun 22 00:37:32.732: INFO: Pod "downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662" satisfied condition "Succeeded or Failed"
Jun 22 00:37:32.742: INFO: Trying to get logs from node 10.10.24.208 pod downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662 container dapi-container: <nil>
STEP: delete the pod
Jun 22 00:37:32.809: INFO: Waiting for pod downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662 to disappear
Jun 22 00:37:32.821: INFO: Pod downward-api-cf88cdc0-ea37-41b7-a473-d9b0bf855662 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:37:32.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-195" for this suite.

• [SLOW TEST:6.537 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6014,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:37:32.885: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-8215
STEP: creating replication controller nodeport-test in namespace services-8215
I0622 00:37:33.287422      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8215, replica count: 2
Jun 22 00:37:36.339: INFO: Creating new exec pod
I0622 00:37:36.339030      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 00:37:41.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-8215 exec execpodt7swz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 22 00:37:42.270: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 22 00:37:42.270: INFO: stdout: "nodeport-test-ng75h"
Jun 22 00:37:42.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-8215 exec execpodt7swz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.104.25 80'
Jun 22 00:37:42.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.104.25 80\nConnection to 172.21.104.25 80 port [tcp/http] succeeded!\n"
Jun 22 00:37:42.731: INFO: stdout: "nodeport-test-ng75h"
Jun 22 00:37:42.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-8215 exec execpodt7swz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 30196'
Jun 22 00:37:43.152: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 30196\nConnection to 10.10.24.206 30196 port [tcp/*] succeeded!\n"
Jun 22 00:37:43.152: INFO: stdout: ""
Jun 22 00:37:44.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-8215 exec execpodt7swz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.206 30196'
Jun 22 00:37:44.616: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.206 30196\nConnection to 10.10.24.206 30196 port [tcp/*] succeeded!\n"
Jun 22 00:37:44.616: INFO: stdout: "nodeport-test-5wpvg"
Jun 22 00:37:44.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-379654781 --namespace=services-8215 exec execpodt7swz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.10.24.208 30196'
Jun 22 00:37:45.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.10.24.208 30196\nConnection to 10.10.24.208 30196 port [tcp/*] succeeded!\n"
Jun 22 00:37:45.077: INFO: stdout: "nodeport-test-ng75h"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:37:45.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8215" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.244 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":341,"skipped":6028,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:37:45.130: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 22 00:37:50.103: INFO: Successfully updated pod "adopt-release--1-69v2z"
STEP: Checking that the Job readopts the Pod
Jun 22 00:37:50.103: INFO: Waiting up to 15m0s for pod "adopt-release--1-69v2z" in namespace "job-3472" to be "adopted"
Jun 22 00:37:50.152: INFO: Pod "adopt-release--1-69v2z": Phase="Running", Reason="", readiness=true. Elapsed: 48.729455ms
Jun 22 00:37:50.152: INFO: Pod "adopt-release--1-69v2z" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 22 00:37:50.724: INFO: Successfully updated pod "adopt-release--1-69v2z"
STEP: Checking that the Job releases the Pod
Jun 22 00:37:50.724: INFO: Waiting up to 15m0s for pod "adopt-release--1-69v2z" in namespace "job-3472" to be "released"
Jun 22 00:37:50.740: INFO: Pod "adopt-release--1-69v2z": Phase="Running", Reason="", readiness=true. Elapsed: 15.810157ms
Jun 22 00:37:52.753: INFO: Pod "adopt-release--1-69v2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.028965464s
Jun 22 00:37:52.753: INFO: Pod "adopt-release--1-69v2z" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:37:52.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3472" for this suite.

• [SLOW TEST:7.674 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":342,"skipped":6043,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:37:52.804: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 22 00:39:53.811: INFO: Successfully updated pod "var-expansion-c0a1e0ce-1889-4447-ab9f-33956473e48b"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 22 00:39:55.850: INFO: Deleting pod "var-expansion-c0a1e0ce-1889-4447-ab9f-33956473e48b" in namespace "var-expansion-4239"
Jun 22 00:39:55.877: INFO: Wait up to 5m0s for pod "var-expansion-c0a1e0ce-1889-4447-ab9f-33956473e48b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:40:27.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4239" for this suite.

• [SLOW TEST:155.162 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":343,"skipped":6055,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:40:27.966: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun 22 00:40:28.311: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40" in namespace "downward-api-6026" to be "Succeeded or Failed"
Jun 22 00:40:28.339: INFO: Pod "downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40": Phase="Pending", Reason="", readiness=false. Elapsed: 27.599127ms
Jun 22 00:40:30.357: INFO: Pod "downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046131399s
Jun 22 00:40:32.374: INFO: Pod "downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062221666s
Jun 22 00:40:34.386: INFO: Pod "downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07509874s
STEP: Saw pod success
Jun 22 00:40:34.386: INFO: Pod "downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40" satisfied condition "Succeeded or Failed"
Jun 22 00:40:34.398: INFO: Trying to get logs from node 10.10.24.208 pod downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40 container client-container: <nil>
STEP: delete the pod
Jun 22 00:40:34.547: INFO: Waiting for pod downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40 to disappear
Jun 22 00:40:34.559: INFO: Pod downwardapi-volume-22b20f2f-c008-41b0-b3af-d489698eec40 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:40:34.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6026" for this suite.

• [SLOW TEST:6.637 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6062,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:40:34.606: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 00:40:34.899: INFO: Waiting up to 5m0s for pod "pod-92f9e7cd-c9ba-4e53-bc36-469b97698243" in namespace "emptydir-6919" to be "Succeeded or Failed"
Jun 22 00:40:34.913: INFO: Pod "pod-92f9e7cd-c9ba-4e53-bc36-469b97698243": Phase="Pending", Reason="", readiness=false. Elapsed: 13.629111ms
Jun 22 00:40:36.937: INFO: Pod "pod-92f9e7cd-c9ba-4e53-bc36-469b97698243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037714748s
Jun 22 00:40:38.957: INFO: Pod "pod-92f9e7cd-c9ba-4e53-bc36-469b97698243": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058412227s
Jun 22 00:40:40.992: INFO: Pod "pod-92f9e7cd-c9ba-4e53-bc36-469b97698243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092938676s
STEP: Saw pod success
Jun 22 00:40:40.992: INFO: Pod "pod-92f9e7cd-c9ba-4e53-bc36-469b97698243" satisfied condition "Succeeded or Failed"
Jun 22 00:40:41.026: INFO: Trying to get logs from node 10.10.24.208 pod pod-92f9e7cd-c9ba-4e53-bc36-469b97698243 container test-container: <nil>
STEP: delete the pod
Jun 22 00:40:41.096: INFO: Waiting for pod pod-92f9e7cd-c9ba-4e53-bc36-469b97698243 to disappear
Jun 22 00:40:41.108: INFO: Pod pod-92f9e7cd-c9ba-4e53-bc36-469b97698243 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:40:41.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6919" for this suite.

• [SLOW TEST:6.558 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6064,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun 22 00:40:41.164: INFO: >>> kubeConfig: /tmp/kubeconfig-379654781
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun 22 00:40:41.431: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-2aa806ad-1172-45b5-81f3-316ea50ca288
STEP: Creating configMap with name cm-test-opt-upd-dddec31f-ccda-424f-aaf4-7d2ce61fd0eb
STEP: Creating the pod
Jun 22 00:40:41.574: INFO: The status of Pod pod-configmaps-c6da6362-adbb-41c5-aca3-320be640142d is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:40:43.591: INFO: The status of Pod pod-configmaps-c6da6362-adbb-41c5-aca3-320be640142d is Pending, waiting for it to be Running (with Ready = true)
Jun 22 00:40:45.587: INFO: The status of Pod pod-configmaps-c6da6362-adbb-41c5-aca3-320be640142d is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-2aa806ad-1172-45b5-81f3-316ea50ca288
STEP: Updating configmap cm-test-opt-upd-dddec31f-ccda-424f-aaf4-7d2ce61fd0eb
STEP: Creating configMap with name cm-test-opt-create-1140f963-090f-4025-a1f1-ca06eb6d1d4b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun 22 00:40:47.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2382" for this suite.

• [SLOW TEST:6.856 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSJun 22 00:40:48.021: INFO: Running AfterSuite actions on all nodes
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun 22 00:40:48.021: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jun 22 00:40:48.021: INFO: Running AfterSuite actions on node 1
Jun 22 00:40:48.021: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6088,"failed":0}

Ran 346 of 6434 Specs in 7009.949 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6088 Skipped
PASS

Ginkgo ran 1 suite in 1h56m52.34934784s
Test Suite Passed

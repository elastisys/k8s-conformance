I0824 11:39:13.004980      14 e2e.go:126] Starting e2e run "e37f2036-3a54-4653-ada1-c01489d8d1f1" on Ginkgo node 1
Aug 24 11:39:13.051: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692877152 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 24 11:39:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:39:13.278: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 24 11:39:13.307: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 24 11:39:13.359: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 24 11:39:13.360: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Aug 24 11:39:13.360: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 24 11:39:13.373: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Aug 24 11:39:13.373: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
Aug 24 11:39:13.373: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 24 11:39:13.373: INFO: e2e test version: v1.26.8
Aug 24 11:39:13.374: INFO: kube-apiserver version: v1.26.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 24 11:39:13.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:39:13.384: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.110 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 24 11:39:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:39:13.278: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 24 11:39:13.307: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 24 11:39:13.359: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 24 11:39:13.360: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Aug 24 11:39:13.360: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 24 11:39:13.373: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Aug 24 11:39:13.373: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
    Aug 24 11:39:13.373: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Aug 24 11:39:13.373: INFO: e2e test version: v1.26.8
    Aug 24 11:39:13.374: INFO: kube-apiserver version: v1.26.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 24 11:39:13.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:39:13.384: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:39:13.435
Aug 24 11:39:13.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 11:39:13.437
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:13.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:13.479
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 11:39:13.485
Aug 24 11:39:13.510: INFO: Waiting up to 5m0s for pod "pod-9d704f62-4d95-49c5-b380-04509ac78237" in namespace "emptydir-582" to be "Succeeded or Failed"
Aug 24 11:39:13.518: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185248ms
Aug 24 11:39:15.533: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021717467s
Aug 24 11:39:17.526: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014170556s
Aug 24 11:39:19.528: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016760088s
Aug 24 11:39:21.523: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011754703s
Aug 24 11:39:23.525: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013286028s
Aug 24 11:39:25.527: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015187536s
Aug 24 11:39:27.526: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014417955s
Aug 24 11:39:29.527: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.01509674s
STEP: Saw pod success 08/24/23 11:39:29.527
Aug 24 11:39:29.528: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237" satisfied condition "Succeeded or Failed"
Aug 24 11:39:29.534: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-9d704f62-4d95-49c5-b380-04509ac78237 container test-container: <nil>
STEP: delete the pod 08/24/23 11:39:29.568
Aug 24 11:39:29.590: INFO: Waiting for pod pod-9d704f62-4d95-49c5-b380-04509ac78237 to disappear
Aug 24 11:39:29.597: INFO: Pod pod-9d704f62-4d95-49c5-b380-04509ac78237 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:39:29.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-582" for this suite. 08/24/23 11:39:29.606
------------------------------
â€¢ [SLOW TEST] [16.182 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:39:13.435
    Aug 24 11:39:13.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:39:13.437
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:13.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:13.479
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 11:39:13.485
    Aug 24 11:39:13.510: INFO: Waiting up to 5m0s for pod "pod-9d704f62-4d95-49c5-b380-04509ac78237" in namespace "emptydir-582" to be "Succeeded or Failed"
    Aug 24 11:39:13.518: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185248ms
    Aug 24 11:39:15.533: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021717467s
    Aug 24 11:39:17.526: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014170556s
    Aug 24 11:39:19.528: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016760088s
    Aug 24 11:39:21.523: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011754703s
    Aug 24 11:39:23.525: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013286028s
    Aug 24 11:39:25.527: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 12.015187536s
    Aug 24 11:39:27.526: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014417955s
    Aug 24 11:39:29.527: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.01509674s
    STEP: Saw pod success 08/24/23 11:39:29.527
    Aug 24 11:39:29.528: INFO: Pod "pod-9d704f62-4d95-49c5-b380-04509ac78237" satisfied condition "Succeeded or Failed"
    Aug 24 11:39:29.534: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-9d704f62-4d95-49c5-b380-04509ac78237 container test-container: <nil>
    STEP: delete the pod 08/24/23 11:39:29.568
    Aug 24 11:39:29.590: INFO: Waiting for pod pod-9d704f62-4d95-49c5-b380-04509ac78237 to disappear
    Aug 24 11:39:29.597: INFO: Pod pod-9d704f62-4d95-49c5-b380-04509ac78237 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:39:29.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-582" for this suite. 08/24/23 11:39:29.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:39:29.619
Aug 24 11:39:29.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 11:39:29.623
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:29.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:29.654
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-27c0ae38-d4e5-4f42-bd9f-0ae4fc607487 08/24/23 11:39:29.657
STEP: Creating a pod to test consume configMaps 08/24/23 11:39:29.665
Aug 24 11:39:29.679: INFO: Waiting up to 5m0s for pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e" in namespace "configmap-8803" to be "Succeeded or Failed"
Aug 24 11:39:29.689: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.846731ms
Aug 24 11:39:31.698: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018640343s
Aug 24 11:39:33.701: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022000598s
STEP: Saw pod success 08/24/23 11:39:33.701
Aug 24 11:39:33.702: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e" satisfied condition "Succeeded or Failed"
Aug 24 11:39:33.708: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:39:33.72
Aug 24 11:39:33.744: INFO: Waiting for pod pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e to disappear
Aug 24 11:39:33.751: INFO: Pod pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:39:33.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8803" for this suite. 08/24/23 11:39:33.758
------------------------------
â€¢ [4.157 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:39:29.619
    Aug 24 11:39:29.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 11:39:29.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:29.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:29.654
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-27c0ae38-d4e5-4f42-bd9f-0ae4fc607487 08/24/23 11:39:29.657
    STEP: Creating a pod to test consume configMaps 08/24/23 11:39:29.665
    Aug 24 11:39:29.679: INFO: Waiting up to 5m0s for pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e" in namespace "configmap-8803" to be "Succeeded or Failed"
    Aug 24 11:39:29.689: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.846731ms
    Aug 24 11:39:31.698: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018640343s
    Aug 24 11:39:33.701: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022000598s
    STEP: Saw pod success 08/24/23 11:39:33.701
    Aug 24 11:39:33.702: INFO: Pod "pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e" satisfied condition "Succeeded or Failed"
    Aug 24 11:39:33.708: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:39:33.72
    Aug 24 11:39:33.744: INFO: Waiting for pod pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e to disappear
    Aug 24 11:39:33.751: INFO: Pod pod-configmaps-0735dabf-60e7-4156-87cc-6b76bd7de40e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:39:33.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8803" for this suite. 08/24/23 11:39:33.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:39:33.78
Aug 24 11:39:33.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:39:33.781
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:33.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:33.825
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/24/23 11:39:33.835
Aug 24 11:39:33.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/24/23 11:39:43.357
Aug 24 11:39:43.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:39:46.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:39:56.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9510" for this suite. 08/24/23 11:39:56.573
------------------------------
â€¢ [SLOW TEST] [22.804 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:39:33.78
    Aug 24 11:39:33.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:39:33.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:33.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:33.825
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/24/23 11:39:33.835
    Aug 24 11:39:33.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/24/23 11:39:43.357
    Aug 24 11:39:43.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:39:46.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:39:56.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9510" for this suite. 08/24/23 11:39:56.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:39:56.585
Aug 24 11:39:56.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 11:39:56.587
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:56.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:56.635
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-512a52d9-1e8f-4493-a78a-c9174abb4025 08/24/23 11:39:56.653
STEP: Creating configMap with name cm-test-opt-upd-25bb1507-5430-47ff-a71c-7ff77df1b903 08/24/23 11:39:56.665
STEP: Creating the pod 08/24/23 11:39:56.673
Aug 24 11:39:56.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc" in namespace "configmap-141" to be "running and ready"
Aug 24 11:39:56.698: INFO: Pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.315763ms
Aug 24 11:39:56.698: INFO: The phase of Pod pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:39:58.708: INFO: Pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.0191577s
Aug 24 11:39:58.709: INFO: The phase of Pod pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc is Running (Ready = true)
Aug 24 11:39:58.709: INFO: Pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-512a52d9-1e8f-4493-a78a-c9174abb4025 08/24/23 11:39:58.771
STEP: Updating configmap cm-test-opt-upd-25bb1507-5430-47ff-a71c-7ff77df1b903 08/24/23 11:39:58.785
STEP: Creating configMap with name cm-test-opt-create-92d519e1-9723-49b0-9fad-a7486fda7de6 08/24/23 11:39:58.798
STEP: waiting to observe update in volume 08/24/23 11:39:58.807
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-141" for this suite. 08/24/23 11:40:00.897
------------------------------
â€¢ [4.330 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:39:56.585
    Aug 24 11:39:56.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 11:39:56.587
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:39:56.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:39:56.635
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-512a52d9-1e8f-4493-a78a-c9174abb4025 08/24/23 11:39:56.653
    STEP: Creating configMap with name cm-test-opt-upd-25bb1507-5430-47ff-a71c-7ff77df1b903 08/24/23 11:39:56.665
    STEP: Creating the pod 08/24/23 11:39:56.673
    Aug 24 11:39:56.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc" in namespace "configmap-141" to be "running and ready"
    Aug 24 11:39:56.698: INFO: Pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.315763ms
    Aug 24 11:39:56.698: INFO: The phase of Pod pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:39:58.708: INFO: Pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.0191577s
    Aug 24 11:39:58.709: INFO: The phase of Pod pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc is Running (Ready = true)
    Aug 24 11:39:58.709: INFO: Pod "pod-configmaps-c36d4986-7b91-46e5-866d-47e627c07fcc" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-512a52d9-1e8f-4493-a78a-c9174abb4025 08/24/23 11:39:58.771
    STEP: Updating configmap cm-test-opt-upd-25bb1507-5430-47ff-a71c-7ff77df1b903 08/24/23 11:39:58.785
    STEP: Creating configMap with name cm-test-opt-create-92d519e1-9723-49b0-9fad-a7486fda7de6 08/24/23 11:39:58.798
    STEP: waiting to observe update in volume 08/24/23 11:39:58.807
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:00.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-141" for this suite. 08/24/23 11:40:00.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:00.925
Aug 24 11:40:00.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename watch 08/24/23 11:40:00.93
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:00.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:00.972
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/24/23 11:40:00.978
STEP: creating a watch on configmaps with label B 08/24/23 11:40:00.98
STEP: creating a watch on configmaps with label A or B 08/24/23 11:40:00.983
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/24/23 11:40:00.985
Aug 24 11:40:00.997: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3696 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:40:00.998: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3696 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/24/23 11:40:00.998
Aug 24 11:40:01.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3697 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:40:01.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3697 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/24/23 11:40:01.02
Aug 24 11:40:01.042: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3698 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:40:01.042: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3698 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/24/23 11:40:01.043
Aug 24 11:40:01.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3699 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:40:01.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3699 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/24/23 11:40:01.062
Aug 24 11:40:01.074: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3700 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:40:01.075: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3700 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/24/23 11:40:11.076
Aug 24 11:40:11.119: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3747 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:40:11.119: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3747 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:21.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4634" for this suite. 08/24/23 11:40:21.135
------------------------------
â€¢ [SLOW TEST] [20.221 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:00.925
    Aug 24 11:40:00.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename watch 08/24/23 11:40:00.93
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:00.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:00.972
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/24/23 11:40:00.978
    STEP: creating a watch on configmaps with label B 08/24/23 11:40:00.98
    STEP: creating a watch on configmaps with label A or B 08/24/23 11:40:00.983
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/24/23 11:40:00.985
    Aug 24 11:40:00.997: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3696 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:40:00.998: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3696 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/24/23 11:40:00.998
    Aug 24 11:40:01.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3697 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:40:01.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3697 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/24/23 11:40:01.02
    Aug 24 11:40:01.042: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3698 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:40:01.042: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3698 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/24/23 11:40:01.043
    Aug 24 11:40:01.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3699 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:40:01.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4634  8c60aa99-b907-465d-995d-4acb603372e5 3699 0 2023-08-24 11:40:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/24/23 11:40:01.062
    Aug 24 11:40:01.074: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3700 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:40:01.075: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3700 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/24/23 11:40:11.076
    Aug 24 11:40:11.119: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3747 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:40:11.119: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4634  43048ee0-21f0-4410-a51e-c19b47370d8c 3747 0 2023-08-24 11:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:21.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4634" for this suite. 08/24/23 11:40:21.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:21.152
Aug 24 11:40:21.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:40:21.156
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:21.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:21.193
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/24/23 11:40:21.197
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/24/23 11:40:21.199
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 11:40:21.2
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/24/23 11:40:21.2
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/24/23 11:40:21.201
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:40:21.202
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:40:21.203
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:21.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3968" for this suite. 08/24/23 11:40:21.211
------------------------------
â€¢ [0.072 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:21.152
    Aug 24 11:40:21.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:40:21.156
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:21.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:21.193
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/24/23 11:40:21.197
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/24/23 11:40:21.199
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 11:40:21.2
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/24/23 11:40:21.2
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/24/23 11:40:21.201
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:40:21.202
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:40:21.203
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:21.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3968" for this suite. 08/24/23 11:40:21.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:21.228
Aug 24 11:40:21.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 11:40:21.229
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:21.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:21.258
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-8924e332-627e-4966-a6f7-3b6f225f8d8a 08/24/23 11:40:21.262
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:21.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4205" for this suite. 08/24/23 11:40:21.272
------------------------------
â€¢ [0.054 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:21.228
    Aug 24 11:40:21.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 11:40:21.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:21.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:21.258
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-8924e332-627e-4966-a6f7-3b6f225f8d8a 08/24/23 11:40:21.262
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:21.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4205" for this suite. 08/24/23 11:40:21.272
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:21.282
Aug 24 11:40:21.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 11:40:21.285
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:21.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:21.31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:40:21.314
Aug 24 11:40:21.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 24 11:40:21.663: INFO: stderr: ""
Aug 24 11:40:21.663: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/24/23 11:40:21.663
STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 11:40:31.718
Aug 24 11:40:31.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 get pod e2e-test-httpd-pod -o json'
Aug 24 11:40:31.861: INFO: stderr: ""
Aug 24 11:40:31.861: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-24T11:40:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6093\",\n        \"resourceVersion\": \"3828\",\n        \"uid\": \"f98da024-0669-45a1-9d3c-f59305a59de6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-slqvb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pe9deep4seen-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-slqvb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://361aac20fcc4b35fad82a28383a1c9f4914a894be0238bf4d906c21dd2523ccb\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-24T11:40:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.130\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.206\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.206\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-24T11:40:21Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/24/23 11:40:31.862
Aug 24 11:40:31.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 replace -f -'
Aug 24 11:40:32.970: INFO: stderr: ""
Aug 24 11:40:32.970: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/24/23 11:40:32.97
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 24 11:40:32.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 delete pods e2e-test-httpd-pod'
Aug 24 11:40:36.675: INFO: stderr: ""
Aug 24 11:40:36.675: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:36.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6093" for this suite. 08/24/23 11:40:36.689
------------------------------
â€¢ [SLOW TEST] [15.423 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:21.282
    Aug 24 11:40:21.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:40:21.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:21.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:21.31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:40:21.314
    Aug 24 11:40:21.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 24 11:40:21.663: INFO: stderr: ""
    Aug 24 11:40:21.663: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/24/23 11:40:21.663
    STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 11:40:31.718
    Aug 24 11:40:31.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 get pod e2e-test-httpd-pod -o json'
    Aug 24 11:40:31.861: INFO: stderr: ""
    Aug 24 11:40:31.861: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-24T11:40:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6093\",\n        \"resourceVersion\": \"3828\",\n        \"uid\": \"f98da024-0669-45a1-9d3c-f59305a59de6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-slqvb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"pe9deep4seen-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-slqvb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:40:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://361aac20fcc4b35fad82a28383a1c9f4914a894be0238bf4d906c21dd2523ccb\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-24T11:40:30Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.130\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.206\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.206\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-24T11:40:21Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/24/23 11:40:31.862
    Aug 24 11:40:31.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 replace -f -'
    Aug 24 11:40:32.970: INFO: stderr: ""
    Aug 24 11:40:32.970: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/24/23 11:40:32.97
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 24 11:40:32.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6093 delete pods e2e-test-httpd-pod'
    Aug 24 11:40:36.675: INFO: stderr: ""
    Aug 24 11:40:36.675: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:36.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6093" for this suite. 08/24/23 11:40:36.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:36.709
Aug 24 11:40:36.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename controllerrevisions 08/24/23 11:40:36.712
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:36.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:36.746
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-xww5l-daemon-set" 08/24/23 11:40:36.796
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 11:40:36.81
Aug 24 11:40:36.827: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 0
Aug 24 11:40:36.827: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:37.859: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 0
Aug 24 11:40:37.859: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:38.842: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:38.843: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:39.848: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:39.848: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:40.845: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:40.846: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:41.844: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:41.844: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:42.861: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:42.861: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:43.844: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:43.844: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:44.851: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:44.851: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:45.845: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
Aug 24 11:40:45.845: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:46.841: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:46.842: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:47.851: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:47.851: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:48.842: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:48.842: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:49.847: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:49.847: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:50.864: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:50.864: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:51.842: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:51.842: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:52.853: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:52.853: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:53.844: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:53.844: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:54.846: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:54.846: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:55.846: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:55.846: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:56.843: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
Aug 24 11:40:56.843: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:40:57.843: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 3
Aug 24 11:40:57.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-xww5l-daemon-set
STEP: Confirm DaemonSet "e2e-xww5l-daemon-set" successfully created with "daemonset-name=e2e-xww5l-daemon-set" label 08/24/23 11:40:57.851
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xww5l-daemon-set" 08/24/23 11:40:57.865
Aug 24 11:40:57.870: INFO: Located ControllerRevision: "e2e-xww5l-daemon-set-7f6fbc98bd"
STEP: Patching ControllerRevision "e2e-xww5l-daemon-set-7f6fbc98bd" 08/24/23 11:40:57.875
Aug 24 11:40:57.920: INFO: e2e-xww5l-daemon-set-7f6fbc98bd has been patched
STEP: Create a new ControllerRevision 08/24/23 11:40:57.92
Aug 24 11:40:57.931: INFO: Created ControllerRevision: e2e-xww5l-daemon-set-677f49f974
STEP: Confirm that there are two ControllerRevisions 08/24/23 11:40:57.931
Aug 24 11:40:57.932: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 11:40:57.939: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-xww5l-daemon-set-7f6fbc98bd" 08/24/23 11:40:57.939
STEP: Confirm that there is only one ControllerRevision 08/24/23 11:40:57.953
Aug 24 11:40:57.953: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 11:40:57.959: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-xww5l-daemon-set-677f49f974" 08/24/23 11:40:57.964
Aug 24 11:40:57.978: INFO: e2e-xww5l-daemon-set-677f49f974 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/24/23 11:40:57.978
W0824 11:40:57.989363      14 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/24/23 11:40:57.989
Aug 24 11:40:57.989: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 11:40:58.999: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 11:40:59.009: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xww5l-daemon-set-677f49f974=updated" 08/24/23 11:40:59.009
STEP: Confirm that there is only one ControllerRevision 08/24/23 11:40:59.021
Aug 24 11:40:59.022: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 11:40:59.027: INFO: Found 1 ControllerRevisions
Aug 24 11:40:59.032: INFO: ControllerRevision "e2e-xww5l-daemon-set-854b796487" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-xww5l-daemon-set" 08/24/23 11:40:59.037
STEP: deleting DaemonSet.extensions e2e-xww5l-daemon-set in namespace controllerrevisions-7151, will wait for the garbage collector to delete the pods 08/24/23 11:40:59.037
Aug 24 11:40:59.109: INFO: Deleting DaemonSet.extensions e2e-xww5l-daemon-set took: 13.920995ms
Aug 24 11:40:59.329: INFO: Terminating DaemonSet.extensions e2e-xww5l-daemon-set pods took: 220.25128ms
Aug 24 11:41:01.836: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 0
Aug 24 11:41:01.836: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xww5l-daemon-set
Aug 24 11:41:01.844: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4014"},"items":null}

Aug 24 11:41:01.850: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4014"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:01.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-7151" for this suite. 08/24/23 11:41:01.879
------------------------------
â€¢ [SLOW TEST] [25.182 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:36.709
    Aug 24 11:40:36.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename controllerrevisions 08/24/23 11:40:36.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:36.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:36.746
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-xww5l-daemon-set" 08/24/23 11:40:36.796
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 11:40:36.81
    Aug 24 11:40:36.827: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 0
    Aug 24 11:40:36.827: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:37.859: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 0
    Aug 24 11:40:37.859: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:38.842: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:38.843: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:39.848: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:39.848: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:40.845: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:40.846: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:41.844: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:41.844: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:42.861: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:42.861: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:43.844: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:43.844: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:44.851: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:44.851: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:45.845: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 1
    Aug 24 11:40:45.845: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:46.841: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:46.842: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:47.851: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:47.851: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:48.842: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:48.842: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:49.847: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:49.847: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:50.864: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:50.864: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:51.842: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:51.842: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:52.853: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:52.853: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:53.844: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:53.844: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:54.846: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:54.846: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:55.846: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:55.846: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:56.843: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 2
    Aug 24 11:40:56.843: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:40:57.843: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 3
    Aug 24 11:40:57.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-xww5l-daemon-set
    STEP: Confirm DaemonSet "e2e-xww5l-daemon-set" successfully created with "daemonset-name=e2e-xww5l-daemon-set" label 08/24/23 11:40:57.851
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xww5l-daemon-set" 08/24/23 11:40:57.865
    Aug 24 11:40:57.870: INFO: Located ControllerRevision: "e2e-xww5l-daemon-set-7f6fbc98bd"
    STEP: Patching ControllerRevision "e2e-xww5l-daemon-set-7f6fbc98bd" 08/24/23 11:40:57.875
    Aug 24 11:40:57.920: INFO: e2e-xww5l-daemon-set-7f6fbc98bd has been patched
    STEP: Create a new ControllerRevision 08/24/23 11:40:57.92
    Aug 24 11:40:57.931: INFO: Created ControllerRevision: e2e-xww5l-daemon-set-677f49f974
    STEP: Confirm that there are two ControllerRevisions 08/24/23 11:40:57.931
    Aug 24 11:40:57.932: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 11:40:57.939: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-xww5l-daemon-set-7f6fbc98bd" 08/24/23 11:40:57.939
    STEP: Confirm that there is only one ControllerRevision 08/24/23 11:40:57.953
    Aug 24 11:40:57.953: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 11:40:57.959: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-xww5l-daemon-set-677f49f974" 08/24/23 11:40:57.964
    Aug 24 11:40:57.978: INFO: e2e-xww5l-daemon-set-677f49f974 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/24/23 11:40:57.978
    W0824 11:40:57.989363      14 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/24/23 11:40:57.989
    Aug 24 11:40:57.989: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 11:40:58.999: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 11:40:59.009: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xww5l-daemon-set-677f49f974=updated" 08/24/23 11:40:59.009
    STEP: Confirm that there is only one ControllerRevision 08/24/23 11:40:59.021
    Aug 24 11:40:59.022: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 11:40:59.027: INFO: Found 1 ControllerRevisions
    Aug 24 11:40:59.032: INFO: ControllerRevision "e2e-xww5l-daemon-set-854b796487" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-xww5l-daemon-set" 08/24/23 11:40:59.037
    STEP: deleting DaemonSet.extensions e2e-xww5l-daemon-set in namespace controllerrevisions-7151, will wait for the garbage collector to delete the pods 08/24/23 11:40:59.037
    Aug 24 11:40:59.109: INFO: Deleting DaemonSet.extensions e2e-xww5l-daemon-set took: 13.920995ms
    Aug 24 11:40:59.329: INFO: Terminating DaemonSet.extensions e2e-xww5l-daemon-set pods took: 220.25128ms
    Aug 24 11:41:01.836: INFO: Number of nodes with available pods controlled by daemonset e2e-xww5l-daemon-set: 0
    Aug 24 11:41:01.836: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xww5l-daemon-set
    Aug 24 11:41:01.844: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4014"},"items":null}

    Aug 24 11:41:01.850: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4014"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:01.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-7151" for this suite. 08/24/23 11:41:01.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:01.9
Aug 24 11:41:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename csistoragecapacity 08/24/23 11:41:01.902
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:01.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:01.935
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/24/23 11:41:01.938
STEP: getting /apis/storage.k8s.io 08/24/23 11:41:01.942
STEP: getting /apis/storage.k8s.io/v1 08/24/23 11:41:01.944
STEP: creating 08/24/23 11:41:01.946
STEP: watching 08/24/23 11:41:01.974
Aug 24 11:41:01.975: INFO: starting watch
STEP: getting 08/24/23 11:41:01.985
STEP: listing in namespace 08/24/23 11:41:01.989
STEP: listing across namespaces 08/24/23 11:41:01.994
STEP: patching 08/24/23 11:41:01.999
STEP: updating 08/24/23 11:41:02.007
Aug 24 11:41:02.015: INFO: waiting for watch events with expected annotations in namespace
Aug 24 11:41:02.015: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/24/23 11:41:02.015
STEP: deleting a collection 08/24/23 11:41:02.036
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:02.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6038" for this suite. 08/24/23 11:41:02.074
------------------------------
â€¢ [0.201 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:01.9
    Aug 24 11:41:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename csistoragecapacity 08/24/23 11:41:01.902
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:01.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:01.935
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/24/23 11:41:01.938
    STEP: getting /apis/storage.k8s.io 08/24/23 11:41:01.942
    STEP: getting /apis/storage.k8s.io/v1 08/24/23 11:41:01.944
    STEP: creating 08/24/23 11:41:01.946
    STEP: watching 08/24/23 11:41:01.974
    Aug 24 11:41:01.975: INFO: starting watch
    STEP: getting 08/24/23 11:41:01.985
    STEP: listing in namespace 08/24/23 11:41:01.989
    STEP: listing across namespaces 08/24/23 11:41:01.994
    STEP: patching 08/24/23 11:41:01.999
    STEP: updating 08/24/23 11:41:02.007
    Aug 24 11:41:02.015: INFO: waiting for watch events with expected annotations in namespace
    Aug 24 11:41:02.015: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/24/23 11:41:02.015
    STEP: deleting a collection 08/24/23 11:41:02.036
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:02.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6038" for this suite. 08/24/23 11:41:02.074
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:02.104
Aug 24 11:41:02.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename init-container 08/24/23 11:41:02.107
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:02.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:02.185
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/24/23 11:41:02.208
Aug 24 11:41:02.209: INFO: PodSpec: initContainers in spec.initContainers
Aug 24 11:41:43.857: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2941ac8c-c70b-4878-b01a-5ab6e1e53abc", GenerateName:"", Namespace:"init-container-5099", SelfLink:"", UID:"d4d8a34c-64fd-4b87-86d9-f40af842551f", ResourceVersion:"4183", Generation:0, CreationTimestamp:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"208999300"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014c1320), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 41, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014c1368), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-4pwnk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003aa71c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4pwnk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4pwnk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4pwnk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007152760), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pe9deep4seen-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000736460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0071527f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007152810)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007152818), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00715281c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000aff790), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.130", PodIP:"10.233.66.105", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.105"}}, StartTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000736540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007365b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://3ff443c9a48c63f15cf2e36319b63ea982667e37cedd53313b9bf936d54a4d78", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa7240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa7220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc007152894)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:43.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5099" for this suite. 08/24/23 11:41:43.882
------------------------------
â€¢ [SLOW TEST] [41.794 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:02.104
    Aug 24 11:41:02.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename init-container 08/24/23 11:41:02.107
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:02.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:02.185
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/24/23 11:41:02.208
    Aug 24 11:41:02.209: INFO: PodSpec: initContainers in spec.initContainers
    Aug 24 11:41:43.857: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2941ac8c-c70b-4878-b01a-5ab6e1e53abc", GenerateName:"", Namespace:"init-container-5099", SelfLink:"", UID:"d4d8a34c-64fd-4b87-86d9-f40af842551f", ResourceVersion:"4183", Generation:0, CreationTimestamp:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"208999300"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014c1320), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 41, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014c1368), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-4pwnk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003aa71c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4pwnk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4pwnk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4pwnk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007152760), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"pe9deep4seen-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000736460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0071527f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007152810)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007152818), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00715281c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000aff790), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.130", PodIP:"10.233.66.105", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.105"}}, StartTime:time.Date(2023, time.August, 24, 11, 41, 2, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000736540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0007365b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://3ff443c9a48c63f15cf2e36319b63ea982667e37cedd53313b9bf936d54a4d78", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa7240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003aa7220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc007152894)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:43.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5099" for this suite. 08/24/23 11:41:43.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:43.911
Aug 24 11:41:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 11:41:43.916
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:43.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:44.001
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:41:44.005
Aug 24 11:41:44.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313" in namespace "projected-1818" to be "Succeeded or Failed"
Aug 24 11:41:44.024: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745515ms
Aug 24 11:41:46.037: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01728521s
Aug 24 11:41:48.032: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012904847s
STEP: Saw pod success 08/24/23 11:41:48.032
Aug 24 11:41:48.033: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313" satisfied condition "Succeeded or Failed"
Aug 24 11:41:48.037: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313 container client-container: <nil>
STEP: delete the pod 08/24/23 11:41:48.062
Aug 24 11:41:48.094: INFO: Waiting for pod downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313 to disappear
Aug 24 11:41:48.100: INFO: Pod downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1818" for this suite. 08/24/23 11:41:48.113
------------------------------
â€¢ [4.216 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:43.911
    Aug 24 11:41:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 11:41:43.916
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:43.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:44.001
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:41:44.005
    Aug 24 11:41:44.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313" in namespace "projected-1818" to be "Succeeded or Failed"
    Aug 24 11:41:44.024: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745515ms
    Aug 24 11:41:46.037: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01728521s
    Aug 24 11:41:48.032: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012904847s
    STEP: Saw pod success 08/24/23 11:41:48.032
    Aug 24 11:41:48.033: INFO: Pod "downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313" satisfied condition "Succeeded or Failed"
    Aug 24 11:41:48.037: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:41:48.062
    Aug 24 11:41:48.094: INFO: Waiting for pod downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313 to disappear
    Aug 24 11:41:48.100: INFO: Pod downwardapi-volume-a8e582bf-bf8e-4ff5-8cd6-e635d4fb3313 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1818" for this suite. 08/24/23 11:41:48.113
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:48.132
Aug 24 11:41:48.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replicaset 08/24/23 11:41:48.134
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:48.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:48.165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/24/23 11:41:48.167
Aug 24 11:41:48.183: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6082" to be "running and ready"
Aug 24 11:41:48.192: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.992884ms
Aug 24 11:41:48.192: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:41:50.211: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.02836089s
Aug 24 11:41:50.211: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 24 11:41:50.211: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/24/23 11:41:50.237
STEP: Then the orphan pod is adopted 08/24/23 11:41:50.248
STEP: When the matched label of one of its pods change 08/24/23 11:41:51.264
Aug 24 11:41:51.270: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/24/23 11:41:51.287
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:52.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6082" for this suite. 08/24/23 11:41:52.32
------------------------------
â€¢ [4.200 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:48.132
    Aug 24 11:41:48.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replicaset 08/24/23 11:41:48.134
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:48.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:48.165
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/24/23 11:41:48.167
    Aug 24 11:41:48.183: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6082" to be "running and ready"
    Aug 24 11:41:48.192: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.992884ms
    Aug 24 11:41:48.192: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:41:50.211: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.02836089s
    Aug 24 11:41:50.211: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 24 11:41:50.211: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/24/23 11:41:50.237
    STEP: Then the orphan pod is adopted 08/24/23 11:41:50.248
    STEP: When the matched label of one of its pods change 08/24/23 11:41:51.264
    Aug 24 11:41:51.270: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/24/23 11:41:51.287
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:52.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6082" for this suite. 08/24/23 11:41:52.32
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:52.342
Aug 24 11:41:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 11:41:52.347
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:52.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:52.382
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-d895e7da-b0de-4775-be72-0c1b9455dfe1 08/24/23 11:41:52.386
STEP: Creating a pod to test consume secrets 08/24/23 11:41:52.397
Aug 24 11:41:52.414: INFO: Waiting up to 5m0s for pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404" in namespace "secrets-9933" to be "Succeeded or Failed"
Aug 24 11:41:52.425: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404": Phase="Pending", Reason="", readiness=false. Elapsed: 10.52098ms
Aug 24 11:41:54.435: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020555948s
Aug 24 11:41:56.437: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022931478s
STEP: Saw pod success 08/24/23 11:41:56.438
Aug 24 11:41:56.438: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404" satisfied condition "Succeeded or Failed"
Aug 24 11:41:56.444: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:41:56.456
Aug 24 11:41:56.474: INFO: Waiting for pod pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404 to disappear
Aug 24 11:41:56.479: INFO: Pod pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:56.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9933" for this suite. 08/24/23 11:41:56.488
------------------------------
â€¢ [4.157 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:52.342
    Aug 24 11:41:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 11:41:52.347
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:52.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:52.382
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-d895e7da-b0de-4775-be72-0c1b9455dfe1 08/24/23 11:41:52.386
    STEP: Creating a pod to test consume secrets 08/24/23 11:41:52.397
    Aug 24 11:41:52.414: INFO: Waiting up to 5m0s for pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404" in namespace "secrets-9933" to be "Succeeded or Failed"
    Aug 24 11:41:52.425: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404": Phase="Pending", Reason="", readiness=false. Elapsed: 10.52098ms
    Aug 24 11:41:54.435: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020555948s
    Aug 24 11:41:56.437: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022931478s
    STEP: Saw pod success 08/24/23 11:41:56.438
    Aug 24 11:41:56.438: INFO: Pod "pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404" satisfied condition "Succeeded or Failed"
    Aug 24 11:41:56.444: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:41:56.456
    Aug 24 11:41:56.474: INFO: Waiting for pod pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404 to disappear
    Aug 24 11:41:56.479: INFO: Pod pod-secrets-b51e94bd-0928-4a49-a5e0-35a0b9ac0404 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:56.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9933" for this suite. 08/24/23 11:41:56.488
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:56.5
Aug 24 11:41:56.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 11:41:56.501
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:56.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:56.544
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Aug 24 11:41:56.601: INFO: Create a RollingUpdate DaemonSet
Aug 24 11:41:56.621: INFO: Check that daemon pods launch on every node of the cluster
Aug 24 11:41:56.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 11:41:56.637: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:41:57.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 11:41:57.664: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:41:58.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 11:41:58.675: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 11:41:59.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 11:41:59.652: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 24 11:41:59.652: INFO: Update the DaemonSet to trigger a rollout
Aug 24 11:41:59.673: INFO: Updating DaemonSet daemon-set
Aug 24 11:42:02.727: INFO: Roll back the DaemonSet before rollout is complete
Aug 24 11:42:02.745: INFO: Updating DaemonSet daemon-set
Aug 24 11:42:02.745: INFO: Make sure DaemonSet rollback is complete
Aug 24 11:42:02.760: INFO: Wrong image for pod: daemon-set-gznn7. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 24 11:42:02.760: INFO: Pod daemon-set-gznn7 is not available
Aug 24 11:42:08.775: INFO: Pod daemon-set-zbhft is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 11:42:08.794
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-790, will wait for the garbage collector to delete the pods 08/24/23 11:42:08.794
Aug 24 11:42:08.860: INFO: Deleting DaemonSet.extensions daemon-set took: 10.458247ms
Aug 24 11:42:08.961: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.903484ms
Aug 24 11:42:12.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 11:42:12.367: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 11:42:12.374: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4480"},"items":null}

Aug 24 11:42:12.380: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4480"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-790" for this suite. 08/24/23 11:42:12.415
------------------------------
â€¢ [SLOW TEST] [15.930 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:56.5
    Aug 24 11:41:56.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 11:41:56.501
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:56.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:56.544
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Aug 24 11:41:56.601: INFO: Create a RollingUpdate DaemonSet
    Aug 24 11:41:56.621: INFO: Check that daemon pods launch on every node of the cluster
    Aug 24 11:41:56.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 11:41:56.637: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:41:57.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 11:41:57.664: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:41:58.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 11:41:58.675: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 11:41:59.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 11:41:59.652: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Aug 24 11:41:59.652: INFO: Update the DaemonSet to trigger a rollout
    Aug 24 11:41:59.673: INFO: Updating DaemonSet daemon-set
    Aug 24 11:42:02.727: INFO: Roll back the DaemonSet before rollout is complete
    Aug 24 11:42:02.745: INFO: Updating DaemonSet daemon-set
    Aug 24 11:42:02.745: INFO: Make sure DaemonSet rollback is complete
    Aug 24 11:42:02.760: INFO: Wrong image for pod: daemon-set-gznn7. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 24 11:42:02.760: INFO: Pod daemon-set-gznn7 is not available
    Aug 24 11:42:08.775: INFO: Pod daemon-set-zbhft is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 11:42:08.794
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-790, will wait for the garbage collector to delete the pods 08/24/23 11:42:08.794
    Aug 24 11:42:08.860: INFO: Deleting DaemonSet.extensions daemon-set took: 10.458247ms
    Aug 24 11:42:08.961: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.903484ms
    Aug 24 11:42:12.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 11:42:12.367: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 11:42:12.374: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4480"},"items":null}

    Aug 24 11:42:12.380: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4480"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-790" for this suite. 08/24/23 11:42:12.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:12.433
Aug 24 11:42:12.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 11:42:12.436
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:12.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:12.477
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 11:42:12.483
Aug 24 11:42:12.496: INFO: Waiting up to 5m0s for pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b" in namespace "emptydir-7113" to be "Succeeded or Failed"
Aug 24 11:42:12.502: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.929422ms
Aug 24 11:42:14.518: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021613182s
Aug 24 11:42:16.531: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034391378s
STEP: Saw pod success 08/24/23 11:42:16.531
Aug 24 11:42:16.532: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b" satisfied condition "Succeeded or Failed"
Aug 24 11:42:16.539: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-7803c93c-9051-4327-a2e2-9c878715fe0b container test-container: <nil>
STEP: delete the pod 08/24/23 11:42:16.551
Aug 24 11:42:16.572: INFO: Waiting for pod pod-7803c93c-9051-4327-a2e2-9c878715fe0b to disappear
Aug 24 11:42:16.579: INFO: Pod pod-7803c93c-9051-4327-a2e2-9c878715fe0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7113" for this suite. 08/24/23 11:42:16.592
------------------------------
â€¢ [4.172 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:12.433
    Aug 24 11:42:12.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:42:12.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:12.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:12.477
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 11:42:12.483
    Aug 24 11:42:12.496: INFO: Waiting up to 5m0s for pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b" in namespace "emptydir-7113" to be "Succeeded or Failed"
    Aug 24 11:42:12.502: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.929422ms
    Aug 24 11:42:14.518: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021613182s
    Aug 24 11:42:16.531: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034391378s
    STEP: Saw pod success 08/24/23 11:42:16.531
    Aug 24 11:42:16.532: INFO: Pod "pod-7803c93c-9051-4327-a2e2-9c878715fe0b" satisfied condition "Succeeded or Failed"
    Aug 24 11:42:16.539: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-7803c93c-9051-4327-a2e2-9c878715fe0b container test-container: <nil>
    STEP: delete the pod 08/24/23 11:42:16.551
    Aug 24 11:42:16.572: INFO: Waiting for pod pod-7803c93c-9051-4327-a2e2-9c878715fe0b to disappear
    Aug 24 11:42:16.579: INFO: Pod pod-7803c93c-9051-4327-a2e2-9c878715fe0b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7113" for this suite. 08/24/23 11:42:16.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:16.611
Aug 24 11:42:16.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename endpointslicemirroring 08/24/23 11:42:16.613
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:16.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:16.651
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/24/23 11:42:16.672
Aug 24 11:42:16.690: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/24/23 11:42:18.701
STEP: mirroring deletion of a custom Endpoint 08/24/23 11:42:18.724
Aug 24 11:42:18.750: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:20.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-36" for this suite. 08/24/23 11:42:20.764
------------------------------
â€¢ [4.165 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:16.611
    Aug 24 11:42:16.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename endpointslicemirroring 08/24/23 11:42:16.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:16.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:16.651
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/24/23 11:42:16.672
    Aug 24 11:42:16.690: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/24/23 11:42:18.701
    STEP: mirroring deletion of a custom Endpoint 08/24/23 11:42:18.724
    Aug 24 11:42:18.750: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:20.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-36" for this suite. 08/24/23 11:42:20.764
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:20.778
Aug 24 11:42:20.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename subpath 08/24/23 11:42:20.781
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:20.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:20.816
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 11:42:20.821
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-d4g6 08/24/23 11:42:20.838
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:42:20.839
Aug 24 11:42:20.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-d4g6" in namespace "subpath-855" to be "Succeeded or Failed"
Aug 24 11:42:20.870: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.553165ms
Aug 24 11:42:22.879: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023861689s
Aug 24 11:42:24.880: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 4.024412624s
Aug 24 11:42:26.880: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 6.024250691s
Aug 24 11:42:28.888: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 8.032609071s
Aug 24 11:42:30.882: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 10.026461422s
Aug 24 11:42:32.882: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 12.026184906s
Aug 24 11:42:34.883: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 14.027264497s
Aug 24 11:42:36.876: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 16.020733632s
Aug 24 11:42:38.879: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 18.023149428s
Aug 24 11:42:40.881: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 20.025053116s
Aug 24 11:42:42.880: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=false. Elapsed: 22.024434299s
Aug 24 11:42:44.882: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02619949s
STEP: Saw pod success 08/24/23 11:42:44.882
Aug 24 11:42:44.882: INFO: Pod "pod-subpath-test-projected-d4g6" satisfied condition "Succeeded or Failed"
Aug 24 11:42:44.892: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-projected-d4g6 container test-container-subpath-projected-d4g6: <nil>
STEP: delete the pod 08/24/23 11:42:44.908
Aug 24 11:42:44.928: INFO: Waiting for pod pod-subpath-test-projected-d4g6 to disappear
Aug 24 11:42:44.935: INFO: Pod pod-subpath-test-projected-d4g6 no longer exists
STEP: Deleting pod pod-subpath-test-projected-d4g6 08/24/23 11:42:44.935
Aug 24 11:42:44.936: INFO: Deleting pod "pod-subpath-test-projected-d4g6" in namespace "subpath-855"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:44.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-855" for this suite. 08/24/23 11:42:44.95
------------------------------
â€¢ [SLOW TEST] [24.183 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:20.778
    Aug 24 11:42:20.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename subpath 08/24/23 11:42:20.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:20.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:20.816
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 11:42:20.821
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-d4g6 08/24/23 11:42:20.838
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:42:20.839
    Aug 24 11:42:20.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-d4g6" in namespace "subpath-855" to be "Succeeded or Failed"
    Aug 24 11:42:20.870: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.553165ms
    Aug 24 11:42:22.879: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023861689s
    Aug 24 11:42:24.880: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 4.024412624s
    Aug 24 11:42:26.880: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 6.024250691s
    Aug 24 11:42:28.888: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 8.032609071s
    Aug 24 11:42:30.882: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 10.026461422s
    Aug 24 11:42:32.882: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 12.026184906s
    Aug 24 11:42:34.883: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 14.027264497s
    Aug 24 11:42:36.876: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 16.020733632s
    Aug 24 11:42:38.879: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 18.023149428s
    Aug 24 11:42:40.881: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=true. Elapsed: 20.025053116s
    Aug 24 11:42:42.880: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Running", Reason="", readiness=false. Elapsed: 22.024434299s
    Aug 24 11:42:44.882: INFO: Pod "pod-subpath-test-projected-d4g6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02619949s
    STEP: Saw pod success 08/24/23 11:42:44.882
    Aug 24 11:42:44.882: INFO: Pod "pod-subpath-test-projected-d4g6" satisfied condition "Succeeded or Failed"
    Aug 24 11:42:44.892: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-projected-d4g6 container test-container-subpath-projected-d4g6: <nil>
    STEP: delete the pod 08/24/23 11:42:44.908
    Aug 24 11:42:44.928: INFO: Waiting for pod pod-subpath-test-projected-d4g6 to disappear
    Aug 24 11:42:44.935: INFO: Pod pod-subpath-test-projected-d4g6 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-d4g6 08/24/23 11:42:44.935
    Aug 24 11:42:44.936: INFO: Deleting pod "pod-subpath-test-projected-d4g6" in namespace "subpath-855"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:44.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-855" for this suite. 08/24/23 11:42:44.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:44.963
Aug 24 11:42:44.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 11:42:44.965
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:44.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:45.007
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-3037 08/24/23 11:42:45.012
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[] 08/24/23 11:42:45.033
Aug 24 11:42:45.050: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 24 11:42:46.066: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3037 08/24/23 11:42:46.066
Aug 24 11:42:46.084: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3037" to be "running and ready"
Aug 24 11:42:46.092: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.855046ms
Aug 24 11:42:46.092: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:42:48.099: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014771713s
Aug 24 11:42:48.099: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 24 11:42:48.099: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[pod1:[80]] 08/24/23 11:42:48.105
Aug 24 11:42:48.137: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/24/23 11:42:48.137
Aug 24 11:42:48.138: INFO: Creating new exec pod
Aug 24 11:42:48.152: INFO: Waiting up to 5m0s for pod "execpodsvgdv" in namespace "services-3037" to be "running"
Aug 24 11:42:48.161: INFO: Pod "execpodsvgdv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.104013ms
Aug 24 11:42:50.174: INFO: Pod "execpodsvgdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.021758893s
Aug 24 11:42:50.174: INFO: Pod "execpodsvgdv" satisfied condition "running"
Aug 24 11:42:51.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 24 11:42:51.495: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 24 11:42:51.496: INFO: stdout: ""
Aug 24 11:42:51.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 10.233.31.118 80'
Aug 24 11:42:51.736: INFO: stderr: "+ nc -v -z -w 2 10.233.31.118 80\nConnection to 10.233.31.118 80 port [tcp/http] succeeded!\n"
Aug 24 11:42:51.736: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-3037 08/24/23 11:42:51.736
Aug 24 11:42:51.747: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3037" to be "running and ready"
Aug 24 11:42:51.757: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.905344ms
Aug 24 11:42:51.757: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:42:53.767: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020571959s
Aug 24 11:42:53.768: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:42:55.766: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018963437s
Aug 24 11:42:55.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:42:57.767: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019811529s
Aug 24 11:42:57.767: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:42:59.765: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018620494s
Aug 24 11:42:59.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:43:01.765: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018643718s
Aug 24 11:43:01.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:43:03.764: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 12.016763586s
Aug 24 11:43:03.764: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 24 11:43:03.764: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[pod1:[80] pod2:[80]] 08/24/23 11:43:03.769
Aug 24 11:43:03.790: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/24/23 11:43:03.79
Aug 24 11:43:04.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 24 11:43:05.089: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 24 11:43:05.089: INFO: stdout: ""
Aug 24 11:43:05.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 10.233.31.118 80'
Aug 24 11:43:05.393: INFO: stderr: "+ nc -v -z -w 2 10.233.31.118 80\nConnection to 10.233.31.118 80 port [tcp/http] succeeded!\n"
Aug 24 11:43:05.393: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3037 08/24/23 11:43:05.393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[pod2:[80]] 08/24/23 11:43:05.423
Aug 24 11:43:06.495: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/24/23 11:43:06.495
Aug 24 11:43:07.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 24 11:43:07.788: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 24 11:43:07.788: INFO: stdout: ""
Aug 24 11:43:07.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 10.233.31.118 80'
Aug 24 11:43:08.037: INFO: stderr: "+ nc -v -z -w 2 10.233.31.118 80\nConnection to 10.233.31.118 80 port [tcp/http] succeeded!\n"
Aug 24 11:43:08.037: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-3037 08/24/23 11:43:08.037
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[] 08/24/23 11:43:08.115
Aug 24 11:43:09.155: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:09.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3037" for this suite. 08/24/23 11:43:09.23
------------------------------
â€¢ [SLOW TEST] [24.285 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:44.963
    Aug 24 11:42:44.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 11:42:44.965
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:44.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:45.007
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-3037 08/24/23 11:42:45.012
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[] 08/24/23 11:42:45.033
    Aug 24 11:42:45.050: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Aug 24 11:42:46.066: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3037 08/24/23 11:42:46.066
    Aug 24 11:42:46.084: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3037" to be "running and ready"
    Aug 24 11:42:46.092: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.855046ms
    Aug 24 11:42:46.092: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:42:48.099: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014771713s
    Aug 24 11:42:48.099: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 24 11:42:48.099: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[pod1:[80]] 08/24/23 11:42:48.105
    Aug 24 11:42:48.137: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/24/23 11:42:48.137
    Aug 24 11:42:48.138: INFO: Creating new exec pod
    Aug 24 11:42:48.152: INFO: Waiting up to 5m0s for pod "execpodsvgdv" in namespace "services-3037" to be "running"
    Aug 24 11:42:48.161: INFO: Pod "execpodsvgdv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.104013ms
    Aug 24 11:42:50.174: INFO: Pod "execpodsvgdv": Phase="Running", Reason="", readiness=true. Elapsed: 2.021758893s
    Aug 24 11:42:50.174: INFO: Pod "execpodsvgdv" satisfied condition "running"
    Aug 24 11:42:51.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 24 11:42:51.495: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 24 11:42:51.496: INFO: stdout: ""
    Aug 24 11:42:51.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 10.233.31.118 80'
    Aug 24 11:42:51.736: INFO: stderr: "+ nc -v -z -w 2 10.233.31.118 80\nConnection to 10.233.31.118 80 port [tcp/http] succeeded!\n"
    Aug 24 11:42:51.736: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-3037 08/24/23 11:42:51.736
    Aug 24 11:42:51.747: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3037" to be "running and ready"
    Aug 24 11:42:51.757: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.905344ms
    Aug 24 11:42:51.757: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:42:53.767: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020571959s
    Aug 24 11:42:53.768: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:42:55.766: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018963437s
    Aug 24 11:42:55.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:42:57.767: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019811529s
    Aug 24 11:42:57.767: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:42:59.765: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018620494s
    Aug 24 11:42:59.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:43:01.765: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018643718s
    Aug 24 11:43:01.766: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:43:03.764: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 12.016763586s
    Aug 24 11:43:03.764: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 24 11:43:03.764: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[pod1:[80] pod2:[80]] 08/24/23 11:43:03.769
    Aug 24 11:43:03.790: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/24/23 11:43:03.79
    Aug 24 11:43:04.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 24 11:43:05.089: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 24 11:43:05.089: INFO: stdout: ""
    Aug 24 11:43:05.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 10.233.31.118 80'
    Aug 24 11:43:05.393: INFO: stderr: "+ nc -v -z -w 2 10.233.31.118 80\nConnection to 10.233.31.118 80 port [tcp/http] succeeded!\n"
    Aug 24 11:43:05.393: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3037 08/24/23 11:43:05.393
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[pod2:[80]] 08/24/23 11:43:05.423
    Aug 24 11:43:06.495: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/24/23 11:43:06.495
    Aug 24 11:43:07.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 24 11:43:07.788: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 24 11:43:07.788: INFO: stdout: ""
    Aug 24 11:43:07.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-3037 exec execpodsvgdv -- /bin/sh -x -c nc -v -z -w 2 10.233.31.118 80'
    Aug 24 11:43:08.037: INFO: stderr: "+ nc -v -z -w 2 10.233.31.118 80\nConnection to 10.233.31.118 80 port [tcp/http] succeeded!\n"
    Aug 24 11:43:08.037: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-3037 08/24/23 11:43:08.037
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3037 to expose endpoints map[] 08/24/23 11:43:08.115
    Aug 24 11:43:09.155: INFO: successfully validated that service endpoint-test2 in namespace services-3037 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:09.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3037" for this suite. 08/24/23 11:43:09.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:09.249
Aug 24 11:43:09.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:43:09.254
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:09.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:09.291
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 24 11:43:09.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:09.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6027" for this suite. 08/24/23 11:43:09.875
------------------------------
â€¢ [0.646 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:09.249
    Aug 24 11:43:09.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:43:09.254
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:09.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:09.291
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 24 11:43:09.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:09.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6027" for this suite. 08/24/23 11:43:09.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:09.898
Aug 24 11:43:09.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 11:43:09.9
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:09.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:09.949
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:43:09.976
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:43:10.53
STEP: Deploying the webhook pod 08/24/23 11:43:10.544
STEP: Wait for the deployment to be ready 08/24/23 11:43:10.569
Aug 24 11:43:10.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 11:43:12.62
STEP: Verifying the service has paired with the endpoint 08/24/23 11:43:12.635
Aug 24 11:43:13.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/24/23 11:43:13.746
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 11:43:13.813
STEP: Deleting the collection of validation webhooks 08/24/23 11:43:13.86
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 11:43:13.944
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:13.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9625" for this suite. 08/24/23 11:43:14.088
STEP: Destroying namespace "webhook-9625-markers" for this suite. 08/24/23 11:43:14.154
------------------------------
â€¢ [4.280 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:09.898
    Aug 24 11:43:09.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 11:43:09.9
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:09.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:09.949
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:43:09.976
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:43:10.53
    STEP: Deploying the webhook pod 08/24/23 11:43:10.544
    STEP: Wait for the deployment to be ready 08/24/23 11:43:10.569
    Aug 24 11:43:10.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 11:43:12.62
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:43:12.635
    Aug 24 11:43:13.635: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/24/23 11:43:13.746
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 11:43:13.813
    STEP: Deleting the collection of validation webhooks 08/24/23 11:43:13.86
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 11:43:13.944
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:13.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9625" for this suite. 08/24/23 11:43:14.088
    STEP: Destroying namespace "webhook-9625-markers" for this suite. 08/24/23 11:43:14.154
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:14.179
Aug 24 11:43:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 11:43:14.19
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:14.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:14.281
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:43:14.332
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:43:15.395
STEP: Deploying the webhook pod 08/24/23 11:43:15.432
STEP: Wait for the deployment to be ready 08/24/23 11:43:15.486
Aug 24 11:43:15.502: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 11:43:17.521
STEP: Verifying the service has paired with the endpoint 08/24/23 11:43:17.545
Aug 24 11:43:18.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 11:43:18.553
STEP: create a pod 08/24/23 11:43:18.588
Aug 24 11:43:18.605: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2697" to be "running"
Aug 24 11:43:18.611: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.160101ms
Aug 24 11:43:20.618: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013378871s
Aug 24 11:43:20.618: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/24/23 11:43:20.619
Aug 24 11:43:20.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=webhook-2697 attach --namespace=webhook-2697 to-be-attached-pod -i -c=container1'
Aug 24 11:43:20.777: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:20.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2697" for this suite. 08/24/23 11:43:20.876
STEP: Destroying namespace "webhook-2697-markers" for this suite. 08/24/23 11:43:20.91
------------------------------
â€¢ [SLOW TEST] [6.794 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:14.179
    Aug 24 11:43:14.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 11:43:14.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:14.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:14.281
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:43:14.332
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:43:15.395
    STEP: Deploying the webhook pod 08/24/23 11:43:15.432
    STEP: Wait for the deployment to be ready 08/24/23 11:43:15.486
    Aug 24 11:43:15.502: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 11:43:17.521
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:43:17.545
    Aug 24 11:43:18.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 11:43:18.553
    STEP: create a pod 08/24/23 11:43:18.588
    Aug 24 11:43:18.605: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2697" to be "running"
    Aug 24 11:43:18.611: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.160101ms
    Aug 24 11:43:20.618: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013378871s
    Aug 24 11:43:20.618: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/24/23 11:43:20.619
    Aug 24 11:43:20.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=webhook-2697 attach --namespace=webhook-2697 to-be-attached-pod -i -c=container1'
    Aug 24 11:43:20.777: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:20.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2697" for this suite. 08/24/23 11:43:20.876
    STEP: Destroying namespace "webhook-2697-markers" for this suite. 08/24/23 11:43:20.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:20.975
Aug 24 11:43:20.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-watch 08/24/23 11:43:20.98
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:21.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:21.027
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 24 11:43:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Creating first CR  08/24/23 11:43:23.752
Aug 24 11:43:23.762: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:23Z]] name:name1 resourceVersion:5002 uid:c69e0bd2-4587-49e2-a1c6-c393762769af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/24/23 11:43:33.763
Aug 24 11:43:33.778: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:33Z]] name:name2 resourceVersion:5049 uid:5642a16e-f335-413d-8ece-db363cf93380] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/24/23 11:43:43.779
Aug 24 11:43:43.802: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:43Z]] name:name1 resourceVersion:5072 uid:c69e0bd2-4587-49e2-a1c6-c393762769af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/24/23 11:43:53.804
Aug 24 11:43:53.816: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:53Z]] name:name2 resourceVersion:5095 uid:5642a16e-f335-413d-8ece-db363cf93380] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/24/23 11:44:03.817
Aug 24 11:44:03.833: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:43Z]] name:name1 resourceVersion:5118 uid:c69e0bd2-4587-49e2-a1c6-c393762769af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/24/23 11:44:13.834
Aug 24 11:44:13.852: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:53Z]] name:name2 resourceVersion:5141 uid:5642a16e-f335-413d-8ece-db363cf93380] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:44:24.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-896" for this suite. 08/24/23 11:44:24.396
------------------------------
â€¢ [SLOW TEST] [63.442 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:20.975
    Aug 24 11:43:20.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-watch 08/24/23 11:43:20.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:21.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:21.027
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 24 11:43:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Creating first CR  08/24/23 11:43:23.752
    Aug 24 11:43:23.762: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:23Z]] name:name1 resourceVersion:5002 uid:c69e0bd2-4587-49e2-a1c6-c393762769af] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/24/23 11:43:33.763
    Aug 24 11:43:33.778: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:33Z]] name:name2 resourceVersion:5049 uid:5642a16e-f335-413d-8ece-db363cf93380] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/24/23 11:43:43.779
    Aug 24 11:43:43.802: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:43Z]] name:name1 resourceVersion:5072 uid:c69e0bd2-4587-49e2-a1c6-c393762769af] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/24/23 11:43:53.804
    Aug 24 11:43:53.816: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:53Z]] name:name2 resourceVersion:5095 uid:5642a16e-f335-413d-8ece-db363cf93380] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/24/23 11:44:03.817
    Aug 24 11:44:03.833: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:43Z]] name:name1 resourceVersion:5118 uid:c69e0bd2-4587-49e2-a1c6-c393762769af] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/24/23 11:44:13.834
    Aug 24 11:44:13.852: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:43:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:43:53Z]] name:name2 resourceVersion:5141 uid:5642a16e-f335-413d-8ece-db363cf93380] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:44:24.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-896" for this suite. 08/24/23 11:44:24.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:44:24.422
Aug 24 11:44:24.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename taint-single-pod 08/24/23 11:44:24.426
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:24.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:24.488
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 24 11:44:24.495: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 11:45:24.558: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 24 11:45:24.566: INFO: Starting informer...
STEP: Starting pod... 08/24/23 11:45:24.566
Aug 24 11:45:24.800: INFO: Pod is running on pe9deep4seen-3. Tainting Node
STEP: Trying to apply a taint on the Node 08/24/23 11:45:24.8
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:45:24.824
STEP: Waiting short time to make sure Pod is queued for deletion 08/24/23 11:45:24.835
Aug 24 11:45:24.835: INFO: Pod wasn't evicted. Proceeding
Aug 24 11:45:24.835: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:45:24.87
STEP: Waiting some time to make sure that toleration time passed. 08/24/23 11:45:24.889
Aug 24 11:46:39.892: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:46:39.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-7256" for this suite. 08/24/23 11:46:39.915
------------------------------
â€¢ [SLOW TEST] [135.509 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:44:24.422
    Aug 24 11:44:24.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename taint-single-pod 08/24/23 11:44:24.426
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:24.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:24.488
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 24 11:44:24.495: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 11:45:24.558: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 24 11:45:24.566: INFO: Starting informer...
    STEP: Starting pod... 08/24/23 11:45:24.566
    Aug 24 11:45:24.800: INFO: Pod is running on pe9deep4seen-3. Tainting Node
    STEP: Trying to apply a taint on the Node 08/24/23 11:45:24.8
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:45:24.824
    STEP: Waiting short time to make sure Pod is queued for deletion 08/24/23 11:45:24.835
    Aug 24 11:45:24.835: INFO: Pod wasn't evicted. Proceeding
    Aug 24 11:45:24.835: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:45:24.87
    STEP: Waiting some time to make sure that toleration time passed. 08/24/23 11:45:24.889
    Aug 24 11:46:39.892: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:46:39.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-7256" for this suite. 08/24/23 11:46:39.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:46:39.932
Aug 24 11:46:39.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:46:39.937
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:39.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:39.97
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/24/23 11:46:39.975
Aug 24 11:46:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: mark a version not serverd 08/24/23 11:46:45.731
STEP: check the unserved version gets removed 08/24/23 11:46:45.769
STEP: check the other version is not changed 08/24/23 11:46:48.242
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:46:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-754" for this suite. 08/24/23 11:46:52.723
------------------------------
â€¢ [SLOW TEST] [12.808 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:46:39.932
    Aug 24 11:46:39.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:46:39.937
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:39.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:39.97
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/24/23 11:46:39.975
    Aug 24 11:46:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: mark a version not serverd 08/24/23 11:46:45.731
    STEP: check the unserved version gets removed 08/24/23 11:46:45.769
    STEP: check the other version is not changed 08/24/23 11:46:48.242
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:46:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-754" for this suite. 08/24/23 11:46:52.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:46:52.75
Aug 24 11:46:52.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 11:46:52.753
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:52.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:52.8
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/24/23 11:46:52.805
STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 11:46:52.816
STEP: delete the deployment 08/24/23 11:46:53.336
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/24/23 11:46:53.357
STEP: Gathering metrics 08/24/23 11:46:53.92
Aug 24 11:46:53.973: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
Aug 24 11:46:53.979: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.548052ms
Aug 24 11:46:53.980: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
Aug 24 11:46:53.980: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
Aug 24 11:46:54.208: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 11:46:54.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1344" for this suite. 08/24/23 11:46:54.234
------------------------------
â€¢ [1.496 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:46:52.75
    Aug 24 11:46:52.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 11:46:52.753
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:52.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:52.8
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/24/23 11:46:52.805
    STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 11:46:52.816
    STEP: delete the deployment 08/24/23 11:46:53.336
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/24/23 11:46:53.357
    STEP: Gathering metrics 08/24/23 11:46:53.92
    Aug 24 11:46:53.973: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
    Aug 24 11:46:53.979: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.548052ms
    Aug 24 11:46:53.980: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
    Aug 24 11:46:53.980: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
    Aug 24 11:46:54.208: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:46:54.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1344" for this suite. 08/24/23 11:46:54.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:46:54.258
Aug 24 11:46:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 11:46:54.262
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:54.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:54.299
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/24/23 11:46:54.304
Aug 24 11:46:54.317: INFO: Waiting up to 5m0s for pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b" in namespace "pods-6749" to be "running and ready"
Aug 24 11:46:54.324: INFO: Pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389582ms
Aug 24 11:46:54.324: INFO: The phase of Pod pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:46:56.331: INFO: Pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013816788s
Aug 24 11:46:56.331: INFO: The phase of Pod pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b is Running (Ready = true)
Aug 24 11:46:56.331: INFO: Pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b" satisfied condition "running and ready"
Aug 24 11:46:56.346: INFO: Pod pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b has hostIP: 192.168.121.130
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:46:56.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6749" for this suite. 08/24/23 11:46:56.355
------------------------------
â€¢ [2.109 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:46:54.258
    Aug 24 11:46:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 11:46:54.262
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:54.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:54.299
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/24/23 11:46:54.304
    Aug 24 11:46:54.317: INFO: Waiting up to 5m0s for pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b" in namespace "pods-6749" to be "running and ready"
    Aug 24 11:46:54.324: INFO: Pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.389582ms
    Aug 24 11:46:54.324: INFO: The phase of Pod pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:46:56.331: INFO: Pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013816788s
    Aug 24 11:46:56.331: INFO: The phase of Pod pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b is Running (Ready = true)
    Aug 24 11:46:56.331: INFO: Pod "pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b" satisfied condition "running and ready"
    Aug 24 11:46:56.346: INFO: Pod pod-hostip-db059b64-d36f-47d3-8532-4d70641d761b has hostIP: 192.168.121.130
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:46:56.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6749" for this suite. 08/24/23 11:46:56.355
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:46:56.367
Aug 24 11:46:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:46:56.369
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:56.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:56.399
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-5zjtq"  08/24/23 11:46:56.404
Aug 24 11:46:56.414: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-5zjtq"  08/24/23 11:46:56.414
Aug 24 11:46:56.426: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 11:46:56.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7286" for this suite. 08/24/23 11:46:56.435
------------------------------
â€¢ [0.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:46:56.367
    Aug 24 11:46:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:46:56.369
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:56.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:56.399
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-5zjtq"  08/24/23 11:46:56.404
    Aug 24 11:46:56.414: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-5zjtq"  08/24/23 11:46:56.414
    Aug 24 11:46:56.426: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:46:56.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7286" for this suite. 08/24/23 11:46:56.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:46:56.464
Aug 24 11:46:56.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 11:46:56.467
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:56.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:56.493
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:46:56.522
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:46:57.564
STEP: Deploying the webhook pod 08/24/23 11:46:57.588
STEP: Wait for the deployment to be ready 08/24/23 11:46:57.611
Aug 24 11:46:57.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 11:46:59.649
STEP: Verifying the service has paired with the endpoint 08/24/23 11:46:59.671
Aug 24 11:47:00.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 24 11:47:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8009-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 11:47:01.196
STEP: Creating a custom resource while v1 is storage version 08/24/23 11:47:01.232
STEP: Patching Custom Resource Definition to set v2 as storage 08/24/23 11:47:03.522
STEP: Patching the custom resource while v2 is storage version 08/24/23 11:47:03.563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:04.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2010" for this suite. 08/24/23 11:47:04.459
STEP: Destroying namespace "webhook-2010-markers" for this suite. 08/24/23 11:47:04.479
------------------------------
â€¢ [SLOW TEST] [8.035 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:46:56.464
    Aug 24 11:46:56.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 11:46:56.467
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:56.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:56.493
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:46:56.522
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:46:57.564
    STEP: Deploying the webhook pod 08/24/23 11:46:57.588
    STEP: Wait for the deployment to be ready 08/24/23 11:46:57.611
    Aug 24 11:46:57.632: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 11:46:59.649
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:46:59.671
    Aug 24 11:47:00.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 24 11:47:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8009-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 11:47:01.196
    STEP: Creating a custom resource while v1 is storage version 08/24/23 11:47:01.232
    STEP: Patching Custom Resource Definition to set v2 as storage 08/24/23 11:47:03.522
    STEP: Patching the custom resource while v2 is storage version 08/24/23 11:47:03.563
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:04.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2010" for this suite. 08/24/23 11:47:04.459
    STEP: Destroying namespace "webhook-2010-markers" for this suite. 08/24/23 11:47:04.479
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:04.501
Aug 24 11:47:04.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-webhook 08/24/23 11:47:04.505
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:04.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:04.561
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/24/23 11:47:04.566
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 11:47:05.714
STEP: Deploying the custom resource conversion webhook pod 08/24/23 11:47:05.723
STEP: Wait for the deployment to be ready 08/24/23 11:47:05.741
Aug 24 11:47:05.751: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 11:47:07.769
STEP: Verifying the service has paired with the endpoint 08/24/23 11:47:07.786
Aug 24 11:47:08.787: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 24 11:47:08.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Creating a v1 custom resource 08/24/23 11:47:11.645
STEP: v2 custom resource should be converted 08/24/23 11:47:11.653
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:12.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1440" for this suite. 08/24/23 11:47:12.28
------------------------------
â€¢ [SLOW TEST] [7.807 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:04.501
    Aug 24 11:47:04.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-webhook 08/24/23 11:47:04.505
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:04.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:04.561
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/24/23 11:47:04.566
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 11:47:05.714
    STEP: Deploying the custom resource conversion webhook pod 08/24/23 11:47:05.723
    STEP: Wait for the deployment to be ready 08/24/23 11:47:05.741
    Aug 24 11:47:05.751: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 11:47:07.769
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:47:07.786
    Aug 24 11:47:08.787: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 24 11:47:08.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Creating a v1 custom resource 08/24/23 11:47:11.645
    STEP: v2 custom resource should be converted 08/24/23 11:47:11.653
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:12.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1440" for this suite. 08/24/23 11:47:12.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:12.325
Aug 24 11:47:12.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 11:47:12.334
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:12.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:12.375
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4230 08/24/23 11:47:12.381
STEP: creating service affinity-nodeport in namespace services-4230 08/24/23 11:47:12.381
STEP: creating replication controller affinity-nodeport in namespace services-4230 08/24/23 11:47:12.407
I0824 11:47:12.421424      14 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4230, replica count: 3
I0824 11:47:15.473628      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:47:18.474330      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:47:21.475756      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:47:24.477081      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:47:27.477665      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:47:27.497: INFO: Creating new exec pod
Aug 24 11:47:27.517: INFO: Waiting up to 5m0s for pod "execpod-affinitywvbq2" in namespace "services-4230" to be "running"
Aug 24 11:47:27.530: INFO: Pod "execpod-affinitywvbq2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.292084ms
Aug 24 11:47:29.546: INFO: Pod "execpod-affinitywvbq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028964449s
Aug 24 11:47:29.546: INFO: Pod "execpod-affinitywvbq2" satisfied condition "running"
Aug 24 11:47:30.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 24 11:47:30.875: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 24 11:47:30.875: INFO: stdout: ""
Aug 24 11:47:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 10.233.19.15 80'
Aug 24 11:47:31.164: INFO: stderr: "+ nc -v -z -w 2 10.233.19.15 80\nConnection to 10.233.19.15 80 port [tcp/http] succeeded!\n"
Aug 24 11:47:31.164: INFO: stdout: ""
Aug 24 11:47:31.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 31524'
Aug 24 11:47:31.429: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 31524\nConnection to 192.168.121.111 31524 port [tcp/*] succeeded!\n"
Aug 24 11:47:31.429: INFO: stdout: ""
Aug 24 11:47:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.127 31524'
Aug 24 11:47:31.689: INFO: stderr: "+ nc -v -z -w 2 192.168.121.127 31524\nConnection to 192.168.121.127 31524 port [tcp/*] succeeded!\n"
Aug 24 11:47:31.689: INFO: stdout: ""
Aug 24 11:47:31.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.127:31524/ ; done'
Aug 24 11:47:32.211: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n"
Aug 24 11:47:32.211: INFO: stdout: "\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp"
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
Aug 24 11:47:32.211: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4230, will wait for the garbage collector to delete the pods 08/24/23 11:47:32.237
Aug 24 11:47:32.313: INFO: Deleting ReplicationController affinity-nodeport took: 12.726578ms
Aug 24 11:47:32.414: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.890611ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:34.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4230" for this suite. 08/24/23 11:47:34.488
------------------------------
â€¢ [SLOW TEST] [22.181 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:12.325
    Aug 24 11:47:12.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 11:47:12.334
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:12.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:12.375
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4230 08/24/23 11:47:12.381
    STEP: creating service affinity-nodeport in namespace services-4230 08/24/23 11:47:12.381
    STEP: creating replication controller affinity-nodeport in namespace services-4230 08/24/23 11:47:12.407
    I0824 11:47:12.421424      14 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4230, replica count: 3
    I0824 11:47:15.473628      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:47:18.474330      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:47:21.475756      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:47:24.477081      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:47:27.477665      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:47:27.497: INFO: Creating new exec pod
    Aug 24 11:47:27.517: INFO: Waiting up to 5m0s for pod "execpod-affinitywvbq2" in namespace "services-4230" to be "running"
    Aug 24 11:47:27.530: INFO: Pod "execpod-affinitywvbq2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.292084ms
    Aug 24 11:47:29.546: INFO: Pod "execpod-affinitywvbq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028964449s
    Aug 24 11:47:29.546: INFO: Pod "execpod-affinitywvbq2" satisfied condition "running"
    Aug 24 11:47:30.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 24 11:47:30.875: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 24 11:47:30.875: INFO: stdout: ""
    Aug 24 11:47:30.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 10.233.19.15 80'
    Aug 24 11:47:31.164: INFO: stderr: "+ nc -v -z -w 2 10.233.19.15 80\nConnection to 10.233.19.15 80 port [tcp/http] succeeded!\n"
    Aug 24 11:47:31.164: INFO: stdout: ""
    Aug 24 11:47:31.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 31524'
    Aug 24 11:47:31.429: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 31524\nConnection to 192.168.121.111 31524 port [tcp/*] succeeded!\n"
    Aug 24 11:47:31.429: INFO: stdout: ""
    Aug 24 11:47:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.127 31524'
    Aug 24 11:47:31.689: INFO: stderr: "+ nc -v -z -w 2 192.168.121.127 31524\nConnection to 192.168.121.127 31524 port [tcp/*] succeeded!\n"
    Aug 24 11:47:31.689: INFO: stdout: ""
    Aug 24 11:47:31.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-4230 exec execpod-affinitywvbq2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.127:31524/ ; done'
    Aug 24 11:47:32.211: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:31524/\n"
    Aug 24 11:47:32.211: INFO: stdout: "\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp\naffinity-nodeport-hrvkp"
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Received response from host: affinity-nodeport-hrvkp
    Aug 24 11:47:32.211: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4230, will wait for the garbage collector to delete the pods 08/24/23 11:47:32.237
    Aug 24 11:47:32.313: INFO: Deleting ReplicationController affinity-nodeport took: 12.726578ms
    Aug 24 11:47:32.414: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.890611ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:34.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4230" for this suite. 08/24/23 11:47:34.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:34.514
Aug 24 11:47:34.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pod-network-test 08/24/23 11:47:34.516
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:34.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:34.56
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9005 08/24/23 11:47:34.565
STEP: creating a selector 08/24/23 11:47:34.566
STEP: Creating the service pods in kubernetes 08/24/23 11:47:34.567
Aug 24 11:47:34.568: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 11:47:34.620: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9005" to be "running and ready"
Aug 24 11:47:34.627: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851551ms
Aug 24 11:47:34.628: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:47:36.638: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017744014s
Aug 24 11:47:36.638: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:38.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015512625s
Aug 24 11:47:38.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:40.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017006785s
Aug 24 11:47:40.638: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:42.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016525048s
Aug 24 11:47:42.637: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:44.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018047301s
Aug 24 11:47:44.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:46.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015275775s
Aug 24 11:47:46.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:48.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016273993s
Aug 24 11:47:48.637: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:50.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016177954s
Aug 24 11:47:50.637: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:52.638: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017936834s
Aug 24 11:47:52.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:54.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018778916s
Aug 24 11:47:54.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:47:56.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018579242s
Aug 24 11:47:56.639: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 11:47:56.639: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 11:47:56.647: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9005" to be "running and ready"
Aug 24 11:47:56.654: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.893002ms
Aug 24 11:47:56.654: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 11:47:56.654: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 11:47:56.660: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9005" to be "running and ready"
Aug 24 11:47:56.666: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.280723ms
Aug 24 11:47:56.666: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 11:47:56.666: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 11:47:56.671
Aug 24 11:47:56.702: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9005" to be "running"
Aug 24 11:47:56.721: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.289454ms
Aug 24 11:47:58.728: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025393799s
Aug 24 11:47:58.728: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 11:47:58.733: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9005" to be "running"
Aug 24 11:47:58.737: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.751608ms
Aug 24 11:47:58.737: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 24 11:47:58.741: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 11:47:58.742: INFO: Going to poll 10.233.64.34 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 24 11:47:58.748: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9005 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:47:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:47:58.749: INFO: ExecWithOptions: Clientset creation
Aug 24 11:47:58.749: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9005/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.34+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 11:47:59.893: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 24 11:47:59.893: INFO: Going to poll 10.233.65.44 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 24 11:47:59.902: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.44 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9005 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:47:59.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:47:59.905: INFO: ExecWithOptions: Clientset creation
Aug 24 11:47:59.905: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9005/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.44+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 11:48:01.046: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 24 11:48:01.046: INFO: Going to poll 10.233.66.108 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 24 11:48:01.053: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9005 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:48:01.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:48:01.056: INFO: ExecWithOptions: Clientset creation
Aug 24 11:48:01.056: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9005/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.108+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 11:48:02.240: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 11:48:02.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9005" for this suite. 08/24/23 11:48:02.249
------------------------------
â€¢ [SLOW TEST] [27.748 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:34.514
    Aug 24 11:47:34.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 11:47:34.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:34.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:34.56
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9005 08/24/23 11:47:34.565
    STEP: creating a selector 08/24/23 11:47:34.566
    STEP: Creating the service pods in kubernetes 08/24/23 11:47:34.567
    Aug 24 11:47:34.568: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 11:47:34.620: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9005" to be "running and ready"
    Aug 24 11:47:34.627: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.851551ms
    Aug 24 11:47:34.628: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:47:36.638: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017744014s
    Aug 24 11:47:36.638: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:38.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015512625s
    Aug 24 11:47:38.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:40.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017006785s
    Aug 24 11:47:40.638: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:42.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016525048s
    Aug 24 11:47:42.637: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:44.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018047301s
    Aug 24 11:47:44.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:46.636: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.015275775s
    Aug 24 11:47:46.636: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:48.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016273993s
    Aug 24 11:47:48.637: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:50.637: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016177954s
    Aug 24 11:47:50.637: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:52.638: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017936834s
    Aug 24 11:47:52.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:54.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018778916s
    Aug 24 11:47:54.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:47:56.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018579242s
    Aug 24 11:47:56.639: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 11:47:56.639: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 11:47:56.647: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9005" to be "running and ready"
    Aug 24 11:47:56.654: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.893002ms
    Aug 24 11:47:56.654: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 11:47:56.654: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 11:47:56.660: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9005" to be "running and ready"
    Aug 24 11:47:56.666: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.280723ms
    Aug 24 11:47:56.666: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 11:47:56.666: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 11:47:56.671
    Aug 24 11:47:56.702: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9005" to be "running"
    Aug 24 11:47:56.721: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.289454ms
    Aug 24 11:47:58.728: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025393799s
    Aug 24 11:47:58.728: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 11:47:58.733: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9005" to be "running"
    Aug 24 11:47:58.737: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.751608ms
    Aug 24 11:47:58.737: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 24 11:47:58.741: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 11:47:58.742: INFO: Going to poll 10.233.64.34 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 11:47:58.748: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9005 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:47:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:47:58.749: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:47:58.749: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9005/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.34+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 11:47:59.893: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 24 11:47:59.893: INFO: Going to poll 10.233.65.44 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 11:47:59.902: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.44 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9005 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:47:59.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:47:59.905: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:47:59.905: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9005/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.44+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 11:48:01.046: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 24 11:48:01.046: INFO: Going to poll 10.233.66.108 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 11:48:01.053: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9005 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:48:01.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:48:01.056: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:48:01.056: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9005/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.108+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 11:48:02.240: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:48:02.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9005" for this suite. 08/24/23 11:48:02.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:48:02.264
Aug 24 11:48:02.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename subpath 08/24/23 11:48:02.266
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:48:02.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:48:02.301
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 11:48:02.306
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-cxg5 08/24/23 11:48:02.32
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:48:02.32
Aug 24 11:48:02.331: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cxg5" in namespace "subpath-9761" to be "Succeeded or Failed"
Aug 24 11:48:02.337: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.316589ms
Aug 24 11:48:04.348: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016715891s
Aug 24 11:48:06.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.01329935s
Aug 24 11:48:08.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.014706116s
Aug 24 11:48:10.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.01387875s
Aug 24 11:48:12.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.013308441s
Aug 24 11:48:14.352: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.020790809s
Aug 24 11:48:16.347: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.015538335s
Aug 24 11:48:18.344: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.012664788s
Aug 24 11:48:20.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.01328703s
Aug 24 11:48:22.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.013497619s
Aug 24 11:48:24.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 22.014156048s
Aug 24 11:48:26.347: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=false. Elapsed: 24.014988158s
Aug 24 11:48:28.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013927887s
STEP: Saw pod success 08/24/23 11:48:28.346
Aug 24 11:48:28.346: INFO: Pod "pod-subpath-test-secret-cxg5" satisfied condition "Succeeded or Failed"
Aug 24 11:48:28.352: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-secret-cxg5 container test-container-subpath-secret-cxg5: <nil>
STEP: delete the pod 08/24/23 11:48:28.383
Aug 24 11:48:28.400: INFO: Waiting for pod pod-subpath-test-secret-cxg5 to disappear
Aug 24 11:48:28.404: INFO: Pod pod-subpath-test-secret-cxg5 no longer exists
STEP: Deleting pod pod-subpath-test-secret-cxg5 08/24/23 11:48:28.405
Aug 24 11:48:28.405: INFO: Deleting pod "pod-subpath-test-secret-cxg5" in namespace "subpath-9761"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 11:48:28.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9761" for this suite. 08/24/23 11:48:28.423
------------------------------
â€¢ [SLOW TEST] [26.170 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:48:02.264
    Aug 24 11:48:02.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename subpath 08/24/23 11:48:02.266
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:48:02.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:48:02.301
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 11:48:02.306
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-cxg5 08/24/23 11:48:02.32
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:48:02.32
    Aug 24 11:48:02.331: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cxg5" in namespace "subpath-9761" to be "Succeeded or Failed"
    Aug 24 11:48:02.337: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.316589ms
    Aug 24 11:48:04.348: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016715891s
    Aug 24 11:48:06.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.01329935s
    Aug 24 11:48:08.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.014706116s
    Aug 24 11:48:10.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.01387875s
    Aug 24 11:48:12.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.013308441s
    Aug 24 11:48:14.352: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.020790809s
    Aug 24 11:48:16.347: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.015538335s
    Aug 24 11:48:18.344: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.012664788s
    Aug 24 11:48:20.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.01328703s
    Aug 24 11:48:22.345: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.013497619s
    Aug 24 11:48:24.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=true. Elapsed: 22.014156048s
    Aug 24 11:48:26.347: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Running", Reason="", readiness=false. Elapsed: 24.014988158s
    Aug 24 11:48:28.346: INFO: Pod "pod-subpath-test-secret-cxg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013927887s
    STEP: Saw pod success 08/24/23 11:48:28.346
    Aug 24 11:48:28.346: INFO: Pod "pod-subpath-test-secret-cxg5" satisfied condition "Succeeded or Failed"
    Aug 24 11:48:28.352: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-secret-cxg5 container test-container-subpath-secret-cxg5: <nil>
    STEP: delete the pod 08/24/23 11:48:28.383
    Aug 24 11:48:28.400: INFO: Waiting for pod pod-subpath-test-secret-cxg5 to disappear
    Aug 24 11:48:28.404: INFO: Pod pod-subpath-test-secret-cxg5 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-cxg5 08/24/23 11:48:28.405
    Aug 24 11:48:28.405: INFO: Deleting pod "pod-subpath-test-secret-cxg5" in namespace "subpath-9761"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:48:28.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9761" for this suite. 08/24/23 11:48:28.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:48:28.436
Aug 24 11:48:28.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pod-network-test 08/24/23 11:48:28.441
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:48:28.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:48:28.479
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7785 08/24/23 11:48:28.484
STEP: creating a selector 08/24/23 11:48:28.484
STEP: Creating the service pods in kubernetes 08/24/23 11:48:28.485
Aug 24 11:48:28.485: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 11:48:28.538: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7785" to be "running and ready"
Aug 24 11:48:28.562: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.050126ms
Aug 24 11:48:28.562: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:48:30.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.032510686s
Aug 24 11:48:30.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:32.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032401252s
Aug 24 11:48:32.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:34.572: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03419807s
Aug 24 11:48:34.573: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:36.573: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.034500771s
Aug 24 11:48:36.573: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:38.572: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.034163006s
Aug 24 11:48:38.572: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:40.570: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.031581291s
Aug 24 11:48:40.570: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:42.573: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.034929922s
Aug 24 11:48:42.573: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:44.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.033036196s
Aug 24 11:48:44.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:46.569: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.030866902s
Aug 24 11:48:46.569: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:48.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.03285701s
Aug 24 11:48:48.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:48:50.570: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.031853274s
Aug 24 11:48:50.570: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 11:48:50.570: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 11:48:50.575: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7785" to be "running and ready"
Aug 24 11:48:50.580: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.616506ms
Aug 24 11:48:50.580: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 11:48:50.580: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 11:48:50.588: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7785" to be "running and ready"
Aug 24 11:48:50.595: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.076866ms
Aug 24 11:48:50.595: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 11:48:50.596: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 11:48:50.602
Aug 24 11:48:50.615: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7785" to be "running"
Aug 24 11:48:50.622: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.830877ms
Aug 24 11:48:52.631: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015465032s
Aug 24 11:48:52.631: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 11:48:52.637: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 11:48:52.637: INFO: Breadth first check of 10.233.64.60 on host 192.168.121.127...
Aug 24 11:48:52.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.45:9080/dial?request=hostname&protocol=http&host=10.233.64.60&port=8083&tries=1'] Namespace:pod-network-test-7785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:48:52.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:48:52.648: INFO: ExecWithOptions: Clientset creation
Aug 24 11:48:52.649: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.45%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.60%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 11:48:52.853: INFO: Waiting for responses: map[]
Aug 24 11:48:52.853: INFO: reached 10.233.64.60 after 0/1 tries
Aug 24 11:48:52.853: INFO: Breadth first check of 10.233.65.123 on host 192.168.121.111...
Aug 24 11:48:52.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.45:9080/dial?request=hostname&protocol=http&host=10.233.65.123&port=8083&tries=1'] Namespace:pod-network-test-7785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:48:52.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:48:52.862: INFO: ExecWithOptions: Clientset creation
Aug 24 11:48:52.863: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.45%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.123%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 11:48:53.020: INFO: Waiting for responses: map[]
Aug 24 11:48:53.020: INFO: reached 10.233.65.123 after 0/1 tries
Aug 24 11:48:53.021: INFO: Breadth first check of 10.233.66.207 on host 192.168.121.130...
Aug 24 11:48:53.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.45:9080/dial?request=hostname&protocol=http&host=10.233.66.207&port=8083&tries=1'] Namespace:pod-network-test-7785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:48:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:48:53.030: INFO: ExecWithOptions: Clientset creation
Aug 24 11:48:53.030: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.45%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.207%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 11:48:53.159: INFO: Waiting for responses: map[]
Aug 24 11:48:53.161: INFO: reached 10.233.66.207 after 0/1 tries
Aug 24 11:48:53.161: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 11:48:53.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7785" for this suite. 08/24/23 11:48:53.173
------------------------------
â€¢ [SLOW TEST] [24.747 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:48:28.436
    Aug 24 11:48:28.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 11:48:28.441
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:48:28.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:48:28.479
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7785 08/24/23 11:48:28.484
    STEP: creating a selector 08/24/23 11:48:28.484
    STEP: Creating the service pods in kubernetes 08/24/23 11:48:28.485
    Aug 24 11:48:28.485: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 11:48:28.538: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7785" to be "running and ready"
    Aug 24 11:48:28.562: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.050126ms
    Aug 24 11:48:28.562: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:48:30.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.032510686s
    Aug 24 11:48:30.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:32.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032401252s
    Aug 24 11:48:32.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:34.572: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03419807s
    Aug 24 11:48:34.573: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:36.573: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.034500771s
    Aug 24 11:48:36.573: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:38.572: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.034163006s
    Aug 24 11:48:38.572: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:40.570: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.031581291s
    Aug 24 11:48:40.570: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:42.573: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.034929922s
    Aug 24 11:48:42.573: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:44.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.033036196s
    Aug 24 11:48:44.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:46.569: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.030866902s
    Aug 24 11:48:46.569: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:48.571: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.03285701s
    Aug 24 11:48:48.571: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:48:50.570: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.031853274s
    Aug 24 11:48:50.570: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 11:48:50.570: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 11:48:50.575: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7785" to be "running and ready"
    Aug 24 11:48:50.580: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.616506ms
    Aug 24 11:48:50.580: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 11:48:50.580: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 11:48:50.588: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7785" to be "running and ready"
    Aug 24 11:48:50.595: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.076866ms
    Aug 24 11:48:50.595: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 11:48:50.596: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 11:48:50.602
    Aug 24 11:48:50.615: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7785" to be "running"
    Aug 24 11:48:50.622: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.830877ms
    Aug 24 11:48:52.631: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015465032s
    Aug 24 11:48:52.631: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 11:48:52.637: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 11:48:52.637: INFO: Breadth first check of 10.233.64.60 on host 192.168.121.127...
    Aug 24 11:48:52.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.45:9080/dial?request=hostname&protocol=http&host=10.233.64.60&port=8083&tries=1'] Namespace:pod-network-test-7785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:48:52.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:48:52.648: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:48:52.649: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.45%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.60%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 11:48:52.853: INFO: Waiting for responses: map[]
    Aug 24 11:48:52.853: INFO: reached 10.233.64.60 after 0/1 tries
    Aug 24 11:48:52.853: INFO: Breadth first check of 10.233.65.123 on host 192.168.121.111...
    Aug 24 11:48:52.860: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.45:9080/dial?request=hostname&protocol=http&host=10.233.65.123&port=8083&tries=1'] Namespace:pod-network-test-7785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:48:52.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:48:52.862: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:48:52.863: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.45%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.123%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 11:48:53.020: INFO: Waiting for responses: map[]
    Aug 24 11:48:53.020: INFO: reached 10.233.65.123 after 0/1 tries
    Aug 24 11:48:53.021: INFO: Breadth first check of 10.233.66.207 on host 192.168.121.130...
    Aug 24 11:48:53.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.45:9080/dial?request=hostname&protocol=http&host=10.233.66.207&port=8083&tries=1'] Namespace:pod-network-test-7785 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:48:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:48:53.030: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:48:53.030: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7785/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.45%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.207%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 11:48:53.159: INFO: Waiting for responses: map[]
    Aug 24 11:48:53.161: INFO: reached 10.233.66.207 after 0/1 tries
    Aug 24 11:48:53.161: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:48:53.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7785" for this suite. 08/24/23 11:48:53.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:48:53.188
Aug 24 11:48:53.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename taint-multiple-pods 08/24/23 11:48:53.19
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:48:53.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:48:53.221
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 24 11:48:53.226: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 11:49:53.269: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 24 11:49:53.274: INFO: Starting informer...
STEP: Starting pods... 08/24/23 11:49:53.274
Aug 24 11:49:53.511: INFO: Pod1 is running on pe9deep4seen-3. Tainting Node
Aug 24 11:49:53.728: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-428" to be "running"
Aug 24 11:49:53.734: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02959ms
Aug 24 11:49:55.743: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014328611s
Aug 24 11:49:55.743: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 24 11:49:55.743: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-428" to be "running"
Aug 24 11:49:55.747: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.708448ms
Aug 24 11:49:55.748: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 24 11:49:55.748: INFO: Pod2 is running on pe9deep4seen-3. Tainting Node
STEP: Trying to apply a taint on the Node 08/24/23 11:49:55.748
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:49:55.771
STEP: Waiting for Pod1 and Pod2 to be deleted 08/24/23 11:49:55.778
Aug 24 11:50:01.712: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 24 11:50:21.771: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:50:21.797
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:50:21.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-428" for this suite. 08/24/23 11:50:21.814
------------------------------
â€¢ [SLOW TEST] [88.640 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:48:53.188
    Aug 24 11:48:53.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename taint-multiple-pods 08/24/23 11:48:53.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:48:53.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:48:53.221
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 24 11:48:53.226: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 11:49:53.269: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 24 11:49:53.274: INFO: Starting informer...
    STEP: Starting pods... 08/24/23 11:49:53.274
    Aug 24 11:49:53.511: INFO: Pod1 is running on pe9deep4seen-3. Tainting Node
    Aug 24 11:49:53.728: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-428" to be "running"
    Aug 24 11:49:53.734: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02959ms
    Aug 24 11:49:55.743: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014328611s
    Aug 24 11:49:55.743: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 24 11:49:55.743: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-428" to be "running"
    Aug 24 11:49:55.747: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.708448ms
    Aug 24 11:49:55.748: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 24 11:49:55.748: INFO: Pod2 is running on pe9deep4seen-3. Tainting Node
    STEP: Trying to apply a taint on the Node 08/24/23 11:49:55.748
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:49:55.771
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/24/23 11:49:55.778
    Aug 24 11:50:01.712: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 24 11:50:21.771: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 11:50:21.797
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:50:21.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-428" for this suite. 08/24/23 11:50:21.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:50:21.83
Aug 24 11:50:21.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 11:50:21.833
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:21.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:21.87
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 11:50:21.882
Aug 24 11:50:21.898: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4178" to be "running and ready"
Aug 24 11:50:21.904: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802929ms
Aug 24 11:50:21.904: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:50:23.912: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014488162s
Aug 24 11:50:23.913: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 11:50:23.913: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/24/23 11:50:23.92
Aug 24 11:50:23.930: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4178" to be "running and ready"
Aug 24 11:50:23.936: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.152224ms
Aug 24 11:50:23.936: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:50:25.944: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013249729s
Aug 24 11:50:25.944: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 24 11:50:25.944: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/24/23 11:50:25.949
Aug 24 11:50:25.962: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 24 11:50:25.968: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 24 11:50:27.969: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 24 11:50:27.977: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 24 11:50:29.968: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 24 11:50:29.977: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/24/23 11:50:29.977
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 11:50:30.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4178" for this suite. 08/24/23 11:50:30.012
------------------------------
â€¢ [SLOW TEST] [8.195 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:50:21.83
    Aug 24 11:50:21.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 11:50:21.833
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:21.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:21.87
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 11:50:21.882
    Aug 24 11:50:21.898: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4178" to be "running and ready"
    Aug 24 11:50:21.904: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.802929ms
    Aug 24 11:50:21.904: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:50:23.912: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014488162s
    Aug 24 11:50:23.913: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 11:50:23.913: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/24/23 11:50:23.92
    Aug 24 11:50:23.930: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4178" to be "running and ready"
    Aug 24 11:50:23.936: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.152224ms
    Aug 24 11:50:23.936: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:50:25.944: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013249729s
    Aug 24 11:50:25.944: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 24 11:50:25.944: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/24/23 11:50:25.949
    Aug 24 11:50:25.962: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 24 11:50:25.968: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 24 11:50:27.969: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 24 11:50:27.977: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 24 11:50:29.968: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 24 11:50:29.977: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/24/23 11:50:29.977
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:50:30.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4178" for this suite. 08/24/23 11:50:30.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:50:30.028
Aug 24 11:50:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:50:30.03
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:30.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:30.069
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 24 11:50:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:50:31.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7262" for this suite. 08/24/23 11:50:31.137
------------------------------
â€¢ [1.123 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:50:30.028
    Aug 24 11:50:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:50:30.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:30.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:30.069
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 24 11:50:30.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:50:31.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7262" for this suite. 08/24/23 11:50:31.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:50:31.156
Aug 24 11:50:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename hostport 08/24/23 11:50:31.16
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:31.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:31.198
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/24/23 11:50:31.215
Aug 24 11:50:31.236: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9701" to be "running and ready"
Aug 24 11:50:31.257: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.631154ms
Aug 24 11:50:31.257: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:50:33.265: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02951089s
Aug 24 11:50:33.266: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 24 11:50:33.266: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.127 on the node which pod1 resides and expect scheduled 08/24/23 11:50:33.266
Aug 24 11:50:33.276: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9701" to be "running and ready"
Aug 24 11:50:33.282: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.215385ms
Aug 24 11:50:33.282: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:50:35.293: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.017284722s
Aug 24 11:50:35.293: INFO: The phase of Pod pod2 is Running (Ready = false)
Aug 24 11:50:37.290: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014333805s
Aug 24 11:50:37.290: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 24 11:50:37.290: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.127 but use UDP protocol on the node which pod2 resides 08/24/23 11:50:37.291
Aug 24 11:50:37.302: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9701" to be "running and ready"
Aug 24 11:50:37.318: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.665067ms
Aug 24 11:50:37.318: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:50:39.327: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.024230923s
Aug 24 11:50:39.327: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 24 11:50:39.327: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 24 11:50:39.337: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9701" to be "running and ready"
Aug 24 11:50:39.344: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223517ms
Aug 24 11:50:39.344: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:50:41.351: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014027218s
Aug 24 11:50:41.351: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 24 11:50:41.351: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/24/23 11:50:41.358
Aug 24 11:50:41.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.127 http://127.0.0.1:54323/hostname] Namespace:hostport-9701 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:50:41.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:50:41.359: INFO: ExecWithOptions: Clientset creation
Aug 24 11:50:41.360: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9701/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.127+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.127, port: 54323 08/24/23 11:50:41.539
Aug 24 11:50:41.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.127:54323/hostname] Namespace:hostport-9701 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:50:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:50:41.541: INFO: ExecWithOptions: Clientset creation
Aug 24 11:50:41.541: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9701/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.127%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.127, port: 54323 UDP 08/24/23 11:50:41.681
Aug 24 11:50:41.682: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.127 54323] Namespace:hostport-9701 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:50:41.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:50:41.685: INFO: ExecWithOptions: Clientset creation
Aug 24 11:50:41.685: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9701/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.127+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 24 11:50:46.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9701" for this suite. 08/24/23 11:50:46.818
------------------------------
â€¢ [SLOW TEST] [15.672 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:50:31.156
    Aug 24 11:50:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename hostport 08/24/23 11:50:31.16
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:31.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:31.198
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/24/23 11:50:31.215
    Aug 24 11:50:31.236: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9701" to be "running and ready"
    Aug 24 11:50:31.257: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.631154ms
    Aug 24 11:50:31.257: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:50:33.265: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02951089s
    Aug 24 11:50:33.266: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 24 11:50:33.266: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.127 on the node which pod1 resides and expect scheduled 08/24/23 11:50:33.266
    Aug 24 11:50:33.276: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9701" to be "running and ready"
    Aug 24 11:50:33.282: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.215385ms
    Aug 24 11:50:33.282: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:50:35.293: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.017284722s
    Aug 24 11:50:35.293: INFO: The phase of Pod pod2 is Running (Ready = false)
    Aug 24 11:50:37.290: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014333805s
    Aug 24 11:50:37.290: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 24 11:50:37.290: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.127 but use UDP protocol on the node which pod2 resides 08/24/23 11:50:37.291
    Aug 24 11:50:37.302: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9701" to be "running and ready"
    Aug 24 11:50:37.318: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.665067ms
    Aug 24 11:50:37.318: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:50:39.327: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.024230923s
    Aug 24 11:50:39.327: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 24 11:50:39.327: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 24 11:50:39.337: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9701" to be "running and ready"
    Aug 24 11:50:39.344: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.223517ms
    Aug 24 11:50:39.344: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:50:41.351: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014027218s
    Aug 24 11:50:41.351: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 24 11:50:41.351: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/24/23 11:50:41.358
    Aug 24 11:50:41.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.127 http://127.0.0.1:54323/hostname] Namespace:hostport-9701 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:50:41.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:50:41.359: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:50:41.360: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9701/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.127+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.127, port: 54323 08/24/23 11:50:41.539
    Aug 24 11:50:41.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.127:54323/hostname] Namespace:hostport-9701 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:50:41.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:50:41.541: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:50:41.541: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9701/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.127%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.127, port: 54323 UDP 08/24/23 11:50:41.681
    Aug 24 11:50:41.682: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.127 54323] Namespace:hostport-9701 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:50:41.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:50:41.685: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:50:41.685: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9701/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.127+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:50:46.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9701" for this suite. 08/24/23 11:50:46.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:50:46.833
Aug 24 11:50:46.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename job 08/24/23 11:50:46.835
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:46.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:46.878
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/24/23 11:50:46.884
STEP: Ensure pods equal to parallelism count is attached to the job 08/24/23 11:50:46.897
STEP: patching /status 08/24/23 11:50:48.908
STEP: updating /status 08/24/23 11:50:48.926
STEP: get /status 08/24/23 11:50:48.97
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 11:50:48.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5732" for this suite. 08/24/23 11:50:48.99
------------------------------
â€¢ [2.172 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:50:46.833
    Aug 24 11:50:46.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename job 08/24/23 11:50:46.835
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:46.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:46.878
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/24/23 11:50:46.884
    STEP: Ensure pods equal to parallelism count is attached to the job 08/24/23 11:50:46.897
    STEP: patching /status 08/24/23 11:50:48.908
    STEP: updating /status 08/24/23 11:50:48.926
    STEP: get /status 08/24/23 11:50:48.97
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:50:48.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5732" for this suite. 08/24/23 11:50:48.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:50:49.015
Aug 24 11:50:49.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 11:50:49.016
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:49.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:49.051
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5111 08/24/23 11:50:49.057
STEP: changing the ExternalName service to type=ClusterIP 08/24/23 11:50:49.071
STEP: creating replication controller externalname-service in namespace services-5111 08/24/23 11:50:49.101
I0824 11:50:49.114731      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5111, replica count: 2
I0824 11:50:52.166255      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:50:52.166: INFO: Creating new exec pod
Aug 24 11:50:52.182: INFO: Waiting up to 5m0s for pod "execpodllvn6" in namespace "services-5111" to be "running"
Aug 24 11:50:52.189: INFO: Pod "execpodllvn6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200063ms
Aug 24 11:50:54.198: INFO: Pod "execpodllvn6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01522407s
Aug 24 11:50:54.198: INFO: Pod "execpodllvn6" satisfied condition "running"
Aug 24 11:50:55.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-5111 exec execpodllvn6 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 24 11:50:55.513: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 24 11:50:55.513: INFO: stdout: ""
Aug 24 11:50:55.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-5111 exec execpodllvn6 -- /bin/sh -x -c nc -v -z -w 2 10.233.53.209 80'
Aug 24 11:50:55.746: INFO: stderr: "+ nc -v -z -w 2 10.233.53.209 80\nConnection to 10.233.53.209 80 port [tcp/http] succeeded!\n"
Aug 24 11:50:55.746: INFO: stdout: ""
Aug 24 11:50:55.746: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:50:55.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5111" for this suite. 08/24/23 11:50:55.795
------------------------------
â€¢ [SLOW TEST] [6.796 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:50:49.015
    Aug 24 11:50:49.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 11:50:49.016
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:49.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:49.051
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5111 08/24/23 11:50:49.057
    STEP: changing the ExternalName service to type=ClusterIP 08/24/23 11:50:49.071
    STEP: creating replication controller externalname-service in namespace services-5111 08/24/23 11:50:49.101
    I0824 11:50:49.114731      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5111, replica count: 2
    I0824 11:50:52.166255      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:50:52.166: INFO: Creating new exec pod
    Aug 24 11:50:52.182: INFO: Waiting up to 5m0s for pod "execpodllvn6" in namespace "services-5111" to be "running"
    Aug 24 11:50:52.189: INFO: Pod "execpodllvn6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200063ms
    Aug 24 11:50:54.198: INFO: Pod "execpodllvn6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01522407s
    Aug 24 11:50:54.198: INFO: Pod "execpodllvn6" satisfied condition "running"
    Aug 24 11:50:55.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-5111 exec execpodllvn6 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 24 11:50:55.513: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 24 11:50:55.513: INFO: stdout: ""
    Aug 24 11:50:55.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-5111 exec execpodllvn6 -- /bin/sh -x -c nc -v -z -w 2 10.233.53.209 80'
    Aug 24 11:50:55.746: INFO: stderr: "+ nc -v -z -w 2 10.233.53.209 80\nConnection to 10.233.53.209 80 port [tcp/http] succeeded!\n"
    Aug 24 11:50:55.746: INFO: stdout: ""
    Aug 24 11:50:55.746: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:50:55.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5111" for this suite. 08/24/23 11:50:55.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:50:55.813
Aug 24 11:50:55.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename disruption 08/24/23 11:50:55.818
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:55.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:55.86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/24/23 11:50:55.865
STEP: Waiting for the pdb to be processed 08/24/23 11:50:55.875
STEP: First trying to evict a pod which shouldn't be evictable 08/24/23 11:50:57.905
STEP: Waiting for all pods to be running 08/24/23 11:50:57.905
Aug 24 11:50:57.913: INFO: pods: 0 < 3
Aug 24 11:50:59.924: INFO: running pods: 2 < 3
STEP: locating a running pod 08/24/23 11:51:01.926
STEP: Updating the pdb to allow a pod to be evicted 08/24/23 11:51:01.974
STEP: Waiting for the pdb to be processed 08/24/23 11:51:02
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:51:04.012
STEP: Waiting for all pods to be running 08/24/23 11:51:04.013
STEP: Waiting for the pdb to observed all healthy pods 08/24/23 11:51:04.02
STEP: Patching the pdb to disallow a pod to be evicted 08/24/23 11:51:04.067
STEP: Waiting for the pdb to be processed 08/24/23 11:51:04.14
STEP: Waiting for all pods to be running 08/24/23 11:51:06.164
STEP: locating a running pod 08/24/23 11:51:06.17
STEP: Deleting the pdb to allow a pod to be evicted 08/24/23 11:51:06.196
STEP: Waiting for the pdb to be deleted 08/24/23 11:51:06.207
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:51:06.213
STEP: Waiting for all pods to be running 08/24/23 11:51:06.213
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 11:51:06.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3505" for this suite. 08/24/23 11:51:06.319
------------------------------
â€¢ [SLOW TEST] [10.532 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:50:55.813
    Aug 24 11:50:55.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename disruption 08/24/23 11:50:55.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:50:55.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:50:55.86
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/24/23 11:50:55.865
    STEP: Waiting for the pdb to be processed 08/24/23 11:50:55.875
    STEP: First trying to evict a pod which shouldn't be evictable 08/24/23 11:50:57.905
    STEP: Waiting for all pods to be running 08/24/23 11:50:57.905
    Aug 24 11:50:57.913: INFO: pods: 0 < 3
    Aug 24 11:50:59.924: INFO: running pods: 2 < 3
    STEP: locating a running pod 08/24/23 11:51:01.926
    STEP: Updating the pdb to allow a pod to be evicted 08/24/23 11:51:01.974
    STEP: Waiting for the pdb to be processed 08/24/23 11:51:02
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:51:04.012
    STEP: Waiting for all pods to be running 08/24/23 11:51:04.013
    STEP: Waiting for the pdb to observed all healthy pods 08/24/23 11:51:04.02
    STEP: Patching the pdb to disallow a pod to be evicted 08/24/23 11:51:04.067
    STEP: Waiting for the pdb to be processed 08/24/23 11:51:04.14
    STEP: Waiting for all pods to be running 08/24/23 11:51:06.164
    STEP: locating a running pod 08/24/23 11:51:06.17
    STEP: Deleting the pdb to allow a pod to be evicted 08/24/23 11:51:06.196
    STEP: Waiting for the pdb to be deleted 08/24/23 11:51:06.207
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:51:06.213
    STEP: Waiting for all pods to be running 08/24/23 11:51:06.213
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:51:06.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3505" for this suite. 08/24/23 11:51:06.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:51:06.349
Aug 24 11:51:06.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename podtemplate 08/24/23 11:51:06.355
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:06.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:06.404
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/24/23 11:51:06.408
Aug 24 11:51:06.418: INFO: created test-podtemplate-1
Aug 24 11:51:06.435: INFO: created test-podtemplate-2
Aug 24 11:51:06.443: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/24/23 11:51:06.443
STEP: delete collection of pod templates 08/24/23 11:51:06.45
Aug 24 11:51:06.451: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/24/23 11:51:06.486
Aug 24 11:51:06.486: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 24 11:51:06.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9143" for this suite. 08/24/23 11:51:06.504
------------------------------
â€¢ [0.166 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:51:06.349
    Aug 24 11:51:06.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename podtemplate 08/24/23 11:51:06.355
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:06.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:06.404
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/24/23 11:51:06.408
    Aug 24 11:51:06.418: INFO: created test-podtemplate-1
    Aug 24 11:51:06.435: INFO: created test-podtemplate-2
    Aug 24 11:51:06.443: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/24/23 11:51:06.443
    STEP: delete collection of pod templates 08/24/23 11:51:06.45
    Aug 24 11:51:06.451: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/24/23 11:51:06.486
    Aug 24 11:51:06.486: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:51:06.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9143" for this suite. 08/24/23 11:51:06.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:51:06.524
Aug 24 11:51:06.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 11:51:06.526
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:06.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:06.569
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/24/23 11:51:06.573
Aug 24 11:51:06.589: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0" in namespace "emptydir-4369" to be "running"
Aug 24 11:51:06.597: INFO: Pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.820148ms
Aug 24 11:51:08.604: INFO: Pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01464932s
Aug 24 11:51:08.604: INFO: Pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/24/23 11:51:08.604
Aug 24 11:51:08.604: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4369 PodName:pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:51:08.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:51:08.606: INFO: ExecWithOptions: Clientset creation
Aug 24 11:51:08.606: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4369/pods/pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 24 11:51:08.714: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:51:08.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4369" for this suite. 08/24/23 11:51:08.726
------------------------------
â€¢ [2.215 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:51:06.524
    Aug 24 11:51:06.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:51:06.526
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:06.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:06.569
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/24/23 11:51:06.573
    Aug 24 11:51:06.589: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0" in namespace "emptydir-4369" to be "running"
    Aug 24 11:51:06.597: INFO: Pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.820148ms
    Aug 24 11:51:08.604: INFO: Pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01464932s
    Aug 24 11:51:08.604: INFO: Pod "pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/24/23 11:51:08.604
    Aug 24 11:51:08.604: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4369 PodName:pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:51:08.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:51:08.606: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:51:08.606: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4369/pods/pod-sharedvolume-45d1634b-9b4f-4302-ab49-15c1bbbb48a0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 24 11:51:08.714: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:51:08.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4369" for this suite. 08/24/23 11:51:08.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:51:08.745
Aug 24 11:51:08.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename subpath 08/24/23 11:51:08.747
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:08.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:08.79
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 11:51:08.794
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-xwxv 08/24/23 11:51:08.814
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:51:08.814
Aug 24 11:51:08.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xwxv" in namespace "subpath-4536" to be "Succeeded or Failed"
Aug 24 11:51:08.840: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.236803ms
Aug 24 11:51:10.845: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013858099s
Aug 24 11:51:12.846: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 4.015018463s
Aug 24 11:51:14.848: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 6.016409679s
Aug 24 11:51:16.845: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 8.01321208s
Aug 24 11:51:18.849: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 10.017513528s
Aug 24 11:51:20.847: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 12.015663426s
Aug 24 11:51:22.847: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 14.01510698s
Aug 24 11:51:24.847: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 16.015420477s
Aug 24 11:51:26.848: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 18.016170052s
Aug 24 11:51:28.848: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 20.016617987s
Aug 24 11:51:30.850: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=false. Elapsed: 22.018437238s
Aug 24 11:51:32.851: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019973761s
STEP: Saw pod success 08/24/23 11:51:32.852
Aug 24 11:51:32.852: INFO: Pod "pod-subpath-test-configmap-xwxv" satisfied condition "Succeeded or Failed"
Aug 24 11:51:32.860: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-configmap-xwxv container test-container-subpath-configmap-xwxv: <nil>
STEP: delete the pod 08/24/23 11:51:32.879
Aug 24 11:51:32.905: INFO: Waiting for pod pod-subpath-test-configmap-xwxv to disappear
Aug 24 11:51:32.911: INFO: Pod pod-subpath-test-configmap-xwxv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xwxv 08/24/23 11:51:32.912
Aug 24 11:51:32.912: INFO: Deleting pod "pod-subpath-test-configmap-xwxv" in namespace "subpath-4536"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 11:51:32.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4536" for this suite. 08/24/23 11:51:32.929
------------------------------
â€¢ [SLOW TEST] [24.201 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:51:08.745
    Aug 24 11:51:08.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename subpath 08/24/23 11:51:08.747
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:08.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:08.79
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 11:51:08.794
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-xwxv 08/24/23 11:51:08.814
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:51:08.814
    Aug 24 11:51:08.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xwxv" in namespace "subpath-4536" to be "Succeeded or Failed"
    Aug 24 11:51:08.840: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.236803ms
    Aug 24 11:51:10.845: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013858099s
    Aug 24 11:51:12.846: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 4.015018463s
    Aug 24 11:51:14.848: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 6.016409679s
    Aug 24 11:51:16.845: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 8.01321208s
    Aug 24 11:51:18.849: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 10.017513528s
    Aug 24 11:51:20.847: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 12.015663426s
    Aug 24 11:51:22.847: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 14.01510698s
    Aug 24 11:51:24.847: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 16.015420477s
    Aug 24 11:51:26.848: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 18.016170052s
    Aug 24 11:51:28.848: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=true. Elapsed: 20.016617987s
    Aug 24 11:51:30.850: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Running", Reason="", readiness=false. Elapsed: 22.018437238s
    Aug 24 11:51:32.851: INFO: Pod "pod-subpath-test-configmap-xwxv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019973761s
    STEP: Saw pod success 08/24/23 11:51:32.852
    Aug 24 11:51:32.852: INFO: Pod "pod-subpath-test-configmap-xwxv" satisfied condition "Succeeded or Failed"
    Aug 24 11:51:32.860: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-configmap-xwxv container test-container-subpath-configmap-xwxv: <nil>
    STEP: delete the pod 08/24/23 11:51:32.879
    Aug 24 11:51:32.905: INFO: Waiting for pod pod-subpath-test-configmap-xwxv to disappear
    Aug 24 11:51:32.911: INFO: Pod pod-subpath-test-configmap-xwxv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-xwxv 08/24/23 11:51:32.912
    Aug 24 11:51:32.912: INFO: Deleting pod "pod-subpath-test-configmap-xwxv" in namespace "subpath-4536"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:51:32.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4536" for this suite. 08/24/23 11:51:32.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:51:32.957
Aug 24 11:51:32.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename proxy 08/24/23 11:51:32.963
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:32.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:33.007
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/24/23 11:51:33.045
STEP: creating replication controller proxy-service-xtkzj in namespace proxy-8717 08/24/23 11:51:33.051
I0824 11:51:33.069739      14 runners.go:193] Created replication controller with name: proxy-service-xtkzj, namespace: proxy-8717, replica count: 1
I0824 11:51:34.122105      14 runners.go:193] proxy-service-xtkzj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0824 11:51:35.122784      14 runners.go:193] proxy-service-xtkzj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:51:35.130: INFO: Endpoint proxy-8717/proxy-service-xtkzj is not ready yet
Aug 24 11:51:37.138: INFO: setup took 4.126570161s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/24/23 11:51:37.138
Aug 24 11:51:37.169: INFO: (0) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 28.522903ms)
Aug 24 11:51:37.169: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 29.398076ms)
Aug 24 11:51:37.172: INFO: (0) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 30.634616ms)
Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 33.824865ms)
Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 32.173479ms)
Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 32.50786ms)
Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 32.042531ms)
Aug 24 11:51:37.178: INFO: (0) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 36.218758ms)
Aug 24 11:51:37.178: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 37.308887ms)
Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 39.715858ms)
Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 40.088371ms)
Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 39.903441ms)
Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 40.486589ms)
Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 42.497437ms)
Aug 24 11:51:37.183: INFO: (0) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 41.635021ms)
Aug 24 11:51:37.184: INFO: (0) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 42.008312ms)
Aug 24 11:51:37.202: INFO: (1) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 17.791463ms)
Aug 24 11:51:37.203: INFO: (1) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 18.069255ms)
Aug 24 11:51:37.209: INFO: (1) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 24.861353ms)
Aug 24 11:51:37.209: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 24.574287ms)
Aug 24 11:51:37.210: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 25.086291ms)
Aug 24 11:51:37.211: INFO: (1) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 26.613043ms)
Aug 24 11:51:37.212: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 26.753585ms)
Aug 24 11:51:37.217: INFO: (1) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 31.906206ms)
Aug 24 11:51:37.217: INFO: (1) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 32.873998ms)
Aug 24 11:51:37.219: INFO: (1) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 33.484772ms)
Aug 24 11:51:37.219: INFO: (1) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 34.803439ms)
Aug 24 11:51:37.220: INFO: (1) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 34.594548ms)
Aug 24 11:51:37.221: INFO: (1) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 35.540902ms)
Aug 24 11:51:37.222: INFO: (1) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 36.927147ms)
Aug 24 11:51:37.223: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 36.886405ms)
Aug 24 11:51:37.224: INFO: (1) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 38.556353ms)
Aug 24 11:51:37.245: INFO: (2) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 20.433121ms)
Aug 24 11:51:37.245: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 20.303791ms)
Aug 24 11:51:37.247: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 21.027376ms)
Aug 24 11:51:37.248: INFO: (2) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 23.018981ms)
Aug 24 11:51:37.250: INFO: (2) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 24.96934ms)
Aug 24 11:51:37.250: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 25.118302ms)
Aug 24 11:51:37.251: INFO: (2) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 24.502841ms)
Aug 24 11:51:37.252: INFO: (2) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 27.460364ms)
Aug 24 11:51:37.252: INFO: (2) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 27.809561ms)
Aug 24 11:51:37.252: INFO: (2) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 26.407599ms)
Aug 24 11:51:37.253: INFO: (2) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 26.923552ms)
Aug 24 11:51:37.258: INFO: (2) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 33.175362ms)
Aug 24 11:51:37.259: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 33.582068ms)
Aug 24 11:51:37.259: INFO: (2) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 34.048449ms)
Aug 24 11:51:37.259: INFO: (2) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 33.692441ms)
Aug 24 11:51:37.262: INFO: (2) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 36.861813ms)
Aug 24 11:51:37.274: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 11.337526ms)
Aug 24 11:51:37.275: INFO: (3) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 11.520741ms)
Aug 24 11:51:37.278: INFO: (3) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 15.460722ms)
Aug 24 11:51:37.282: INFO: (3) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 19.336439ms)
Aug 24 11:51:37.282: INFO: (3) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 18.871227ms)
Aug 24 11:51:37.284: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.459922ms)
Aug 24 11:51:37.286: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.808705ms)
Aug 24 11:51:37.287: INFO: (3) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.186352ms)
Aug 24 11:51:37.291: INFO: (3) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 27.054353ms)
Aug 24 11:51:37.291: INFO: (3) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.940457ms)
Aug 24 11:51:37.291: INFO: (3) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 27.187897ms)
Aug 24 11:51:37.293: INFO: (3) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 28.807852ms)
Aug 24 11:51:37.296: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 31.859408ms)
Aug 24 11:51:37.297: INFO: (3) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 33.376647ms)
Aug 24 11:51:37.298: INFO: (3) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 33.805351ms)
Aug 24 11:51:37.299: INFO: (3) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 34.501386ms)
Aug 24 11:51:37.323: INFO: (4) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 23.419651ms)
Aug 24 11:51:37.331: INFO: (4) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 30.844785ms)
Aug 24 11:51:37.334: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 30.93905ms)
Aug 24 11:51:37.335: INFO: (4) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 33.699125ms)
Aug 24 11:51:37.335: INFO: (4) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 32.987935ms)
Aug 24 11:51:37.343: INFO: (4) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 43.537878ms)
Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 45.108472ms)
Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 46.236119ms)
Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 46.690546ms)
Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 46.267377ms)
Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 47.467086ms)
Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 47.267427ms)
Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 48.11728ms)
Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 48.114615ms)
Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 48.688187ms)
Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 47.160844ms)
Aug 24 11:51:37.364: INFO: (5) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 15.522742ms)
Aug 24 11:51:37.370: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.439746ms)
Aug 24 11:51:37.371: INFO: (5) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 21.79586ms)
Aug 24 11:51:37.373: INFO: (5) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 23.3018ms)
Aug 24 11:51:37.373: INFO: (5) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 22.620725ms)
Aug 24 11:51:37.374: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 23.823797ms)
Aug 24 11:51:37.380: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 30.690922ms)
Aug 24 11:51:37.380: INFO: (5) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 29.846445ms)
Aug 24 11:51:37.381: INFO: (5) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 32.052231ms)
Aug 24 11:51:37.381: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 31.186761ms)
Aug 24 11:51:37.382: INFO: (5) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 31.231351ms)
Aug 24 11:51:37.382: INFO: (5) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 33.160176ms)
Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 34.103866ms)
Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 34.157641ms)
Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 33.661279ms)
Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 33.432515ms)
Aug 24 11:51:37.396: INFO: (6) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 10.944071ms)
Aug 24 11:51:37.403: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 17.069376ms)
Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 24.992351ms)
Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 24.489349ms)
Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 22.721276ms)
Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.633074ms)
Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 24.892041ms)
Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 25.378997ms)
Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 26.88738ms)
Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.593402ms)
Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 26.997856ms)
Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 26.85147ms)
Aug 24 11:51:37.415: INFO: (6) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 27.356661ms)
Aug 24 11:51:37.415: INFO: (6) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 27.724933ms)
Aug 24 11:51:37.416: INFO: (6) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 29.225761ms)
Aug 24 11:51:37.417: INFO: (6) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 29.965273ms)
Aug 24 11:51:37.430: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 12.445567ms)
Aug 24 11:51:37.430: INFO: (7) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 12.5745ms)
Aug 24 11:51:37.436: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 18.115475ms)
Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 19.1481ms)
Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 18.849719ms)
Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 18.688084ms)
Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 18.24083ms)
Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 18.344107ms)
Aug 24 11:51:37.439: INFO: (7) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 20.532539ms)
Aug 24 11:51:37.439: INFO: (7) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 20.538945ms)
Aug 24 11:51:37.439: INFO: (7) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 21.105668ms)
Aug 24 11:51:37.441: INFO: (7) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 23.065386ms)
Aug 24 11:51:37.442: INFO: (7) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 24.860047ms)
Aug 24 11:51:37.442: INFO: (7) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 23.830203ms)
Aug 24 11:51:37.442: INFO: (7) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 24.472511ms)
Aug 24 11:51:37.443: INFO: (7) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 24.893369ms)
Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 13.456889ms)
Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 13.909277ms)
Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 14.491022ms)
Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 14.29584ms)
Aug 24 11:51:37.458: INFO: (8) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 14.723869ms)
Aug 24 11:51:37.465: INFO: (8) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 21.475991ms)
Aug 24 11:51:37.467: INFO: (8) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.534775ms)
Aug 24 11:51:37.467: INFO: (8) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 23.809043ms)
Aug 24 11:51:37.468: INFO: (8) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 24.228281ms)
Aug 24 11:51:37.471: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 27.230416ms)
Aug 24 11:51:37.472: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 28.085565ms)
Aug 24 11:51:37.472: INFO: (8) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 29.256385ms)
Aug 24 11:51:37.472: INFO: (8) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 28.519903ms)
Aug 24 11:51:37.474: INFO: (8) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 31.270202ms)
Aug 24 11:51:37.475: INFO: (8) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 31.219872ms)
Aug 24 11:51:37.475: INFO: (8) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 31.075299ms)
Aug 24 11:51:37.488: INFO: (9) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 13.32175ms)
Aug 24 11:51:37.489: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 12.983146ms)
Aug 24 11:51:37.491: INFO: (9) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 15.909536ms)
Aug 24 11:51:37.496: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.757405ms)
Aug 24 11:51:37.499: INFO: (9) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 22.628104ms)
Aug 24 11:51:37.499: INFO: (9) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 23.974649ms)
Aug 24 11:51:37.499: INFO: (9) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 23.795444ms)
Aug 24 11:51:37.500: INFO: (9) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 23.92096ms)
Aug 24 11:51:37.500: INFO: (9) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 24.451544ms)
Aug 24 11:51:37.500: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 24.17641ms)
Aug 24 11:51:37.501: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 26.219438ms)
Aug 24 11:51:37.502: INFO: (9) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.749255ms)
Aug 24 11:51:37.502: INFO: (9) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 27.06246ms)
Aug 24 11:51:37.503: INFO: (9) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 27.673484ms)
Aug 24 11:51:37.503: INFO: (9) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 27.150857ms)
Aug 24 11:51:37.504: INFO: (9) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 27.591189ms)
Aug 24 11:51:37.520: INFO: (10) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 16.125613ms)
Aug 24 11:51:37.522: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 17.650153ms)
Aug 24 11:51:37.522: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 17.864148ms)
Aug 24 11:51:37.524: INFO: (10) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 19.388775ms)
Aug 24 11:51:37.525: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.259796ms)
Aug 24 11:51:37.527: INFO: (10) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 22.09975ms)
Aug 24 11:51:37.527: INFO: (10) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.40776ms)
Aug 24 11:51:37.529: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 24.204708ms)
Aug 24 11:51:37.530: INFO: (10) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 24.628524ms)
Aug 24 11:51:37.530: INFO: (10) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 24.963742ms)
Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 26.423323ms)
Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 26.467246ms)
Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 26.471007ms)
Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 26.814154ms)
Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 26.170129ms)
Aug 24 11:51:37.532: INFO: (10) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 27.318892ms)
Aug 24 11:51:37.552: INFO: (11) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 19.412225ms)
Aug 24 11:51:37.553: INFO: (11) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 20.378284ms)
Aug 24 11:51:37.558: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 24.662806ms)
Aug 24 11:51:37.561: INFO: (11) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 27.645139ms)
Aug 24 11:51:37.564: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 30.470394ms)
Aug 24 11:51:37.565: INFO: (11) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 30.58965ms)
Aug 24 11:51:37.566: INFO: (11) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 33.409102ms)
Aug 24 11:51:37.566: INFO: (11) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 32.544232ms)
Aug 24 11:51:37.567: INFO: (11) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 31.992134ms)
Aug 24 11:51:37.567: INFO: (11) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 31.841556ms)
Aug 24 11:51:37.568: INFO: (11) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 34.48422ms)
Aug 24 11:51:37.568: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 34.156377ms)
Aug 24 11:51:37.569: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 34.095838ms)
Aug 24 11:51:37.570: INFO: (11) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 36.928613ms)
Aug 24 11:51:37.571: INFO: (11) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 36.714517ms)
Aug 24 11:51:37.572: INFO: (11) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 38.537793ms)
Aug 24 11:51:37.585: INFO: (12) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 12.090371ms)
Aug 24 11:51:37.587: INFO: (12) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 14.090782ms)
Aug 24 11:51:37.590: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 16.88185ms)
Aug 24 11:51:37.591: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 17.591153ms)
Aug 24 11:51:37.591: INFO: (12) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 17.817852ms)
Aug 24 11:51:37.592: INFO: (12) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 19.293253ms)
Aug 24 11:51:37.592: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 18.331589ms)
Aug 24 11:51:37.596: INFO: (12) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 21.828908ms)
Aug 24 11:51:37.596: INFO: (12) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 22.86005ms)
Aug 24 11:51:37.596: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 22.666103ms)
Aug 24 11:51:37.599: INFO: (12) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.078056ms)
Aug 24 11:51:37.601: INFO: (12) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 26.890166ms)
Aug 24 11:51:37.602: INFO: (12) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 27.659208ms)
Aug 24 11:51:37.605: INFO: (12) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 31.751536ms)
Aug 24 11:51:37.607: INFO: (12) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 33.198796ms)
Aug 24 11:51:37.608: INFO: (12) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 33.613248ms)
Aug 24 11:51:37.622: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 13.820007ms)
Aug 24 11:51:37.626: INFO: (13) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 17.091822ms)
Aug 24 11:51:37.626: INFO: (13) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 17.918363ms)
Aug 24 11:51:37.626: INFO: (13) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 18.280871ms)
Aug 24 11:51:37.633: INFO: (13) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.016009ms)
Aug 24 11:51:37.636: INFO: (13) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 26.94423ms)
Aug 24 11:51:37.636: INFO: (13) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.524289ms)
Aug 24 11:51:37.636: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 27.437282ms)
Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 27.937118ms)
Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 28.614409ms)
Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 28.895745ms)
Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 28.626462ms)
Aug 24 11:51:37.638: INFO: (13) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 29.407067ms)
Aug 24 11:51:37.638: INFO: (13) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 30.270699ms)
Aug 24 11:51:37.639: INFO: (13) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 30.101543ms)
Aug 24 11:51:37.639: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 30.72754ms)
Aug 24 11:51:37.650: INFO: (14) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 10.364019ms)
Aug 24 11:51:37.651: INFO: (14) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 11.109575ms)
Aug 24 11:51:37.652: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 11.972416ms)
Aug 24 11:51:37.658: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 18.147397ms)
Aug 24 11:51:37.659: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 19.440583ms)
Aug 24 11:51:37.661: INFO: (14) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 20.392691ms)
Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.222153ms)
Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 25.134679ms)
Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 26.17759ms)
Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 24.899931ms)
Aug 24 11:51:37.666: INFO: (14) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 26.65573ms)
Aug 24 11:51:37.667: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 25.720598ms)
Aug 24 11:51:37.668: INFO: (14) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 27.237623ms)
Aug 24 11:51:37.669: INFO: (14) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 29.068173ms)
Aug 24 11:51:37.669: INFO: (14) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 28.152505ms)
Aug 24 11:51:37.670: INFO: (14) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 29.195396ms)
Aug 24 11:51:37.692: INFO: (15) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 20.40674ms)
Aug 24 11:51:37.693: INFO: (15) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 22.040497ms)
Aug 24 11:51:37.693: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 23.107868ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.27314ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 23.374256ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 23.791547ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 23.138342ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.869505ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 22.65983ms)
Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 22.894564ms)
Aug 24 11:51:37.695: INFO: (15) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 24.516646ms)
Aug 24 11:51:37.696: INFO: (15) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 23.969469ms)
Aug 24 11:51:37.697: INFO: (15) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 26.830505ms)
Aug 24 11:51:37.698: INFO: (15) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 27.081836ms)
Aug 24 11:51:37.698: INFO: (15) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 27.726206ms)
Aug 24 11:51:37.699: INFO: (15) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 28.563069ms)
Aug 24 11:51:37.743: INFO: (16) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 43.321949ms)
Aug 24 11:51:37.748: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 48.682343ms)
Aug 24 11:51:37.756: INFO: (16) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 56.95181ms)
Aug 24 11:51:37.774: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 74.688539ms)
Aug 24 11:51:37.778: INFO: (16) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 79.435773ms)
Aug 24 11:51:37.779: INFO: (16) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 79.897017ms)
Aug 24 11:51:37.786: INFO: (16) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 86.61411ms)
Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 87.297453ms)
Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 87.447915ms)
Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 88.249581ms)
Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 88.009258ms)
Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 87.66611ms)
Aug 24 11:51:37.788: INFO: (16) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 88.725693ms)
Aug 24 11:51:37.790: INFO: (16) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 90.322596ms)
Aug 24 11:51:37.791: INFO: (16) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 91.494283ms)
Aug 24 11:51:37.792: INFO: (16) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 92.612271ms)
Aug 24 11:51:37.823: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 30.261926ms)
Aug 24 11:51:37.827: INFO: (17) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 34.565478ms)
Aug 24 11:51:37.846: INFO: (17) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 51.839624ms)
Aug 24 11:51:37.846: INFO: (17) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 52.952777ms)
Aug 24 11:51:37.846: INFO: (17) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 53.651048ms)
Aug 24 11:51:37.850: INFO: (17) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 56.529603ms)
Aug 24 11:51:37.867: INFO: (17) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 72.833709ms)
Aug 24 11:51:37.868: INFO: (17) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 73.804659ms)
Aug 24 11:51:37.868: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 74.185744ms)
Aug 24 11:51:37.869: INFO: (17) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 74.911447ms)
Aug 24 11:51:37.871: INFO: (17) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 76.525487ms)
Aug 24 11:51:37.873: INFO: (17) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 79.967238ms)
Aug 24 11:51:37.878: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 83.908016ms)
Aug 24 11:51:37.878: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 86.212052ms)
Aug 24 11:51:37.879: INFO: (17) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 84.4285ms)
Aug 24 11:51:37.879: INFO: (17) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 84.681732ms)
Aug 24 11:51:37.917: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 36.832525ms)
Aug 24 11:51:37.921: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 41.415566ms)
Aug 24 11:51:37.921: INFO: (18) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 41.538356ms)
Aug 24 11:51:37.924: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 43.493345ms)
Aug 24 11:51:37.924: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 43.79925ms)
Aug 24 11:51:37.925: INFO: (18) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 44.596733ms)
Aug 24 11:51:37.925: INFO: (18) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 45.023057ms)
Aug 24 11:51:37.925: INFO: (18) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 45.048769ms)
Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 46.237382ms)
Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 46.291773ms)
Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 47.75284ms)
Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 47.857356ms)
Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 47.170776ms)
Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 47.755337ms)
Aug 24 11:51:37.928: INFO: (18) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 47.625982ms)
Aug 24 11:51:37.932: INFO: (18) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 53.01254ms)
Aug 24 11:51:37.955: INFO: (19) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.094268ms)
Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 22.428466ms)
Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.387552ms)
Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 22.80696ms)
Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 23.431499ms)
Aug 24 11:51:37.960: INFO: (19) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 27.527471ms)
Aug 24 11:51:37.961: INFO: (19) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.819525ms)
Aug 24 11:51:37.966: INFO: (19) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 31.777711ms)
Aug 24 11:51:37.967: INFO: (19) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 34.150825ms)
Aug 24 11:51:37.969: INFO: (19) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 35.472317ms)
Aug 24 11:51:37.970: INFO: (19) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 37.34738ms)
Aug 24 11:51:37.970: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 35.818124ms)
Aug 24 11:51:37.972: INFO: (19) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 39.074963ms)
Aug 24 11:51:37.972: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 37.878072ms)
Aug 24 11:51:37.973: INFO: (19) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 40.015063ms)
Aug 24 11:51:37.976: INFO: (19) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 42.566629ms)
STEP: deleting ReplicationController proxy-service-xtkzj in namespace proxy-8717, will wait for the garbage collector to delete the pods 08/24/23 11:51:37.976
Aug 24 11:51:38.043: INFO: Deleting ReplicationController proxy-service-xtkzj took: 8.744417ms
Aug 24 11:51:38.143: INFO: Terminating ReplicationController proxy-service-xtkzj pods took: 100.890478ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 24 11:51:39.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8717" for this suite. 08/24/23 11:51:39.165
------------------------------
â€¢ [SLOW TEST] [6.227 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:51:32.957
    Aug 24 11:51:32.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename proxy 08/24/23 11:51:32.963
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:32.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:33.007
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/24/23 11:51:33.045
    STEP: creating replication controller proxy-service-xtkzj in namespace proxy-8717 08/24/23 11:51:33.051
    I0824 11:51:33.069739      14 runners.go:193] Created replication controller with name: proxy-service-xtkzj, namespace: proxy-8717, replica count: 1
    I0824 11:51:34.122105      14 runners.go:193] proxy-service-xtkzj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0824 11:51:35.122784      14 runners.go:193] proxy-service-xtkzj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:51:35.130: INFO: Endpoint proxy-8717/proxy-service-xtkzj is not ready yet
    Aug 24 11:51:37.138: INFO: setup took 4.126570161s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/24/23 11:51:37.138
    Aug 24 11:51:37.169: INFO: (0) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 28.522903ms)
    Aug 24 11:51:37.169: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 29.398076ms)
    Aug 24 11:51:37.172: INFO: (0) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 30.634616ms)
    Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 33.824865ms)
    Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 32.173479ms)
    Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 32.50786ms)
    Aug 24 11:51:37.173: INFO: (0) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 32.042531ms)
    Aug 24 11:51:37.178: INFO: (0) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 36.218758ms)
    Aug 24 11:51:37.178: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 37.308887ms)
    Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 39.715858ms)
    Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 40.088371ms)
    Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 39.903441ms)
    Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 40.486589ms)
    Aug 24 11:51:37.182: INFO: (0) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 42.497437ms)
    Aug 24 11:51:37.183: INFO: (0) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 41.635021ms)
    Aug 24 11:51:37.184: INFO: (0) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 42.008312ms)
    Aug 24 11:51:37.202: INFO: (1) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 17.791463ms)
    Aug 24 11:51:37.203: INFO: (1) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 18.069255ms)
    Aug 24 11:51:37.209: INFO: (1) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 24.861353ms)
    Aug 24 11:51:37.209: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 24.574287ms)
    Aug 24 11:51:37.210: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 25.086291ms)
    Aug 24 11:51:37.211: INFO: (1) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 26.613043ms)
    Aug 24 11:51:37.212: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 26.753585ms)
    Aug 24 11:51:37.217: INFO: (1) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 31.906206ms)
    Aug 24 11:51:37.217: INFO: (1) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 32.873998ms)
    Aug 24 11:51:37.219: INFO: (1) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 33.484772ms)
    Aug 24 11:51:37.219: INFO: (1) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 34.803439ms)
    Aug 24 11:51:37.220: INFO: (1) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 34.594548ms)
    Aug 24 11:51:37.221: INFO: (1) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 35.540902ms)
    Aug 24 11:51:37.222: INFO: (1) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 36.927147ms)
    Aug 24 11:51:37.223: INFO: (1) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 36.886405ms)
    Aug 24 11:51:37.224: INFO: (1) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 38.556353ms)
    Aug 24 11:51:37.245: INFO: (2) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 20.433121ms)
    Aug 24 11:51:37.245: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 20.303791ms)
    Aug 24 11:51:37.247: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 21.027376ms)
    Aug 24 11:51:37.248: INFO: (2) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 23.018981ms)
    Aug 24 11:51:37.250: INFO: (2) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 24.96934ms)
    Aug 24 11:51:37.250: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 25.118302ms)
    Aug 24 11:51:37.251: INFO: (2) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 24.502841ms)
    Aug 24 11:51:37.252: INFO: (2) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 27.460364ms)
    Aug 24 11:51:37.252: INFO: (2) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 27.809561ms)
    Aug 24 11:51:37.252: INFO: (2) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 26.407599ms)
    Aug 24 11:51:37.253: INFO: (2) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 26.923552ms)
    Aug 24 11:51:37.258: INFO: (2) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 33.175362ms)
    Aug 24 11:51:37.259: INFO: (2) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 33.582068ms)
    Aug 24 11:51:37.259: INFO: (2) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 34.048449ms)
    Aug 24 11:51:37.259: INFO: (2) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 33.692441ms)
    Aug 24 11:51:37.262: INFO: (2) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 36.861813ms)
    Aug 24 11:51:37.274: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 11.337526ms)
    Aug 24 11:51:37.275: INFO: (3) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 11.520741ms)
    Aug 24 11:51:37.278: INFO: (3) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 15.460722ms)
    Aug 24 11:51:37.282: INFO: (3) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 19.336439ms)
    Aug 24 11:51:37.282: INFO: (3) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 18.871227ms)
    Aug 24 11:51:37.284: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.459922ms)
    Aug 24 11:51:37.286: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.808705ms)
    Aug 24 11:51:37.287: INFO: (3) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.186352ms)
    Aug 24 11:51:37.291: INFO: (3) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 27.054353ms)
    Aug 24 11:51:37.291: INFO: (3) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.940457ms)
    Aug 24 11:51:37.291: INFO: (3) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 27.187897ms)
    Aug 24 11:51:37.293: INFO: (3) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 28.807852ms)
    Aug 24 11:51:37.296: INFO: (3) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 31.859408ms)
    Aug 24 11:51:37.297: INFO: (3) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 33.376647ms)
    Aug 24 11:51:37.298: INFO: (3) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 33.805351ms)
    Aug 24 11:51:37.299: INFO: (3) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 34.501386ms)
    Aug 24 11:51:37.323: INFO: (4) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 23.419651ms)
    Aug 24 11:51:37.331: INFO: (4) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 30.844785ms)
    Aug 24 11:51:37.334: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 30.93905ms)
    Aug 24 11:51:37.335: INFO: (4) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 33.699125ms)
    Aug 24 11:51:37.335: INFO: (4) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 32.987935ms)
    Aug 24 11:51:37.343: INFO: (4) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 43.537878ms)
    Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 45.108472ms)
    Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 46.236119ms)
    Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 46.690546ms)
    Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 46.267377ms)
    Aug 24 11:51:37.347: INFO: (4) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 47.467086ms)
    Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 47.267427ms)
    Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 48.11728ms)
    Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 48.114615ms)
    Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 48.688187ms)
    Aug 24 11:51:37.348: INFO: (4) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 47.160844ms)
    Aug 24 11:51:37.364: INFO: (5) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 15.522742ms)
    Aug 24 11:51:37.370: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.439746ms)
    Aug 24 11:51:37.371: INFO: (5) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 21.79586ms)
    Aug 24 11:51:37.373: INFO: (5) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 23.3018ms)
    Aug 24 11:51:37.373: INFO: (5) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 22.620725ms)
    Aug 24 11:51:37.374: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 23.823797ms)
    Aug 24 11:51:37.380: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 30.690922ms)
    Aug 24 11:51:37.380: INFO: (5) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 29.846445ms)
    Aug 24 11:51:37.381: INFO: (5) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 32.052231ms)
    Aug 24 11:51:37.381: INFO: (5) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 31.186761ms)
    Aug 24 11:51:37.382: INFO: (5) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 31.231351ms)
    Aug 24 11:51:37.382: INFO: (5) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 33.160176ms)
    Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 34.103866ms)
    Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 34.157641ms)
    Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 33.661279ms)
    Aug 24 11:51:37.384: INFO: (5) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 33.432515ms)
    Aug 24 11:51:37.396: INFO: (6) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 10.944071ms)
    Aug 24 11:51:37.403: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 17.069376ms)
    Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 24.992351ms)
    Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 24.489349ms)
    Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 22.721276ms)
    Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.633074ms)
    Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 24.892041ms)
    Aug 24 11:51:37.410: INFO: (6) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 25.378997ms)
    Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 26.88738ms)
    Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.593402ms)
    Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 26.997856ms)
    Aug 24 11:51:37.414: INFO: (6) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 26.85147ms)
    Aug 24 11:51:37.415: INFO: (6) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 27.356661ms)
    Aug 24 11:51:37.415: INFO: (6) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 27.724933ms)
    Aug 24 11:51:37.416: INFO: (6) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 29.225761ms)
    Aug 24 11:51:37.417: INFO: (6) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 29.965273ms)
    Aug 24 11:51:37.430: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 12.445567ms)
    Aug 24 11:51:37.430: INFO: (7) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 12.5745ms)
    Aug 24 11:51:37.436: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 18.115475ms)
    Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 19.1481ms)
    Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 18.849719ms)
    Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 18.688084ms)
    Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 18.24083ms)
    Aug 24 11:51:37.437: INFO: (7) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 18.344107ms)
    Aug 24 11:51:37.439: INFO: (7) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 20.532539ms)
    Aug 24 11:51:37.439: INFO: (7) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 20.538945ms)
    Aug 24 11:51:37.439: INFO: (7) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 21.105668ms)
    Aug 24 11:51:37.441: INFO: (7) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 23.065386ms)
    Aug 24 11:51:37.442: INFO: (7) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 24.860047ms)
    Aug 24 11:51:37.442: INFO: (7) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 23.830203ms)
    Aug 24 11:51:37.442: INFO: (7) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 24.472511ms)
    Aug 24 11:51:37.443: INFO: (7) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 24.893369ms)
    Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 13.456889ms)
    Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 13.909277ms)
    Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 14.491022ms)
    Aug 24 11:51:37.457: INFO: (8) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 14.29584ms)
    Aug 24 11:51:37.458: INFO: (8) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 14.723869ms)
    Aug 24 11:51:37.465: INFO: (8) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 21.475991ms)
    Aug 24 11:51:37.467: INFO: (8) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.534775ms)
    Aug 24 11:51:37.467: INFO: (8) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 23.809043ms)
    Aug 24 11:51:37.468: INFO: (8) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 24.228281ms)
    Aug 24 11:51:37.471: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 27.230416ms)
    Aug 24 11:51:37.472: INFO: (8) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 28.085565ms)
    Aug 24 11:51:37.472: INFO: (8) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 29.256385ms)
    Aug 24 11:51:37.472: INFO: (8) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 28.519903ms)
    Aug 24 11:51:37.474: INFO: (8) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 31.270202ms)
    Aug 24 11:51:37.475: INFO: (8) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 31.219872ms)
    Aug 24 11:51:37.475: INFO: (8) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 31.075299ms)
    Aug 24 11:51:37.488: INFO: (9) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 13.32175ms)
    Aug 24 11:51:37.489: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 12.983146ms)
    Aug 24 11:51:37.491: INFO: (9) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 15.909536ms)
    Aug 24 11:51:37.496: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.757405ms)
    Aug 24 11:51:37.499: INFO: (9) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 22.628104ms)
    Aug 24 11:51:37.499: INFO: (9) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 23.974649ms)
    Aug 24 11:51:37.499: INFO: (9) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 23.795444ms)
    Aug 24 11:51:37.500: INFO: (9) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 23.92096ms)
    Aug 24 11:51:37.500: INFO: (9) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 24.451544ms)
    Aug 24 11:51:37.500: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 24.17641ms)
    Aug 24 11:51:37.501: INFO: (9) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 26.219438ms)
    Aug 24 11:51:37.502: INFO: (9) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.749255ms)
    Aug 24 11:51:37.502: INFO: (9) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 27.06246ms)
    Aug 24 11:51:37.503: INFO: (9) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 27.673484ms)
    Aug 24 11:51:37.503: INFO: (9) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 27.150857ms)
    Aug 24 11:51:37.504: INFO: (9) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 27.591189ms)
    Aug 24 11:51:37.520: INFO: (10) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 16.125613ms)
    Aug 24 11:51:37.522: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 17.650153ms)
    Aug 24 11:51:37.522: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 17.864148ms)
    Aug 24 11:51:37.524: INFO: (10) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 19.388775ms)
    Aug 24 11:51:37.525: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 20.259796ms)
    Aug 24 11:51:37.527: INFO: (10) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 22.09975ms)
    Aug 24 11:51:37.527: INFO: (10) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.40776ms)
    Aug 24 11:51:37.529: INFO: (10) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 24.204708ms)
    Aug 24 11:51:37.530: INFO: (10) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 24.628524ms)
    Aug 24 11:51:37.530: INFO: (10) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 24.963742ms)
    Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 26.423323ms)
    Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 26.467246ms)
    Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 26.471007ms)
    Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 26.814154ms)
    Aug 24 11:51:37.531: INFO: (10) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 26.170129ms)
    Aug 24 11:51:37.532: INFO: (10) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 27.318892ms)
    Aug 24 11:51:37.552: INFO: (11) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 19.412225ms)
    Aug 24 11:51:37.553: INFO: (11) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 20.378284ms)
    Aug 24 11:51:37.558: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 24.662806ms)
    Aug 24 11:51:37.561: INFO: (11) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 27.645139ms)
    Aug 24 11:51:37.564: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 30.470394ms)
    Aug 24 11:51:37.565: INFO: (11) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 30.58965ms)
    Aug 24 11:51:37.566: INFO: (11) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 33.409102ms)
    Aug 24 11:51:37.566: INFO: (11) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 32.544232ms)
    Aug 24 11:51:37.567: INFO: (11) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 31.992134ms)
    Aug 24 11:51:37.567: INFO: (11) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 31.841556ms)
    Aug 24 11:51:37.568: INFO: (11) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 34.48422ms)
    Aug 24 11:51:37.568: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 34.156377ms)
    Aug 24 11:51:37.569: INFO: (11) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 34.095838ms)
    Aug 24 11:51:37.570: INFO: (11) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 36.928613ms)
    Aug 24 11:51:37.571: INFO: (11) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 36.714517ms)
    Aug 24 11:51:37.572: INFO: (11) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 38.537793ms)
    Aug 24 11:51:37.585: INFO: (12) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 12.090371ms)
    Aug 24 11:51:37.587: INFO: (12) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 14.090782ms)
    Aug 24 11:51:37.590: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 16.88185ms)
    Aug 24 11:51:37.591: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 17.591153ms)
    Aug 24 11:51:37.591: INFO: (12) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 17.817852ms)
    Aug 24 11:51:37.592: INFO: (12) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 19.293253ms)
    Aug 24 11:51:37.592: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 18.331589ms)
    Aug 24 11:51:37.596: INFO: (12) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 21.828908ms)
    Aug 24 11:51:37.596: INFO: (12) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 22.86005ms)
    Aug 24 11:51:37.596: INFO: (12) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 22.666103ms)
    Aug 24 11:51:37.599: INFO: (12) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.078056ms)
    Aug 24 11:51:37.601: INFO: (12) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 26.890166ms)
    Aug 24 11:51:37.602: INFO: (12) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 27.659208ms)
    Aug 24 11:51:37.605: INFO: (12) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 31.751536ms)
    Aug 24 11:51:37.607: INFO: (12) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 33.198796ms)
    Aug 24 11:51:37.608: INFO: (12) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 33.613248ms)
    Aug 24 11:51:37.622: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 13.820007ms)
    Aug 24 11:51:37.626: INFO: (13) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 17.091822ms)
    Aug 24 11:51:37.626: INFO: (13) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 17.918363ms)
    Aug 24 11:51:37.626: INFO: (13) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 18.280871ms)
    Aug 24 11:51:37.633: INFO: (13) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.016009ms)
    Aug 24 11:51:37.636: INFO: (13) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 26.94423ms)
    Aug 24 11:51:37.636: INFO: (13) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.524289ms)
    Aug 24 11:51:37.636: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 27.437282ms)
    Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 27.937118ms)
    Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 28.614409ms)
    Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 28.895745ms)
    Aug 24 11:51:37.637: INFO: (13) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 28.626462ms)
    Aug 24 11:51:37.638: INFO: (13) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 29.407067ms)
    Aug 24 11:51:37.638: INFO: (13) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 30.270699ms)
    Aug 24 11:51:37.639: INFO: (13) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 30.101543ms)
    Aug 24 11:51:37.639: INFO: (13) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 30.72754ms)
    Aug 24 11:51:37.650: INFO: (14) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 10.364019ms)
    Aug 24 11:51:37.651: INFO: (14) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 11.109575ms)
    Aug 24 11:51:37.652: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 11.972416ms)
    Aug 24 11:51:37.658: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 18.147397ms)
    Aug 24 11:51:37.659: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 19.440583ms)
    Aug 24 11:51:37.661: INFO: (14) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 20.392691ms)
    Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 25.222153ms)
    Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 25.134679ms)
    Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 26.17759ms)
    Aug 24 11:51:37.665: INFO: (14) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 24.899931ms)
    Aug 24 11:51:37.666: INFO: (14) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 26.65573ms)
    Aug 24 11:51:37.667: INFO: (14) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 25.720598ms)
    Aug 24 11:51:37.668: INFO: (14) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 27.237623ms)
    Aug 24 11:51:37.669: INFO: (14) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 29.068173ms)
    Aug 24 11:51:37.669: INFO: (14) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 28.152505ms)
    Aug 24 11:51:37.670: INFO: (14) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 29.195396ms)
    Aug 24 11:51:37.692: INFO: (15) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 20.40674ms)
    Aug 24 11:51:37.693: INFO: (15) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 22.040497ms)
    Aug 24 11:51:37.693: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 23.107868ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.27314ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 23.374256ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 23.791547ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 23.138342ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.869505ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 22.65983ms)
    Aug 24 11:51:37.694: INFO: (15) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 22.894564ms)
    Aug 24 11:51:37.695: INFO: (15) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 24.516646ms)
    Aug 24 11:51:37.696: INFO: (15) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 23.969469ms)
    Aug 24 11:51:37.697: INFO: (15) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 26.830505ms)
    Aug 24 11:51:37.698: INFO: (15) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 27.081836ms)
    Aug 24 11:51:37.698: INFO: (15) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 27.726206ms)
    Aug 24 11:51:37.699: INFO: (15) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 28.563069ms)
    Aug 24 11:51:37.743: INFO: (16) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 43.321949ms)
    Aug 24 11:51:37.748: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 48.682343ms)
    Aug 24 11:51:37.756: INFO: (16) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 56.95181ms)
    Aug 24 11:51:37.774: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 74.688539ms)
    Aug 24 11:51:37.778: INFO: (16) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 79.435773ms)
    Aug 24 11:51:37.779: INFO: (16) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 79.897017ms)
    Aug 24 11:51:37.786: INFO: (16) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 86.61411ms)
    Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 87.297453ms)
    Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 87.447915ms)
    Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 88.249581ms)
    Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 88.009258ms)
    Aug 24 11:51:37.787: INFO: (16) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 87.66611ms)
    Aug 24 11:51:37.788: INFO: (16) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 88.725693ms)
    Aug 24 11:51:37.790: INFO: (16) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 90.322596ms)
    Aug 24 11:51:37.791: INFO: (16) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 91.494283ms)
    Aug 24 11:51:37.792: INFO: (16) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 92.612271ms)
    Aug 24 11:51:37.823: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 30.261926ms)
    Aug 24 11:51:37.827: INFO: (17) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 34.565478ms)
    Aug 24 11:51:37.846: INFO: (17) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 51.839624ms)
    Aug 24 11:51:37.846: INFO: (17) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 52.952777ms)
    Aug 24 11:51:37.846: INFO: (17) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 53.651048ms)
    Aug 24 11:51:37.850: INFO: (17) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 56.529603ms)
    Aug 24 11:51:37.867: INFO: (17) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 72.833709ms)
    Aug 24 11:51:37.868: INFO: (17) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 73.804659ms)
    Aug 24 11:51:37.868: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 74.185744ms)
    Aug 24 11:51:37.869: INFO: (17) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 74.911447ms)
    Aug 24 11:51:37.871: INFO: (17) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 76.525487ms)
    Aug 24 11:51:37.873: INFO: (17) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 79.967238ms)
    Aug 24 11:51:37.878: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 83.908016ms)
    Aug 24 11:51:37.878: INFO: (17) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 86.212052ms)
    Aug 24 11:51:37.879: INFO: (17) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 84.4285ms)
    Aug 24 11:51:37.879: INFO: (17) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 84.681732ms)
    Aug 24 11:51:37.917: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 36.832525ms)
    Aug 24 11:51:37.921: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 41.415566ms)
    Aug 24 11:51:37.921: INFO: (18) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 41.538356ms)
    Aug 24 11:51:37.924: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 43.493345ms)
    Aug 24 11:51:37.924: INFO: (18) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 43.79925ms)
    Aug 24 11:51:37.925: INFO: (18) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 44.596733ms)
    Aug 24 11:51:37.925: INFO: (18) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 45.023057ms)
    Aug 24 11:51:37.925: INFO: (18) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 45.048769ms)
    Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 46.237382ms)
    Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 46.291773ms)
    Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 47.75284ms)
    Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 47.857356ms)
    Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 47.170776ms)
    Aug 24 11:51:37.927: INFO: (18) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 47.755337ms)
    Aug 24 11:51:37.928: INFO: (18) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 47.625982ms)
    Aug 24 11:51:37.932: INFO: (18) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 53.01254ms)
    Aug 24 11:51:37.955: INFO: (19) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:460/proxy/: tls baz (200; 23.094268ms)
    Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 22.428466ms)
    Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 22.387552ms)
    Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">... (200; 22.80696ms)
    Aug 24 11:51:37.956: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7/proxy/rewriteme">test</a> (200; 23.431499ms)
    Aug 24 11:51:37.960: INFO: (19) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname2/proxy/: bar (200; 27.527471ms)
    Aug 24 11:51:37.961: INFO: (19) /api/v1/namespaces/proxy-8717/pods/http:proxy-service-xtkzj-7ftp7:162/proxy/: bar (200; 26.819525ms)
    Aug 24 11:51:37.966: INFO: (19) /api/v1/namespaces/proxy-8717/services/proxy-service-xtkzj:portname1/proxy/: foo (200; 31.777711ms)
    Aug 24 11:51:37.967: INFO: (19) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:443/proxy/tlsrewritem... (200; 34.150825ms)
    Aug 24 11:51:37.969: INFO: (19) /api/v1/namespaces/proxy-8717/pods/https:proxy-service-xtkzj-7ftp7:462/proxy/: tls qux (200; 35.472317ms)
    Aug 24 11:51:37.970: INFO: (19) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname2/proxy/: bar (200; 37.34738ms)
    Aug 24 11:51:37.970: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:1080/proxy/rewriteme">test<... (200; 35.818124ms)
    Aug 24 11:51:37.972: INFO: (19) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname2/proxy/: tls qux (200; 39.074963ms)
    Aug 24 11:51:37.972: INFO: (19) /api/v1/namespaces/proxy-8717/pods/proxy-service-xtkzj-7ftp7:160/proxy/: foo (200; 37.878072ms)
    Aug 24 11:51:37.973: INFO: (19) /api/v1/namespaces/proxy-8717/services/http:proxy-service-xtkzj:portname1/proxy/: foo (200; 40.015063ms)
    Aug 24 11:51:37.976: INFO: (19) /api/v1/namespaces/proxy-8717/services/https:proxy-service-xtkzj:tlsportname1/proxy/: tls baz (200; 42.566629ms)
    STEP: deleting ReplicationController proxy-service-xtkzj in namespace proxy-8717, will wait for the garbage collector to delete the pods 08/24/23 11:51:37.976
    Aug 24 11:51:38.043: INFO: Deleting ReplicationController proxy-service-xtkzj took: 8.744417ms
    Aug 24 11:51:38.143: INFO: Terminating ReplicationController proxy-service-xtkzj pods took: 100.890478ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:51:39.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8717" for this suite. 08/24/23 11:51:39.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:51:39.186
Aug 24 11:51:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename subpath 08/24/23 11:51:39.188
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:39.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:39.24
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 11:51:39.249
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-z7xc 08/24/23 11:51:39.275
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:51:39.275
Aug 24 11:51:39.296: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z7xc" in namespace "subpath-8477" to be "Succeeded or Failed"
Aug 24 11:51:39.303: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.596056ms
Aug 24 11:51:41.311: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014441883s
Aug 24 11:51:43.311: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 4.014578363s
Aug 24 11:51:45.310: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 6.013547541s
Aug 24 11:51:47.309: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 8.012016548s
Aug 24 11:51:49.310: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 10.013934123s
Aug 24 11:51:51.308: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 12.011568026s
Aug 24 11:51:53.310: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 14.013809209s
Aug 24 11:51:55.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 16.015433365s
Aug 24 11:51:57.311: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 18.014939949s
Aug 24 11:51:59.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 20.015222149s
Aug 24 11:52:01.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=false. Elapsed: 22.015429004s
Aug 24 11:52:03.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015505216s
STEP: Saw pod success 08/24/23 11:52:03.312
Aug 24 11:52:03.313: INFO: Pod "pod-subpath-test-downwardapi-z7xc" satisfied condition "Succeeded or Failed"
Aug 24 11:52:03.323: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-downwardapi-z7xc container test-container-subpath-downwardapi-z7xc: <nil>
STEP: delete the pod 08/24/23 11:52:03.339
Aug 24 11:52:03.365: INFO: Waiting for pod pod-subpath-test-downwardapi-z7xc to disappear
Aug 24 11:52:03.378: INFO: Pod pod-subpath-test-downwardapi-z7xc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-z7xc 08/24/23 11:52:03.378
Aug 24 11:52:03.378: INFO: Deleting pod "pod-subpath-test-downwardapi-z7xc" in namespace "subpath-8477"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 11:52:03.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8477" for this suite. 08/24/23 11:52:03.394
------------------------------
â€¢ [SLOW TEST] [24.234 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:51:39.186
    Aug 24 11:51:39.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename subpath 08/24/23 11:51:39.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:51:39.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:51:39.24
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 11:51:39.249
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-z7xc 08/24/23 11:51:39.275
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:51:39.275
    Aug 24 11:51:39.296: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-z7xc" in namespace "subpath-8477" to be "Succeeded or Failed"
    Aug 24 11:51:39.303: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.596056ms
    Aug 24 11:51:41.311: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014441883s
    Aug 24 11:51:43.311: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 4.014578363s
    Aug 24 11:51:45.310: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 6.013547541s
    Aug 24 11:51:47.309: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 8.012016548s
    Aug 24 11:51:49.310: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 10.013934123s
    Aug 24 11:51:51.308: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 12.011568026s
    Aug 24 11:51:53.310: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 14.013809209s
    Aug 24 11:51:55.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 16.015433365s
    Aug 24 11:51:57.311: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 18.014939949s
    Aug 24 11:51:59.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=true. Elapsed: 20.015222149s
    Aug 24 11:52:01.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Running", Reason="", readiness=false. Elapsed: 22.015429004s
    Aug 24 11:52:03.312: INFO: Pod "pod-subpath-test-downwardapi-z7xc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015505216s
    STEP: Saw pod success 08/24/23 11:52:03.312
    Aug 24 11:52:03.313: INFO: Pod "pod-subpath-test-downwardapi-z7xc" satisfied condition "Succeeded or Failed"
    Aug 24 11:52:03.323: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-downwardapi-z7xc container test-container-subpath-downwardapi-z7xc: <nil>
    STEP: delete the pod 08/24/23 11:52:03.339
    Aug 24 11:52:03.365: INFO: Waiting for pod pod-subpath-test-downwardapi-z7xc to disappear
    Aug 24 11:52:03.378: INFO: Pod pod-subpath-test-downwardapi-z7xc no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-z7xc 08/24/23 11:52:03.378
    Aug 24 11:52:03.378: INFO: Deleting pod "pod-subpath-test-downwardapi-z7xc" in namespace "subpath-8477"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:52:03.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8477" for this suite. 08/24/23 11:52:03.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:52:03.429
Aug 24 11:52:03.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 11:52:03.431
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:52:03.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:52:03.468
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2 in namespace container-probe-38 08/24/23 11:52:03.473
Aug 24 11:52:03.491: INFO: Waiting up to 5m0s for pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2" in namespace "container-probe-38" to be "not pending"
Aug 24 11:52:03.510: INFO: Pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.234607ms
Aug 24 11:52:05.517: INFO: Pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025845522s
Aug 24 11:52:05.517: INFO: Pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2" satisfied condition "not pending"
Aug 24 11:52:05.517: INFO: Started pod test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2 in namespace container-probe-38
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:52:05.517
Aug 24 11:52:05.521: INFO: Initial restart count of pod test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2 is 0
STEP: deleting the pod 08/24/23 11:56:06.659
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 11:56:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-38" for this suite. 08/24/23 11:56:06.698
------------------------------
â€¢ [SLOW TEST] [243.281 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:52:03.429
    Aug 24 11:52:03.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:52:03.431
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:52:03.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:52:03.468
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2 in namespace container-probe-38 08/24/23 11:52:03.473
    Aug 24 11:52:03.491: INFO: Waiting up to 5m0s for pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2" in namespace "container-probe-38" to be "not pending"
    Aug 24 11:52:03.510: INFO: Pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.234607ms
    Aug 24 11:52:05.517: INFO: Pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025845522s
    Aug 24 11:52:05.517: INFO: Pod "test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2" satisfied condition "not pending"
    Aug 24 11:52:05.517: INFO: Started pod test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2 in namespace container-probe-38
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:52:05.517
    Aug 24 11:52:05.521: INFO: Initial restart count of pod test-webserver-6d5318e9-1cea-4a4a-8b1b-a0913068c4d2 is 0
    STEP: deleting the pod 08/24/23 11:56:06.659
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:56:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-38" for this suite. 08/24/23 11:56:06.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:56:06.712
Aug 24 11:56:06.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 11:56:06.724
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:06.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:06.773
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/24/23 11:56:06.784
STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 11:56:06.795
STEP: delete the deployment 08/24/23 11:56:07.317
STEP: wait for all rs to be garbage collected 08/24/23 11:56:07.328
STEP: expected 0 rs, got 1 rs 08/24/23 11:56:07.339
STEP: expected 0 pods, got 2 pods 08/24/23 11:56:07.361
STEP: Gathering metrics 08/24/23 11:56:07.889
Aug 24 11:56:07.943: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
Aug 24 11:56:07.949: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.047528ms
Aug 24 11:56:07.949: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
Aug 24 11:56:07.949: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
Aug 24 11:56:08.072: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 11:56:08.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8741" for this suite. 08/24/23 11:56:08.083
------------------------------
â€¢ [1.389 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:56:06.712
    Aug 24 11:56:06.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 11:56:06.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:06.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:06.773
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/24/23 11:56:06.784
    STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 11:56:06.795
    STEP: delete the deployment 08/24/23 11:56:07.317
    STEP: wait for all rs to be garbage collected 08/24/23 11:56:07.328
    STEP: expected 0 rs, got 1 rs 08/24/23 11:56:07.339
    STEP: expected 0 pods, got 2 pods 08/24/23 11:56:07.361
    STEP: Gathering metrics 08/24/23 11:56:07.889
    Aug 24 11:56:07.943: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
    Aug 24 11:56:07.949: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.047528ms
    Aug 24 11:56:07.949: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
    Aug 24 11:56:07.949: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
    Aug 24 11:56:08.072: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:56:08.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8741" for this suite. 08/24/23 11:56:08.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:56:08.106
Aug 24 11:56:08.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 11:56:08.108
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:08.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:08.145
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/24/23 11:56:08.151
STEP: waiting for pod running 08/24/23 11:56:08.181
Aug 24 11:56:08.181: INFO: Waiting up to 2m0s for pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" in namespace "var-expansion-9608" to be "running"
Aug 24 11:56:08.193: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.847109ms
Aug 24 11:56:10.200: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018598807s
Aug 24 11:56:10.200: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" satisfied condition "running"
STEP: creating a file in subpath 08/24/23 11:56:10.2
Aug 24 11:56:10.205: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9608 PodName:var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:56:10.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:56:10.208: INFO: ExecWithOptions: Clientset creation
Aug 24 11:56:10.208: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9608/pods/var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/24/23 11:56:10.316
Aug 24 11:56:10.322: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9608 PodName:var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:56:10.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 11:56:10.324: INFO: ExecWithOptions: Clientset creation
Aug 24 11:56:10.324: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9608/pods/var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/24/23 11:56:10.44
Aug 24 11:56:10.962: INFO: Successfully updated pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8"
STEP: waiting for annotated pod running 08/24/23 11:56:10.962
Aug 24 11:56:10.962: INFO: Waiting up to 2m0s for pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" in namespace "var-expansion-9608" to be "running"
Aug 24 11:56:10.970: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8": Phase="Running", Reason="", readiness=true. Elapsed: 7.314786ms
Aug 24 11:56:10.970: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" satisfied condition "running"
STEP: deleting the pod gracefully 08/24/23 11:56:10.97
Aug 24 11:56:10.970: INFO: Deleting pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" in namespace "var-expansion-9608"
Aug 24 11:56:10.994: INFO: Wait up to 5m0s for pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 11:56:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9608" for this suite. 08/24/23 11:56:45.024
------------------------------
â€¢ [SLOW TEST] [36.931 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:56:08.106
    Aug 24 11:56:08.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 11:56:08.108
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:08.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:08.145
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/24/23 11:56:08.151
    STEP: waiting for pod running 08/24/23 11:56:08.181
    Aug 24 11:56:08.181: INFO: Waiting up to 2m0s for pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" in namespace "var-expansion-9608" to be "running"
    Aug 24 11:56:08.193: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.847109ms
    Aug 24 11:56:10.200: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018598807s
    Aug 24 11:56:10.200: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" satisfied condition "running"
    STEP: creating a file in subpath 08/24/23 11:56:10.2
    Aug 24 11:56:10.205: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9608 PodName:var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:56:10.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:56:10.208: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:56:10.208: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9608/pods/var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/24/23 11:56:10.316
    Aug 24 11:56:10.322: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9608 PodName:var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:56:10.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 11:56:10.324: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:56:10.324: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9608/pods/var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/24/23 11:56:10.44
    Aug 24 11:56:10.962: INFO: Successfully updated pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8"
    STEP: waiting for annotated pod running 08/24/23 11:56:10.962
    Aug 24 11:56:10.962: INFO: Waiting up to 2m0s for pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" in namespace "var-expansion-9608" to be "running"
    Aug 24 11:56:10.970: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8": Phase="Running", Reason="", readiness=true. Elapsed: 7.314786ms
    Aug 24 11:56:10.970: INFO: Pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" satisfied condition "running"
    STEP: deleting the pod gracefully 08/24/23 11:56:10.97
    Aug 24 11:56:10.970: INFO: Deleting pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" in namespace "var-expansion-9608"
    Aug 24 11:56:10.994: INFO: Wait up to 5m0s for pod "var-expansion-1b471ee9-a874-4acd-bcbf-67f3086b66b8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:56:45.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9608" for this suite. 08/24/23 11:56:45.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:56:45.048
Aug 24 11:56:45.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename job 08/24/23 11:56:45.053
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:45.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:45.097
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/24/23 11:56:45.106
STEP: Patching the Job 08/24/23 11:56:45.122
STEP: Watching for Job to be patched 08/24/23 11:56:45.15
Aug 24 11:56:45.153: INFO: Event ADDED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 24 11:56:45.153: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 24 11:56:45.153: INFO: Event MODIFIED found for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/24/23 11:56:45.154
STEP: Watching for Job to be updated 08/24/23 11:56:45.169
Aug 24 11:56:45.172: INFO: Event MODIFIED found for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 11:56:45.172: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/24/23 11:56:45.172
Aug 24 11:56:45.177: INFO: Job: e2e-wknj6 as labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched]
STEP: Waiting for job to complete 08/24/23 11:56:45.177
STEP: Delete a job collection with a labelselector 08/24/23 11:56:57.184
STEP: Watching for Job to be deleted 08/24/23 11:56:57.202
Aug 24 11:56:57.206: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 11:56:57.207: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 11:56:57.207: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 11:56:57.207: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 11:56:57.208: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 11:56:57.208: INFO: Event DELETED found for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/24/23 11:56:57.208
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 11:56:57.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7777" for this suite. 08/24/23 11:56:57.223
------------------------------
â€¢ [SLOW TEST] [12.188 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:56:45.048
    Aug 24 11:56:45.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename job 08/24/23 11:56:45.053
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:45.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:45.097
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/24/23 11:56:45.106
    STEP: Patching the Job 08/24/23 11:56:45.122
    STEP: Watching for Job to be patched 08/24/23 11:56:45.15
    Aug 24 11:56:45.153: INFO: Event ADDED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 24 11:56:45.153: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 24 11:56:45.153: INFO: Event MODIFIED found for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/24/23 11:56:45.154
    STEP: Watching for Job to be updated 08/24/23 11:56:45.169
    Aug 24 11:56:45.172: INFO: Event MODIFIED found for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 11:56:45.172: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/24/23 11:56:45.172
    Aug 24 11:56:45.177: INFO: Job: e2e-wknj6 as labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched]
    STEP: Waiting for job to complete 08/24/23 11:56:45.177
    STEP: Delete a job collection with a labelselector 08/24/23 11:56:57.184
    STEP: Watching for Job to be deleted 08/24/23 11:56:57.202
    Aug 24 11:56:57.206: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 11:56:57.207: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 11:56:57.207: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 11:56:57.207: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 11:56:57.208: INFO: Event MODIFIED observed for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 11:56:57.208: INFO: Event DELETED found for Job e2e-wknj6 in namespace job-7777 with labels: map[e2e-job-label:e2e-wknj6 e2e-wknj6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/24/23 11:56:57.208
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:56:57.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7777" for this suite. 08/24/23 11:56:57.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:56:57.258
Aug 24 11:56:57.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 11:56:57.26
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:57.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:57.333
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4 in namespace container-probe-4704 08/24/23 11:56:57.338
Aug 24 11:56:57.352: INFO: Waiting up to 5m0s for pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4" in namespace "container-probe-4704" to be "not pending"
Aug 24 11:56:57.361: INFO: Pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134284ms
Aug 24 11:56:59.370: INFO: Pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018339203s
Aug 24 11:56:59.370: INFO: Pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4" satisfied condition "not pending"
Aug 24 11:56:59.370: INFO: Started pod liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4 in namespace container-probe-4704
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:56:59.37
Aug 24 11:56:59.376: INFO: Initial restart count of pod liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4 is 0
STEP: deleting the pod 08/24/23 12:01:00.53
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:00.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4704" for this suite. 08/24/23 12:01:00.569
------------------------------
â€¢ [SLOW TEST] [243.346 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:56:57.258
    Aug 24 11:56:57.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:56:57.26
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:57.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:57.333
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4 in namespace container-probe-4704 08/24/23 11:56:57.338
    Aug 24 11:56:57.352: INFO: Waiting up to 5m0s for pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4" in namespace "container-probe-4704" to be "not pending"
    Aug 24 11:56:57.361: INFO: Pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.134284ms
    Aug 24 11:56:59.370: INFO: Pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018339203s
    Aug 24 11:56:59.370: INFO: Pod "liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4" satisfied condition "not pending"
    Aug 24 11:56:59.370: INFO: Started pod liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4 in namespace container-probe-4704
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:56:59.37
    Aug 24 11:56:59.376: INFO: Initial restart count of pod liveness-4bcf1f9d-8bc6-466a-920f-0ee72d0e22a4 is 0
    STEP: deleting the pod 08/24/23 12:01:00.53
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:00.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4704" for this suite. 08/24/23 12:01:00.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:00.609
Aug 24 12:01:00.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:01:00.614
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:00.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:00.659
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:00.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5040" for this suite. 08/24/23 12:01:00.689
------------------------------
â€¢ [0.091 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:00.609
    Aug 24 12:01:00.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:01:00.614
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:00.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:00.659
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:00.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5040" for this suite. 08/24/23 12:01:00.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:00.708
Aug 24 12:01:00.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:01:00.71
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:00.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:00.743
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:01:00.774
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:01:02.292
STEP: Deploying the webhook pod 08/24/23 12:01:02.308
STEP: Wait for the deployment to be ready 08/24/23 12:01:02.331
Aug 24 12:01:02.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:01:04.374
STEP: Verifying the service has paired with the endpoint 08/24/23 12:01:04.407
Aug 24 12:01:05.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/24/23 12:01:05.416
STEP: create a configmap that should be updated by the webhook 08/24/23 12:01:05.45
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:05.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1878" for this suite. 08/24/23 12:01:05.59
STEP: Destroying namespace "webhook-1878-markers" for this suite. 08/24/23 12:01:05.606
------------------------------
â€¢ [4.914 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:00.708
    Aug 24 12:01:00.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:01:00.71
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:00.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:00.743
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:01:00.774
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:01:02.292
    STEP: Deploying the webhook pod 08/24/23 12:01:02.308
    STEP: Wait for the deployment to be ready 08/24/23 12:01:02.331
    Aug 24 12:01:02.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:01:04.374
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:01:04.407
    Aug 24 12:01:05.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/24/23 12:01:05.416
    STEP: create a configmap that should be updated by the webhook 08/24/23 12:01:05.45
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:05.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1878" for this suite. 08/24/23 12:01:05.59
    STEP: Destroying namespace "webhook-1878-markers" for this suite. 08/24/23 12:01:05.606
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:05.625
Aug 24 12:01:05.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:05.633
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:05.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:05.717
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/24/23 12:01:05.729
Aug 24 12:01:05.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 create -f -'
Aug 24 12:01:07.849: INFO: stderr: ""
Aug 24 12:01:07.849: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:07.849
Aug 24 12:01:07.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:08.069: INFO: stderr: ""
Aug 24 12:01:08.069: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
Aug 24 12:01:08.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:08.217: INFO: stderr: ""
Aug 24 12:01:08.217: INFO: stdout: ""
Aug 24 12:01:08.217: INFO: update-demo-nautilus-gvjxq is created but not running
Aug 24 12:01:13.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:13.390: INFO: stderr: ""
Aug 24 12:01:13.390: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
Aug 24 12:01:13.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:13.578: INFO: stderr: ""
Aug 24 12:01:13.578: INFO: stdout: ""
Aug 24 12:01:13.578: INFO: update-demo-nautilus-gvjxq is created but not running
Aug 24 12:01:18.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:18.756: INFO: stderr: ""
Aug 24 12:01:18.756: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
Aug 24 12:01:18.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:18.917: INFO: stderr: ""
Aug 24 12:01:18.917: INFO: stdout: ""
Aug 24 12:01:18.917: INFO: update-demo-nautilus-gvjxq is created but not running
Aug 24 12:01:23.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:24.071: INFO: stderr: ""
Aug 24 12:01:24.071: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
Aug 24 12:01:24.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:24.201: INFO: stderr: ""
Aug 24 12:01:24.201: INFO: stdout: "true"
Aug 24 12:01:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:01:24.335: INFO: stderr: ""
Aug 24 12:01:24.335: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:01:24.335: INFO: validating pod update-demo-nautilus-gvjxq
Aug 24 12:01:24.353: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:01:24.354: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:01:24.354: INFO: update-demo-nautilus-gvjxq is verified up and running
Aug 24 12:01:24.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:24.503: INFO: stderr: ""
Aug 24 12:01:24.503: INFO: stdout: "true"
Aug 24 12:01:24.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:01:24.646: INFO: stderr: ""
Aug 24 12:01:24.646: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:01:24.646: INFO: validating pod update-demo-nautilus-krfsl
Aug 24 12:01:24.673: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:01:24.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:01:24.673: INFO: update-demo-nautilus-krfsl is verified up and running
STEP: scaling down the replication controller 08/24/23 12:01:24.673
Aug 24 12:01:24.691: INFO: scanned /root for discovery docs: <nil>
Aug 24 12:01:24.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 24 12:01:24.878: INFO: stderr: ""
Aug 24 12:01:24.878: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:24.878
Aug 24 12:01:24.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:25.072: INFO: stderr: ""
Aug 24 12:01:25.072: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/24/23 12:01:25.072
Aug 24 12:01:30.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:30.212: INFO: stderr: ""
Aug 24 12:01:30.212: INFO: stdout: "update-demo-nautilus-krfsl "
Aug 24 12:01:30.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:30.340: INFO: stderr: ""
Aug 24 12:01:30.340: INFO: stdout: "true"
Aug 24 12:01:30.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:01:30.469: INFO: stderr: ""
Aug 24 12:01:30.469: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:01:30.469: INFO: validating pod update-demo-nautilus-krfsl
Aug 24 12:01:30.483: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:01:30.483: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:01:30.483: INFO: update-demo-nautilus-krfsl is verified up and running
STEP: scaling up the replication controller 08/24/23 12:01:30.483
Aug 24 12:01:30.492: INFO: scanned /root for discovery docs: <nil>
Aug 24 12:01:30.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 24 12:01:31.687: INFO: stderr: ""
Aug 24 12:01:31.687: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:31.687
Aug 24 12:01:31.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:31.829: INFO: stderr: ""
Aug 24 12:01:31.829: INFO: stdout: "update-demo-nautilus-krfsl update-demo-nautilus-v6mpd "
Aug 24 12:01:31.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:31.986: INFO: stderr: ""
Aug 24 12:01:31.986: INFO: stdout: "true"
Aug 24 12:01:31.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:01:32.149: INFO: stderr: ""
Aug 24 12:01:32.149: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:01:32.149: INFO: validating pod update-demo-nautilus-krfsl
Aug 24 12:01:32.158: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:01:32.158: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:01:32.158: INFO: update-demo-nautilus-krfsl is verified up and running
Aug 24 12:01:32.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-v6mpd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:32.307: INFO: stderr: ""
Aug 24 12:01:32.307: INFO: stdout: "true"
Aug 24 12:01:32.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-v6mpd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:01:32.480: INFO: stderr: ""
Aug 24 12:01:32.480: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:01:32.480: INFO: validating pod update-demo-nautilus-v6mpd
Aug 24 12:01:32.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:01:32.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:01:32.503: INFO: update-demo-nautilus-v6mpd is verified up and running
STEP: using delete to clean up resources 08/24/23 12:01:32.504
Aug 24 12:01:32.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 delete --grace-period=0 --force -f -'
Aug 24 12:01:32.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:01:32.631: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 24 12:01:32.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get rc,svc -l name=update-demo --no-headers'
Aug 24 12:01:32.811: INFO: stderr: "No resources found in kubectl-110 namespace.\n"
Aug 24 12:01:32.811: INFO: stdout: ""
Aug 24 12:01:32.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 24 12:01:32.965: INFO: stderr: ""
Aug 24 12:01:32.965: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-110" for this suite. 08/24/23 12:01:32.977
------------------------------
â€¢ [SLOW TEST] [27.365 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:05.625
    Aug 24 12:01:05.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:05.633
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:05.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:05.717
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/24/23 12:01:05.729
    Aug 24 12:01:05.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 create -f -'
    Aug 24 12:01:07.849: INFO: stderr: ""
    Aug 24 12:01:07.849: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:07.849
    Aug 24 12:01:07.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:08.069: INFO: stderr: ""
    Aug 24 12:01:08.069: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
    Aug 24 12:01:08.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:08.217: INFO: stderr: ""
    Aug 24 12:01:08.217: INFO: stdout: ""
    Aug 24 12:01:08.217: INFO: update-demo-nautilus-gvjxq is created but not running
    Aug 24 12:01:13.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:13.390: INFO: stderr: ""
    Aug 24 12:01:13.390: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
    Aug 24 12:01:13.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:13.578: INFO: stderr: ""
    Aug 24 12:01:13.578: INFO: stdout: ""
    Aug 24 12:01:13.578: INFO: update-demo-nautilus-gvjxq is created but not running
    Aug 24 12:01:18.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:18.756: INFO: stderr: ""
    Aug 24 12:01:18.756: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
    Aug 24 12:01:18.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:18.917: INFO: stderr: ""
    Aug 24 12:01:18.917: INFO: stdout: ""
    Aug 24 12:01:18.917: INFO: update-demo-nautilus-gvjxq is created but not running
    Aug 24 12:01:23.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:24.071: INFO: stderr: ""
    Aug 24 12:01:24.071: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
    Aug 24 12:01:24.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:24.201: INFO: stderr: ""
    Aug 24 12:01:24.201: INFO: stdout: "true"
    Aug 24 12:01:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-gvjxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:01:24.335: INFO: stderr: ""
    Aug 24 12:01:24.335: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:01:24.335: INFO: validating pod update-demo-nautilus-gvjxq
    Aug 24 12:01:24.353: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:01:24.354: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:01:24.354: INFO: update-demo-nautilus-gvjxq is verified up and running
    Aug 24 12:01:24.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:24.503: INFO: stderr: ""
    Aug 24 12:01:24.503: INFO: stdout: "true"
    Aug 24 12:01:24.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:01:24.646: INFO: stderr: ""
    Aug 24 12:01:24.646: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:01:24.646: INFO: validating pod update-demo-nautilus-krfsl
    Aug 24 12:01:24.673: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:01:24.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:01:24.673: INFO: update-demo-nautilus-krfsl is verified up and running
    STEP: scaling down the replication controller 08/24/23 12:01:24.673
    Aug 24 12:01:24.691: INFO: scanned /root for discovery docs: <nil>
    Aug 24 12:01:24.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 24 12:01:24.878: INFO: stderr: ""
    Aug 24 12:01:24.878: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:24.878
    Aug 24 12:01:24.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:25.072: INFO: stderr: ""
    Aug 24 12:01:25.072: INFO: stdout: "update-demo-nautilus-gvjxq update-demo-nautilus-krfsl "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/24/23 12:01:25.072
    Aug 24 12:01:30.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:30.212: INFO: stderr: ""
    Aug 24 12:01:30.212: INFO: stdout: "update-demo-nautilus-krfsl "
    Aug 24 12:01:30.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:30.340: INFO: stderr: ""
    Aug 24 12:01:30.340: INFO: stdout: "true"
    Aug 24 12:01:30.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:01:30.469: INFO: stderr: ""
    Aug 24 12:01:30.469: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:01:30.469: INFO: validating pod update-demo-nautilus-krfsl
    Aug 24 12:01:30.483: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:01:30.483: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:01:30.483: INFO: update-demo-nautilus-krfsl is verified up and running
    STEP: scaling up the replication controller 08/24/23 12:01:30.483
    Aug 24 12:01:30.492: INFO: scanned /root for discovery docs: <nil>
    Aug 24 12:01:30.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 24 12:01:31.687: INFO: stderr: ""
    Aug 24 12:01:31.687: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:31.687
    Aug 24 12:01:31.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:31.829: INFO: stderr: ""
    Aug 24 12:01:31.829: INFO: stdout: "update-demo-nautilus-krfsl update-demo-nautilus-v6mpd "
    Aug 24 12:01:31.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:31.986: INFO: stderr: ""
    Aug 24 12:01:31.986: INFO: stdout: "true"
    Aug 24 12:01:31.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-krfsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:01:32.149: INFO: stderr: ""
    Aug 24 12:01:32.149: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:01:32.149: INFO: validating pod update-demo-nautilus-krfsl
    Aug 24 12:01:32.158: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:01:32.158: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:01:32.158: INFO: update-demo-nautilus-krfsl is verified up and running
    Aug 24 12:01:32.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-v6mpd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:32.307: INFO: stderr: ""
    Aug 24 12:01:32.307: INFO: stdout: "true"
    Aug 24 12:01:32.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods update-demo-nautilus-v6mpd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:01:32.480: INFO: stderr: ""
    Aug 24 12:01:32.480: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:01:32.480: INFO: validating pod update-demo-nautilus-v6mpd
    Aug 24 12:01:32.503: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:01:32.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:01:32.503: INFO: update-demo-nautilus-v6mpd is verified up and running
    STEP: using delete to clean up resources 08/24/23 12:01:32.504
    Aug 24 12:01:32.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 delete --grace-period=0 --force -f -'
    Aug 24 12:01:32.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:01:32.631: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 24 12:01:32.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get rc,svc -l name=update-demo --no-headers'
    Aug 24 12:01:32.811: INFO: stderr: "No resources found in kubectl-110 namespace.\n"
    Aug 24 12:01:32.811: INFO: stdout: ""
    Aug 24 12:01:32.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-110 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 24 12:01:32.965: INFO: stderr: ""
    Aug 24 12:01:32.965: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-110" for this suite. 08/24/23 12:01:32.977
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:32.992
Aug 24 12:01:32.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:01:32.996
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:33.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:33.035
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/24/23 12:01:33.042
Aug 24 12:01:33.063: INFO: Waiting up to 5m0s for pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa" in namespace "emptydir-2210" to be "Succeeded or Failed"
Aug 24 12:01:33.075: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.127414ms
Aug 24 12:01:35.091: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.028684277s
Aug 24 12:01:37.081: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018398152s
STEP: Saw pod success 08/24/23 12:01:37.081
Aug 24 12:01:37.081: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa" satisfied condition "Succeeded or Failed"
Aug 24 12:01:37.087: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa container test-container: <nil>
STEP: delete the pod 08/24/23 12:01:37.119
Aug 24 12:01:37.139: INFO: Waiting for pod pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa to disappear
Aug 24 12:01:37.149: INFO: Pod pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:37.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2210" for this suite. 08/24/23 12:01:37.161
------------------------------
â€¢ [4.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:32.992
    Aug 24 12:01:32.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:01:32.996
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:33.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:33.035
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/24/23 12:01:33.042
    Aug 24 12:01:33.063: INFO: Waiting up to 5m0s for pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa" in namespace "emptydir-2210" to be "Succeeded or Failed"
    Aug 24 12:01:33.075: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.127414ms
    Aug 24 12:01:35.091: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.028684277s
    Aug 24 12:01:37.081: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018398152s
    STEP: Saw pod success 08/24/23 12:01:37.081
    Aug 24 12:01:37.081: INFO: Pod "pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa" satisfied condition "Succeeded or Failed"
    Aug 24 12:01:37.087: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa container test-container: <nil>
    STEP: delete the pod 08/24/23 12:01:37.119
    Aug 24 12:01:37.139: INFO: Waiting for pod pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa to disappear
    Aug 24 12:01:37.149: INFO: Pod pod-cfbcdf5b-1ecf-4ed8-8df6-0019e6f3feaa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:37.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2210" for this suite. 08/24/23 12:01:37.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:37.175
Aug 24 12:01:37.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:01:37.177
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:37.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:37.216
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/24/23 12:01:37.223
STEP: submitting the pod to kubernetes 08/24/23 12:01:37.223
STEP: verifying QOS class is set on the pod 08/24/23 12:01:37.237
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:37.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5142" for this suite. 08/24/23 12:01:37.261
------------------------------
â€¢ [0.104 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:37.175
    Aug 24 12:01:37.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:01:37.177
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:37.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:37.216
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/24/23 12:01:37.223
    STEP: submitting the pod to kubernetes 08/24/23 12:01:37.223
    STEP: verifying QOS class is set on the pod 08/24/23 12:01:37.237
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:37.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5142" for this suite. 08/24/23 12:01:37.261
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:37.282
Aug 24 12:01:37.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:01:37.285
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:37.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:37.322
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:01:37.333
Aug 24 12:01:37.348: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4257" to be "running and ready"
Aug 24 12:01:37.354: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.419517ms
Aug 24 12:01:37.355: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:01:39.361: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013652621s
Aug 24 12:01:39.362: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 12:01:39.362: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/24/23 12:01:39.367
Aug 24 12:01:39.376: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4257" to be "running and ready"
Aug 24 12:01:39.385: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34406ms
Aug 24 12:01:39.385: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:01:41.394: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018131496s
Aug 24 12:01:41.394: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 24 12:01:41.394: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/24/23 12:01:41.399
STEP: delete the pod with lifecycle hook 08/24/23 12:01:41.432
Aug 24 12:01:41.446: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 24 12:01:41.453: INFO: Pod pod-with-poststart-http-hook still exists
Aug 24 12:01:43.453: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 24 12:01:43.459: INFO: Pod pod-with-poststart-http-hook still exists
Aug 24 12:01:45.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 24 12:01:45.462: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:45.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4257" for this suite. 08/24/23 12:01:45.474
------------------------------
â€¢ [SLOW TEST] [8.205 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:37.282
    Aug 24 12:01:37.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:01:37.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:37.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:37.322
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:01:37.333
    Aug 24 12:01:37.348: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4257" to be "running and ready"
    Aug 24 12:01:37.354: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.419517ms
    Aug 24 12:01:37.355: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:01:39.361: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013652621s
    Aug 24 12:01:39.362: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 12:01:39.362: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/24/23 12:01:39.367
    Aug 24 12:01:39.376: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4257" to be "running and ready"
    Aug 24 12:01:39.385: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34406ms
    Aug 24 12:01:39.385: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:01:41.394: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018131496s
    Aug 24 12:01:41.394: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 24 12:01:41.394: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/24/23 12:01:41.399
    STEP: delete the pod with lifecycle hook 08/24/23 12:01:41.432
    Aug 24 12:01:41.446: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 24 12:01:41.453: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 24 12:01:43.453: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 24 12:01:43.459: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 24 12:01:45.454: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 24 12:01:45.462: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:45.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4257" for this suite. 08/24/23 12:01:45.474
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:45.488
Aug 24 12:01:45.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:45.491
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:45.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:45.534
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/24/23 12:01:45.539
Aug 24 12:01:45.541: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6656 proxy --unix-socket=/tmp/kubectl-proxy-unix3741190031/test'
STEP: retrieving proxy /api/ output 08/24/23 12:01:45.665
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:45.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6656" for this suite. 08/24/23 12:01:45.678
------------------------------
â€¢ [0.203 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:45.488
    Aug 24 12:01:45.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:45.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:45.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:45.534
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/24/23 12:01:45.539
    Aug 24 12:01:45.541: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-6656 proxy --unix-socket=/tmp/kubectl-proxy-unix3741190031/test'
    STEP: retrieving proxy /api/ output 08/24/23 12:01:45.665
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:45.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6656" for this suite. 08/24/23 12:01:45.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:45.696
Aug 24 12:01:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:01:45.699
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:45.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:45.733
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-8fd696fc-a305-4657-b734-7a06bfc81f62 08/24/23 12:01:45.737
STEP: Creating a pod to test consume configMaps 08/24/23 12:01:45.747
Aug 24 12:01:45.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a" in namespace "configmap-6201" to be "Succeeded or Failed"
Aug 24 12:01:45.773: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.029773ms
Aug 24 12:01:47.780: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014469325s
Aug 24 12:01:49.781: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015868159s
STEP: Saw pod success 08/24/23 12:01:49.782
Aug 24 12:01:49.782: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a" satisfied condition "Succeeded or Failed"
Aug 24 12:01:49.787: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:01:49.801
Aug 24 12:01:49.828: INFO: Waiting for pod pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a to disappear
Aug 24 12:01:49.835: INFO: Pod pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:49.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6201" for this suite. 08/24/23 12:01:49.849
------------------------------
â€¢ [4.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:45.696
    Aug 24 12:01:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:01:45.699
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:45.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:45.733
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-8fd696fc-a305-4657-b734-7a06bfc81f62 08/24/23 12:01:45.737
    STEP: Creating a pod to test consume configMaps 08/24/23 12:01:45.747
    Aug 24 12:01:45.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a" in namespace "configmap-6201" to be "Succeeded or Failed"
    Aug 24 12:01:45.773: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.029773ms
    Aug 24 12:01:47.780: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014469325s
    Aug 24 12:01:49.781: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015868159s
    STEP: Saw pod success 08/24/23 12:01:49.782
    Aug 24 12:01:49.782: INFO: Pod "pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a" satisfied condition "Succeeded or Failed"
    Aug 24 12:01:49.787: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:01:49.801
    Aug 24 12:01:49.828: INFO: Waiting for pod pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a to disappear
    Aug 24 12:01:49.835: INFO: Pod pod-configmaps-325b6199-ad91-4c71-b596-ba6803b78c8a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:49.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6201" for this suite. 08/24/23 12:01:49.849
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:49.869
Aug 24 12:01:49.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:49.871
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:49.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:49.908
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/24/23 12:01:49.916
Aug 24 12:01:49.917: INFO: namespace kubectl-8510
Aug 24 12:01:49.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 create -f -'
Aug 24 12:01:50.466: INFO: stderr: ""
Aug 24 12:01:50.466: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/24/23 12:01:50.466
Aug 24 12:01:51.482: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:01:51.482: INFO: Found 0 / 1
Aug 24 12:01:52.475: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:01:52.475: INFO: Found 1 / 1
Aug 24 12:01:52.475: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 24 12:01:52.480: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:01:52.480: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 24 12:01:52.480: INFO: wait on agnhost-primary startup in kubectl-8510 
Aug 24 12:01:52.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 logs agnhost-primary-8m9gp agnhost-primary'
Aug 24 12:01:52.660: INFO: stderr: ""
Aug 24 12:01:52.660: INFO: stdout: "Paused\n"
STEP: exposing RC 08/24/23 12:01:52.66
Aug 24 12:01:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 24 12:01:52.858: INFO: stderr: ""
Aug 24 12:01:52.858: INFO: stdout: "service/rm2 exposed\n"
Aug 24 12:01:52.873: INFO: Service rm2 in namespace kubectl-8510 found.
STEP: exposing service 08/24/23 12:01:54.889
Aug 24 12:01:54.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 24 12:01:55.138: INFO: stderr: ""
Aug 24 12:01:55.138: INFO: stdout: "service/rm3 exposed\n"
Aug 24 12:01:55.149: INFO: Service rm3 in namespace kubectl-8510 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8510" for this suite. 08/24/23 12:01:57.17
------------------------------
â€¢ [SLOW TEST] [7.316 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:49.869
    Aug 24 12:01:49.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:49.871
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:49.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:49.908
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/24/23 12:01:49.916
    Aug 24 12:01:49.917: INFO: namespace kubectl-8510
    Aug 24 12:01:49.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 create -f -'
    Aug 24 12:01:50.466: INFO: stderr: ""
    Aug 24 12:01:50.466: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/24/23 12:01:50.466
    Aug 24 12:01:51.482: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:01:51.482: INFO: Found 0 / 1
    Aug 24 12:01:52.475: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:01:52.475: INFO: Found 1 / 1
    Aug 24 12:01:52.475: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 24 12:01:52.480: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:01:52.480: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 24 12:01:52.480: INFO: wait on agnhost-primary startup in kubectl-8510 
    Aug 24 12:01:52.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 logs agnhost-primary-8m9gp agnhost-primary'
    Aug 24 12:01:52.660: INFO: stderr: ""
    Aug 24 12:01:52.660: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/24/23 12:01:52.66
    Aug 24 12:01:52.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 24 12:01:52.858: INFO: stderr: ""
    Aug 24 12:01:52.858: INFO: stdout: "service/rm2 exposed\n"
    Aug 24 12:01:52.873: INFO: Service rm2 in namespace kubectl-8510 found.
    STEP: exposing service 08/24/23 12:01:54.889
    Aug 24 12:01:54.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8510 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 24 12:01:55.138: INFO: stderr: ""
    Aug 24 12:01:55.138: INFO: stdout: "service/rm3 exposed\n"
    Aug 24 12:01:55.149: INFO: Service rm3 in namespace kubectl-8510 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:57.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8510" for this suite. 08/24/23 12:01:57.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:57.188
Aug 24 12:01:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:57.19
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:57.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:57.225
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/24/23 12:01:57.23
Aug 24 12:01:57.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 create -f -'
Aug 24 12:01:57.639: INFO: stderr: ""
Aug 24 12:01:57.639: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:57.639
Aug 24 12:01:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:01:57.823: INFO: stderr: ""
Aug 24 12:01:57.823: INFO: stdout: "update-demo-nautilus-wxkl7 update-demo-nautilus-z8qwl "
Aug 24 12:01:57.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-wxkl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:01:57.985: INFO: stderr: ""
Aug 24 12:01:57.985: INFO: stdout: ""
Aug 24 12:01:57.985: INFO: update-demo-nautilus-wxkl7 is created but not running
Aug 24 12:02:02.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 12:02:03.159: INFO: stderr: ""
Aug 24 12:02:03.159: INFO: stdout: "update-demo-nautilus-wxkl7 update-demo-nautilus-z8qwl "
Aug 24 12:02:03.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-wxkl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:02:03.340: INFO: stderr: ""
Aug 24 12:02:03.340: INFO: stdout: "true"
Aug 24 12:02:03.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-wxkl7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:02:03.510: INFO: stderr: ""
Aug 24 12:02:03.510: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:02:03.510: INFO: validating pod update-demo-nautilus-wxkl7
Aug 24 12:02:03.524: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:02:03.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:02:03.524: INFO: update-demo-nautilus-wxkl7 is verified up and running
Aug 24 12:02:03.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-z8qwl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 12:02:03.662: INFO: stderr: ""
Aug 24 12:02:03.662: INFO: stdout: "true"
Aug 24 12:02:03.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-z8qwl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 12:02:03.801: INFO: stderr: ""
Aug 24 12:02:03.801: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 12:02:03.801: INFO: validating pod update-demo-nautilus-z8qwl
Aug 24 12:02:03.815: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 12:02:03.815: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 12:02:03.815: INFO: update-demo-nautilus-z8qwl is verified up and running
STEP: using delete to clean up resources 08/24/23 12:02:03.815
Aug 24 12:02:03.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 delete --grace-period=0 --force -f -'
Aug 24 12:02:04.000: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:02:04.000: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 24 12:02:04.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get rc,svc -l name=update-demo --no-headers'
Aug 24 12:02:04.230: INFO: stderr: "No resources found in kubectl-3825 namespace.\n"
Aug 24 12:02:04.230: INFO: stdout: ""
Aug 24 12:02:04.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 24 12:02:04.419: INFO: stderr: ""
Aug 24 12:02:04.419: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:02:04.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3825" for this suite. 08/24/23 12:02:04.432
------------------------------
â€¢ [SLOW TEST] [7.260 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:57.188
    Aug 24 12:01:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:01:57.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:57.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:57.225
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/24/23 12:01:57.23
    Aug 24 12:01:57.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 create -f -'
    Aug 24 12:01:57.639: INFO: stderr: ""
    Aug 24 12:01:57.639: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 12:01:57.639
    Aug 24 12:01:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:01:57.823: INFO: stderr: ""
    Aug 24 12:01:57.823: INFO: stdout: "update-demo-nautilus-wxkl7 update-demo-nautilus-z8qwl "
    Aug 24 12:01:57.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-wxkl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:01:57.985: INFO: stderr: ""
    Aug 24 12:01:57.985: INFO: stdout: ""
    Aug 24 12:01:57.985: INFO: update-demo-nautilus-wxkl7 is created but not running
    Aug 24 12:02:02.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 12:02:03.159: INFO: stderr: ""
    Aug 24 12:02:03.159: INFO: stdout: "update-demo-nautilus-wxkl7 update-demo-nautilus-z8qwl "
    Aug 24 12:02:03.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-wxkl7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:02:03.340: INFO: stderr: ""
    Aug 24 12:02:03.340: INFO: stdout: "true"
    Aug 24 12:02:03.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-wxkl7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:02:03.510: INFO: stderr: ""
    Aug 24 12:02:03.510: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:02:03.510: INFO: validating pod update-demo-nautilus-wxkl7
    Aug 24 12:02:03.524: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:02:03.524: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:02:03.524: INFO: update-demo-nautilus-wxkl7 is verified up and running
    Aug 24 12:02:03.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-z8qwl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 12:02:03.662: INFO: stderr: ""
    Aug 24 12:02:03.662: INFO: stdout: "true"
    Aug 24 12:02:03.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods update-demo-nautilus-z8qwl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 12:02:03.801: INFO: stderr: ""
    Aug 24 12:02:03.801: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 12:02:03.801: INFO: validating pod update-demo-nautilus-z8qwl
    Aug 24 12:02:03.815: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 12:02:03.815: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 12:02:03.815: INFO: update-demo-nautilus-z8qwl is verified up and running
    STEP: using delete to clean up resources 08/24/23 12:02:03.815
    Aug 24 12:02:03.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 delete --grace-period=0 --force -f -'
    Aug 24 12:02:04.000: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:02:04.000: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 24 12:02:04.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get rc,svc -l name=update-demo --no-headers'
    Aug 24 12:02:04.230: INFO: stderr: "No resources found in kubectl-3825 namespace.\n"
    Aug 24 12:02:04.230: INFO: stdout: ""
    Aug 24 12:02:04.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3825 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 24 12:02:04.419: INFO: stderr: ""
    Aug 24 12:02:04.419: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:02:04.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3825" for this suite. 08/24/23 12:02:04.432
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:02:04.45
Aug 24 12:02:04.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename ingressclass 08/24/23 12:02:04.454
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:04.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:04.491
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/24/23 12:02:04.503
STEP: getting /apis/networking.k8s.io 08/24/23 12:02:04.512
STEP: getting /apis/networking.k8s.iov1 08/24/23 12:02:04.515
STEP: creating 08/24/23 12:02:04.518
STEP: getting 08/24/23 12:02:04.547
STEP: listing 08/24/23 12:02:04.555
STEP: watching 08/24/23 12:02:04.563
Aug 24 12:02:04.563: INFO: starting watch
STEP: patching 08/24/23 12:02:04.565
STEP: updating 08/24/23 12:02:04.575
Aug 24 12:02:04.583: INFO: waiting for watch events with expected annotations
Aug 24 12:02:04.583: INFO: saw patched and updated annotations
STEP: deleting 08/24/23 12:02:04.583
STEP: deleting a collection 08/24/23 12:02:04.602
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:02:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-8680" for this suite. 08/24/23 12:02:04.644
------------------------------
â€¢ [0.208 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:02:04.45
    Aug 24 12:02:04.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename ingressclass 08/24/23 12:02:04.454
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:04.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:04.491
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/24/23 12:02:04.503
    STEP: getting /apis/networking.k8s.io 08/24/23 12:02:04.512
    STEP: getting /apis/networking.k8s.iov1 08/24/23 12:02:04.515
    STEP: creating 08/24/23 12:02:04.518
    STEP: getting 08/24/23 12:02:04.547
    STEP: listing 08/24/23 12:02:04.555
    STEP: watching 08/24/23 12:02:04.563
    Aug 24 12:02:04.563: INFO: starting watch
    STEP: patching 08/24/23 12:02:04.565
    STEP: updating 08/24/23 12:02:04.575
    Aug 24 12:02:04.583: INFO: waiting for watch events with expected annotations
    Aug 24 12:02:04.583: INFO: saw patched and updated annotations
    STEP: deleting 08/24/23 12:02:04.583
    STEP: deleting a collection 08/24/23 12:02:04.602
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:02:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-8680" for this suite. 08/24/23 12:02:04.644
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:02:04.658
Aug 24 12:02:04.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 12:02:04.66
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:04.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:04.698
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7243 08/24/23 12:02:04.707
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 24 12:02:04.769: INFO: Found 0 stateful pods, waiting for 1
Aug 24 12:02:14.779: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/24/23 12:02:14.79
W0824 12:02:14.814464      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 24 12:02:14.828: INFO: Found 1 stateful pods, waiting for 2
Aug 24 12:02:24.838: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:02:24.838: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/24/23 12:02:24.852
STEP: Delete all of the StatefulSets 08/24/23 12:02:24.858
STEP: Verify that StatefulSets have been deleted 08/24/23 12:02:24.878
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:02:24.888: INFO: Deleting all statefulset in ns statefulset-7243
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:02:24.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7243" for this suite. 08/24/23 12:02:24.914
------------------------------
â€¢ [SLOW TEST] [20.270 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:02:04.658
    Aug 24 12:02:04.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:02:04.66
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:04.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:04.698
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7243 08/24/23 12:02:04.707
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 24 12:02:04.769: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 12:02:14.779: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/24/23 12:02:14.79
    W0824 12:02:14.814464      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 24 12:02:14.828: INFO: Found 1 stateful pods, waiting for 2
    Aug 24 12:02:24.838: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:02:24.838: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/24/23 12:02:24.852
    STEP: Delete all of the StatefulSets 08/24/23 12:02:24.858
    STEP: Verify that StatefulSets have been deleted 08/24/23 12:02:24.878
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:02:24.888: INFO: Deleting all statefulset in ns statefulset-7243
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:02:24.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7243" for this suite. 08/24/23 12:02:24.914
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:02:24.929
Aug 24 12:02:24.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:02:24.932
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:25.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:25.05
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/24/23 12:02:25.054
Aug 24 12:02:25.089: INFO: Waiting up to 5m0s for pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f" in namespace "var-expansion-3254" to be "Succeeded or Failed"
Aug 24 12:02:25.103: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.29147ms
Aug 24 12:02:27.112: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022193154s
Aug 24 12:02:29.110: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020125314s
Aug 24 12:02:31.113: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022988239s
STEP: Saw pod success 08/24/23 12:02:31.113
Aug 24 12:02:31.113: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f" satisfied condition "Succeeded or Failed"
Aug 24 12:02:31.119: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:02:31.133
Aug 24 12:02:31.157: INFO: Waiting for pod var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f to disappear
Aug 24 12:02:31.162: INFO: Pod var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:02:31.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3254" for this suite. 08/24/23 12:02:31.172
------------------------------
â€¢ [SLOW TEST] [6.256 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:02:24.929
    Aug 24 12:02:24.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:02:24.932
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:25.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:25.05
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/24/23 12:02:25.054
    Aug 24 12:02:25.089: INFO: Waiting up to 5m0s for pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f" in namespace "var-expansion-3254" to be "Succeeded or Failed"
    Aug 24 12:02:25.103: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.29147ms
    Aug 24 12:02:27.112: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022193154s
    Aug 24 12:02:29.110: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020125314s
    Aug 24 12:02:31.113: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022988239s
    STEP: Saw pod success 08/24/23 12:02:31.113
    Aug 24 12:02:31.113: INFO: Pod "var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f" satisfied condition "Succeeded or Failed"
    Aug 24 12:02:31.119: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:02:31.133
    Aug 24 12:02:31.157: INFO: Waiting for pod var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f to disappear
    Aug 24 12:02:31.162: INFO: Pod var-expansion-e5d0acf8-6b08-4cb6-b7df-21c005351c2f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:02:31.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3254" for this suite. 08/24/23 12:02:31.172
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:02:31.186
Aug 24 12:02:31.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:02:31.188
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:31.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:31.234
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/24/23 12:02:31.24
Aug 24 12:02:31.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3619 create -f -'
Aug 24 12:02:31.748: INFO: stderr: ""
Aug 24 12:02:31.749: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/24/23 12:02:31.749
Aug 24 12:02:31.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3619 diff -f -'
Aug 24 12:02:32.284: INFO: rc: 1
Aug 24 12:02:32.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3619 delete -f -'
Aug 24 12:02:32.536: INFO: stderr: ""
Aug 24 12:02:32.537: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:02:32.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3619" for this suite. 08/24/23 12:02:32.547
------------------------------
â€¢ [1.372 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:02:31.186
    Aug 24 12:02:31.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:02:31.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:31.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:31.234
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/24/23 12:02:31.24
    Aug 24 12:02:31.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3619 create -f -'
    Aug 24 12:02:31.748: INFO: stderr: ""
    Aug 24 12:02:31.749: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/24/23 12:02:31.749
    Aug 24 12:02:31.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3619 diff -f -'
    Aug 24 12:02:32.284: INFO: rc: 1
    Aug 24 12:02:32.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-3619 delete -f -'
    Aug 24 12:02:32.536: INFO: stderr: ""
    Aug 24 12:02:32.537: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:02:32.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3619" for this suite. 08/24/23 12:02:32.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:02:32.558
Aug 24 12:02:32.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:02:32.561
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:32.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:32.601
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/24/23 12:02:32.609
STEP: Creating a ResourceQuota 08/24/23 12:02:37.619
STEP: Ensuring resource quota status is calculated 08/24/23 12:02:37.629
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:02:39.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1450" for this suite. 08/24/23 12:02:39.647
------------------------------
â€¢ [SLOW TEST] [7.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:02:32.558
    Aug 24 12:02:32.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:02:32.561
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:32.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:32.601
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/24/23 12:02:32.609
    STEP: Creating a ResourceQuota 08/24/23 12:02:37.619
    STEP: Ensuring resource quota status is calculated 08/24/23 12:02:37.629
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:02:39.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1450" for this suite. 08/24/23 12:02:39.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:02:39.665
Aug 24 12:02:39.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 12:02:39.667
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:39.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:39.702
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/24/23 12:02:39.708
STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:02:40.086
Aug 24 12:02:40.110: INFO: Pod name wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216: Found 0 pods out of 5
Aug 24 12:02:45.128: INFO: Pod name wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/24/23 12:02:45.128
Aug 24 12:02:45.128: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:02:45.136: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.359166ms
Aug 24 12:02:47.152: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023857548s
Aug 24 12:02:49.151: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022990404s
Aug 24 12:02:51.149: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020813805s
Aug 24 12:02:53.146: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017778866s
Aug 24 12:02:55.180: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051730301s
Aug 24 12:02:57.149: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Running", Reason="", readiness=true. Elapsed: 12.021524891s
Aug 24 12:02:57.150: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k" satisfied condition "running"
Aug 24 12:02:57.150: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-g784n" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:02:57.160: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-g784n": Phase="Running", Reason="", readiness=true. Elapsed: 10.026625ms
Aug 24 12:02:57.160: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-g784n" satisfied condition "running"
Aug 24 12:02:57.160: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-hw98m" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:02:57.171: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-hw98m": Phase="Running", Reason="", readiness=true. Elapsed: 10.581572ms
Aug 24 12:02:57.171: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-hw98m" satisfied condition "running"
Aug 24 12:02:57.171: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-k5cmn" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:02:57.180: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-k5cmn": Phase="Running", Reason="", readiness=true. Elapsed: 8.173669ms
Aug 24 12:02:57.180: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-k5cmn" satisfied condition "running"
Aug 24 12:02:57.180: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-kc8hz" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:02:57.187: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-kc8hz": Phase="Running", Reason="", readiness=true. Elapsed: 6.772071ms
Aug 24 12:02:57.187: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-kc8hz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216 in namespace emptydir-wrapper-3153, will wait for the garbage collector to delete the pods 08/24/23 12:02:57.187
Aug 24 12:02:57.266: INFO: Deleting ReplicationController wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216 took: 17.273278ms
Aug 24 12:02:57.367: INFO: Terminating ReplicationController wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216 pods took: 100.484286ms
STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:03:00.181
Aug 24 12:03:00.208: INFO: Pod name wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827: Found 0 pods out of 5
Aug 24 12:03:05.232: INFO: Pod name wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/24/23 12:03:05.232
Aug 24 12:03:05.233: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:05.245: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.202784ms
Aug 24 12:03:07.253: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020635752s
Aug 24 12:03:09.257: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024338191s
Aug 24 12:03:11.256: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023106052s
Aug 24 12:03:13.474: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.241409927s
Aug 24 12:03:15.256: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Running", Reason="", readiness=true. Elapsed: 10.023400714s
Aug 24 12:03:15.256: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq" satisfied condition "running"
Aug 24 12:03:15.256: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-n4fpx" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:15.264: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-n4fpx": Phase="Running", Reason="", readiness=true. Elapsed: 7.508693ms
Aug 24 12:03:15.264: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-n4fpx" satisfied condition "running"
Aug 24 12:03:15.264: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:15.271: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.624897ms
Aug 24 12:03:17.283: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.01886981s
Aug 24 12:03:17.283: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5" satisfied condition "running"
Aug 24 12:03:17.284: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-w2z9k" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:17.295: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-w2z9k": Phase="Running", Reason="", readiness=true. Elapsed: 11.081999ms
Aug 24 12:03:17.295: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-w2z9k" satisfied condition "running"
Aug 24 12:03:17.295: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-wskkg" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:17.304: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-wskkg": Phase="Running", Reason="", readiness=true. Elapsed: 8.656357ms
Aug 24 12:03:17.304: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-wskkg" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827 in namespace emptydir-wrapper-3153, will wait for the garbage collector to delete the pods 08/24/23 12:03:17.305
Aug 24 12:03:17.388: INFO: Deleting ReplicationController wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827 took: 20.324822ms
Aug 24 12:03:17.588: INFO: Terminating ReplicationController wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827 pods took: 200.369357ms
STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:03:20.301
Aug 24 12:03:20.338: INFO: Pod name wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359: Found 0 pods out of 5
Aug 24 12:03:25.356: INFO: Pod name wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/24/23 12:03:25.356
Aug 24 12:03:25.357: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:25.366: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 9.292609ms
Aug 24 12:03:27.387: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030901421s
Aug 24 12:03:29.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018589702s
Aug 24 12:03:31.381: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02487661s
Aug 24 12:03:33.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017944368s
Aug 24 12:03:35.395: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038159108s
Aug 24 12:03:37.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Running", Reason="", readiness=true. Elapsed: 12.018574073s
Aug 24 12:03:37.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks" satisfied condition "running"
Aug 24 12:03:37.375: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-blr4x" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:37.383: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-blr4x": Phase="Running", Reason="", readiness=true. Elapsed: 7.809901ms
Aug 24 12:03:37.383: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-blr4x" satisfied condition "running"
Aug 24 12:03:37.383: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-h7f2t" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:37.392: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-h7f2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.678785ms
Aug 24 12:03:37.392: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-h7f2t" satisfied condition "running"
Aug 24 12:03:37.392: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-p7kwt" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:37.400: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-p7kwt": Phase="Running", Reason="", readiness=true. Elapsed: 7.719113ms
Aug 24 12:03:37.400: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-p7kwt" satisfied condition "running"
Aug 24 12:03:37.400: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-tbfm4" in namespace "emptydir-wrapper-3153" to be "running"
Aug 24 12:03:37.408: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-tbfm4": Phase="Running", Reason="", readiness=true. Elapsed: 7.546671ms
Aug 24 12:03:37.408: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-tbfm4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359 in namespace emptydir-wrapper-3153, will wait for the garbage collector to delete the pods 08/24/23 12:03:37.408
Aug 24 12:03:37.479: INFO: Deleting ReplicationController wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359 took: 12.279694ms
Aug 24 12:03:37.679: INFO: Terminating ReplicationController wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359 pods took: 200.810427ms
STEP: Cleaning up the configMaps 08/24/23 12:03:40.98
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:41.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3153" for this suite. 08/24/23 12:03:41.551
------------------------------
â€¢ [SLOW TEST] [61.901 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:02:39.665
    Aug 24 12:02:39.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 12:02:39.667
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:02:39.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:02:39.702
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/24/23 12:02:39.708
    STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:02:40.086
    Aug 24 12:02:40.110: INFO: Pod name wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216: Found 0 pods out of 5
    Aug 24 12:02:45.128: INFO: Pod name wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/24/23 12:02:45.128
    Aug 24 12:02:45.128: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:02:45.136: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.359166ms
    Aug 24 12:02:47.152: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023857548s
    Aug 24 12:02:49.151: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022990404s
    Aug 24 12:02:51.149: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020813805s
    Aug 24 12:02:53.146: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017778866s
    Aug 24 12:02:55.180: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051730301s
    Aug 24 12:02:57.149: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k": Phase="Running", Reason="", readiness=true. Elapsed: 12.021524891s
    Aug 24 12:02:57.150: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-8dt4k" satisfied condition "running"
    Aug 24 12:02:57.150: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-g784n" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:02:57.160: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-g784n": Phase="Running", Reason="", readiness=true. Elapsed: 10.026625ms
    Aug 24 12:02:57.160: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-g784n" satisfied condition "running"
    Aug 24 12:02:57.160: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-hw98m" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:02:57.171: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-hw98m": Phase="Running", Reason="", readiness=true. Elapsed: 10.581572ms
    Aug 24 12:02:57.171: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-hw98m" satisfied condition "running"
    Aug 24 12:02:57.171: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-k5cmn" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:02:57.180: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-k5cmn": Phase="Running", Reason="", readiness=true. Elapsed: 8.173669ms
    Aug 24 12:02:57.180: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-k5cmn" satisfied condition "running"
    Aug 24 12:02:57.180: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-kc8hz" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:02:57.187: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-kc8hz": Phase="Running", Reason="", readiness=true. Elapsed: 6.772071ms
    Aug 24 12:02:57.187: INFO: Pod "wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216-kc8hz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216 in namespace emptydir-wrapper-3153, will wait for the garbage collector to delete the pods 08/24/23 12:02:57.187
    Aug 24 12:02:57.266: INFO: Deleting ReplicationController wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216 took: 17.273278ms
    Aug 24 12:02:57.367: INFO: Terminating ReplicationController wrapped-volume-race-885a3d96-9376-4fc3-b626-5bc2293a5216 pods took: 100.484286ms
    STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:03:00.181
    Aug 24 12:03:00.208: INFO: Pod name wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827: Found 0 pods out of 5
    Aug 24 12:03:05.232: INFO: Pod name wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/24/23 12:03:05.232
    Aug 24 12:03:05.233: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:05.245: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.202784ms
    Aug 24 12:03:07.253: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020635752s
    Aug 24 12:03:09.257: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024338191s
    Aug 24 12:03:11.256: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023106052s
    Aug 24 12:03:13.474: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.241409927s
    Aug 24 12:03:15.256: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq": Phase="Running", Reason="", readiness=true. Elapsed: 10.023400714s
    Aug 24 12:03:15.256: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-cbwqq" satisfied condition "running"
    Aug 24 12:03:15.256: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-n4fpx" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:15.264: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-n4fpx": Phase="Running", Reason="", readiness=true. Elapsed: 7.508693ms
    Aug 24 12:03:15.264: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-n4fpx" satisfied condition "running"
    Aug 24 12:03:15.264: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:15.271: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.624897ms
    Aug 24 12:03:17.283: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.01886981s
    Aug 24 12:03:17.283: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-njbh5" satisfied condition "running"
    Aug 24 12:03:17.284: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-w2z9k" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:17.295: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-w2z9k": Phase="Running", Reason="", readiness=true. Elapsed: 11.081999ms
    Aug 24 12:03:17.295: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-w2z9k" satisfied condition "running"
    Aug 24 12:03:17.295: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-wskkg" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:17.304: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-wskkg": Phase="Running", Reason="", readiness=true. Elapsed: 8.656357ms
    Aug 24 12:03:17.304: INFO: Pod "wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827-wskkg" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827 in namespace emptydir-wrapper-3153, will wait for the garbage collector to delete the pods 08/24/23 12:03:17.305
    Aug 24 12:03:17.388: INFO: Deleting ReplicationController wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827 took: 20.324822ms
    Aug 24 12:03:17.588: INFO: Terminating ReplicationController wrapped-volume-race-e3b461cc-1f2a-43bb-a525-aa014985d827 pods took: 200.369357ms
    STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:03:20.301
    Aug 24 12:03:20.338: INFO: Pod name wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359: Found 0 pods out of 5
    Aug 24 12:03:25.356: INFO: Pod name wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/24/23 12:03:25.356
    Aug 24 12:03:25.357: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:25.366: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 9.292609ms
    Aug 24 12:03:27.387: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030901421s
    Aug 24 12:03:29.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018589702s
    Aug 24 12:03:31.381: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02487661s
    Aug 24 12:03:33.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017944368s
    Aug 24 12:03:35.395: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038159108s
    Aug 24 12:03:37.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks": Phase="Running", Reason="", readiness=true. Elapsed: 12.018574073s
    Aug 24 12:03:37.375: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-9g7ks" satisfied condition "running"
    Aug 24 12:03:37.375: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-blr4x" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:37.383: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-blr4x": Phase="Running", Reason="", readiness=true. Elapsed: 7.809901ms
    Aug 24 12:03:37.383: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-blr4x" satisfied condition "running"
    Aug 24 12:03:37.383: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-h7f2t" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:37.392: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-h7f2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.678785ms
    Aug 24 12:03:37.392: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-h7f2t" satisfied condition "running"
    Aug 24 12:03:37.392: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-p7kwt" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:37.400: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-p7kwt": Phase="Running", Reason="", readiness=true. Elapsed: 7.719113ms
    Aug 24 12:03:37.400: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-p7kwt" satisfied condition "running"
    Aug 24 12:03:37.400: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-tbfm4" in namespace "emptydir-wrapper-3153" to be "running"
    Aug 24 12:03:37.408: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-tbfm4": Phase="Running", Reason="", readiness=true. Elapsed: 7.546671ms
    Aug 24 12:03:37.408: INFO: Pod "wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359-tbfm4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359 in namespace emptydir-wrapper-3153, will wait for the garbage collector to delete the pods 08/24/23 12:03:37.408
    Aug 24 12:03:37.479: INFO: Deleting ReplicationController wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359 took: 12.279694ms
    Aug 24 12:03:37.679: INFO: Terminating ReplicationController wrapped-volume-race-f2a480b8-649c-4693-b9b1-79ed3ea8b359 pods took: 200.810427ms
    STEP: Cleaning up the configMaps 08/24/23 12:03:40.98
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:41.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3153" for this suite. 08/24/23 12:03:41.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:41.587
Aug 24 12:03:41.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:03:41.59
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:41.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:41.629
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:03:41.636
Aug 24 12:03:41.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673" in namespace "downward-api-7237" to be "Succeeded or Failed"
Aug 24 12:03:41.666: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673": Phase="Pending", Reason="", readiness=false. Elapsed: 7.223134ms
Aug 24 12:03:43.675: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01590422s
Aug 24 12:03:45.674: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01563273s
STEP: Saw pod success 08/24/23 12:03:45.676
Aug 24 12:03:45.676: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673" satisfied condition "Succeeded or Failed"
Aug 24 12:03:45.680: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673 container client-container: <nil>
STEP: delete the pod 08/24/23 12:03:45.691
Aug 24 12:03:45.712: INFO: Waiting for pod downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673 to disappear
Aug 24 12:03:45.716: INFO: Pod downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:45.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7237" for this suite. 08/24/23 12:03:45.725
------------------------------
â€¢ [4.149 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:41.587
    Aug 24 12:03:41.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:03:41.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:41.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:41.629
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:03:41.636
    Aug 24 12:03:41.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673" in namespace "downward-api-7237" to be "Succeeded or Failed"
    Aug 24 12:03:41.666: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673": Phase="Pending", Reason="", readiness=false. Elapsed: 7.223134ms
    Aug 24 12:03:43.675: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01590422s
    Aug 24 12:03:45.674: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01563273s
    STEP: Saw pod success 08/24/23 12:03:45.676
    Aug 24 12:03:45.676: INFO: Pod "downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673" satisfied condition "Succeeded or Failed"
    Aug 24 12:03:45.680: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:03:45.691
    Aug 24 12:03:45.712: INFO: Waiting for pod downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673 to disappear
    Aug 24 12:03:45.716: INFO: Pod downwardapi-volume-e90807c3-2b4f-4f8d-9145-e0d231350673 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:45.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7237" for this suite. 08/24/23 12:03:45.725
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:45.74
Aug 24 12:03:45.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:03:45.744
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:45.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:45.778
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/24/23 12:03:45.783
Aug 24 12:03:45.799: INFO: Waiting up to 5m0s for pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c" in namespace "svcaccounts-3287" to be "Succeeded or Failed"
Aug 24 12:03:45.811: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.134521ms
Aug 24 12:03:47.828: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Running", Reason="", readiness=false. Elapsed: 2.027967742s
Aug 24 12:03:49.822: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Running", Reason="", readiness=false. Elapsed: 4.021868426s
Aug 24 12:03:51.821: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020965843s
STEP: Saw pod success 08/24/23 12:03:51.821
Aug 24 12:03:51.822: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c" satisfied condition "Succeeded or Failed"
Aug 24 12:03:51.828: INFO: Trying to get logs from node pe9deep4seen-3 pod test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:03:51.84
Aug 24 12:03:51.858: INFO: Waiting for pod test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c to disappear
Aug 24 12:03:51.864: INFO: Pod test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:51.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3287" for this suite. 08/24/23 12:03:51.873
------------------------------
â€¢ [SLOW TEST] [6.145 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:45.74
    Aug 24 12:03:45.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:03:45.744
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:45.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:45.778
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/24/23 12:03:45.783
    Aug 24 12:03:45.799: INFO: Waiting up to 5m0s for pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c" in namespace "svcaccounts-3287" to be "Succeeded or Failed"
    Aug 24 12:03:45.811: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.134521ms
    Aug 24 12:03:47.828: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Running", Reason="", readiness=false. Elapsed: 2.027967742s
    Aug 24 12:03:49.822: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Running", Reason="", readiness=false. Elapsed: 4.021868426s
    Aug 24 12:03:51.821: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020965843s
    STEP: Saw pod success 08/24/23 12:03:51.821
    Aug 24 12:03:51.822: INFO: Pod "test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c" satisfied condition "Succeeded or Failed"
    Aug 24 12:03:51.828: INFO: Trying to get logs from node pe9deep4seen-3 pod test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:03:51.84
    Aug 24 12:03:51.858: INFO: Waiting for pod test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c to disappear
    Aug 24 12:03:51.864: INFO: Pod test-pod-ac12e53d-62c9-4ee9-a368-d61c79e1944c no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:51.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3287" for this suite. 08/24/23 12:03:51.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:51.891
Aug 24 12:03:51.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename proxy 08/24/23 12:03:51.893
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:51.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:51.926
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 24 12:03:51.930: INFO: Creating pod...
Aug 24 12:03:51.944: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6638" to be "running"
Aug 24 12:03:51.948: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.747703ms
Aug 24 12:03:53.955: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.01166265s
Aug 24 12:03:53.956: INFO: Pod "agnhost" satisfied condition "running"
Aug 24 12:03:53.956: INFO: Creating service...
Aug 24 12:03:53.972: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=DELETE
Aug 24 12:03:53.989: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 12:03:53.990: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=OPTIONS
Aug 24 12:03:54.001: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 12:03:54.001: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=PATCH
Aug 24 12:03:54.011: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 12:03:54.011: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=POST
Aug 24 12:03:54.019: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 12:03:54.019: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=PUT
Aug 24 12:03:54.026: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 24 12:03:54.027: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 24 12:03:54.038: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 12:03:54.038: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 24 12:03:54.052: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 12:03:54.052: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 24 12:03:54.062: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 12:03:54.062: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=POST
Aug 24 12:03:54.073: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 12:03:54.073: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=PUT
Aug 24 12:03:54.105: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 24 12:03:54.105: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=GET
Aug 24 12:03:54.115: INFO: http.Client request:GET StatusCode:301
Aug 24 12:03:54.115: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=GET
Aug 24 12:03:54.126: INFO: http.Client request:GET StatusCode:301
Aug 24 12:03:54.127: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=HEAD
Aug 24 12:03:54.132: INFO: http.Client request:HEAD StatusCode:301
Aug 24 12:03:54.132: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 24 12:03:54.140: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:54.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6638" for this suite. 08/24/23 12:03:54.148
------------------------------
â€¢ [2.272 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:51.891
    Aug 24 12:03:51.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename proxy 08/24/23 12:03:51.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:51.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:51.926
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 24 12:03:51.930: INFO: Creating pod...
    Aug 24 12:03:51.944: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6638" to be "running"
    Aug 24 12:03:51.948: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.747703ms
    Aug 24 12:03:53.955: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.01166265s
    Aug 24 12:03:53.956: INFO: Pod "agnhost" satisfied condition "running"
    Aug 24 12:03:53.956: INFO: Creating service...
    Aug 24 12:03:53.972: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=DELETE
    Aug 24 12:03:53.989: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 12:03:53.990: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=OPTIONS
    Aug 24 12:03:54.001: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 12:03:54.001: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=PATCH
    Aug 24 12:03:54.011: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 12:03:54.011: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=POST
    Aug 24 12:03:54.019: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 12:03:54.019: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=PUT
    Aug 24 12:03:54.026: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 24 12:03:54.027: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 24 12:03:54.038: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 12:03:54.038: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 24 12:03:54.052: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 12:03:54.052: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 24 12:03:54.062: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 12:03:54.062: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=POST
    Aug 24 12:03:54.073: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 12:03:54.073: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 24 12:03:54.105: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 24 12:03:54.105: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=GET
    Aug 24 12:03:54.115: INFO: http.Client request:GET StatusCode:301
    Aug 24 12:03:54.115: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=GET
    Aug 24 12:03:54.126: INFO: http.Client request:GET StatusCode:301
    Aug 24 12:03:54.127: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/pods/agnhost/proxy?method=HEAD
    Aug 24 12:03:54.132: INFO: http.Client request:HEAD StatusCode:301
    Aug 24 12:03:54.132: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6638/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 24 12:03:54.140: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:54.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6638" for this suite. 08/24/23 12:03:54.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:54.174
Aug 24 12:03:54.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename endpointslice 08/24/23 12:03:54.176
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:54.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:54.23
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:56.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9907" for this suite. 08/24/23 12:03:56.332
------------------------------
â€¢ [2.169 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:54.174
    Aug 24 12:03:54.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename endpointslice 08/24/23 12:03:54.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:54.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:54.23
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:56.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9907" for this suite. 08/24/23 12:03:56.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:56.346
Aug 24 12:03:56.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename namespaces 08/24/23 12:03:56.348
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:56.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:56.383
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5952" 08/24/23 12:03:56.388
Aug 24 12:03:56.403: INFO: Namespace "namespaces-5952" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e37f2036-3a54-4653-ada1-c01489d8d1f1", "kubernetes.io/metadata.name":"namespaces-5952", "namespaces-5952":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:56.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5952" for this suite. 08/24/23 12:03:56.412
------------------------------
â€¢ [0.077 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:56.346
    Aug 24 12:03:56.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:03:56.348
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:56.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:56.383
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5952" 08/24/23 12:03:56.388
    Aug 24 12:03:56.403: INFO: Namespace "namespaces-5952" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e37f2036-3a54-4653-ada1-c01489d8d1f1", "kubernetes.io/metadata.name":"namespaces-5952", "namespaces-5952":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:56.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5952" for this suite. 08/24/23 12:03:56.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:56.428
Aug 24 12:03:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename security-context-test 08/24/23 12:03:56.43
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:56.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:56.466
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 24 12:03:56.487: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112" in namespace "security-context-test-849" to be "Succeeded or Failed"
Aug 24 12:03:56.493: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890233ms
Aug 24 12:03:58.500: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012621821s
Aug 24 12:04:00.501: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013446268s
Aug 24 12:04:00.501: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112" satisfied condition "Succeeded or Failed"
Aug 24 12:04:00.512: INFO: Got logs for pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 12:04:00.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-849" for this suite. 08/24/23 12:04:00.522
------------------------------
â€¢ [4.104 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:56.428
    Aug 24 12:03:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename security-context-test 08/24/23 12:03:56.43
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:56.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:56.466
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 24 12:03:56.487: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112" in namespace "security-context-test-849" to be "Succeeded or Failed"
    Aug 24 12:03:56.493: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890233ms
    Aug 24 12:03:58.500: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012621821s
    Aug 24 12:04:00.501: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013446268s
    Aug 24 12:04:00.501: INFO: Pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112" satisfied condition "Succeeded or Failed"
    Aug 24 12:04:00.512: INFO: Got logs for pod "busybox-privileged-false-7b10d0cf-7d8c-4fd9-9390-a19de1db3112": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:04:00.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-849" for this suite. 08/24/23 12:04:00.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:04:00.536
Aug 24 12:04:00.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:04:00.538
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:00.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:00.574
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Aug 24 12:04:00.625: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/24/23 12:04:00.634
Aug 24 12:04:00.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:00.640: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/24/23 12:04:00.64
Aug 24 12:04:00.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:00.681: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:01.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:01.690: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:02.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:02.692: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:03.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 12:04:03.690: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/24/23 12:04:03.697
Aug 24 12:04:03.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 12:04:03.731: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 24 12:04:04.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:04.740: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/24/23 12:04:04.74
Aug 24 12:04:04.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:04.771: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:05.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:05.779: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:06.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:06.783: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:07.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:07.780: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:04:08.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 12:04:08.780: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:04:08.795
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2169, will wait for the garbage collector to delete the pods 08/24/23 12:04:08.796
Aug 24 12:04:08.866: INFO: Deleting DaemonSet.extensions daemon-set took: 12.602697ms
Aug 24 12:04:08.967: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.099951ms
Aug 24 12:04:11.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:04:11.481: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:04:11.488: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10987"},"items":null}

Aug 24 12:04:11.494: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10987"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:04:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2169" for this suite. 08/24/23 12:04:11.581
------------------------------
â€¢ [SLOW TEST] [11.055 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:04:00.536
    Aug 24 12:04:00.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:04:00.538
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:00.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:00.574
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Aug 24 12:04:00.625: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/24/23 12:04:00.634
    Aug 24 12:04:00.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:00.640: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/24/23 12:04:00.64
    Aug 24 12:04:00.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:00.681: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:01.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:01.690: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:02.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:02.692: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:03.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 12:04:03.690: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/24/23 12:04:03.697
    Aug 24 12:04:03.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 12:04:03.731: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 24 12:04:04.740: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:04.740: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/24/23 12:04:04.74
    Aug 24 12:04:04.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:04.771: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:05.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:05.779: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:06.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:06.783: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:07.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:07.780: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:04:08.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 12:04:08.780: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:04:08.795
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2169, will wait for the garbage collector to delete the pods 08/24/23 12:04:08.796
    Aug 24 12:04:08.866: INFO: Deleting DaemonSet.extensions daemon-set took: 12.602697ms
    Aug 24 12:04:08.967: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.099951ms
    Aug 24 12:04:11.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:04:11.481: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:04:11.488: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10987"},"items":null}

    Aug 24 12:04:11.494: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10987"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:04:11.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2169" for this suite. 08/24/23 12:04:11.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:04:11.597
Aug 24 12:04:11.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pod-network-test 08/24/23 12:04:11.599
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:11.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:11.646
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-818 08/24/23 12:04:11.652
STEP: creating a selector 08/24/23 12:04:11.653
STEP: Creating the service pods in kubernetes 08/24/23 12:04:11.653
Aug 24 12:04:11.653: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 12:04:11.713: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-818" to be "running and ready"
Aug 24 12:04:11.720: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992813ms
Aug 24 12:04:11.721: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:04:13.729: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016074344s
Aug 24 12:04:13.729: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:04:15.729: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016243222s
Aug 24 12:04:15.730: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:04:17.730: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016556208s
Aug 24 12:04:17.730: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:04:19.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018140404s
Aug 24 12:04:19.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:04:21.728: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015181878s
Aug 24 12:04:21.729: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:04:23.730: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.017018802s
Aug 24 12:04:23.730: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 12:04:23.730: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 12:04:23.735: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-818" to be "running and ready"
Aug 24 12:04:23.740: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.222563ms
Aug 24 12:04:23.740: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 12:04:23.741: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 12:04:23.746: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-818" to be "running and ready"
Aug 24 12:04:23.752: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.130213ms
Aug 24 12:04:23.752: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 12:04:23.752: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 12:04:23.758
Aug 24 12:04:23.780: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-818" to be "running"
Aug 24 12:04:23.786: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481253ms
Aug 24 12:04:25.795: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014726039s
Aug 24 12:04:25.796: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 12:04:25.803: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-818" to be "running"
Aug 24 12:04:25.811: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.557919ms
Aug 24 12:04:25.811: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 24 12:04:25.819: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 12:04:25.819: INFO: Going to poll 10.233.64.193 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 24 12:04:25.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.193:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:04:25.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:04:25.830: INFO: ExecWithOptions: Clientset creation
Aug 24 12:04:25.830: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.193%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 12:04:26.012: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 24 12:04:26.012: INFO: Going to poll 10.233.65.33 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 24 12:04:26.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:04:26.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:04:26.020: INFO: ExecWithOptions: Clientset creation
Aug 24 12:04:26.021: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 12:04:26.178: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 24 12:04:26.178: INFO: Going to poll 10.233.66.52 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 24 12:04:26.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:04:26.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:04:26.188: INFO: ExecWithOptions: Clientset creation
Aug 24 12:04:26.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 12:04:26.304: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 12:04:26.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-818" for this suite. 08/24/23 12:04:26.317
------------------------------
â€¢ [SLOW TEST] [14.738 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:04:11.597
    Aug 24 12:04:11.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 12:04:11.599
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:11.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:11.646
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-818 08/24/23 12:04:11.652
    STEP: creating a selector 08/24/23 12:04:11.653
    STEP: Creating the service pods in kubernetes 08/24/23 12:04:11.653
    Aug 24 12:04:11.653: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 12:04:11.713: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-818" to be "running and ready"
    Aug 24 12:04:11.720: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992813ms
    Aug 24 12:04:11.721: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:04:13.729: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016074344s
    Aug 24 12:04:13.729: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:04:15.729: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016243222s
    Aug 24 12:04:15.730: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:04:17.730: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016556208s
    Aug 24 12:04:17.730: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:04:19.731: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018140404s
    Aug 24 12:04:19.731: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:04:21.728: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015181878s
    Aug 24 12:04:21.729: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:04:23.730: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.017018802s
    Aug 24 12:04:23.730: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 12:04:23.730: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 12:04:23.735: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-818" to be "running and ready"
    Aug 24 12:04:23.740: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.222563ms
    Aug 24 12:04:23.740: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 12:04:23.741: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 12:04:23.746: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-818" to be "running and ready"
    Aug 24 12:04:23.752: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.130213ms
    Aug 24 12:04:23.752: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 12:04:23.752: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 12:04:23.758
    Aug 24 12:04:23.780: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-818" to be "running"
    Aug 24 12:04:23.786: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481253ms
    Aug 24 12:04:25.795: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014726039s
    Aug 24 12:04:25.796: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 12:04:25.803: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-818" to be "running"
    Aug 24 12:04:25.811: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.557919ms
    Aug 24 12:04:25.811: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 24 12:04:25.819: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 12:04:25.819: INFO: Going to poll 10.233.64.193 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 12:04:25.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.193:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:04:25.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:04:25.830: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:04:25.830: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.193%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 12:04:26.012: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 24 12:04:26.012: INFO: Going to poll 10.233.65.33 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 12:04:26.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.33:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:04:26.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:04:26.020: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:04:26.021: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.33%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 12:04:26.178: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 24 12:04:26.178: INFO: Going to poll 10.233.66.52 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 12:04:26.186: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.52:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-818 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:04:26.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:04:26.188: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:04:26.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-818/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.52%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 12:04:26.304: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:04:26.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-818" for this suite. 08/24/23 12:04:26.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:04:26.339
Aug 24 12:04:26.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename limitrange 08/24/23 12:04:26.342
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:26.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:26.387
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/24/23 12:04:26.395
STEP: Setting up watch 08/24/23 12:04:26.396
STEP: Submitting a LimitRange 08/24/23 12:04:26.504
STEP: Verifying LimitRange creation was observed 08/24/23 12:04:26.517
STEP: Fetching the LimitRange to ensure it has proper values 08/24/23 12:04:26.518
Aug 24 12:04:26.527: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 24 12:04:26.527: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/24/23 12:04:26.527
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/24/23 12:04:26.537
Aug 24 12:04:26.543: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 24 12:04:26.543: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/24/23 12:04:26.544
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/24/23 12:04:26.555
Aug 24 12:04:26.566: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 24 12:04:26.566: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/24/23 12:04:26.567
STEP: Failing to create a Pod with more than max resources 08/24/23 12:04:26.575
STEP: Updating a LimitRange 08/24/23 12:04:26.582
STEP: Verifying LimitRange updating is effective 08/24/23 12:04:26.6
STEP: Creating a Pod with less than former min resources 08/24/23 12:04:28.611
STEP: Failing to create a Pod with more than max resources 08/24/23 12:04:28.627
STEP: Deleting a LimitRange 08/24/23 12:04:28.632
STEP: Verifying the LimitRange was deleted 08/24/23 12:04:28.644
Aug 24 12:04:33.652: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/24/23 12:04:33.652
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 24 12:04:33.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-284" for this suite. 08/24/23 12:04:33.684
------------------------------
â€¢ [SLOW TEST] [7.357 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:04:26.339
    Aug 24 12:04:26.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename limitrange 08/24/23 12:04:26.342
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:26.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:26.387
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/24/23 12:04:26.395
    STEP: Setting up watch 08/24/23 12:04:26.396
    STEP: Submitting a LimitRange 08/24/23 12:04:26.504
    STEP: Verifying LimitRange creation was observed 08/24/23 12:04:26.517
    STEP: Fetching the LimitRange to ensure it has proper values 08/24/23 12:04:26.518
    Aug 24 12:04:26.527: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 24 12:04:26.527: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/24/23 12:04:26.527
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/24/23 12:04:26.537
    Aug 24 12:04:26.543: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 24 12:04:26.543: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/24/23 12:04:26.544
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/24/23 12:04:26.555
    Aug 24 12:04:26.566: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 24 12:04:26.566: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/24/23 12:04:26.567
    STEP: Failing to create a Pod with more than max resources 08/24/23 12:04:26.575
    STEP: Updating a LimitRange 08/24/23 12:04:26.582
    STEP: Verifying LimitRange updating is effective 08/24/23 12:04:26.6
    STEP: Creating a Pod with less than former min resources 08/24/23 12:04:28.611
    STEP: Failing to create a Pod with more than max resources 08/24/23 12:04:28.627
    STEP: Deleting a LimitRange 08/24/23 12:04:28.632
    STEP: Verifying the LimitRange was deleted 08/24/23 12:04:28.644
    Aug 24 12:04:33.652: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/24/23 12:04:33.652
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:04:33.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-284" for this suite. 08/24/23 12:04:33.684
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:04:33.702
Aug 24 12:04:33.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:04:33.706
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:33.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:33.742
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:04:33.748
Aug 24 12:04:33.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb" in namespace "downward-api-5948" to be "Succeeded or Failed"
Aug 24 12:04:33.774: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.356768ms
Aug 24 12:04:35.782: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017725016s
Aug 24 12:04:37.787: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021804014s
STEP: Saw pod success 08/24/23 12:04:37.787
Aug 24 12:04:37.788: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb" satisfied condition "Succeeded or Failed"
Aug 24 12:04:37.793: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb container client-container: <nil>
STEP: delete the pod 08/24/23 12:04:37.805
Aug 24 12:04:37.830: INFO: Waiting for pod downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb to disappear
Aug 24 12:04:37.843: INFO: Pod downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:04:37.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5948" for this suite. 08/24/23 12:04:37.856
------------------------------
â€¢ [4.171 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:04:33.702
    Aug 24 12:04:33.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:04:33.706
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:33.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:33.742
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:04:33.748
    Aug 24 12:04:33.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb" in namespace "downward-api-5948" to be "Succeeded or Failed"
    Aug 24 12:04:33.774: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.356768ms
    Aug 24 12:04:35.782: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017725016s
    Aug 24 12:04:37.787: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021804014s
    STEP: Saw pod success 08/24/23 12:04:37.787
    Aug 24 12:04:37.788: INFO: Pod "downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb" satisfied condition "Succeeded or Failed"
    Aug 24 12:04:37.793: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb container client-container: <nil>
    STEP: delete the pod 08/24/23 12:04:37.805
    Aug 24 12:04:37.830: INFO: Waiting for pod downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb to disappear
    Aug 24 12:04:37.843: INFO: Pod downwardapi-volume-f00ba1bc-a5e4-449d-b394-013f45f95dfb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:04:37.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5948" for this suite. 08/24/23 12:04:37.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:04:37.881
Aug 24 12:04:37.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 12:04:37.883
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:37.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:37.929
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/24/23 12:04:37.938
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/24/23 12:04:37.983
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/24/23 12:04:37.983
STEP: creating a pod to probe DNS 08/24/23 12:04:37.984
STEP: submitting the pod to kubernetes 08/24/23 12:04:37.984
Aug 24 12:04:38.011: INFO: Waiting up to 15m0s for pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1" in namespace "dns-9729" to be "running"
Aug 24 12:04:38.019: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090243ms
Aug 24 12:04:40.030: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019016863s
Aug 24 12:04:42.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018142982s
Aug 24 12:04:44.028: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016569138s
Aug 24 12:04:46.027: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015468253s
Aug 24 12:04:48.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018173038s
Aug 24 12:04:50.027: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01619479s
Aug 24 12:04:52.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017392913s
Aug 24 12:04:54.033: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022158599s
Aug 24 12:04:56.028: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016404547s
Aug 24 12:04:58.028: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016865109s
Aug 24 12:05:00.027: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016198711s
Aug 24 12:05:02.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Running", Reason="", readiness=true. Elapsed: 24.017481083s
Aug 24 12:05:02.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:05:02.029
STEP: looking for the results for each expected name from probers 08/24/23 12:05:02.035
Aug 24 12:05:02.065: INFO: DNS probes using dns-9729/dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1 succeeded

STEP: deleting the pod 08/24/23 12:05:02.065
STEP: deleting the test headless service 08/24/23 12:05:02.103
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9729" for this suite. 08/24/23 12:05:02.139
------------------------------
â€¢ [SLOW TEST] [24.293 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:04:37.881
    Aug 24 12:04:37.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 12:04:37.883
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:04:37.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:04:37.929
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/24/23 12:04:37.938
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/24/23 12:04:37.983
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9729.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/24/23 12:04:37.983
    STEP: creating a pod to probe DNS 08/24/23 12:04:37.984
    STEP: submitting the pod to kubernetes 08/24/23 12:04:37.984
    Aug 24 12:04:38.011: INFO: Waiting up to 15m0s for pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1" in namespace "dns-9729" to be "running"
    Aug 24 12:04:38.019: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090243ms
    Aug 24 12:04:40.030: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019016863s
    Aug 24 12:04:42.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018142982s
    Aug 24 12:04:44.028: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016569138s
    Aug 24 12:04:46.027: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015468253s
    Aug 24 12:04:48.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018173038s
    Aug 24 12:04:50.027: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01619479s
    Aug 24 12:04:52.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017392913s
    Aug 24 12:04:54.033: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022158599s
    Aug 24 12:04:56.028: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016404547s
    Aug 24 12:04:58.028: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016865109s
    Aug 24 12:05:00.027: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016198711s
    Aug 24 12:05:02.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1": Phase="Running", Reason="", readiness=true. Elapsed: 24.017481083s
    Aug 24 12:05:02.029: INFO: Pod "dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:05:02.029
    STEP: looking for the results for each expected name from probers 08/24/23 12:05:02.035
    Aug 24 12:05:02.065: INFO: DNS probes using dns-9729/dns-test-0d1f5060-6b46-4f2b-9158-4e8cb2a56be1 succeeded

    STEP: deleting the pod 08/24/23 12:05:02.065
    STEP: deleting the test headless service 08/24/23 12:05:02.103
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9729" for this suite. 08/24/23 12:05:02.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:02.177
Aug 24 12:05:02.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:05:02.185
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:02.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:02.256
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 24 12:05:02.262: INFO: Creating deployment "webserver-deployment"
Aug 24 12:05:02.271: INFO: Waiting for observed generation 1
Aug 24 12:05:04.339: INFO: Waiting for all required pods to come up
Aug 24 12:05:04.355: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/24/23 12:05:04.355
Aug 24 12:05:04.355: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5rsbd" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-59lhj" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6lh4w" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vc7xn" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l2l4z" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-92c7g" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-h7szf" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nr2hf" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qkvvf" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qx8n4" in namespace "deployment-6551" to be "running"
Aug 24 12:05:04.369: INFO: Pod "webserver-deployment-7f5969cbc7-5rsbd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.291498ms
Aug 24 12:05:04.369: INFO: Pod "webserver-deployment-7f5969cbc7-59lhj": Phase="Pending", Reason="", readiness=false. Elapsed: 13.055108ms
Aug 24 12:05:04.372: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.577046ms
Aug 24 12:05:04.372: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.724481ms
Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g": Phase="Pending", Reason="", readiness=false. Elapsed: 15.26906ms
Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011588ms
Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.943674ms
Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.155825ms
Aug 24 12:05:04.376: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379466ms
Aug 24 12:05:04.378: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z": Phase="Pending", Reason="", readiness=false. Elapsed: 20.606816ms
Aug 24 12:05:06.378: INFO: Pod "webserver-deployment-7f5969cbc7-5rsbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.02270839s
Aug 24 12:05:06.379: INFO: Pod "webserver-deployment-7f5969cbc7-5rsbd" satisfied condition "running"
Aug 24 12:05:06.379: INFO: Pod "webserver-deployment-7f5969cbc7-59lhj": Phase="Running", Reason="", readiness=true. Elapsed: 2.022784921s
Aug 24 12:05:06.379: INFO: Pod "webserver-deployment-7f5969cbc7-59lhj" satisfied condition "running"
Aug 24 12:05:06.384: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025602468s
Aug 24 12:05:06.384: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf" satisfied condition "running"
Aug 24 12:05:06.384: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn": Phase="Running", Reason="", readiness=true. Elapsed: 2.027336512s
Aug 24 12:05:06.385: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn" satisfied condition "running"
Aug 24 12:05:06.385: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.028936045s
Aug 24 12:05:06.385: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w" satisfied condition "running"
Aug 24 12:05:06.388: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g": Phase="Running", Reason="", readiness=true. Elapsed: 2.030065101s
Aug 24 12:05:06.388: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g" satisfied condition "running"
Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.031347373s
Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf" satisfied condition "running"
Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031278272s
Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4" satisfied condition "running"
Aug 24 12:05:06.391: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z": Phase="Running", Reason="", readiness=true. Elapsed: 2.033725408s
Aug 24 12:05:06.391: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z" satisfied condition "running"
Aug 24 12:05:06.392: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf": Phase="Running", Reason="", readiness=true. Elapsed: 2.034295724s
Aug 24 12:05:06.392: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf" satisfied condition "running"
Aug 24 12:05:06.392: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 24 12:05:06.410: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 24 12:05:06.432: INFO: Updating deployment webserver-deployment
Aug 24 12:05:06.432: INFO: Waiting for observed generation 2
Aug 24 12:05:08.449: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 24 12:05:08.455: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 24 12:05:08.464: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 24 12:05:08.489: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 24 12:05:08.489: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 24 12:05:08.494: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 24 12:05:08.514: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 24 12:05:08.514: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 24 12:05:08.536: INFO: Updating deployment webserver-deployment
Aug 24 12:05:08.536: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 24 12:05:08.551: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 24 12:05:08.558: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:05:08.575: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6551  6a2f7ce3-9382-421a-8b0a-61c7919aed38 11559 3 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00440e898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:05:05 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-24 12:05:06 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 24 12:05:08.587: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6551  cda339a5-cb12-4559-b433-9ea4e42c5328 11562 3 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6a2f7ce3-9382-421a-8b0a-61c7919aed38 0xc0043f6787 0xc0043f6788}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2f7ce3-9382-421a-8b0a-61c7919aed38\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043f6828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:05:08.587: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 24 12:05:08.588: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6551  5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 11560 3 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6a2f7ce3-9382-421a-8b0a-61c7919aed38 0xc0043f6697 0xc0043f6698}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2f7ce3-9382-421a-8b0a-61c7919aed38\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043f6728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:05:08.605: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6lh4w webserver-deployment-7f5969cbc7- deployment-6551  a0a34292-7d66-4c09-b687-01accdb065aa 11471 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f6d27 0xc0043f6d28}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbm9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbm9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.247,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6f799be3f2774bb983b0587ac4e086acea528410c157b09fb3e43f6bad9349dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.609: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92c7g webserver-deployment-7f5969cbc7- deployment-6551  e70b300e-8634-4ae1-b83d-a59abc96c108 11431 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f6f17 0xc0043f6f18}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n529b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n529b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.97,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0f010483760a5f62718aee5733f7f3dad5cbf4b2ece76b7c0dcb4dd662d60dd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.610: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h7szf webserver-deployment-7f5969cbc7- deployment-6551  9e650a5e-b2b3-47d2-bfd7-f345e31172b9 11461 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7107 0xc0043f7108}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x7jcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x7jcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.36,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dd8f88223ef13d49ba60c65b078ffc7a3d9f9596231c62c21d645c7042afca60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.611: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l2l4z webserver-deployment-7f5969cbc7- deployment-6551  08a07cad-dc35-4813-aa21-919801abc458 11469 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f72f7 0xc0043f72f8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xpvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xpvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.21,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3db3ae17578b4e51008b894ce901fb43b9fc6bcaca533a3beb85af1faacac539,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.612: INFO: Pod "webserver-deployment-7f5969cbc7-llg98" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-llg98 webserver-deployment-7f5969cbc7- deployment-6551  60745212-95cb-4f98-99ea-eff9c4338dc6 11563 0 2023-08-24 12:05:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f74e7 0xc0043f74e8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wsc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wsc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.613: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nr2hf webserver-deployment-7f5969cbc7- deployment-6551  e1dbe24c-d890-4809-8f8f-4dcedf8ee7a8 11434 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7627 0xc0043f7628}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h25cb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h25cb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.120,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fb3e7a3a7ee14030722237f222ddff1884d051b8488759cdf92c517a05c9503c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.614: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qkvvf webserver-deployment-7f5969cbc7- deployment-6551  feff3bef-8605-491f-9cfc-2e211b70b3f0 11438 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7817 0xc0043f7818}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5snl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5snl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.252,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5ee0d52a75649116fdb307e178b56705f62e96dd7faddd2a62f4473a8db3fb86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.622: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qx8n4 webserver-deployment-7f5969cbc7- deployment-6551  62ed2318-0402-4fa7-b3b1-2d285225f36c 11444 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7a07 0xc0043f7a08}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts289,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts289,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.124,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3cb4baa38f3fb1e7c44935fe4bec2d8fd35202f39a7aaad54f538a3658a394c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.623: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vc7xn webserver-deployment-7f5969cbc7- deployment-6551  8eab0c91-1efd-46d3-a77b-ca52e3aaa7b4 11464 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7bf7 0xc0043f7bf8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4xm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4xm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.37,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://214dd2d3eaf62e6bf3ed389c5ea8f2add3c0e4eedecf2a6d3bb4ed10ce760a94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.623: INFO: Pod "webserver-deployment-d9f79cb5-46656" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-46656 webserver-deployment-d9f79cb5- deployment-6551  18ec467f-e668-49cd-9632-092d96446fc6 11524 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0043f7de7 0xc0043f7de8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bdh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bdh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.624: INFO: Pod "webserver-deployment-d9f79cb5-bx9wf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bx9wf webserver-deployment-d9f79cb5- deployment-6551  9be5550a-a806-4341-b4d6-91bacefd8062 11523 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0043f7fd7 0xc0043f7fd8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vl8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vl8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.626: INFO: Pod "webserver-deployment-d9f79cb5-d5xxg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d5xxg webserver-deployment-d9f79cb5- deployment-6551  47fa12d9-2dd9-4ded-9d9e-9eeecc874a3d 11489 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0045001c7 0xc0045001c8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ss2z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ss2z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.627: INFO: Pod "webserver-deployment-d9f79cb5-d6p6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d6p6s webserver-deployment-d9f79cb5- deployment-6551  8a9df42a-beb4-41dd-af82-78aaea9a464f 11500 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0045003b7 0xc0045003b8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xlkcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xlkcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:05:08.627: INFO: Pod "webserver-deployment-d9f79cb5-lc8zr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lc8zr webserver-deployment-d9f79cb5- deployment-6551  d9dbc594-5e24-4e64-8bc0-14f65f3cf35b 11505 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0045005a7 0xc0045005a8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktlp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktlp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6551" for this suite. 08/24/23 12:05:08.647
------------------------------
â€¢ [SLOW TEST] [6.492 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:02.177
    Aug 24 12:05:02.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:05:02.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:02.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:02.256
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 24 12:05:02.262: INFO: Creating deployment "webserver-deployment"
    Aug 24 12:05:02.271: INFO: Waiting for observed generation 1
    Aug 24 12:05:04.339: INFO: Waiting for all required pods to come up
    Aug 24 12:05:04.355: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/24/23 12:05:04.355
    Aug 24 12:05:04.355: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5rsbd" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-59lhj" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6lh4w" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.356: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vc7xn" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-l2l4z" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-92c7g" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-h7szf" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nr2hf" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qkvvf" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.357: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qx8n4" in namespace "deployment-6551" to be "running"
    Aug 24 12:05:04.369: INFO: Pod "webserver-deployment-7f5969cbc7-5rsbd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.291498ms
    Aug 24 12:05:04.369: INFO: Pod "webserver-deployment-7f5969cbc7-59lhj": Phase="Pending", Reason="", readiness=false. Elapsed: 13.055108ms
    Aug 24 12:05:04.372: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.577046ms
    Aug 24 12:05:04.372: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.724481ms
    Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g": Phase="Pending", Reason="", readiness=false. Elapsed: 15.26906ms
    Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011588ms
    Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.943674ms
    Aug 24 12:05:04.373: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w": Phase="Pending", Reason="", readiness=false. Elapsed: 17.155825ms
    Aug 24 12:05:04.376: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379466ms
    Aug 24 12:05:04.378: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z": Phase="Pending", Reason="", readiness=false. Elapsed: 20.606816ms
    Aug 24 12:05:06.378: INFO: Pod "webserver-deployment-7f5969cbc7-5rsbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.02270839s
    Aug 24 12:05:06.379: INFO: Pod "webserver-deployment-7f5969cbc7-5rsbd" satisfied condition "running"
    Aug 24 12:05:06.379: INFO: Pod "webserver-deployment-7f5969cbc7-59lhj": Phase="Running", Reason="", readiness=true. Elapsed: 2.022784921s
    Aug 24 12:05:06.379: INFO: Pod "webserver-deployment-7f5969cbc7-59lhj" satisfied condition "running"
    Aug 24 12:05:06.384: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025602468s
    Aug 24 12:05:06.384: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf" satisfied condition "running"
    Aug 24 12:05:06.384: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn": Phase="Running", Reason="", readiness=true. Elapsed: 2.027336512s
    Aug 24 12:05:06.385: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn" satisfied condition "running"
    Aug 24 12:05:06.385: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.028936045s
    Aug 24 12:05:06.385: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w" satisfied condition "running"
    Aug 24 12:05:06.388: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g": Phase="Running", Reason="", readiness=true. Elapsed: 2.030065101s
    Aug 24 12:05:06.388: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g" satisfied condition "running"
    Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf": Phase="Running", Reason="", readiness=true. Elapsed: 2.031347373s
    Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf" satisfied condition "running"
    Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031278272s
    Aug 24 12:05:06.390: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4" satisfied condition "running"
    Aug 24 12:05:06.391: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z": Phase="Running", Reason="", readiness=true. Elapsed: 2.033725408s
    Aug 24 12:05:06.391: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z" satisfied condition "running"
    Aug 24 12:05:06.392: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf": Phase="Running", Reason="", readiness=true. Elapsed: 2.034295724s
    Aug 24 12:05:06.392: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf" satisfied condition "running"
    Aug 24 12:05:06.392: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 24 12:05:06.410: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 24 12:05:06.432: INFO: Updating deployment webserver-deployment
    Aug 24 12:05:06.432: INFO: Waiting for observed generation 2
    Aug 24 12:05:08.449: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 24 12:05:08.455: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 24 12:05:08.464: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 24 12:05:08.489: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 24 12:05:08.489: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 24 12:05:08.494: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 24 12:05:08.514: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 24 12:05:08.514: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 24 12:05:08.536: INFO: Updating deployment webserver-deployment
    Aug 24 12:05:08.536: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 24 12:05:08.551: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 24 12:05:08.558: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:05:08.575: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-6551  6a2f7ce3-9382-421a-8b0a-61c7919aed38 11559 3 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00440e898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:05:05 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-24 12:05:06 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 24 12:05:08.587: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-6551  cda339a5-cb12-4559-b433-9ea4e42c5328 11562 3 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6a2f7ce3-9382-421a-8b0a-61c7919aed38 0xc0043f6787 0xc0043f6788}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2f7ce3-9382-421a-8b0a-61c7919aed38\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043f6828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:05:08.587: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 24 12:05:08.588: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-6551  5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 11560 3 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6a2f7ce3-9382-421a-8b0a-61c7919aed38 0xc0043f6697 0xc0043f6698}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2f7ce3-9382-421a-8b0a-61c7919aed38\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043f6728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:05:08.605: INFO: Pod "webserver-deployment-7f5969cbc7-6lh4w" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6lh4w webserver-deployment-7f5969cbc7- deployment-6551  a0a34292-7d66-4c09-b687-01accdb065aa 11471 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f6d27 0xc0043f6d28}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbm9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbm9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.247,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6f799be3f2774bb983b0587ac4e086acea528410c157b09fb3e43f6bad9349dc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.609: INFO: Pod "webserver-deployment-7f5969cbc7-92c7g" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-92c7g webserver-deployment-7f5969cbc7- deployment-6551  e70b300e-8634-4ae1-b83d-a59abc96c108 11431 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f6f17 0xc0043f6f18}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n529b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n529b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.97,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0f010483760a5f62718aee5733f7f3dad5cbf4b2ece76b7c0dcb4dd662d60dd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.610: INFO: Pod "webserver-deployment-7f5969cbc7-h7szf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h7szf webserver-deployment-7f5969cbc7- deployment-6551  9e650a5e-b2b3-47d2-bfd7-f345e31172b9 11461 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7107 0xc0043f7108}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x7jcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x7jcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.36,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dd8f88223ef13d49ba60c65b078ffc7a3d9f9596231c62c21d645c7042afca60,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.611: INFO: Pod "webserver-deployment-7f5969cbc7-l2l4z" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l2l4z webserver-deployment-7f5969cbc7- deployment-6551  08a07cad-dc35-4813-aa21-919801abc458 11469 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f72f7 0xc0043f72f8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xpvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xpvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.21,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3db3ae17578b4e51008b894ce901fb43b9fc6bcaca533a3beb85af1faacac539,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.612: INFO: Pod "webserver-deployment-7f5969cbc7-llg98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-llg98 webserver-deployment-7f5969cbc7- deployment-6551  60745212-95cb-4f98-99ea-eff9c4338dc6 11563 0 2023-08-24 12:05:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f74e7 0xc0043f74e8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wsc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wsc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.613: INFO: Pod "webserver-deployment-7f5969cbc7-nr2hf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nr2hf webserver-deployment-7f5969cbc7- deployment-6551  e1dbe24c-d890-4809-8f8f-4dcedf8ee7a8 11434 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7627 0xc0043f7628}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h25cb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h25cb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.120,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fb3e7a3a7ee14030722237f222ddff1884d051b8488759cdf92c517a05c9503c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.614: INFO: Pod "webserver-deployment-7f5969cbc7-qkvvf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qkvvf webserver-deployment-7f5969cbc7- deployment-6551  feff3bef-8605-491f-9cfc-2e211b70b3f0 11438 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7817 0xc0043f7818}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5snl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5snl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.252,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5ee0d52a75649116fdb307e178b56705f62e96dd7faddd2a62f4473a8db3fb86,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.622: INFO: Pod "webserver-deployment-7f5969cbc7-qx8n4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qx8n4 webserver-deployment-7f5969cbc7- deployment-6551  62ed2318-0402-4fa7-b3b1-2d285225f36c 11444 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7a07 0xc0043f7a08}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts289,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts289,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.124,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3cb4baa38f3fb1e7c44935fe4bec2d8fd35202f39a7aaad54f538a3658a394c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.623: INFO: Pod "webserver-deployment-7f5969cbc7-vc7xn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vc7xn webserver-deployment-7f5969cbc7- deployment-6551  8eab0c91-1efd-46d3-a77b-ca52e3aaa7b4 11464 0 2023-08-24 12:05:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74 0xc0043f7bf7 0xc0043f7bf8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b4e2f01-5673-41c4-a6ab-50b5a1cb1f74\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4xm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4xm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.37,StartTime:2023-08-24 12:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://214dd2d3eaf62e6bf3ed389c5ea8f2add3c0e4eedecf2a6d3bb4ed10ce760a94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.623: INFO: Pod "webserver-deployment-d9f79cb5-46656" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-46656 webserver-deployment-d9f79cb5- deployment-6551  18ec467f-e668-49cd-9632-092d96446fc6 11524 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0043f7de7 0xc0043f7de8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bdh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bdh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.624: INFO: Pod "webserver-deployment-d9f79cb5-bx9wf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bx9wf webserver-deployment-d9f79cb5- deployment-6551  9be5550a-a806-4341-b4d6-91bacefd8062 11523 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0043f7fd7 0xc0043f7fd8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vl8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vl8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.626: INFO: Pod "webserver-deployment-d9f79cb5-d5xxg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d5xxg webserver-deployment-d9f79cb5- deployment-6551  47fa12d9-2dd9-4ded-9d9e-9eeecc874a3d 11489 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0045001c7 0xc0045001c8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ss2z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ss2z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.627: INFO: Pod "webserver-deployment-d9f79cb5-d6p6s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d6p6s webserver-deployment-d9f79cb5- deployment-6551  8a9df42a-beb4-41dd-af82-78aaea9a464f 11500 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0045003b7 0xc0045003b8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xlkcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xlkcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:05:08.627: INFO: Pod "webserver-deployment-d9f79cb5-lc8zr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lc8zr webserver-deployment-d9f79cb5- deployment-6551  d9dbc594-5e24-4e64-8bc0-14f65f3cf35b 11505 0 2023-08-24 12:05:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 cda339a5-cb12-4559-b433-9ea4e42c5328 0xc0045005a7 0xc0045005a8}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cda339a5-cb12-4559-b433-9ea4e42c5328\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktlp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktlp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:,StartTime:2023-08-24 12:05:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:08.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6551" for this suite. 08/24/23 12:05:08.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:08.675
Aug 24 12:05:08.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:05:08.68
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:08.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:08.805
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:08.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5320" for this suite. 08/24/23 12:05:08.87
------------------------------
â€¢ [0.213 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:08.675
    Aug 24 12:05:08.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:05:08.68
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:08.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:08.805
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:08.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5320" for this suite. 08/24/23 12:05:08.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:08.89
Aug 24 12:05:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename conformance-tests 08/24/23 12:05:08.893
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:08.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:08.992
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/24/23 12:05:08.999
Aug 24 12:05:09.000: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:09.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-691" for this suite. 08/24/23 12:05:09.024
------------------------------
â€¢ [0.148 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:08.89
    Aug 24 12:05:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename conformance-tests 08/24/23 12:05:08.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:08.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:08.992
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/24/23 12:05:08.999
    Aug 24 12:05:09.000: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:09.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-691" for this suite. 08/24/23 12:05:09.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:09.042
Aug 24 12:05:09.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename prestop 08/24/23 12:05:09.045
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:09.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:09.087
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5044 08/24/23 12:05:09.09
STEP: Waiting for pods to come up. 08/24/23 12:05:09.106
Aug 24 12:05:09.106: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5044" to be "running"
Aug 24 12:05:09.115: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.132261ms
Aug 24 12:05:11.127: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020197005s
Aug 24 12:05:11.127: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5044 08/24/23 12:05:11.133
Aug 24 12:05:11.144: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5044" to be "running"
Aug 24 12:05:11.156: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 11.308871ms
Aug 24 12:05:13.163: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.018204767s
Aug 24 12:05:13.163: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/24/23 12:05:13.163
Aug 24 12:05:18.191: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/24/23 12:05:18.192
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5044" for this suite. 08/24/23 12:05:18.251
------------------------------
â€¢ [SLOW TEST] [9.223 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:09.042
    Aug 24 12:05:09.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename prestop 08/24/23 12:05:09.045
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:09.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:09.087
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5044 08/24/23 12:05:09.09
    STEP: Waiting for pods to come up. 08/24/23 12:05:09.106
    Aug 24 12:05:09.106: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5044" to be "running"
    Aug 24 12:05:09.115: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.132261ms
    Aug 24 12:05:11.127: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020197005s
    Aug 24 12:05:11.127: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5044 08/24/23 12:05:11.133
    Aug 24 12:05:11.144: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5044" to be "running"
    Aug 24 12:05:11.156: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 11.308871ms
    Aug 24 12:05:13.163: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.018204767s
    Aug 24 12:05:13.163: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/24/23 12:05:13.163
    Aug 24 12:05:18.191: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/24/23 12:05:18.192
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5044" for this suite. 08/24/23 12:05:18.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:18.269
Aug 24 12:05:18.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:05:18.272
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:18.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:18.31
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-b329a5cd-f94b-45c7-944a-e11a4d385cea 08/24/23 12:05:18.314
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:18.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5753" for this suite. 08/24/23 12:05:18.327
------------------------------
â€¢ [0.070 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:18.269
    Aug 24 12:05:18.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:05:18.272
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:18.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:18.31
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-b329a5cd-f94b-45c7-944a-e11a4d385cea 08/24/23 12:05:18.314
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:18.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5753" for this suite. 08/24/23 12:05:18.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:18.34
Aug 24 12:05:18.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:05:18.342
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:18.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:18.386
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/24/23 12:05:18.416
STEP: waiting for Deployment to be created 08/24/23 12:05:18.427
STEP: waiting for all Replicas to be Ready 08/24/23 12:05:18.429
Aug 24 12:05:18.432: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.432: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.467: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.467: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.511: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.511: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.585: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:18.585: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 12:05:19.517: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 24 12:05:19.517: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 24 12:05:20.578: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/24/23 12:05:20.578
W0824 12:05:20.594981      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 24 12:05:20.598: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/24/23 12:05:20.599
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.629: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.629: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.678: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.678: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:20.704: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:20.704: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:20.717: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:20.717: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:22.538: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:22.538: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:22.625: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
STEP: listing Deployments 08/24/23 12:05:22.625
Aug 24 12:05:22.634: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/24/23 12:05:22.634
Aug 24 12:05:22.693: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/24/23 12:05:22.693
Aug 24 12:05:22.725: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:22.746: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:22.858: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:22.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:22.990: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:24.551: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:24.608: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:24.664: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 12:05:25.766: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/24/23 12:05:25.906
STEP: fetching the DeploymentStatus 08/24/23 12:05:25.921
Aug 24 12:05:25.935: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:25.936: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:25.936: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:25.937: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:25.937: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
Aug 24 12:05:25.937: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:25.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:25.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
Aug 24 12:05:25.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 3
STEP: deleting the Deployment 08/24/23 12:05:25.938
Aug 24 12:05:25.960: INFO: observed event type MODIFIED
Aug 24 12:05:25.960: INFO: observed event type MODIFIED
Aug 24 12:05:25.961: INFO: observed event type MODIFIED
Aug 24 12:05:25.961: INFO: observed event type MODIFIED
Aug 24 12:05:25.963: INFO: observed event type MODIFIED
Aug 24 12:05:25.963: INFO: observed event type MODIFIED
Aug 24 12:05:25.963: INFO: observed event type MODIFIED
Aug 24 12:05:25.963: INFO: observed event type MODIFIED
Aug 24 12:05:25.964: INFO: observed event type MODIFIED
Aug 24 12:05:25.964: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:05:25.971: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 24 12:05:25.976: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-4924  4ec7a836-a017-419a-86fd-f2f85670194c 12017 2 2023-08-24 12:05:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ebda576a-4732-4827-bc12-b32ffb9fb026 0xc003e79c27 0xc003e79c28}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebda576a-4732-4827-bc12-b32ffb9fb026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e79cb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 24 12:05:25.986: INFO: pod: "test-deployment-7b7876f9d6-tn8g4":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-tn8g4 test-deployment-7b7876f9d6- deployment-4924  89ae3ae1-cebc-4eed-8440-6dbfd470fd0b 12015 0 2023-08-24 12:05:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4ec7a836-a017-419a-86fd-f2f85670194c 0xc003fdc587 0xc003fdc588}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec7a836-a017-419a-86fd-f2f85670194c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjlxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjlxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.20,StartTime:2023-08-24 12:05:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://84c815d43f419769d71787f4db41ecbf3542c94ee93daf75cec6bb7dc92aacbf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 12:05:25.987: INFO: pod: "test-deployment-7b7876f9d6-zvs52":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-zvs52 test-deployment-7b7876f9d6- deployment-4924  d67c5712-5ee1-4638-95b4-1485380e87c3 11986 0 2023-08-24 12:05:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4ec7a836-a017-419a-86fd-f2f85670194c 0xc003fdc777 0xc003fdc778}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec7a836-a017-419a-86fd-f2f85670194c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8mh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8mh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.245,StartTime:2023-08-24 12:05:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0057bcf04dd54e84e4300442876ccc675b80d8f822f60600471aaaa8fb3b6f4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 12:05:25.987: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-4924  1103d062-3854-4a2c-be90-c1085d9a1768 12026 4 2023-08-24 12:05:20 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ebda576a-4732-4827-bc12-b32ffb9fb026 0xc003e79d17 0xc003e79d18}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebda576a-4732-4827-bc12-b32ffb9fb026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e79da0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 24 12:05:25.999: INFO: pod: "test-deployment-7df74c55ff-gtb68":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-gtb68 test-deployment-7df74c55ff- deployment-4924  f939e517-649e-493d-ae66-dfd747d0a5c0 11995 0 2023-08-24 12:05:22 +0000 UTC 2023-08-24 12:05:25 +0000 UTC 0xc003fddb28 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1103d062-3854-4a2c-be90-c1085d9a1768 0xc003fddb57 0xc003fddb58}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1103d062-3854-4a2c-be90-c1085d9a1768\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfg5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfg5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.141,StartTime:2023-08-24 12:05:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://1d6491adadd3414d5a23904e52d31104fec7b0ff2da6bb960008354d831b7bda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 12:05:26.000: INFO: pod: "test-deployment-7df74c55ff-p666g":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-p666g test-deployment-7df74c55ff- deployment-4924  a347370d-96b3-416c-9947-aa26244b1bb2 12021 0 2023-08-24 12:05:20 +0000 UTC 2023-08-24 12:05:26 +0000 UTC 0xc003fddd20 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1103d062-3854-4a2c-be90-c1085d9a1768 0xc003fddd57 0xc003fddd58}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1103d062-3854-4a2c-be90-c1085d9a1768\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bpx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bpx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.143,StartTime:2023-08-24 12:05:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://b9726124fbcfd7dbba7b79569ad13a366c60acf553c1a6393909223a2e2361ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 12:05:26.000: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-4924  ab5f1146-d6bc-444e-9957-780b45019236 11924 3 2023-08-24 12:05:18 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ebda576a-4732-4827-bc12-b32ffb9fb026 0xc003e79e07 0xc003e79e08}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebda576a-4732-4827-bc12-b32ffb9fb026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e79e90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:26.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4924" for this suite. 08/24/23 12:05:26.024
------------------------------
â€¢ [SLOW TEST] [7.695 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:18.34
    Aug 24 12:05:18.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:05:18.342
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:18.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:18.386
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/24/23 12:05:18.416
    STEP: waiting for Deployment to be created 08/24/23 12:05:18.427
    STEP: waiting for all Replicas to be Ready 08/24/23 12:05:18.429
    Aug 24 12:05:18.432: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.432: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.467: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.467: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.511: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.511: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.585: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:18.585: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 12:05:19.517: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 24 12:05:19.517: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 24 12:05:20.578: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/24/23 12:05:20.578
    W0824 12:05:20.594981      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 24 12:05:20.598: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/24/23 12:05:20.599
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 0
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:20.604: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.605: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.629: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.629: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.678: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.678: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:20.704: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:20.704: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:20.717: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:20.717: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:22.538: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:22.538: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:22.625: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    STEP: listing Deployments 08/24/23 12:05:22.625
    Aug 24 12:05:22.634: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/24/23 12:05:22.634
    Aug 24 12:05:22.693: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/24/23 12:05:22.693
    Aug 24 12:05:22.725: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:22.746: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:22.858: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:22.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:22.990: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:24.551: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:24.608: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:24.664: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 12:05:25.766: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/24/23 12:05:25.906
    STEP: fetching the DeploymentStatus 08/24/23 12:05:25.921
    Aug 24 12:05:25.935: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:25.936: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:25.936: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:25.937: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:25.937: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 1
    Aug 24 12:05:25.937: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:25.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:25.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 2
    Aug 24 12:05:25.938: INFO: observed Deployment test-deployment in namespace deployment-4924 with ReadyReplicas 3
    STEP: deleting the Deployment 08/24/23 12:05:25.938
    Aug 24 12:05:25.960: INFO: observed event type MODIFIED
    Aug 24 12:05:25.960: INFO: observed event type MODIFIED
    Aug 24 12:05:25.961: INFO: observed event type MODIFIED
    Aug 24 12:05:25.961: INFO: observed event type MODIFIED
    Aug 24 12:05:25.963: INFO: observed event type MODIFIED
    Aug 24 12:05:25.963: INFO: observed event type MODIFIED
    Aug 24 12:05:25.963: INFO: observed event type MODIFIED
    Aug 24 12:05:25.963: INFO: observed event type MODIFIED
    Aug 24 12:05:25.964: INFO: observed event type MODIFIED
    Aug 24 12:05:25.964: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:05:25.971: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 24 12:05:25.976: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-4924  4ec7a836-a017-419a-86fd-f2f85670194c 12017 2 2023-08-24 12:05:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ebda576a-4732-4827-bc12-b32ffb9fb026 0xc003e79c27 0xc003e79c28}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebda576a-4732-4827-bc12-b32ffb9fb026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e79cb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 24 12:05:25.986: INFO: pod: "test-deployment-7b7876f9d6-tn8g4":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-tn8g4 test-deployment-7b7876f9d6- deployment-4924  89ae3ae1-cebc-4eed-8440-6dbfd470fd0b 12015 0 2023-08-24 12:05:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4ec7a836-a017-419a-86fd-f2f85670194c 0xc003fdc587 0xc003fdc588}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec7a836-a017-419a-86fd-f2f85670194c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjlxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjlxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.127,PodIP:10.233.64.20,StartTime:2023-08-24 12:05:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://84c815d43f419769d71787f4db41ecbf3542c94ee93daf75cec6bb7dc92aacbf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 12:05:25.987: INFO: pod: "test-deployment-7b7876f9d6-zvs52":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-zvs52 test-deployment-7b7876f9d6- deployment-4924  d67c5712-5ee1-4638-95b4-1485380e87c3 11986 0 2023-08-24 12:05:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4ec7a836-a017-419a-86fd-f2f85670194c 0xc003fdc777 0xc003fdc778}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec7a836-a017-419a-86fd-f2f85670194c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8mh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8mh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.245,StartTime:2023-08-24 12:05:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0057bcf04dd54e84e4300442876ccc675b80d8f822f60600471aaaa8fb3b6f4e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 12:05:25.987: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-4924  1103d062-3854-4a2c-be90-c1085d9a1768 12026 4 2023-08-24 12:05:20 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ebda576a-4732-4827-bc12-b32ffb9fb026 0xc003e79d17 0xc003e79d18}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebda576a-4732-4827-bc12-b32ffb9fb026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e79da0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 24 12:05:25.999: INFO: pod: "test-deployment-7df74c55ff-gtb68":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-gtb68 test-deployment-7df74c55ff- deployment-4924  f939e517-649e-493d-ae66-dfd747d0a5c0 11995 0 2023-08-24 12:05:22 +0000 UTC 2023-08-24 12:05:25 +0000 UTC 0xc003fddb28 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1103d062-3854-4a2c-be90-c1085d9a1768 0xc003fddb57 0xc003fddb58}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1103d062-3854-4a2c-be90-c1085d9a1768\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfg5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfg5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.141,StartTime:2023-08-24 12:05:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://1d6491adadd3414d5a23904e52d31104fec7b0ff2da6bb960008354d831b7bda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 12:05:26.000: INFO: pod: "test-deployment-7df74c55ff-p666g":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-p666g test-deployment-7df74c55ff- deployment-4924  a347370d-96b3-416c-9947-aa26244b1bb2 12021 0 2023-08-24 12:05:20 +0000 UTC 2023-08-24 12:05:26 +0000 UTC 0xc003fddd20 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1103d062-3854-4a2c-be90-c1085d9a1768 0xc003fddd57 0xc003fddd58}] [] [{kube-controller-manager Update v1 2023-08-24 12:05:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1103d062-3854-4a2c-be90-c1085d9a1768\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bpx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bpx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:05:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.143,StartTime:2023-08-24 12:05:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:05:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://b9726124fbcfd7dbba7b79569ad13a366c60acf553c1a6393909223a2e2361ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 12:05:26.000: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-4924  ab5f1146-d6bc-444e-9957-780b45019236 11924 3 2023-08-24 12:05:18 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ebda576a-4732-4827-bc12-b32ffb9fb026 0xc003e79e07 0xc003e79e08}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebda576a-4732-4827-bc12-b32ffb9fb026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:05:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e79e90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:26.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4924" for this suite. 08/24/23 12:05:26.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:26.04
Aug 24 12:05:26.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:05:26.043
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:26.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:26.08
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:05:26.128
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:05:27.575
STEP: Deploying the webhook pod 08/24/23 12:05:27.589
STEP: Wait for the deployment to be ready 08/24/23 12:05:27.607
Aug 24 12:05:27.620: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:05:29.656
STEP: Verifying the service has paired with the endpoint 08/24/23 12:05:29.685
Aug 24 12:05:30.686: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 12:05:30.693
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 12:05:30.724
STEP: Creating a dummy validating-webhook-configuration object 08/24/23 12:05:30.75
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/24/23 12:05:30.765
STEP: Creating a dummy mutating-webhook-configuration object 08/24/23 12:05:30.778
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/24/23 12:05:30.792
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:30.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5068" for this suite. 08/24/23 12:05:30.919
STEP: Destroying namespace "webhook-5068-markers" for this suite. 08/24/23 12:05:30.948
------------------------------
â€¢ [4.932 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:26.04
    Aug 24 12:05:26.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:05:26.043
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:26.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:26.08
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:05:26.128
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:05:27.575
    STEP: Deploying the webhook pod 08/24/23 12:05:27.589
    STEP: Wait for the deployment to be ready 08/24/23 12:05:27.607
    Aug 24 12:05:27.620: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:05:29.656
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:05:29.685
    Aug 24 12:05:30.686: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 12:05:30.693
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 12:05:30.724
    STEP: Creating a dummy validating-webhook-configuration object 08/24/23 12:05:30.75
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/24/23 12:05:30.765
    STEP: Creating a dummy mutating-webhook-configuration object 08/24/23 12:05:30.778
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/24/23 12:05:30.792
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:30.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5068" for this suite. 08/24/23 12:05:30.919
    STEP: Destroying namespace "webhook-5068-markers" for this suite. 08/24/23 12:05:30.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:30.989
Aug 24 12:05:30.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:05:31.01
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:31.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:31.076
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/24/23 12:05:31.086
Aug 24 12:05:31.109: INFO: Waiting up to 5m0s for pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539" in namespace "downward-api-5981" to be "running and ready"
Aug 24 12:05:31.121: INFO: Pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539": Phase="Pending", Reason="", readiness=false. Elapsed: 11.513924ms
Aug 24 12:05:31.121: INFO: The phase of Pod annotationupdate7d969f22-c79c-459d-8168-2416c4e51539 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:05:33.138: INFO: Pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539": Phase="Running", Reason="", readiness=true. Elapsed: 2.027852365s
Aug 24 12:05:33.138: INFO: The phase of Pod annotationupdate7d969f22-c79c-459d-8168-2416c4e51539 is Running (Ready = true)
Aug 24 12:05:33.138: INFO: Pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539" satisfied condition "running and ready"
Aug 24 12:05:33.675: INFO: Successfully updated pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5981" for this suite. 08/24/23 12:05:37.727
------------------------------
â€¢ [SLOW TEST] [6.755 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:30.989
    Aug 24 12:05:30.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:05:31.01
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:31.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:31.076
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/24/23 12:05:31.086
    Aug 24 12:05:31.109: INFO: Waiting up to 5m0s for pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539" in namespace "downward-api-5981" to be "running and ready"
    Aug 24 12:05:31.121: INFO: Pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539": Phase="Pending", Reason="", readiness=false. Elapsed: 11.513924ms
    Aug 24 12:05:31.121: INFO: The phase of Pod annotationupdate7d969f22-c79c-459d-8168-2416c4e51539 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:05:33.138: INFO: Pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539": Phase="Running", Reason="", readiness=true. Elapsed: 2.027852365s
    Aug 24 12:05:33.138: INFO: The phase of Pod annotationupdate7d969f22-c79c-459d-8168-2416c4e51539 is Running (Ready = true)
    Aug 24 12:05:33.138: INFO: Pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539" satisfied condition "running and ready"
    Aug 24 12:05:33.675: INFO: Successfully updated pod "annotationupdate7d969f22-c79c-459d-8168-2416c4e51539"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5981" for this suite. 08/24/23 12:05:37.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:37.75
Aug 24 12:05:37.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:05:37.752
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:37.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:37.789
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 24 12:05:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: creating the pod 08/24/23 12:05:37.798
STEP: submitting the pod to kubernetes 08/24/23 12:05:37.799
Aug 24 12:05:37.820: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a" in namespace "pods-238" to be "running and ready"
Aug 24 12:05:37.831: INFO: Pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.852251ms
Aug 24 12:05:37.831: INFO: The phase of Pod pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:05:39.839: INFO: Pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019202831s
Aug 24 12:05:39.839: INFO: The phase of Pod pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a is Running (Ready = true)
Aug 24 12:05:39.839: INFO: Pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:39.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-238" for this suite. 08/24/23 12:05:40.007
------------------------------
â€¢ [2.274 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:37.75
    Aug 24 12:05:37.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:05:37.752
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:37.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:37.789
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 24 12:05:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: creating the pod 08/24/23 12:05:37.798
    STEP: submitting the pod to kubernetes 08/24/23 12:05:37.799
    Aug 24 12:05:37.820: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a" in namespace "pods-238" to be "running and ready"
    Aug 24 12:05:37.831: INFO: Pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.852251ms
    Aug 24 12:05:37.831: INFO: The phase of Pod pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:05:39.839: INFO: Pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019202831s
    Aug 24 12:05:39.839: INFO: The phase of Pod pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a is Running (Ready = true)
    Aug 24 12:05:39.839: INFO: Pod "pod-exec-websocket-6ae657dc-92eb-4e3d-b650-d679b9c16a2a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:39.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-238" for this suite. 08/24/23 12:05:40.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:40.033
Aug 24 12:05:40.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:05:40.035
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:40.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:40.076
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:05:40.082
Aug 24 12:05:40.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90" in namespace "downward-api-8606" to be "Succeeded or Failed"
Aug 24 12:05:40.111: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90": Phase="Pending", Reason="", readiness=false. Elapsed: 9.554132ms
Aug 24 12:05:42.122: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021036023s
Aug 24 12:05:44.118: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017008637s
STEP: Saw pod success 08/24/23 12:05:44.118
Aug 24 12:05:44.119: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90" satisfied condition "Succeeded or Failed"
Aug 24 12:05:44.126: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90 container client-container: <nil>
STEP: delete the pod 08/24/23 12:05:44.138
Aug 24 12:05:44.166: INFO: Waiting for pod downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90 to disappear
Aug 24 12:05:44.171: INFO: Pod downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:44.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8606" for this suite. 08/24/23 12:05:44.182
------------------------------
â€¢ [4.161 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:40.033
    Aug 24 12:05:40.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:05:40.035
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:40.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:40.076
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:05:40.082
    Aug 24 12:05:40.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90" in namespace "downward-api-8606" to be "Succeeded or Failed"
    Aug 24 12:05:40.111: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90": Phase="Pending", Reason="", readiness=false. Elapsed: 9.554132ms
    Aug 24 12:05:42.122: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021036023s
    Aug 24 12:05:44.118: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017008637s
    STEP: Saw pod success 08/24/23 12:05:44.118
    Aug 24 12:05:44.119: INFO: Pod "downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90" satisfied condition "Succeeded or Failed"
    Aug 24 12:05:44.126: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:05:44.138
    Aug 24 12:05:44.166: INFO: Waiting for pod downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90 to disappear
    Aug 24 12:05:44.171: INFO: Pod downwardapi-volume-630de7db-62f0-4385-be1b-1b3e81659c90 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:44.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8606" for this suite. 08/24/23 12:05:44.182
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:44.196
Aug 24 12:05:44.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-runtime 08/24/23 12:05:44.199
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:44.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:44.231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/24/23 12:05:44.237
STEP: wait for the container to reach Succeeded 08/24/23 12:05:44.253
STEP: get the container status 08/24/23 12:05:47.281
STEP: the container should be terminated 08/24/23 12:05:47.288
STEP: the termination message should be set 08/24/23 12:05:47.288
Aug 24 12:05:47.288: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/24/23 12:05:47.288
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:47.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9501" for this suite. 08/24/23 12:05:47.325
------------------------------
â€¢ [3.140 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:44.196
    Aug 24 12:05:44.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-runtime 08/24/23 12:05:44.199
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:44.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:44.231
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/24/23 12:05:44.237
    STEP: wait for the container to reach Succeeded 08/24/23 12:05:44.253
    STEP: get the container status 08/24/23 12:05:47.281
    STEP: the container should be terminated 08/24/23 12:05:47.288
    STEP: the termination message should be set 08/24/23 12:05:47.288
    Aug 24 12:05:47.288: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/24/23 12:05:47.288
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:47.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9501" for this suite. 08/24/23 12:05:47.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:47.352
Aug 24 12:05:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replicaset 08/24/23 12:05:47.355
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:47.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:47.388
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 24 12:05:47.394: INFO: Creating ReplicaSet my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce
Aug 24 12:05:47.407: INFO: Pod name my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce: Found 0 pods out of 1
Aug 24 12:05:52.418: INFO: Pod name my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce: Found 1 pods out of 1
Aug 24 12:05:52.418: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce" is running
Aug 24 12:05:52.418: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479" in namespace "replicaset-8202" to be "running"
Aug 24 12:05:52.425: INFO: Pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479": Phase="Running", Reason="", readiness=true. Elapsed: 7.00628ms
Aug 24 12:05:52.425: INFO: Pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479" satisfied condition "running"
Aug 24 12:05:52.425: INFO: Pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:47 +0000 UTC Reason: Message:}])
Aug 24 12:05:52.425: INFO: Trying to dial the pod
Aug 24 12:05:57.450: INFO: Controller my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce: Got expected result from replica 1 [my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479]: "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:05:57.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8202" for this suite. 08/24/23 12:05:57.461
------------------------------
â€¢ [SLOW TEST] [10.123 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:47.352
    Aug 24 12:05:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replicaset 08/24/23 12:05:47.355
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:47.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:47.388
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 24 12:05:47.394: INFO: Creating ReplicaSet my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce
    Aug 24 12:05:47.407: INFO: Pod name my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce: Found 0 pods out of 1
    Aug 24 12:05:52.418: INFO: Pod name my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce: Found 1 pods out of 1
    Aug 24 12:05:52.418: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce" is running
    Aug 24 12:05:52.418: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479" in namespace "replicaset-8202" to be "running"
    Aug 24 12:05:52.425: INFO: Pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479": Phase="Running", Reason="", readiness=true. Elapsed: 7.00628ms
    Aug 24 12:05:52.425: INFO: Pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479" satisfied condition "running"
    Aug 24 12:05:52.425: INFO: Pod "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:05:47 +0000 UTC Reason: Message:}])
    Aug 24 12:05:52.425: INFO: Trying to dial the pod
    Aug 24 12:05:57.450: INFO: Controller my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce: Got expected result from replica 1 [my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479]: "my-hostname-basic-a31f4a5b-9411-4160-9537-d3c30c7164ce-ls479", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:05:57.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8202" for this suite. 08/24/23 12:05:57.461
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:05:57.48
Aug 24 12:05:57.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:05:57.483
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:57.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:57.523
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8b7567d7-2a9f-4dad-9e60-ffe4121282c8 08/24/23 12:05:57.534
STEP: Creating the pod 08/24/23 12:05:57.54
Aug 24 12:05:57.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b" in namespace "projected-5905" to be "running and ready"
Aug 24 12:05:57.560: INFO: Pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274666ms
Aug 24 12:05:57.560: INFO: The phase of Pod pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:05:59.569: INFO: Pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015087636s
Aug 24 12:05:59.569: INFO: The phase of Pod pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b is Running (Ready = true)
Aug 24 12:05:59.569: INFO: Pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-8b7567d7-2a9f-4dad-9e60-ffe4121282c8 08/24/23 12:05:59.585
STEP: waiting to observe update in volume 08/24/23 12:05:59.594
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:06:01.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5905" for this suite. 08/24/23 12:06:01.628
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:05:57.48
    Aug 24 12:05:57.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:05:57.483
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:05:57.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:05:57.523
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-8b7567d7-2a9f-4dad-9e60-ffe4121282c8 08/24/23 12:05:57.534
    STEP: Creating the pod 08/24/23 12:05:57.54
    Aug 24 12:05:57.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b" in namespace "projected-5905" to be "running and ready"
    Aug 24 12:05:57.560: INFO: Pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274666ms
    Aug 24 12:05:57.560: INFO: The phase of Pod pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:05:59.569: INFO: Pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015087636s
    Aug 24 12:05:59.569: INFO: The phase of Pod pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b is Running (Ready = true)
    Aug 24 12:05:59.569: INFO: Pod "pod-projected-configmaps-e2ca7f0b-448b-46c0-a0b8-662cd7ed797b" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-8b7567d7-2a9f-4dad-9e60-ffe4121282c8 08/24/23 12:05:59.585
    STEP: waiting to observe update in volume 08/24/23 12:05:59.594
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:06:01.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5905" for this suite. 08/24/23 12:06:01.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:06:01.641
Aug 24 12:06:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:06:01.646
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:01.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:01.685
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/24/23 12:06:01.691
STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:06:01.699
STEP: Creating a ResourceQuota with not terminating scope 08/24/23 12:06:03.712
STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:06:03.722
STEP: Creating a long running pod 08/24/23 12:06:05.732
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/24/23 12:06:05.762
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/24/23 12:06:07.772
STEP: Deleting the pod 08/24/23 12:06:09.778
STEP: Ensuring resource quota status released the pod usage 08/24/23 12:06:09.802
STEP: Creating a terminating pod 08/24/23 12:06:11.811
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/24/23 12:06:11.833
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/24/23 12:06:13.841
STEP: Deleting the pod 08/24/23 12:06:15.851
STEP: Ensuring resource quota status released the pod usage 08/24/23 12:06:15.886
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:06:17.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-749" for this suite. 08/24/23 12:06:17.907
------------------------------
â€¢ [SLOW TEST] [16.278 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:06:01.641
    Aug 24 12:06:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:06:01.646
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:01.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:01.685
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/24/23 12:06:01.691
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:06:01.699
    STEP: Creating a ResourceQuota with not terminating scope 08/24/23 12:06:03.712
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:06:03.722
    STEP: Creating a long running pod 08/24/23 12:06:05.732
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/24/23 12:06:05.762
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/24/23 12:06:07.772
    STEP: Deleting the pod 08/24/23 12:06:09.778
    STEP: Ensuring resource quota status released the pod usage 08/24/23 12:06:09.802
    STEP: Creating a terminating pod 08/24/23 12:06:11.811
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/24/23 12:06:11.833
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/24/23 12:06:13.841
    STEP: Deleting the pod 08/24/23 12:06:15.851
    STEP: Ensuring resource quota status released the pod usage 08/24/23 12:06:15.886
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:06:17.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-749" for this suite. 08/24/23 12:06:17.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:06:17.926
Aug 24 12:06:17.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename disruption 08/24/23 12:06:17.929
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:18.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:18.006
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/24/23 12:06:18.012
STEP: Waiting for the pdb to be processed 08/24/23 12:06:18.021
STEP: updating the pdb 08/24/23 12:06:20.034
STEP: Waiting for the pdb to be processed 08/24/23 12:06:20.052
STEP: patching the pdb 08/24/23 12:06:22.067
STEP: Waiting for the pdb to be processed 08/24/23 12:06:22.084
STEP: Waiting for the pdb to be deleted 08/24/23 12:06:24.113
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:06:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3539" for this suite. 08/24/23 12:06:24.126
------------------------------
â€¢ [SLOW TEST] [6.210 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:06:17.926
    Aug 24 12:06:17.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename disruption 08/24/23 12:06:17.929
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:18.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:18.006
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/24/23 12:06:18.012
    STEP: Waiting for the pdb to be processed 08/24/23 12:06:18.021
    STEP: updating the pdb 08/24/23 12:06:20.034
    STEP: Waiting for the pdb to be processed 08/24/23 12:06:20.052
    STEP: patching the pdb 08/24/23 12:06:22.067
    STEP: Waiting for the pdb to be processed 08/24/23 12:06:22.084
    STEP: Waiting for the pdb to be deleted 08/24/23 12:06:24.113
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:06:24.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3539" for this suite. 08/24/23 12:06:24.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:06:24.15
Aug 24 12:06:24.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename aggregator 08/24/23 12:06:24.152
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:24.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:24.184
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 24 12:06:24.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/24/23 12:06:24.189
Aug 24 12:06:25.036: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 24 12:06:27.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:29.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:31.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:33.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:35.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:37.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:39.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:41.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:43.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:45.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:47.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:49.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:06:51.303: INFO: Waited 144.268942ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/24/23 12:06:51.434
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/24/23 12:06:51.444
STEP: List APIServices 08/24/23 12:06:51.466
Aug 24 12:06:51.482: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 24 12:06:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-4095" for this suite. 08/24/23 12:06:51.736
------------------------------
â€¢ [SLOW TEST] [27.606 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:06:24.15
    Aug 24 12:06:24.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename aggregator 08/24/23 12:06:24.152
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:24.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:24.184
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 24 12:06:24.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/24/23 12:06:24.189
    Aug 24 12:06:25.036: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Aug 24 12:06:27.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:29.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:31.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:33.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:35.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:37.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:39.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:41.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:43.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:45.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:47.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:49.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 6, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:06:51.303: INFO: Waited 144.268942ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/24/23 12:06:51.434
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/24/23 12:06:51.444
    STEP: List APIServices 08/24/23 12:06:51.466
    Aug 24 12:06:51.482: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:06:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-4095" for this suite. 08/24/23 12:06:51.736
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:06:51.76
Aug 24 12:06:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename events 08/24/23 12:06:51.766
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:51.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:51.836
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/24/23 12:06:51.845
Aug 24 12:06:51.855: INFO: created test-event-1
Aug 24 12:06:51.869: INFO: created test-event-2
Aug 24 12:06:51.880: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/24/23 12:06:51.88
STEP: delete collection of events 08/24/23 12:06:51.886
Aug 24 12:06:51.887: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/24/23 12:06:51.95
Aug 24 12:06:51.950: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 24 12:06:51.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7681" for this suite. 08/24/23 12:06:51.973
------------------------------
â€¢ [0.233 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:06:51.76
    Aug 24 12:06:51.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename events 08/24/23 12:06:51.766
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:51.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:51.836
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/24/23 12:06:51.845
    Aug 24 12:06:51.855: INFO: created test-event-1
    Aug 24 12:06:51.869: INFO: created test-event-2
    Aug 24 12:06:51.880: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/24/23 12:06:51.88
    STEP: delete collection of events 08/24/23 12:06:51.886
    Aug 24 12:06:51.887: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/24/23 12:06:51.95
    Aug 24 12:06:51.950: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:06:51.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7681" for this suite. 08/24/23 12:06:51.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:06:51.993
Aug 24 12:06:51.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:06:51.996
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:52.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:52.041
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/24/23 12:06:52.047
STEP: Creating a ResourceQuota 08/24/23 12:06:57.056
STEP: Ensuring resource quota status is calculated 08/24/23 12:06:57.065
STEP: Creating a Service 08/24/23 12:06:59.074
STEP: Creating a NodePort Service 08/24/23 12:06:59.109
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/24/23 12:06:59.172
STEP: Ensuring resource quota status captures service creation 08/24/23 12:06:59.245
STEP: Deleting Services 08/24/23 12:07:01.252
STEP: Ensuring resource quota status released usage 08/24/23 12:07:01.337
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:03.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-871" for this suite. 08/24/23 12:07:03.361
------------------------------
â€¢ [SLOW TEST] [11.402 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:06:51.993
    Aug 24 12:06:51.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:06:51.996
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:06:52.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:06:52.041
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/24/23 12:06:52.047
    STEP: Creating a ResourceQuota 08/24/23 12:06:57.056
    STEP: Ensuring resource quota status is calculated 08/24/23 12:06:57.065
    STEP: Creating a Service 08/24/23 12:06:59.074
    STEP: Creating a NodePort Service 08/24/23 12:06:59.109
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/24/23 12:06:59.172
    STEP: Ensuring resource quota status captures service creation 08/24/23 12:06:59.245
    STEP: Deleting Services 08/24/23 12:07:01.252
    STEP: Ensuring resource quota status released usage 08/24/23 12:07:01.337
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:03.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-871" for this suite. 08/24/23 12:07:03.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:03.404
Aug 24 12:07:03.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:07:03.408
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:03.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:03.468
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 12:07:03.501: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 12:08:03.586: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:03.594
Aug 24 12:08:03.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 12:08:03.598
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:03.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:03.65
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/24/23 12:08:03.657
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 12:08:03.657
Aug 24 12:08:03.673: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2018" to be "running"
Aug 24 12:08:03.682: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.189073ms
Aug 24 12:08:05.690: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016628331s
Aug 24 12:08:05.690: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 12:08:05.697
Aug 24 12:08:05.720: INFO: found a healthy node: pe9deep4seen-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 24 12:08:11.873: INFO: pods created so far: [1 1 1]
Aug 24 12:08:11.874: INFO: length of pods created so far: 3
Aug 24 12:08:13.893: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:20.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:20.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-2018" for this suite. 08/24/23 12:08:21.055
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3895" for this suite. 08/24/23 12:08:21.07
------------------------------
â€¢ [SLOW TEST] [77.681 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:03.404
    Aug 24 12:07:03.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:07:03.408
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:03.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:03.468
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 12:07:03.501: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 12:08:03.586: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:03.594
    Aug 24 12:08:03.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 12:08:03.598
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:03.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:03.65
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/24/23 12:08:03.657
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 12:08:03.657
    Aug 24 12:08:03.673: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2018" to be "running"
    Aug 24 12:08:03.682: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.189073ms
    Aug 24 12:08:05.690: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016628331s
    Aug 24 12:08:05.690: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 12:08:05.697
    Aug 24 12:08:05.720: INFO: found a healthy node: pe9deep4seen-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 24 12:08:11.873: INFO: pods created so far: [1 1 1]
    Aug 24 12:08:11.874: INFO: length of pods created so far: 3
    Aug 24 12:08:13.893: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:20.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:20.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-2018" for this suite. 08/24/23 12:08:21.055
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3895" for this suite. 08/24/23 12:08:21.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:21.094
Aug 24 12:08:21.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:08:21.097
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:21.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:21.136
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/24/23 12:08:21.142
Aug 24 12:08:21.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:24.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:34.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8912" for this suite. 08/24/23 12:08:34.878
------------------------------
â€¢ [SLOW TEST] [13.801 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:21.094
    Aug 24 12:08:21.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:08:21.097
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:21.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:21.136
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/24/23 12:08:21.142
    Aug 24 12:08:21.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:24.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:34.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8912" for this suite. 08/24/23 12:08:34.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:34.895
Aug 24 12:08:34.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/24/23 12:08:34.897
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:34.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:34.94
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/24/23 12:08:34.945
STEP: Creating hostNetwork=false pod 08/24/23 12:08:34.946
Aug 24 12:08:34.973: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6083" to be "running and ready"
Aug 24 12:08:34.996: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.457206ms
Aug 24 12:08:34.997: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:08:37.008: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034678038s
Aug 24 12:08:37.008: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 24 12:08:37.008: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/24/23 12:08:37.013
Aug 24 12:08:37.024: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6083" to be "running and ready"
Aug 24 12:08:37.033: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.993361ms
Aug 24 12:08:37.037: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:08:39.045: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020861261s
Aug 24 12:08:39.045: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 24 12:08:39.045: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/24/23 12:08:39.061
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/24/23 12:08:39.061
Aug 24 12:08:39.061: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.063: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.064: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 12:08:39.196: INFO: Exec stderr: ""
Aug 24 12:08:39.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.198: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.199: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 12:08:39.336: INFO: Exec stderr: ""
Aug 24 12:08:39.337: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.338: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.338: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 12:08:39.462: INFO: Exec stderr: ""
Aug 24 12:08:39.462: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.465: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.465: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 12:08:39.579: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/24/23 12:08:39.579
Aug 24 12:08:39.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.581: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.582: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 24 12:08:39.726: INFO: Exec stderr: ""
Aug 24 12:08:39.726: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.729: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.729: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 24 12:08:39.836: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/24/23 12:08:39.836
Aug 24 12:08:39.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.839: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.839: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 12:08:39.978: INFO: Exec stderr: ""
Aug 24 12:08:39.978: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:39.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:39.980: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:39.981: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 12:08:40.116: INFO: Exec stderr: ""
Aug 24 12:08:40.116: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:40.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:40.118: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:40.119: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 12:08:40.263: INFO: Exec stderr: ""
Aug 24 12:08:40.263: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:08:40.265: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:40.265: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 12:08:40.408: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:40.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6083" for this suite. 08/24/23 12:08:40.424
------------------------------
â€¢ [SLOW TEST] [5.541 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:34.895
    Aug 24 12:08:34.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/24/23 12:08:34.897
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:34.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:34.94
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/24/23 12:08:34.945
    STEP: Creating hostNetwork=false pod 08/24/23 12:08:34.946
    Aug 24 12:08:34.973: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6083" to be "running and ready"
    Aug 24 12:08:34.996: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.457206ms
    Aug 24 12:08:34.997: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:08:37.008: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034678038s
    Aug 24 12:08:37.008: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 24 12:08:37.008: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/24/23 12:08:37.013
    Aug 24 12:08:37.024: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6083" to be "running and ready"
    Aug 24 12:08:37.033: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.993361ms
    Aug 24 12:08:37.037: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:08:39.045: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020861261s
    Aug 24 12:08:39.045: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 24 12:08:39.045: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/24/23 12:08:39.061
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/24/23 12:08:39.061
    Aug 24 12:08:39.061: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.063: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.064: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 12:08:39.196: INFO: Exec stderr: ""
    Aug 24 12:08:39.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.198: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.199: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 12:08:39.336: INFO: Exec stderr: ""
    Aug 24 12:08:39.337: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.338: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.338: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 12:08:39.462: INFO: Exec stderr: ""
    Aug 24 12:08:39.462: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.465: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.465: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 12:08:39.579: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/24/23 12:08:39.579
    Aug 24 12:08:39.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.581: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.582: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 24 12:08:39.726: INFO: Exec stderr: ""
    Aug 24 12:08:39.726: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.729: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.729: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 24 12:08:39.836: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/24/23 12:08:39.836
    Aug 24 12:08:39.837: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.839: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.839: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 12:08:39.978: INFO: Exec stderr: ""
    Aug 24 12:08:39.978: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:39.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:39.980: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:39.981: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 12:08:40.116: INFO: Exec stderr: ""
    Aug 24 12:08:40.116: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:40.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:40.118: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:40.119: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 12:08:40.263: INFO: Exec stderr: ""
    Aug 24 12:08:40.263: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6083 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:40.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:08:40.265: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:40.265: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6083/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 12:08:40.408: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:40.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6083" for this suite. 08/24/23 12:08:40.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:40.45
Aug 24 12:08:40.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:08:40.452
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:40.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:40.485
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/24/23 12:08:40.49
Aug 24 12:08:40.504: INFO: Waiting up to 5m0s for pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657" in namespace "downward-api-7059" to be "running and ready"
Aug 24 12:08:40.510: INFO: Pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657": Phase="Pending", Reason="", readiness=false. Elapsed: 5.995477ms
Aug 24 12:08:40.511: INFO: The phase of Pod labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:08:42.518: INFO: Pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657": Phase="Running", Reason="", readiness=true. Elapsed: 2.013559936s
Aug 24 12:08:42.518: INFO: The phase of Pod labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657 is Running (Ready = true)
Aug 24 12:08:42.518: INFO: Pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657" satisfied condition "running and ready"
Aug 24 12:08:43.075: INFO: Successfully updated pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:45.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7059" for this suite. 08/24/23 12:08:45.136
------------------------------
â€¢ [4.699 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:40.45
    Aug 24 12:08:40.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:08:40.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:40.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:40.485
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/24/23 12:08:40.49
    Aug 24 12:08:40.504: INFO: Waiting up to 5m0s for pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657" in namespace "downward-api-7059" to be "running and ready"
    Aug 24 12:08:40.510: INFO: Pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657": Phase="Pending", Reason="", readiness=false. Elapsed: 5.995477ms
    Aug 24 12:08:40.511: INFO: The phase of Pod labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:08:42.518: INFO: Pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657": Phase="Running", Reason="", readiness=true. Elapsed: 2.013559936s
    Aug 24 12:08:42.518: INFO: The phase of Pod labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657 is Running (Ready = true)
    Aug 24 12:08:42.518: INFO: Pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657" satisfied condition "running and ready"
    Aug 24 12:08:43.075: INFO: Successfully updated pod "labelsupdate0558dfc0-d665-48e8-b8e1-da9e4894e657"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:45.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7059" for this suite. 08/24/23 12:08:45.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:45.155
Aug 24 12:08:45.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:08:45.156
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:45.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:45.191
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/24/23 12:08:45.197
STEP: Creating a ResourceQuota 08/24/23 12:08:50.204
STEP: Ensuring resource quota status is calculated 08/24/23 12:08:50.215
STEP: Creating a ReplicaSet 08/24/23 12:08:52.224
STEP: Ensuring resource quota status captures replicaset creation 08/24/23 12:08:52.25
STEP: Deleting a ReplicaSet 08/24/23 12:08:54.259
STEP: Ensuring resource quota status released usage 08/24/23 12:08:54.271
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3263" for this suite. 08/24/23 12:08:56.289
------------------------------
â€¢ [SLOW TEST] [11.149 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:45.155
    Aug 24 12:08:45.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:08:45.156
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:45.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:45.191
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/24/23 12:08:45.197
    STEP: Creating a ResourceQuota 08/24/23 12:08:50.204
    STEP: Ensuring resource quota status is calculated 08/24/23 12:08:50.215
    STEP: Creating a ReplicaSet 08/24/23 12:08:52.224
    STEP: Ensuring resource quota status captures replicaset creation 08/24/23 12:08:52.25
    STEP: Deleting a ReplicaSet 08/24/23 12:08:54.259
    STEP: Ensuring resource quota status released usage 08/24/23 12:08:54.271
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:56.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3263" for this suite. 08/24/23 12:08:56.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:56.311
Aug 24 12:08:56.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:08:56.313
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:56.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:56.349
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/24/23 12:08:56.355
STEP: Getting a ResourceQuota 08/24/23 12:08:56.364
STEP: Listing all ResourceQuotas with LabelSelector 08/24/23 12:08:56.371
STEP: Patching the ResourceQuota 08/24/23 12:08:56.379
STEP: Deleting a Collection of ResourceQuotas 08/24/23 12:08:56.394
STEP: Verifying the deleted ResourceQuota 08/24/23 12:08:56.412
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4148" for this suite. 08/24/23 12:08:56.425
------------------------------
â€¢ [0.127 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:56.311
    Aug 24 12:08:56.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:08:56.313
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:56.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:56.349
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/24/23 12:08:56.355
    STEP: Getting a ResourceQuota 08/24/23 12:08:56.364
    STEP: Listing all ResourceQuotas with LabelSelector 08/24/23 12:08:56.371
    STEP: Patching the ResourceQuota 08/24/23 12:08:56.379
    STEP: Deleting a Collection of ResourceQuotas 08/24/23 12:08:56.394
    STEP: Verifying the deleted ResourceQuota 08/24/23 12:08:56.412
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:56.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4148" for this suite. 08/24/23 12:08:56.425
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:56.44
Aug 24 12:08:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename disruption 08/24/23 12:08:56.443
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:56.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:56.491
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:56.495
Aug 24 12:08:56.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename disruption-2 08/24/23 12:08:56.496
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:56.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:56.533
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/24/23 12:08:56.545
STEP: Waiting for the pdb to be processed 08/24/23 12:08:58.563
STEP: Waiting for the pdb to be processed 08/24/23 12:09:00.587
STEP: listing a collection of PDBs across all namespaces 08/24/23 12:09:02.604
STEP: listing a collection of PDBs in namespace disruption-402 08/24/23 12:09:02.61
STEP: deleting a collection of PDBs 08/24/23 12:09:02.616
STEP: Waiting for the PDB collection to be deleted 08/24/23 12:09:02.638
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 24 12:09:02.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:09:02.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-9370" for this suite. 08/24/23 12:09:02.668
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-402" for this suite. 08/24/23 12:09:02.681
------------------------------
â€¢ [SLOW TEST] [6.253 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:56.44
    Aug 24 12:08:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename disruption 08/24/23 12:08:56.443
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:56.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:56.491
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:56.495
    Aug 24 12:08:56.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename disruption-2 08/24/23 12:08:56.496
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:56.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:56.533
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/24/23 12:08:56.545
    STEP: Waiting for the pdb to be processed 08/24/23 12:08:58.563
    STEP: Waiting for the pdb to be processed 08/24/23 12:09:00.587
    STEP: listing a collection of PDBs across all namespaces 08/24/23 12:09:02.604
    STEP: listing a collection of PDBs in namespace disruption-402 08/24/23 12:09:02.61
    STEP: deleting a collection of PDBs 08/24/23 12:09:02.616
    STEP: Waiting for the PDB collection to be deleted 08/24/23 12:09:02.638
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:09:02.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:09:02.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-9370" for this suite. 08/24/23 12:09:02.668
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-402" for this suite. 08/24/23 12:09:02.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:09:02.699
Aug 24 12:09:02.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:09:02.702
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:09:02.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:09:02.737
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/24/23 12:09:02.749
STEP: waiting for RC to be added 08/24/23 12:09:02.759
STEP: waiting for available Replicas 08/24/23 12:09:02.759
STEP: patching ReplicationController 08/24/23 12:09:04.484
STEP: waiting for RC to be modified 08/24/23 12:09:04.5
STEP: patching ReplicationController status 08/24/23 12:09:04.5
STEP: waiting for RC to be modified 08/24/23 12:09:04.515
STEP: waiting for available Replicas 08/24/23 12:09:04.515
STEP: fetching ReplicationController status 08/24/23 12:09:04.53
STEP: patching ReplicationController scale 08/24/23 12:09:04.54
STEP: waiting for RC to be modified 08/24/23 12:09:04.555
STEP: waiting for ReplicationController's scale to be the max amount 08/24/23 12:09:04.556
STEP: fetching ReplicationController; ensuring that it's patched 08/24/23 12:09:06.279
STEP: updating ReplicationController status 08/24/23 12:09:06.287
STEP: waiting for RC to be modified 08/24/23 12:09:06.296
STEP: listing all ReplicationControllers 08/24/23 12:09:06.298
STEP: checking that ReplicationController has expected values 08/24/23 12:09:06.307
STEP: deleting ReplicationControllers by collection 08/24/23 12:09:06.308
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/24/23 12:09:06.322
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:09:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6548" for this suite. 08/24/23 12:09:06.43
------------------------------
â€¢ [3.743 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:09:02.699
    Aug 24 12:09:02.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:09:02.702
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:09:02.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:09:02.737
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/24/23 12:09:02.749
    STEP: waiting for RC to be added 08/24/23 12:09:02.759
    STEP: waiting for available Replicas 08/24/23 12:09:02.759
    STEP: patching ReplicationController 08/24/23 12:09:04.484
    STEP: waiting for RC to be modified 08/24/23 12:09:04.5
    STEP: patching ReplicationController status 08/24/23 12:09:04.5
    STEP: waiting for RC to be modified 08/24/23 12:09:04.515
    STEP: waiting for available Replicas 08/24/23 12:09:04.515
    STEP: fetching ReplicationController status 08/24/23 12:09:04.53
    STEP: patching ReplicationController scale 08/24/23 12:09:04.54
    STEP: waiting for RC to be modified 08/24/23 12:09:04.555
    STEP: waiting for ReplicationController's scale to be the max amount 08/24/23 12:09:04.556
    STEP: fetching ReplicationController; ensuring that it's patched 08/24/23 12:09:06.279
    STEP: updating ReplicationController status 08/24/23 12:09:06.287
    STEP: waiting for RC to be modified 08/24/23 12:09:06.296
    STEP: listing all ReplicationControllers 08/24/23 12:09:06.298
    STEP: checking that ReplicationController has expected values 08/24/23 12:09:06.307
    STEP: deleting ReplicationControllers by collection 08/24/23 12:09:06.308
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/24/23 12:09:06.322
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:09:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6548" for this suite. 08/24/23 12:09:06.43
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:09:06.443
Aug 24 12:09:06.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:09:06.445
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:09:06.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:09:06.482
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/24/23 12:09:06.487
Aug 24 12:09:06.507: INFO: created test-pod-1
Aug 24 12:09:06.520: INFO: created test-pod-2
Aug 24 12:09:06.543: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/24/23 12:09:06.543
Aug 24 12:09:06.544: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-198' to be running and ready
Aug 24 12:09:06.569: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:09:06.569: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:09:06.569: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:09:06.569: INFO: 0 / 3 pods in namespace 'pods-198' are running and ready (0 seconds elapsed)
Aug 24 12:09:06.569: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
Aug 24 12:09:06.569: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Aug 24 12:09:06.569: INFO: test-pod-1  pe9deep4seen-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  }]
Aug 24 12:09:06.569: INFO: test-pod-2  pe9deep4seen-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  }]
Aug 24 12:09:06.570: INFO: test-pod-3  pe9deep4seen-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  }]
Aug 24 12:09:06.570: INFO: 
Aug 24 12:09:08.588: INFO: 3 / 3 pods in namespace 'pods-198' are running and ready (2 seconds elapsed)
Aug 24 12:09:08.588: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/24/23 12:09:08.623
Aug 24 12:09:08.631: INFO: Pod quantity 3 is different from expected quantity 0
Aug 24 12:09:09.639: INFO: Pod quantity 3 is different from expected quantity 0
Aug 24 12:09:10.641: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:09:11.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-198" for this suite. 08/24/23 12:09:11.647
------------------------------
â€¢ [SLOW TEST] [5.220 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:09:06.443
    Aug 24 12:09:06.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:09:06.445
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:09:06.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:09:06.482
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/24/23 12:09:06.487
    Aug 24 12:09:06.507: INFO: created test-pod-1
    Aug 24 12:09:06.520: INFO: created test-pod-2
    Aug 24 12:09:06.543: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/24/23 12:09:06.543
    Aug 24 12:09:06.544: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-198' to be running and ready
    Aug 24 12:09:06.569: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:09:06.569: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:09:06.569: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:09:06.569: INFO: 0 / 3 pods in namespace 'pods-198' are running and ready (0 seconds elapsed)
    Aug 24 12:09:06.569: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
    Aug 24 12:09:06.569: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Aug 24 12:09:06.569: INFO: test-pod-1  pe9deep4seen-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  }]
    Aug 24 12:09:06.569: INFO: test-pod-2  pe9deep4seen-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  }]
    Aug 24 12:09:06.570: INFO: test-pod-3  pe9deep4seen-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:09:06 +0000 UTC  }]
    Aug 24 12:09:06.570: INFO: 
    Aug 24 12:09:08.588: INFO: 3 / 3 pods in namespace 'pods-198' are running and ready (2 seconds elapsed)
    Aug 24 12:09:08.588: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/24/23 12:09:08.623
    Aug 24 12:09:08.631: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 24 12:09:09.639: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 24 12:09:10.641: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:09:11.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-198" for this suite. 08/24/23 12:09:11.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:09:11.681
Aug 24 12:09:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 12:09:11.686
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:09:11.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:09:11.721
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/24/23 12:09:11.732
STEP: delete the rc 08/24/23 12:09:16.995
STEP: wait for the rc to be deleted 08/24/23 12:09:17.024
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/24/23 12:09:22.297
STEP: Gathering metrics 08/24/23 12:09:52.327
Aug 24 12:09:52.359: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
Aug 24 12:09:52.365: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.609153ms
Aug 24 12:09:52.365: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
Aug 24 12:09:52.365: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
Aug 24 12:09:52.472: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 24 12:09:52.472: INFO: Deleting pod "simpletest.rc-2vc7k" in namespace "gc-4959"
Aug 24 12:09:52.528: INFO: Deleting pod "simpletest.rc-42m6h" in namespace "gc-4959"
Aug 24 12:09:52.568: INFO: Deleting pod "simpletest.rc-442wc" in namespace "gc-4959"
Aug 24 12:09:52.623: INFO: Deleting pod "simpletest.rc-44c2n" in namespace "gc-4959"
Aug 24 12:09:52.707: INFO: Deleting pod "simpletest.rc-46f8q" in namespace "gc-4959"
Aug 24 12:09:52.775: INFO: Deleting pod "simpletest.rc-46txg" in namespace "gc-4959"
Aug 24 12:09:52.863: INFO: Deleting pod "simpletest.rc-4bq76" in namespace "gc-4959"
Aug 24 12:09:52.937: INFO: Deleting pod "simpletest.rc-4qx47" in namespace "gc-4959"
Aug 24 12:09:53.011: INFO: Deleting pod "simpletest.rc-4zq54" in namespace "gc-4959"
Aug 24 12:09:53.050: INFO: Deleting pod "simpletest.rc-54tm6" in namespace "gc-4959"
Aug 24 12:09:53.129: INFO: Deleting pod "simpletest.rc-59gqq" in namespace "gc-4959"
Aug 24 12:09:53.282: INFO: Deleting pod "simpletest.rc-5l2t2" in namespace "gc-4959"
Aug 24 12:09:53.331: INFO: Deleting pod "simpletest.rc-5twlk" in namespace "gc-4959"
Aug 24 12:09:53.395: INFO: Deleting pod "simpletest.rc-659bs" in namespace "gc-4959"
Aug 24 12:09:53.475: INFO: Deleting pod "simpletest.rc-65vnp" in namespace "gc-4959"
Aug 24 12:09:53.566: INFO: Deleting pod "simpletest.rc-68hgd" in namespace "gc-4959"
Aug 24 12:09:53.709: INFO: Deleting pod "simpletest.rc-6hfd5" in namespace "gc-4959"
Aug 24 12:09:53.733: INFO: Deleting pod "simpletest.rc-6lzpm" in namespace "gc-4959"
Aug 24 12:09:53.753: INFO: Deleting pod "simpletest.rc-6mqg4" in namespace "gc-4959"
Aug 24 12:09:53.792: INFO: Deleting pod "simpletest.rc-6pdlg" in namespace "gc-4959"
Aug 24 12:09:53.877: INFO: Deleting pod "simpletest.rc-6rxx8" in namespace "gc-4959"
Aug 24 12:09:53.914: INFO: Deleting pod "simpletest.rc-6xfnl" in namespace "gc-4959"
Aug 24 12:09:53.958: INFO: Deleting pod "simpletest.rc-85pvf" in namespace "gc-4959"
Aug 24 12:09:54.019: INFO: Deleting pod "simpletest.rc-86ms7" in namespace "gc-4959"
Aug 24 12:09:54.082: INFO: Deleting pod "simpletest.rc-89ftv" in namespace "gc-4959"
Aug 24 12:09:54.198: INFO: Deleting pod "simpletest.rc-8rxvm" in namespace "gc-4959"
Aug 24 12:09:54.269: INFO: Deleting pod "simpletest.rc-8t9rv" in namespace "gc-4959"
Aug 24 12:09:54.323: INFO: Deleting pod "simpletest.rc-95sfp" in namespace "gc-4959"
Aug 24 12:09:54.371: INFO: Deleting pod "simpletest.rc-bsffz" in namespace "gc-4959"
Aug 24 12:09:54.449: INFO: Deleting pod "simpletest.rc-bzcsk" in namespace "gc-4959"
Aug 24 12:09:54.508: INFO: Deleting pod "simpletest.rc-c2zdx" in namespace "gc-4959"
Aug 24 12:09:54.558: INFO: Deleting pod "simpletest.rc-cn8mv" in namespace "gc-4959"
Aug 24 12:09:54.619: INFO: Deleting pod "simpletest.rc-cr922" in namespace "gc-4959"
Aug 24 12:09:54.730: INFO: Deleting pod "simpletest.rc-cxpck" in namespace "gc-4959"
Aug 24 12:09:54.845: INFO: Deleting pod "simpletest.rc-dj8p4" in namespace "gc-4959"
Aug 24 12:09:54.905: INFO: Deleting pod "simpletest.rc-djjjk" in namespace "gc-4959"
Aug 24 12:09:54.941: INFO: Deleting pod "simpletest.rc-dms4d" in namespace "gc-4959"
Aug 24 12:09:55.158: INFO: Deleting pod "simpletest.rc-dprgw" in namespace "gc-4959"
Aug 24 12:09:55.236: INFO: Deleting pod "simpletest.rc-drzgp" in namespace "gc-4959"
Aug 24 12:09:55.290: INFO: Deleting pod "simpletest.rc-dx79b" in namespace "gc-4959"
Aug 24 12:09:55.401: INFO: Deleting pod "simpletest.rc-f2mb6" in namespace "gc-4959"
Aug 24 12:09:55.487: INFO: Deleting pod "simpletest.rc-f4xsl" in namespace "gc-4959"
Aug 24 12:09:55.628: INFO: Deleting pod "simpletest.rc-fl2b5" in namespace "gc-4959"
Aug 24 12:09:55.735: INFO: Deleting pod "simpletest.rc-fmdgw" in namespace "gc-4959"
Aug 24 12:09:55.798: INFO: Deleting pod "simpletest.rc-fqxrn" in namespace "gc-4959"
Aug 24 12:09:55.888: INFO: Deleting pod "simpletest.rc-g4dc9" in namespace "gc-4959"
Aug 24 12:09:55.950: INFO: Deleting pod "simpletest.rc-g68vt" in namespace "gc-4959"
Aug 24 12:09:56.058: INFO: Deleting pod "simpletest.rc-gjqbw" in namespace "gc-4959"
Aug 24 12:09:56.173: INFO: Deleting pod "simpletest.rc-gsrh8" in namespace "gc-4959"
Aug 24 12:09:56.227: INFO: Deleting pod "simpletest.rc-hh7kc" in namespace "gc-4959"
Aug 24 12:09:56.313: INFO: Deleting pod "simpletest.rc-hndcc" in namespace "gc-4959"
Aug 24 12:09:56.372: INFO: Deleting pod "simpletest.rc-hv28k" in namespace "gc-4959"
Aug 24 12:09:56.488: INFO: Deleting pod "simpletest.rc-jbxmn" in namespace "gc-4959"
Aug 24 12:09:56.591: INFO: Deleting pod "simpletest.rc-jx8b8" in namespace "gc-4959"
Aug 24 12:09:56.664: INFO: Deleting pod "simpletest.rc-kbp4k" in namespace "gc-4959"
Aug 24 12:09:56.776: INFO: Deleting pod "simpletest.rc-kf8v4" in namespace "gc-4959"
Aug 24 12:09:56.901: INFO: Deleting pod "simpletest.rc-kl9pm" in namespace "gc-4959"
Aug 24 12:09:56.997: INFO: Deleting pod "simpletest.rc-l8dwr" in namespace "gc-4959"
Aug 24 12:09:57.126: INFO: Deleting pod "simpletest.rc-lcs2s" in namespace "gc-4959"
Aug 24 12:09:57.182: INFO: Deleting pod "simpletest.rc-m26gv" in namespace "gc-4959"
Aug 24 12:09:57.222: INFO: Deleting pod "simpletest.rc-m9fqg" in namespace "gc-4959"
Aug 24 12:09:57.256: INFO: Deleting pod "simpletest.rc-m9xb4" in namespace "gc-4959"
Aug 24 12:09:57.301: INFO: Deleting pod "simpletest.rc-mb62s" in namespace "gc-4959"
Aug 24 12:09:57.363: INFO: Deleting pod "simpletest.rc-nc8lf" in namespace "gc-4959"
Aug 24 12:09:57.457: INFO: Deleting pod "simpletest.rc-nnfvm" in namespace "gc-4959"
Aug 24 12:09:57.574: INFO: Deleting pod "simpletest.rc-p2ftm" in namespace "gc-4959"
Aug 24 12:09:57.668: INFO: Deleting pod "simpletest.rc-pmtbf" in namespace "gc-4959"
Aug 24 12:09:57.732: INFO: Deleting pod "simpletest.rc-ppdzg" in namespace "gc-4959"
Aug 24 12:09:57.800: INFO: Deleting pod "simpletest.rc-ppznp" in namespace "gc-4959"
Aug 24 12:09:57.843: INFO: Deleting pod "simpletest.rc-ptsdj" in namespace "gc-4959"
Aug 24 12:09:57.878: INFO: Deleting pod "simpletest.rc-q2rzm" in namespace "gc-4959"
Aug 24 12:09:58.026: INFO: Deleting pod "simpletest.rc-q2vhf" in namespace "gc-4959"
Aug 24 12:09:58.366: INFO: Deleting pod "simpletest.rc-q82mt" in namespace "gc-4959"
Aug 24 12:09:58.388: INFO: Deleting pod "simpletest.rc-qdstk" in namespace "gc-4959"
Aug 24 12:09:58.435: INFO: Deleting pod "simpletest.rc-qkgsm" in namespace "gc-4959"
Aug 24 12:09:58.496: INFO: Deleting pod "simpletest.rc-s542t" in namespace "gc-4959"
Aug 24 12:09:58.548: INFO: Deleting pod "simpletest.rc-s6zv4" in namespace "gc-4959"
Aug 24 12:09:58.655: INFO: Deleting pod "simpletest.rc-s8zs7" in namespace "gc-4959"
Aug 24 12:09:58.737: INFO: Deleting pod "simpletest.rc-smksv" in namespace "gc-4959"
Aug 24 12:09:58.779: INFO: Deleting pod "simpletest.rc-t22z9" in namespace "gc-4959"
Aug 24 12:09:58.835: INFO: Deleting pod "simpletest.rc-t9624" in namespace "gc-4959"
Aug 24 12:09:58.887: INFO: Deleting pod "simpletest.rc-tjxw9" in namespace "gc-4959"
Aug 24 12:09:58.966: INFO: Deleting pod "simpletest.rc-trwkx" in namespace "gc-4959"
Aug 24 12:09:59.144: INFO: Deleting pod "simpletest.rc-txltk" in namespace "gc-4959"
Aug 24 12:09:59.236: INFO: Deleting pod "simpletest.rc-v2wkl" in namespace "gc-4959"
Aug 24 12:09:59.273: INFO: Deleting pod "simpletest.rc-v2zrz" in namespace "gc-4959"
Aug 24 12:09:59.375: INFO: Deleting pod "simpletest.rc-vfmd6" in namespace "gc-4959"
Aug 24 12:09:59.612: INFO: Deleting pod "simpletest.rc-vg6jj" in namespace "gc-4959"
Aug 24 12:09:59.755: INFO: Deleting pod "simpletest.rc-vj9lq" in namespace "gc-4959"
Aug 24 12:09:59.870: INFO: Deleting pod "simpletest.rc-vlwxf" in namespace "gc-4959"
Aug 24 12:09:59.938: INFO: Deleting pod "simpletest.rc-w6vb4" in namespace "gc-4959"
Aug 24 12:10:00.084: INFO: Deleting pod "simpletest.rc-w9mqg" in namespace "gc-4959"
Aug 24 12:10:00.197: INFO: Deleting pod "simpletest.rc-wh8hd" in namespace "gc-4959"
Aug 24 12:10:00.267: INFO: Deleting pod "simpletest.rc-wkpc8" in namespace "gc-4959"
Aug 24 12:10:00.343: INFO: Deleting pod "simpletest.rc-wkwww" in namespace "gc-4959"
Aug 24 12:10:00.451: INFO: Deleting pod "simpletest.rc-wxblj" in namespace "gc-4959"
Aug 24 12:10:00.511: INFO: Deleting pod "simpletest.rc-xhrwf" in namespace "gc-4959"
Aug 24 12:10:00.574: INFO: Deleting pod "simpletest.rc-xmjw8" in namespace "gc-4959"
Aug 24 12:10:00.672: INFO: Deleting pod "simpletest.rc-xnkt5" in namespace "gc-4959"
Aug 24 12:10:00.719: INFO: Deleting pod "simpletest.rc-xvlk4" in namespace "gc-4959"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:00.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4959" for this suite. 08/24/23 12:10:00.911
------------------------------
â€¢ [SLOW TEST] [49.303 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:09:11.681
    Aug 24 12:09:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 12:09:11.686
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:09:11.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:09:11.721
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/24/23 12:09:11.732
    STEP: delete the rc 08/24/23 12:09:16.995
    STEP: wait for the rc to be deleted 08/24/23 12:09:17.024
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/24/23 12:09:22.297
    STEP: Gathering metrics 08/24/23 12:09:52.327
    Aug 24 12:09:52.359: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
    Aug 24 12:09:52.365: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.609153ms
    Aug 24 12:09:52.365: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
    Aug 24 12:09:52.365: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
    Aug 24 12:09:52.472: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 24 12:09:52.472: INFO: Deleting pod "simpletest.rc-2vc7k" in namespace "gc-4959"
    Aug 24 12:09:52.528: INFO: Deleting pod "simpletest.rc-42m6h" in namespace "gc-4959"
    Aug 24 12:09:52.568: INFO: Deleting pod "simpletest.rc-442wc" in namespace "gc-4959"
    Aug 24 12:09:52.623: INFO: Deleting pod "simpletest.rc-44c2n" in namespace "gc-4959"
    Aug 24 12:09:52.707: INFO: Deleting pod "simpletest.rc-46f8q" in namespace "gc-4959"
    Aug 24 12:09:52.775: INFO: Deleting pod "simpletest.rc-46txg" in namespace "gc-4959"
    Aug 24 12:09:52.863: INFO: Deleting pod "simpletest.rc-4bq76" in namespace "gc-4959"
    Aug 24 12:09:52.937: INFO: Deleting pod "simpletest.rc-4qx47" in namespace "gc-4959"
    Aug 24 12:09:53.011: INFO: Deleting pod "simpletest.rc-4zq54" in namespace "gc-4959"
    Aug 24 12:09:53.050: INFO: Deleting pod "simpletest.rc-54tm6" in namespace "gc-4959"
    Aug 24 12:09:53.129: INFO: Deleting pod "simpletest.rc-59gqq" in namespace "gc-4959"
    Aug 24 12:09:53.282: INFO: Deleting pod "simpletest.rc-5l2t2" in namespace "gc-4959"
    Aug 24 12:09:53.331: INFO: Deleting pod "simpletest.rc-5twlk" in namespace "gc-4959"
    Aug 24 12:09:53.395: INFO: Deleting pod "simpletest.rc-659bs" in namespace "gc-4959"
    Aug 24 12:09:53.475: INFO: Deleting pod "simpletest.rc-65vnp" in namespace "gc-4959"
    Aug 24 12:09:53.566: INFO: Deleting pod "simpletest.rc-68hgd" in namespace "gc-4959"
    Aug 24 12:09:53.709: INFO: Deleting pod "simpletest.rc-6hfd5" in namespace "gc-4959"
    Aug 24 12:09:53.733: INFO: Deleting pod "simpletest.rc-6lzpm" in namespace "gc-4959"
    Aug 24 12:09:53.753: INFO: Deleting pod "simpletest.rc-6mqg4" in namespace "gc-4959"
    Aug 24 12:09:53.792: INFO: Deleting pod "simpletest.rc-6pdlg" in namespace "gc-4959"
    Aug 24 12:09:53.877: INFO: Deleting pod "simpletest.rc-6rxx8" in namespace "gc-4959"
    Aug 24 12:09:53.914: INFO: Deleting pod "simpletest.rc-6xfnl" in namespace "gc-4959"
    Aug 24 12:09:53.958: INFO: Deleting pod "simpletest.rc-85pvf" in namespace "gc-4959"
    Aug 24 12:09:54.019: INFO: Deleting pod "simpletest.rc-86ms7" in namespace "gc-4959"
    Aug 24 12:09:54.082: INFO: Deleting pod "simpletest.rc-89ftv" in namespace "gc-4959"
    Aug 24 12:09:54.198: INFO: Deleting pod "simpletest.rc-8rxvm" in namespace "gc-4959"
    Aug 24 12:09:54.269: INFO: Deleting pod "simpletest.rc-8t9rv" in namespace "gc-4959"
    Aug 24 12:09:54.323: INFO: Deleting pod "simpletest.rc-95sfp" in namespace "gc-4959"
    Aug 24 12:09:54.371: INFO: Deleting pod "simpletest.rc-bsffz" in namespace "gc-4959"
    Aug 24 12:09:54.449: INFO: Deleting pod "simpletest.rc-bzcsk" in namespace "gc-4959"
    Aug 24 12:09:54.508: INFO: Deleting pod "simpletest.rc-c2zdx" in namespace "gc-4959"
    Aug 24 12:09:54.558: INFO: Deleting pod "simpletest.rc-cn8mv" in namespace "gc-4959"
    Aug 24 12:09:54.619: INFO: Deleting pod "simpletest.rc-cr922" in namespace "gc-4959"
    Aug 24 12:09:54.730: INFO: Deleting pod "simpletest.rc-cxpck" in namespace "gc-4959"
    Aug 24 12:09:54.845: INFO: Deleting pod "simpletest.rc-dj8p4" in namespace "gc-4959"
    Aug 24 12:09:54.905: INFO: Deleting pod "simpletest.rc-djjjk" in namespace "gc-4959"
    Aug 24 12:09:54.941: INFO: Deleting pod "simpletest.rc-dms4d" in namespace "gc-4959"
    Aug 24 12:09:55.158: INFO: Deleting pod "simpletest.rc-dprgw" in namespace "gc-4959"
    Aug 24 12:09:55.236: INFO: Deleting pod "simpletest.rc-drzgp" in namespace "gc-4959"
    Aug 24 12:09:55.290: INFO: Deleting pod "simpletest.rc-dx79b" in namespace "gc-4959"
    Aug 24 12:09:55.401: INFO: Deleting pod "simpletest.rc-f2mb6" in namespace "gc-4959"
    Aug 24 12:09:55.487: INFO: Deleting pod "simpletest.rc-f4xsl" in namespace "gc-4959"
    Aug 24 12:09:55.628: INFO: Deleting pod "simpletest.rc-fl2b5" in namespace "gc-4959"
    Aug 24 12:09:55.735: INFO: Deleting pod "simpletest.rc-fmdgw" in namespace "gc-4959"
    Aug 24 12:09:55.798: INFO: Deleting pod "simpletest.rc-fqxrn" in namespace "gc-4959"
    Aug 24 12:09:55.888: INFO: Deleting pod "simpletest.rc-g4dc9" in namespace "gc-4959"
    Aug 24 12:09:55.950: INFO: Deleting pod "simpletest.rc-g68vt" in namespace "gc-4959"
    Aug 24 12:09:56.058: INFO: Deleting pod "simpletest.rc-gjqbw" in namespace "gc-4959"
    Aug 24 12:09:56.173: INFO: Deleting pod "simpletest.rc-gsrh8" in namespace "gc-4959"
    Aug 24 12:09:56.227: INFO: Deleting pod "simpletest.rc-hh7kc" in namespace "gc-4959"
    Aug 24 12:09:56.313: INFO: Deleting pod "simpletest.rc-hndcc" in namespace "gc-4959"
    Aug 24 12:09:56.372: INFO: Deleting pod "simpletest.rc-hv28k" in namespace "gc-4959"
    Aug 24 12:09:56.488: INFO: Deleting pod "simpletest.rc-jbxmn" in namespace "gc-4959"
    Aug 24 12:09:56.591: INFO: Deleting pod "simpletest.rc-jx8b8" in namespace "gc-4959"
    Aug 24 12:09:56.664: INFO: Deleting pod "simpletest.rc-kbp4k" in namespace "gc-4959"
    Aug 24 12:09:56.776: INFO: Deleting pod "simpletest.rc-kf8v4" in namespace "gc-4959"
    Aug 24 12:09:56.901: INFO: Deleting pod "simpletest.rc-kl9pm" in namespace "gc-4959"
    Aug 24 12:09:56.997: INFO: Deleting pod "simpletest.rc-l8dwr" in namespace "gc-4959"
    Aug 24 12:09:57.126: INFO: Deleting pod "simpletest.rc-lcs2s" in namespace "gc-4959"
    Aug 24 12:09:57.182: INFO: Deleting pod "simpletest.rc-m26gv" in namespace "gc-4959"
    Aug 24 12:09:57.222: INFO: Deleting pod "simpletest.rc-m9fqg" in namespace "gc-4959"
    Aug 24 12:09:57.256: INFO: Deleting pod "simpletest.rc-m9xb4" in namespace "gc-4959"
    Aug 24 12:09:57.301: INFO: Deleting pod "simpletest.rc-mb62s" in namespace "gc-4959"
    Aug 24 12:09:57.363: INFO: Deleting pod "simpletest.rc-nc8lf" in namespace "gc-4959"
    Aug 24 12:09:57.457: INFO: Deleting pod "simpletest.rc-nnfvm" in namespace "gc-4959"
    Aug 24 12:09:57.574: INFO: Deleting pod "simpletest.rc-p2ftm" in namespace "gc-4959"
    Aug 24 12:09:57.668: INFO: Deleting pod "simpletest.rc-pmtbf" in namespace "gc-4959"
    Aug 24 12:09:57.732: INFO: Deleting pod "simpletest.rc-ppdzg" in namespace "gc-4959"
    Aug 24 12:09:57.800: INFO: Deleting pod "simpletest.rc-ppznp" in namespace "gc-4959"
    Aug 24 12:09:57.843: INFO: Deleting pod "simpletest.rc-ptsdj" in namespace "gc-4959"
    Aug 24 12:09:57.878: INFO: Deleting pod "simpletest.rc-q2rzm" in namespace "gc-4959"
    Aug 24 12:09:58.026: INFO: Deleting pod "simpletest.rc-q2vhf" in namespace "gc-4959"
    Aug 24 12:09:58.366: INFO: Deleting pod "simpletest.rc-q82mt" in namespace "gc-4959"
    Aug 24 12:09:58.388: INFO: Deleting pod "simpletest.rc-qdstk" in namespace "gc-4959"
    Aug 24 12:09:58.435: INFO: Deleting pod "simpletest.rc-qkgsm" in namespace "gc-4959"
    Aug 24 12:09:58.496: INFO: Deleting pod "simpletest.rc-s542t" in namespace "gc-4959"
    Aug 24 12:09:58.548: INFO: Deleting pod "simpletest.rc-s6zv4" in namespace "gc-4959"
    Aug 24 12:09:58.655: INFO: Deleting pod "simpletest.rc-s8zs7" in namespace "gc-4959"
    Aug 24 12:09:58.737: INFO: Deleting pod "simpletest.rc-smksv" in namespace "gc-4959"
    Aug 24 12:09:58.779: INFO: Deleting pod "simpletest.rc-t22z9" in namespace "gc-4959"
    Aug 24 12:09:58.835: INFO: Deleting pod "simpletest.rc-t9624" in namespace "gc-4959"
    Aug 24 12:09:58.887: INFO: Deleting pod "simpletest.rc-tjxw9" in namespace "gc-4959"
    Aug 24 12:09:58.966: INFO: Deleting pod "simpletest.rc-trwkx" in namespace "gc-4959"
    Aug 24 12:09:59.144: INFO: Deleting pod "simpletest.rc-txltk" in namespace "gc-4959"
    Aug 24 12:09:59.236: INFO: Deleting pod "simpletest.rc-v2wkl" in namespace "gc-4959"
    Aug 24 12:09:59.273: INFO: Deleting pod "simpletest.rc-v2zrz" in namespace "gc-4959"
    Aug 24 12:09:59.375: INFO: Deleting pod "simpletest.rc-vfmd6" in namespace "gc-4959"
    Aug 24 12:09:59.612: INFO: Deleting pod "simpletest.rc-vg6jj" in namespace "gc-4959"
    Aug 24 12:09:59.755: INFO: Deleting pod "simpletest.rc-vj9lq" in namespace "gc-4959"
    Aug 24 12:09:59.870: INFO: Deleting pod "simpletest.rc-vlwxf" in namespace "gc-4959"
    Aug 24 12:09:59.938: INFO: Deleting pod "simpletest.rc-w6vb4" in namespace "gc-4959"
    Aug 24 12:10:00.084: INFO: Deleting pod "simpletest.rc-w9mqg" in namespace "gc-4959"
    Aug 24 12:10:00.197: INFO: Deleting pod "simpletest.rc-wh8hd" in namespace "gc-4959"
    Aug 24 12:10:00.267: INFO: Deleting pod "simpletest.rc-wkpc8" in namespace "gc-4959"
    Aug 24 12:10:00.343: INFO: Deleting pod "simpletest.rc-wkwww" in namespace "gc-4959"
    Aug 24 12:10:00.451: INFO: Deleting pod "simpletest.rc-wxblj" in namespace "gc-4959"
    Aug 24 12:10:00.511: INFO: Deleting pod "simpletest.rc-xhrwf" in namespace "gc-4959"
    Aug 24 12:10:00.574: INFO: Deleting pod "simpletest.rc-xmjw8" in namespace "gc-4959"
    Aug 24 12:10:00.672: INFO: Deleting pod "simpletest.rc-xnkt5" in namespace "gc-4959"
    Aug 24 12:10:00.719: INFO: Deleting pod "simpletest.rc-xvlk4" in namespace "gc-4959"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:00.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4959" for this suite. 08/24/23 12:10:00.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:00.986
Aug 24 12:10:00.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 12:10:00.991
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:01.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:01.203
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 24 12:10:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:07.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1007" for this suite. 08/24/23 12:10:07.752
------------------------------
â€¢ [SLOW TEST] [6.782 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:00.986
    Aug 24 12:10:00.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 12:10:00.991
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:01.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:01.203
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 24 12:10:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:07.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1007" for this suite. 08/24/23 12:10:07.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:07.778
Aug 24 12:10:07.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:10:07.781
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:07.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:07.833
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:10:07.868
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:10:08.879
STEP: Deploying the webhook pod 08/24/23 12:10:08.898
STEP: Wait for the deployment to be ready 08/24/23 12:10:08.945
Aug 24 12:10:08.958: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 12:10:10.98
STEP: Verifying the service has paired with the endpoint 08/24/23 12:10:11.044
Aug 24 12:10:12.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 24 12:10:12.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/24/23 12:10:12.586
Aug 24 12:10:12.623: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook 08/24/23 12:10:12.775
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/24/23 12:10:14.933
STEP: Updating the custom resource with disallowed data should be denied 08/24/23 12:10:14.95
STEP: Deleting the custom resource should be denied 08/24/23 12:10:14.972
STEP: Remove the offending key and value from the custom resource data 08/24/23 12:10:15
STEP: Deleting the updated custom resource should be successful 08/24/23 12:10:15.031
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:15.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-229" for this suite. 08/24/23 12:10:15.739
STEP: Destroying namespace "webhook-229-markers" for this suite. 08/24/23 12:10:15.755
------------------------------
â€¢ [SLOW TEST] [8.003 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:07.778
    Aug 24 12:10:07.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:10:07.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:07.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:07.833
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:10:07.868
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:10:08.879
    STEP: Deploying the webhook pod 08/24/23 12:10:08.898
    STEP: Wait for the deployment to be ready 08/24/23 12:10:08.945
    Aug 24 12:10:08.958: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 12:10:10.98
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:10:11.044
    Aug 24 12:10:12.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 24 12:10:12.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/24/23 12:10:12.586
    Aug 24 12:10:12.623: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource that should be denied by the webhook 08/24/23 12:10:12.775
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/24/23 12:10:14.933
    STEP: Updating the custom resource with disallowed data should be denied 08/24/23 12:10:14.95
    STEP: Deleting the custom resource should be denied 08/24/23 12:10:14.972
    STEP: Remove the offending key and value from the custom resource data 08/24/23 12:10:15
    STEP: Deleting the updated custom resource should be successful 08/24/23 12:10:15.031
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:15.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-229" for this suite. 08/24/23 12:10:15.739
    STEP: Destroying namespace "webhook-229-markers" for this suite. 08/24/23 12:10:15.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:15.801
Aug 24 12:10:15.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:10:15.81
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:15.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:15.863
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:10:15.87
Aug 24 12:10:15.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513" in namespace "downward-api-668" to be "Succeeded or Failed"
Aug 24 12:10:15.906: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513": Phase="Pending", Reason="", readiness=false. Elapsed: 11.44222ms
Aug 24 12:10:17.913: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018472001s
Aug 24 12:10:19.915: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02064352s
STEP: Saw pod success 08/24/23 12:10:19.916
Aug 24 12:10:19.916: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513" satisfied condition "Succeeded or Failed"
Aug 24 12:10:19.922: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513 container client-container: <nil>
STEP: delete the pod 08/24/23 12:10:19.958
Aug 24 12:10:19.980: INFO: Waiting for pod downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513 to disappear
Aug 24 12:10:19.988: INFO: Pod downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:19.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-668" for this suite. 08/24/23 12:10:20.002
------------------------------
â€¢ [4.217 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:15.801
    Aug 24 12:10:15.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:10:15.81
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:15.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:15.863
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:10:15.87
    Aug 24 12:10:15.895: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513" in namespace "downward-api-668" to be "Succeeded or Failed"
    Aug 24 12:10:15.906: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513": Phase="Pending", Reason="", readiness=false. Elapsed: 11.44222ms
    Aug 24 12:10:17.913: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018472001s
    Aug 24 12:10:19.915: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02064352s
    STEP: Saw pod success 08/24/23 12:10:19.916
    Aug 24 12:10:19.916: INFO: Pod "downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513" satisfied condition "Succeeded or Failed"
    Aug 24 12:10:19.922: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:10:19.958
    Aug 24 12:10:19.980: INFO: Waiting for pod downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513 to disappear
    Aug 24 12:10:19.988: INFO: Pod downwardapi-volume-6e77840e-5db4-4799-86aa-6928176a6513 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:19.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-668" for this suite. 08/24/23 12:10:20.002
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:20.019
Aug 24 12:10:20.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:10:20.021
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:20.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:20.058
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/24/23 12:10:20.064
Aug 24 12:10:20.087: INFO: Waiting up to 5m0s for pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774" in namespace "projected-2961" to be "running and ready"
Aug 24 12:10:20.107: INFO: Pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774": Phase="Pending", Reason="", readiness=false. Elapsed: 19.376797ms
Aug 24 12:10:20.107: INFO: The phase of Pod labelsupdateff6228dd-9f6f-461d-9375-14130cdad774 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:10:22.114: INFO: Pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774": Phase="Running", Reason="", readiness=true. Elapsed: 2.026925453s
Aug 24 12:10:22.115: INFO: The phase of Pod labelsupdateff6228dd-9f6f-461d-9375-14130cdad774 is Running (Ready = true)
Aug 24 12:10:22.115: INFO: Pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774" satisfied condition "running and ready"
Aug 24 12:10:22.658: INFO: Successfully updated pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:26.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2961" for this suite. 08/24/23 12:10:26.721
------------------------------
â€¢ [SLOW TEST] [6.715 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:20.019
    Aug 24 12:10:20.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:10:20.021
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:20.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:20.058
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/24/23 12:10:20.064
    Aug 24 12:10:20.087: INFO: Waiting up to 5m0s for pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774" in namespace "projected-2961" to be "running and ready"
    Aug 24 12:10:20.107: INFO: Pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774": Phase="Pending", Reason="", readiness=false. Elapsed: 19.376797ms
    Aug 24 12:10:20.107: INFO: The phase of Pod labelsupdateff6228dd-9f6f-461d-9375-14130cdad774 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:10:22.114: INFO: Pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774": Phase="Running", Reason="", readiness=true. Elapsed: 2.026925453s
    Aug 24 12:10:22.115: INFO: The phase of Pod labelsupdateff6228dd-9f6f-461d-9375-14130cdad774 is Running (Ready = true)
    Aug 24 12:10:22.115: INFO: Pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774" satisfied condition "running and ready"
    Aug 24 12:10:22.658: INFO: Successfully updated pod "labelsupdateff6228dd-9f6f-461d-9375-14130cdad774"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:26.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2961" for this suite. 08/24/23 12:10:26.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:26.737
Aug 24 12:10:26.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:10:26.738
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:26.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:26.785
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-875393fe-8068-4b0c-a6d0-a52afdfaacc7 08/24/23 12:10:26.793
STEP: Creating a pod to test consume configMaps 08/24/23 12:10:26.803
Aug 24 12:10:26.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305" in namespace "projected-3007" to be "Succeeded or Failed"
Aug 24 12:10:26.825: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305": Phase="Pending", Reason="", readiness=false. Elapsed: 6.574206ms
Aug 24 12:10:28.833: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015224556s
Aug 24 12:10:30.832: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013571196s
STEP: Saw pod success 08/24/23 12:10:30.832
Aug 24 12:10:30.832: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305" satisfied condition "Succeeded or Failed"
Aug 24 12:10:30.837: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:10:30.849
Aug 24 12:10:30.876: INFO: Waiting for pod pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305 to disappear
Aug 24 12:10:30.881: INFO: Pod pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:30.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3007" for this suite. 08/24/23 12:10:30.891
------------------------------
â€¢ [4.166 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:26.737
    Aug 24 12:10:26.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:10:26.738
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:26.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:26.785
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-875393fe-8068-4b0c-a6d0-a52afdfaacc7 08/24/23 12:10:26.793
    STEP: Creating a pod to test consume configMaps 08/24/23 12:10:26.803
    Aug 24 12:10:26.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305" in namespace "projected-3007" to be "Succeeded or Failed"
    Aug 24 12:10:26.825: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305": Phase="Pending", Reason="", readiness=false. Elapsed: 6.574206ms
    Aug 24 12:10:28.833: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015224556s
    Aug 24 12:10:30.832: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013571196s
    STEP: Saw pod success 08/24/23 12:10:30.832
    Aug 24 12:10:30.832: INFO: Pod "pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305" satisfied condition "Succeeded or Failed"
    Aug 24 12:10:30.837: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:10:30.849
    Aug 24 12:10:30.876: INFO: Waiting for pod pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305 to disappear
    Aug 24 12:10:30.881: INFO: Pod pod-projected-configmaps-3c96dcae-0bd7-4179-ac7d-0f368cf99305 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:30.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3007" for this suite. 08/24/23 12:10:30.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:30.905
Aug 24 12:10:30.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sysctl 08/24/23 12:10:30.907
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:30.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:30.959
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/24/23 12:10:30.966
STEP: Watching for error events or started pod 08/24/23 12:10:30.993
STEP: Waiting for pod completion 08/24/23 12:10:33.002
Aug 24 12:10:33.002: INFO: Waiting up to 3m0s for pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee" in namespace "sysctl-347" to be "completed"
Aug 24 12:10:33.008: INFO: Pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.797782ms
Aug 24 12:10:35.017: INFO: Pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014589688s
Aug 24 12:10:35.017: INFO: Pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/24/23 12:10:35.038
STEP: Getting logs from the pod 08/24/23 12:10:35.043
STEP: Checking that the sysctl is actually updated 08/24/23 12:10:35.06
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:35.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-347" for this suite. 08/24/23 12:10:35.071
------------------------------
â€¢ [4.187 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:30.905
    Aug 24 12:10:30.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sysctl 08/24/23 12:10:30.907
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:30.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:30.959
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/24/23 12:10:30.966
    STEP: Watching for error events or started pod 08/24/23 12:10:30.993
    STEP: Waiting for pod completion 08/24/23 12:10:33.002
    Aug 24 12:10:33.002: INFO: Waiting up to 3m0s for pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee" in namespace "sysctl-347" to be "completed"
    Aug 24 12:10:33.008: INFO: Pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.797782ms
    Aug 24 12:10:35.017: INFO: Pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014589688s
    Aug 24 12:10:35.017: INFO: Pod "sysctl-c799d591-a6fc-4a5c-9f38-2ad632f76aee" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/24/23 12:10:35.038
    STEP: Getting logs from the pod 08/24/23 12:10:35.043
    STEP: Checking that the sysctl is actually updated 08/24/23 12:10:35.06
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:35.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-347" for this suite. 08/24/23 12:10:35.071
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:35.094
Aug 24 12:10:35.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:10:35.097
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:35.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:35.141
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-9a830cdb-b963-4e4f-a541-e82895b71e18 08/24/23 12:10:35.147
STEP: Creating a pod to test consume configMaps 08/24/23 12:10:35.156
Aug 24 12:10:35.173: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7" in namespace "projected-8096" to be "Succeeded or Failed"
Aug 24 12:10:35.180: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.677733ms
Aug 24 12:10:37.189: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507401s
Aug 24 12:10:39.187: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014003534s
STEP: Saw pod success 08/24/23 12:10:39.188
Aug 24 12:10:39.188: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7" satisfied condition "Succeeded or Failed"
Aug 24 12:10:39.198: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:10:39.215
Aug 24 12:10:39.245: INFO: Waiting for pod pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7 to disappear
Aug 24 12:10:39.253: INFO: Pod pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:39.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8096" for this suite. 08/24/23 12:10:39.262
------------------------------
â€¢ [4.182 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:35.094
    Aug 24 12:10:35.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:10:35.097
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:35.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:35.141
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-9a830cdb-b963-4e4f-a541-e82895b71e18 08/24/23 12:10:35.147
    STEP: Creating a pod to test consume configMaps 08/24/23 12:10:35.156
    Aug 24 12:10:35.173: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7" in namespace "projected-8096" to be "Succeeded or Failed"
    Aug 24 12:10:35.180: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.677733ms
    Aug 24 12:10:37.189: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015507401s
    Aug 24 12:10:39.187: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014003534s
    STEP: Saw pod success 08/24/23 12:10:39.188
    Aug 24 12:10:39.188: INFO: Pod "pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7" satisfied condition "Succeeded or Failed"
    Aug 24 12:10:39.198: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:10:39.215
    Aug 24 12:10:39.245: INFO: Waiting for pod pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7 to disappear
    Aug 24 12:10:39.253: INFO: Pod pod-projected-configmaps-75d882e1-3c85-4fc7-9288-d1df276836b7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:39.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8096" for this suite. 08/24/23 12:10:39.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:39.28
Aug 24 12:10:39.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 12:10:39.282
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:39.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:39.317
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-927c9f08-887b-45a0-bda2-115bcb16d40a in namespace container-probe-165 08/24/23 12:10:39.323
Aug 24 12:10:39.337: INFO: Waiting up to 5m0s for pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a" in namespace "container-probe-165" to be "not pending"
Aug 24 12:10:39.343: INFO: Pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.107688ms
Aug 24 12:10:41.349: INFO: Pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011671102s
Aug 24 12:10:41.349: INFO: Pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a" satisfied condition "not pending"
Aug 24 12:10:41.349: INFO: Started pod liveness-927c9f08-887b-45a0-bda2-115bcb16d40a in namespace container-probe-165
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 12:10:41.349
Aug 24 12:10:41.355: INFO: Initial restart count of pod liveness-927c9f08-887b-45a0-bda2-115bcb16d40a is 0
Aug 24 12:11:01.513: INFO: Restart count of pod container-probe-165/liveness-927c9f08-887b-45a0-bda2-115bcb16d40a is now 1 (20.157583514s elapsed)
STEP: deleting the pod 08/24/23 12:11:01.513
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:01.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-165" for this suite. 08/24/23 12:11:01.551
------------------------------
â€¢ [SLOW TEST] [22.284 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:39.28
    Aug 24 12:10:39.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 12:10:39.282
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:39.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:39.317
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-927c9f08-887b-45a0-bda2-115bcb16d40a in namespace container-probe-165 08/24/23 12:10:39.323
    Aug 24 12:10:39.337: INFO: Waiting up to 5m0s for pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a" in namespace "container-probe-165" to be "not pending"
    Aug 24 12:10:39.343: INFO: Pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.107688ms
    Aug 24 12:10:41.349: INFO: Pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011671102s
    Aug 24 12:10:41.349: INFO: Pod "liveness-927c9f08-887b-45a0-bda2-115bcb16d40a" satisfied condition "not pending"
    Aug 24 12:10:41.349: INFO: Started pod liveness-927c9f08-887b-45a0-bda2-115bcb16d40a in namespace container-probe-165
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 12:10:41.349
    Aug 24 12:10:41.355: INFO: Initial restart count of pod liveness-927c9f08-887b-45a0-bda2-115bcb16d40a is 0
    Aug 24 12:11:01.513: INFO: Restart count of pod container-probe-165/liveness-927c9f08-887b-45a0-bda2-115bcb16d40a is now 1 (20.157583514s elapsed)
    STEP: deleting the pod 08/24/23 12:11:01.513
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:01.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-165" for this suite. 08/24/23 12:11:01.551
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:01.565
Aug 24 12:11:01.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename cronjob 08/24/23 12:11:01.568
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:01.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:01.606
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/24/23 12:11:01.611
STEP: Ensuring more than one job is running at a time 08/24/23 12:11:01.623
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/24/23 12:13:01.632
STEP: Removing cronjob 08/24/23 12:13:01.64
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 12:13:01.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2294" for this suite. 08/24/23 12:13:01.667
------------------------------
â€¢ [SLOW TEST] [120.114 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:01.565
    Aug 24 12:11:01.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename cronjob 08/24/23 12:11:01.568
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:01.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:01.606
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/24/23 12:11:01.611
    STEP: Ensuring more than one job is running at a time 08/24/23 12:11:01.623
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/24/23 12:13:01.632
    STEP: Removing cronjob 08/24/23 12:13:01.64
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:13:01.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2294" for this suite. 08/24/23 12:13:01.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:13:01.687
Aug 24 12:13:01.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:13:01.693
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:01.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:01.749
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/24/23 12:13:01.763
STEP: Getting a ResourceQuota 08/24/23 12:13:01.785
STEP: Updating a ResourceQuota 08/24/23 12:13:01.792
STEP: Verifying a ResourceQuota was modified 08/24/23 12:13:01.804
STEP: Deleting a ResourceQuota 08/24/23 12:13:01.812
STEP: Verifying the deleted ResourceQuota 08/24/23 12:13:01.824
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:13:01.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3" for this suite. 08/24/23 12:13:01.837
------------------------------
â€¢ [0.167 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:13:01.687
    Aug 24 12:13:01.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:13:01.693
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:01.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:01.749
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/24/23 12:13:01.763
    STEP: Getting a ResourceQuota 08/24/23 12:13:01.785
    STEP: Updating a ResourceQuota 08/24/23 12:13:01.792
    STEP: Verifying a ResourceQuota was modified 08/24/23 12:13:01.804
    STEP: Deleting a ResourceQuota 08/24/23 12:13:01.812
    STEP: Verifying the deleted ResourceQuota 08/24/23 12:13:01.824
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:13:01.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3" for this suite. 08/24/23 12:13:01.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:13:01.859
Aug 24 12:13:01.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:13:01.864
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:01.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:01.907
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1080 08/24/23 12:13:01.913
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 12:13:01.933
STEP: creating service externalsvc in namespace services-1080 08/24/23 12:13:01.933
STEP: creating replication controller externalsvc in namespace services-1080 08/24/23 12:13:01.962
I0824 12:13:01.978980      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1080, replica count: 2
I0824 12:13:05.030625      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/24/23 12:13:05.039
Aug 24 12:13:05.064: INFO: Creating new exec pod
Aug 24 12:13:05.088: INFO: Waiting up to 5m0s for pod "execpodws98s" in namespace "services-1080" to be "running"
Aug 24 12:13:05.097: INFO: Pod "execpodws98s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.673836ms
Aug 24 12:13:07.108: INFO: Pod "execpodws98s": Phase="Running", Reason="", readiness=true. Elapsed: 2.019625278s
Aug 24 12:13:07.108: INFO: Pod "execpodws98s" satisfied condition "running"
Aug 24 12:13:07.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1080 exec execpodws98s -- /bin/sh -x -c nslookup clusterip-service.services-1080.svc.cluster.local'
Aug 24 12:13:07.568: INFO: stderr: "+ nslookup clusterip-service.services-1080.svc.cluster.local\n"
Aug 24 12:13:07.568: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-1080.svc.cluster.local\tcanonical name = externalsvc.services-1080.svc.cluster.local.\nName:\texternalsvc.services-1080.svc.cluster.local\nAddress: 10.233.15.56\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1080, will wait for the garbage collector to delete the pods 08/24/23 12:13:07.568
Aug 24 12:13:07.638: INFO: Deleting ReplicationController externalsvc took: 13.236209ms
Aug 24 12:13:07.741: INFO: Terminating ReplicationController externalsvc pods took: 102.779854ms
Aug 24 12:13:09.482: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:13:09.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1080" for this suite. 08/24/23 12:13:09.523
------------------------------
â€¢ [SLOW TEST] [7.678 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:13:01.859
    Aug 24 12:13:01.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:13:01.864
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:01.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:01.907
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1080 08/24/23 12:13:01.913
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 12:13:01.933
    STEP: creating service externalsvc in namespace services-1080 08/24/23 12:13:01.933
    STEP: creating replication controller externalsvc in namespace services-1080 08/24/23 12:13:01.962
    I0824 12:13:01.978980      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1080, replica count: 2
    I0824 12:13:05.030625      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/24/23 12:13:05.039
    Aug 24 12:13:05.064: INFO: Creating new exec pod
    Aug 24 12:13:05.088: INFO: Waiting up to 5m0s for pod "execpodws98s" in namespace "services-1080" to be "running"
    Aug 24 12:13:05.097: INFO: Pod "execpodws98s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.673836ms
    Aug 24 12:13:07.108: INFO: Pod "execpodws98s": Phase="Running", Reason="", readiness=true. Elapsed: 2.019625278s
    Aug 24 12:13:07.108: INFO: Pod "execpodws98s" satisfied condition "running"
    Aug 24 12:13:07.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1080 exec execpodws98s -- /bin/sh -x -c nslookup clusterip-service.services-1080.svc.cluster.local'
    Aug 24 12:13:07.568: INFO: stderr: "+ nslookup clusterip-service.services-1080.svc.cluster.local\n"
    Aug 24 12:13:07.568: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-1080.svc.cluster.local\tcanonical name = externalsvc.services-1080.svc.cluster.local.\nName:\texternalsvc.services-1080.svc.cluster.local\nAddress: 10.233.15.56\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1080, will wait for the garbage collector to delete the pods 08/24/23 12:13:07.568
    Aug 24 12:13:07.638: INFO: Deleting ReplicationController externalsvc took: 13.236209ms
    Aug 24 12:13:07.741: INFO: Terminating ReplicationController externalsvc pods took: 102.779854ms
    Aug 24 12:13:09.482: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:13:09.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1080" for this suite. 08/24/23 12:13:09.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:13:09.541
Aug 24 12:13:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:13:09.548
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:09.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:09.587
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-b6c32e62-a20e-4daa-9c7c-64ad10551867 08/24/23 12:13:09.591
STEP: Creating a pod to test consume configMaps 08/24/23 12:13:09.601
Aug 24 12:13:09.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde" in namespace "configmap-1119" to be "Succeeded or Failed"
Aug 24 12:13:09.635: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde": Phase="Pending", Reason="", readiness=false. Elapsed: 13.038741ms
Aug 24 12:13:11.644: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022037673s
Aug 24 12:13:13.644: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022357106s
STEP: Saw pod success 08/24/23 12:13:13.644
Aug 24 12:13:13.645: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde" satisfied condition "Succeeded or Failed"
Aug 24 12:13:13.652: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde container configmap-volume-test: <nil>
STEP: delete the pod 08/24/23 12:13:13.683
Aug 24 12:13:13.704: INFO: Waiting for pod pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde to disappear
Aug 24 12:13:13.709: INFO: Pod pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:13:13.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1119" for this suite. 08/24/23 12:13:13.717
------------------------------
â€¢ [4.189 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:13:09.541
    Aug 24 12:13:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:13:09.548
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:09.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:09.587
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-b6c32e62-a20e-4daa-9c7c-64ad10551867 08/24/23 12:13:09.591
    STEP: Creating a pod to test consume configMaps 08/24/23 12:13:09.601
    Aug 24 12:13:09.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde" in namespace "configmap-1119" to be "Succeeded or Failed"
    Aug 24 12:13:09.635: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde": Phase="Pending", Reason="", readiness=false. Elapsed: 13.038741ms
    Aug 24 12:13:11.644: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022037673s
    Aug 24 12:13:13.644: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022357106s
    STEP: Saw pod success 08/24/23 12:13:13.644
    Aug 24 12:13:13.645: INFO: Pod "pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde" satisfied condition "Succeeded or Failed"
    Aug 24 12:13:13.652: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde container configmap-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:13:13.683
    Aug 24 12:13:13.704: INFO: Waiting for pod pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde to disappear
    Aug 24 12:13:13.709: INFO: Pod pod-configmaps-665919d4-536d-46cd-bb92-107a64b92bde no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:13:13.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1119" for this suite. 08/24/23 12:13:13.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:13:13.733
Aug 24 12:13:13.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:13:13.735
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:13.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:13.766
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-31bf844d-597a-454b-9424-ac47410c966f 08/24/23 12:13:13.77
STEP: Creating a pod to test consume secrets 08/24/23 12:13:13.778
Aug 24 12:13:13.791: INFO: Waiting up to 5m0s for pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f" in namespace "secrets-4426" to be "Succeeded or Failed"
Aug 24 12:13:13.798: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.9004ms
Aug 24 12:13:15.809: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018114195s
Aug 24 12:13:17.811: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020905295s
STEP: Saw pod success 08/24/23 12:13:17.812
Aug 24 12:13:17.812: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f" satisfied condition "Succeeded or Failed"
Aug 24 12:13:17.822: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f container secret-env-test: <nil>
STEP: delete the pod 08/24/23 12:13:17.842
Aug 24 12:13:17.870: INFO: Waiting for pod pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f to disappear
Aug 24 12:13:17.875: INFO: Pod pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:13:17.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4426" for this suite. 08/24/23 12:13:17.884
------------------------------
â€¢ [4.166 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:13:13.733
    Aug 24 12:13:13.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:13:13.735
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:13.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:13.766
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-31bf844d-597a-454b-9424-ac47410c966f 08/24/23 12:13:13.77
    STEP: Creating a pod to test consume secrets 08/24/23 12:13:13.778
    Aug 24 12:13:13.791: INFO: Waiting up to 5m0s for pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f" in namespace "secrets-4426" to be "Succeeded or Failed"
    Aug 24 12:13:13.798: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.9004ms
    Aug 24 12:13:15.809: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018114195s
    Aug 24 12:13:17.811: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020905295s
    STEP: Saw pod success 08/24/23 12:13:17.812
    Aug 24 12:13:17.812: INFO: Pod "pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f" satisfied condition "Succeeded or Failed"
    Aug 24 12:13:17.822: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f container secret-env-test: <nil>
    STEP: delete the pod 08/24/23 12:13:17.842
    Aug 24 12:13:17.870: INFO: Waiting for pod pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f to disappear
    Aug 24 12:13:17.875: INFO: Pod pod-secrets-d3af6ef2-c97c-4f52-afef-da0fc1ff5d9f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:13:17.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4426" for this suite. 08/24/23 12:13:17.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:13:17.903
Aug 24 12:13:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:13:17.906
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:17.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:17.946
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 24 12:13:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 12:13:20.633
Aug 24 12:13:20.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 create -f -'
Aug 24 12:13:22.056: INFO: stderr: ""
Aug 24 12:13:22.056: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 24 12:13:22.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 delete e2e-test-crd-publish-openapi-9137-crds test-cr'
Aug 24 12:13:22.235: INFO: stderr: ""
Aug 24 12:13:22.235: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 24 12:13:22.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 apply -f -'
Aug 24 12:13:22.675: INFO: stderr: ""
Aug 24 12:13:22.675: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 24 12:13:22.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 delete e2e-test-crd-publish-openapi-9137-crds test-cr'
Aug 24 12:13:22.814: INFO: stderr: ""
Aug 24 12:13:22.814: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/24/23 12:13:22.814
Aug 24 12:13:22.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 explain e2e-test-crd-publish-openapi-9137-crds'
Aug 24 12:13:24.256: INFO: stderr: ""
Aug 24 12:13:24.256: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9137-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:13:26.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1046" for this suite. 08/24/23 12:13:26.819
------------------------------
â€¢ [SLOW TEST] [8.927 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:13:17.903
    Aug 24 12:13:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:13:17.906
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:17.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:17.946
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 24 12:13:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 12:13:20.633
    Aug 24 12:13:20.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 create -f -'
    Aug 24 12:13:22.056: INFO: stderr: ""
    Aug 24 12:13:22.056: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 24 12:13:22.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 delete e2e-test-crd-publish-openapi-9137-crds test-cr'
    Aug 24 12:13:22.235: INFO: stderr: ""
    Aug 24 12:13:22.235: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 24 12:13:22.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 apply -f -'
    Aug 24 12:13:22.675: INFO: stderr: ""
    Aug 24 12:13:22.675: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 24 12:13:22.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 --namespace=crd-publish-openapi-1046 delete e2e-test-crd-publish-openapi-9137-crds test-cr'
    Aug 24 12:13:22.814: INFO: stderr: ""
    Aug 24 12:13:22.814: INFO: stdout: "e2e-test-crd-publish-openapi-9137-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/24/23 12:13:22.814
    Aug 24 12:13:22.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-1046 explain e2e-test-crd-publish-openapi-9137-crds'
    Aug 24 12:13:24.256: INFO: stderr: ""
    Aug 24 12:13:24.256: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9137-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:13:26.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1046" for this suite. 08/24/23 12:13:26.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:13:26.838
Aug 24 12:13:26.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 12:13:26.84
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:26.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:26.872
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3788 08/24/23 12:13:26.876
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/24/23 12:13:26.884
Aug 24 12:13:26.917: INFO: Found 0 stateful pods, waiting for 3
Aug 24 12:13:36.930: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:13:36.930: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:13:36.930: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 12:13:36.949
Aug 24 12:13:36.976: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/24/23 12:13:36.976
STEP: Not applying an update when the partition is greater than the number of replicas 08/24/23 12:13:47.058
STEP: Performing a canary update 08/24/23 12:13:47.059
Aug 24 12:13:47.093: INFO: Updating stateful set ss2
Aug 24 12:13:47.122: INFO: Waiting for Pod statefulset-3788/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/24/23 12:13:57.138
Aug 24 12:13:57.214: INFO: Found 1 stateful pods, waiting for 3
Aug 24 12:14:07.225: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:14:07.225: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:14:07.225: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/24/23 12:14:07.239
Aug 24 12:14:07.267: INFO: Updating stateful set ss2
Aug 24 12:14:07.344: INFO: Waiting for Pod statefulset-3788/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 12:14:17.393: INFO: Updating stateful set ss2
Aug 24 12:14:17.412: INFO: Waiting for StatefulSet statefulset-3788/ss2 to complete update
Aug 24 12:14:17.413: INFO: Waiting for Pod statefulset-3788/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:14:27.445: INFO: Deleting all statefulset in ns statefulset-3788
Aug 24 12:14:27.452: INFO: Scaling statefulset ss2 to 0
Aug 24 12:14:37.490: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:14:37.497: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:14:37.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3788" for this suite. 08/24/23 12:14:37.551
------------------------------
â€¢ [SLOW TEST] [70.727 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:13:26.838
    Aug 24 12:13:26.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:13:26.84
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:13:26.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:13:26.872
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3788 08/24/23 12:13:26.876
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/24/23 12:13:26.884
    Aug 24 12:13:26.917: INFO: Found 0 stateful pods, waiting for 3
    Aug 24 12:13:36.930: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:13:36.930: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:13:36.930: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 12:13:36.949
    Aug 24 12:13:36.976: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/24/23 12:13:36.976
    STEP: Not applying an update when the partition is greater than the number of replicas 08/24/23 12:13:47.058
    STEP: Performing a canary update 08/24/23 12:13:47.059
    Aug 24 12:13:47.093: INFO: Updating stateful set ss2
    Aug 24 12:13:47.122: INFO: Waiting for Pod statefulset-3788/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/24/23 12:13:57.138
    Aug 24 12:13:57.214: INFO: Found 1 stateful pods, waiting for 3
    Aug 24 12:14:07.225: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:14:07.225: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:14:07.225: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/24/23 12:14:07.239
    Aug 24 12:14:07.267: INFO: Updating stateful set ss2
    Aug 24 12:14:07.344: INFO: Waiting for Pod statefulset-3788/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 12:14:17.393: INFO: Updating stateful set ss2
    Aug 24 12:14:17.412: INFO: Waiting for StatefulSet statefulset-3788/ss2 to complete update
    Aug 24 12:14:17.413: INFO: Waiting for Pod statefulset-3788/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:14:27.445: INFO: Deleting all statefulset in ns statefulset-3788
    Aug 24 12:14:27.452: INFO: Scaling statefulset ss2 to 0
    Aug 24 12:14:37.490: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:14:37.497: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:14:37.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3788" for this suite. 08/24/23 12:14:37.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:14:37.576
Aug 24 12:14:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:14:37.579
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:14:37.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:14:37.615
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 12:14:37.621
Aug 24 12:14:37.638: INFO: Waiting up to 5m0s for pod "pod-edca0feb-4034-404a-a542-9f105df0d971" in namespace "emptydir-3176" to be "Succeeded or Failed"
Aug 24 12:14:37.650: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971": Phase="Pending", Reason="", readiness=false. Elapsed: 11.8767ms
Aug 24 12:14:39.660: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022002383s
Aug 24 12:14:41.662: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024074548s
STEP: Saw pod success 08/24/23 12:14:41.662
Aug 24 12:14:41.663: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971" satisfied condition "Succeeded or Failed"
Aug 24 12:14:41.678: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-edca0feb-4034-404a-a542-9f105df0d971 container test-container: <nil>
STEP: delete the pod 08/24/23 12:14:41.717
Aug 24 12:14:41.758: INFO: Waiting for pod pod-edca0feb-4034-404a-a542-9f105df0d971 to disappear
Aug 24 12:14:41.765: INFO: Pod pod-edca0feb-4034-404a-a542-9f105df0d971 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:14:41.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3176" for this suite. 08/24/23 12:14:41.776
------------------------------
â€¢ [4.214 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:14:37.576
    Aug 24 12:14:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:14:37.579
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:14:37.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:14:37.615
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 12:14:37.621
    Aug 24 12:14:37.638: INFO: Waiting up to 5m0s for pod "pod-edca0feb-4034-404a-a542-9f105df0d971" in namespace "emptydir-3176" to be "Succeeded or Failed"
    Aug 24 12:14:37.650: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971": Phase="Pending", Reason="", readiness=false. Elapsed: 11.8767ms
    Aug 24 12:14:39.660: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022002383s
    Aug 24 12:14:41.662: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024074548s
    STEP: Saw pod success 08/24/23 12:14:41.662
    Aug 24 12:14:41.663: INFO: Pod "pod-edca0feb-4034-404a-a542-9f105df0d971" satisfied condition "Succeeded or Failed"
    Aug 24 12:14:41.678: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-edca0feb-4034-404a-a542-9f105df0d971 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:14:41.717
    Aug 24 12:14:41.758: INFO: Waiting for pod pod-edca0feb-4034-404a-a542-9f105df0d971 to disappear
    Aug 24 12:14:41.765: INFO: Pod pod-edca0feb-4034-404a-a542-9f105df0d971 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:14:41.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3176" for this suite. 08/24/23 12:14:41.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:14:41.792
Aug 24 12:14:41.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 12:14:41.796
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:14:41.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:14:41.835
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/24/23 12:14:41.844
STEP: getting 08/24/23 12:14:41.881
STEP: listing 08/24/23 12:14:41.892
STEP: deleting 08/24/23 12:14:41.898
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:14:41.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6987" for this suite. 08/24/23 12:14:41.95
------------------------------
â€¢ [0.174 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:14:41.792
    Aug 24 12:14:41.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 12:14:41.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:14:41.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:14:41.835
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/24/23 12:14:41.844
    STEP: getting 08/24/23 12:14:41.881
    STEP: listing 08/24/23 12:14:41.892
    STEP: deleting 08/24/23 12:14:41.898
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:14:41.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6987" for this suite. 08/24/23 12:14:41.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:14:41.97
Aug 24 12:14:41.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:14:41.972
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:14:42.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:14:42.01
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-14023a48-ccee-45e5-b391-e68fb7f9b2b6 08/24/23 12:14:42.027
STEP: Creating configMap with name cm-test-opt-upd-3d78a705-8deb-4f4a-b661-7f4f684a0426 08/24/23 12:14:42.044
STEP: Creating the pod 08/24/23 12:14:42.053
Aug 24 12:14:42.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a" in namespace "projected-909" to be "running and ready"
Aug 24 12:14:42.096: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.802ms
Aug 24 12:14:42.096: INFO: The phase of Pod pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:14:44.105: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031119716s
Aug 24 12:14:44.105: INFO: The phase of Pod pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:14:46.120: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a": Phase="Running", Reason="", readiness=true. Elapsed: 4.04552002s
Aug 24 12:14:46.120: INFO: The phase of Pod pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a is Running (Ready = true)
Aug 24 12:14:46.120: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-14023a48-ccee-45e5-b391-e68fb7f9b2b6 08/24/23 12:14:46.19
STEP: Updating configmap cm-test-opt-upd-3d78a705-8deb-4f4a-b661-7f4f684a0426 08/24/23 12:14:46.201
STEP: Creating configMap with name cm-test-opt-create-42f9daf7-8da3-49eb-a9c0-98d14a7dedde 08/24/23 12:14:46.219
STEP: waiting to observe update in volume 08/24/23 12:14:46.227
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:11.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-909" for this suite. 08/24/23 12:16:11.105
------------------------------
â€¢ [SLOW TEST] [89.149 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:14:41.97
    Aug 24 12:14:41.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:14:41.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:14:42.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:14:42.01
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-14023a48-ccee-45e5-b391-e68fb7f9b2b6 08/24/23 12:14:42.027
    STEP: Creating configMap with name cm-test-opt-upd-3d78a705-8deb-4f4a-b661-7f4f684a0426 08/24/23 12:14:42.044
    STEP: Creating the pod 08/24/23 12:14:42.053
    Aug 24 12:14:42.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a" in namespace "projected-909" to be "running and ready"
    Aug 24 12:14:42.096: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.802ms
    Aug 24 12:14:42.096: INFO: The phase of Pod pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:14:44.105: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031119716s
    Aug 24 12:14:44.105: INFO: The phase of Pod pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:14:46.120: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a": Phase="Running", Reason="", readiness=true. Elapsed: 4.04552002s
    Aug 24 12:14:46.120: INFO: The phase of Pod pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a is Running (Ready = true)
    Aug 24 12:14:46.120: INFO: Pod "pod-projected-configmaps-791c183f-72c3-49a3-b28a-f68de8e46e8a" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-14023a48-ccee-45e5-b391-e68fb7f9b2b6 08/24/23 12:14:46.19
    STEP: Updating configmap cm-test-opt-upd-3d78a705-8deb-4f4a-b661-7f4f684a0426 08/24/23 12:14:46.201
    STEP: Creating configMap with name cm-test-opt-create-42f9daf7-8da3-49eb-a9c0-98d14a7dedde 08/24/23 12:14:46.219
    STEP: waiting to observe update in volume 08/24/23 12:14:46.227
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:11.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-909" for this suite. 08/24/23 12:16:11.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:11.137
Aug 24 12:16:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:16:11.14
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:11.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:11.179
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/24/23 12:16:11.184
Aug 24 12:16:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: rename a version 08/24/23 12:16:17.374
STEP: check the new version name is served 08/24/23 12:16:17.405
STEP: check the old version name is removed 08/24/23 12:16:20.248
STEP: check the other version is not changed 08/24/23 12:16:21.148
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:25.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3412" for this suite. 08/24/23 12:16:25.869
------------------------------
â€¢ [SLOW TEST] [14.746 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:11.137
    Aug 24 12:16:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:16:11.14
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:11.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:11.179
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/24/23 12:16:11.184
    Aug 24 12:16:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: rename a version 08/24/23 12:16:17.374
    STEP: check the new version name is served 08/24/23 12:16:17.405
    STEP: check the old version name is removed 08/24/23 12:16:20.248
    STEP: check the other version is not changed 08/24/23 12:16:21.148
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:25.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3412" for this suite. 08/24/23 12:16:25.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:25.888
Aug 24 12:16:25.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename cronjob 08/24/23 12:16:25.891
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:25.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:25.926
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/24/23 12:16:25.93
STEP: Ensuring no jobs are scheduled 08/24/23 12:16:25.943
STEP: Ensuring no job exists by listing jobs explicitly 08/24/23 12:21:25.958
STEP: Removing cronjob 08/24/23 12:21:25.965
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:25.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9111" for this suite. 08/24/23 12:21:25.989
------------------------------
â€¢ [SLOW TEST] [300.114 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:25.888
    Aug 24 12:16:25.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename cronjob 08/24/23 12:16:25.891
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:25.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:25.926
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/24/23 12:16:25.93
    STEP: Ensuring no jobs are scheduled 08/24/23 12:16:25.943
    STEP: Ensuring no job exists by listing jobs explicitly 08/24/23 12:21:25.958
    STEP: Removing cronjob 08/24/23 12:21:25.965
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:25.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9111" for this suite. 08/24/23 12:21:25.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:26.004
Aug 24 12:21:26.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:21:26.007
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:26.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:26.041
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 12:21:26.046
Aug 24 12:21:26.063: INFO: Waiting up to 5m0s for pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c" in namespace "emptydir-9227" to be "Succeeded or Failed"
Aug 24 12:21:26.080: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.770926ms
Aug 24 12:21:28.091: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027434586s
Aug 24 12:21:30.090: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026609192s
STEP: Saw pod success 08/24/23 12:21:30.09
Aug 24 12:21:30.091: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c" satisfied condition "Succeeded or Failed"
Aug 24 12:21:30.103: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c container test-container: <nil>
STEP: delete the pod 08/24/23 12:21:30.135
Aug 24 12:21:30.165: INFO: Waiting for pod pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c to disappear
Aug 24 12:21:30.171: INFO: Pod pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9227" for this suite. 08/24/23 12:21:30.184
------------------------------
â€¢ [4.191 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:26.004
    Aug 24 12:21:26.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:21:26.007
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:26.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:26.041
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 12:21:26.046
    Aug 24 12:21:26.063: INFO: Waiting up to 5m0s for pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c" in namespace "emptydir-9227" to be "Succeeded or Failed"
    Aug 24 12:21:26.080: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.770926ms
    Aug 24 12:21:28.091: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027434586s
    Aug 24 12:21:30.090: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026609192s
    STEP: Saw pod success 08/24/23 12:21:30.09
    Aug 24 12:21:30.091: INFO: Pod "pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c" satisfied condition "Succeeded or Failed"
    Aug 24 12:21:30.103: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c container test-container: <nil>
    STEP: delete the pod 08/24/23 12:21:30.135
    Aug 24 12:21:30.165: INFO: Waiting for pod pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c to disappear
    Aug 24 12:21:30.171: INFO: Pod pod-4f8d555b-0c10-45e6-b3a8-73a1bf2fce0c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9227" for this suite. 08/24/23 12:21:30.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:30.215
Aug 24 12:21:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename security-context-test 08/24/23 12:21:30.218
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:30.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:30.276
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 24 12:21:30.299: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c" in namespace "security-context-test-3578" to be "Succeeded or Failed"
Aug 24 12:21:30.305: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279307ms
Aug 24 12:21:32.316: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017188845s
Aug 24 12:21:34.313: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01349695s
Aug 24 12:21:34.313: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3578" for this suite. 08/24/23 12:21:34.323
------------------------------
â€¢ [4.119 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:30.215
    Aug 24 12:21:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename security-context-test 08/24/23 12:21:30.218
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:30.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:30.276
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 24 12:21:30.299: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c" in namespace "security-context-test-3578" to be "Succeeded or Failed"
    Aug 24 12:21:30.305: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279307ms
    Aug 24 12:21:32.316: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017188845s
    Aug 24 12:21:34.313: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01349695s
    Aug 24 12:21:34.313: INFO: Pod "busybox-readonly-false-6afcca8f-3a05-4cf4-8302-707b858cb73c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:34.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3578" for this suite. 08/24/23 12:21:34.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:34.337
Aug 24 12:21:34.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename endpointslice 08/24/23 12:21:34.339
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:34.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:34.373
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/24/23 12:21:34.38
STEP: getting /apis/discovery.k8s.io 08/24/23 12:21:34.385
STEP: getting /apis/discovery.k8s.iov1 08/24/23 12:21:34.387
STEP: creating 08/24/23 12:21:34.39
STEP: getting 08/24/23 12:21:34.416
STEP: listing 08/24/23 12:21:34.421
STEP: watching 08/24/23 12:21:34.427
Aug 24 12:21:34.427: INFO: starting watch
STEP: cluster-wide listing 08/24/23 12:21:34.43
STEP: cluster-wide watching 08/24/23 12:21:34.437
Aug 24 12:21:34.437: INFO: starting watch
STEP: patching 08/24/23 12:21:34.443
STEP: updating 08/24/23 12:21:34.452
Aug 24 12:21:34.465: INFO: waiting for watch events with expected annotations
Aug 24 12:21:34.466: INFO: saw patched and updated annotations
STEP: deleting 08/24/23 12:21:34.466
STEP: deleting a collection 08/24/23 12:21:34.491
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:34.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5025" for this suite. 08/24/23 12:21:34.527
------------------------------
â€¢ [0.201 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:34.337
    Aug 24 12:21:34.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename endpointslice 08/24/23 12:21:34.339
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:34.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:34.373
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/24/23 12:21:34.38
    STEP: getting /apis/discovery.k8s.io 08/24/23 12:21:34.385
    STEP: getting /apis/discovery.k8s.iov1 08/24/23 12:21:34.387
    STEP: creating 08/24/23 12:21:34.39
    STEP: getting 08/24/23 12:21:34.416
    STEP: listing 08/24/23 12:21:34.421
    STEP: watching 08/24/23 12:21:34.427
    Aug 24 12:21:34.427: INFO: starting watch
    STEP: cluster-wide listing 08/24/23 12:21:34.43
    STEP: cluster-wide watching 08/24/23 12:21:34.437
    Aug 24 12:21:34.437: INFO: starting watch
    STEP: patching 08/24/23 12:21:34.443
    STEP: updating 08/24/23 12:21:34.452
    Aug 24 12:21:34.465: INFO: waiting for watch events with expected annotations
    Aug 24 12:21:34.466: INFO: saw patched and updated annotations
    STEP: deleting 08/24/23 12:21:34.466
    STEP: deleting a collection 08/24/23 12:21:34.491
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:34.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5025" for this suite. 08/24/23 12:21:34.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:34.538
Aug 24 12:21:34.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename limitrange 08/24/23 12:21:34.54
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:34.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:34.573
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-c5ttz" in namespace "limitrange-8653" 08/24/23 12:21:34.577
STEP: Creating another limitRange in another namespace 08/24/23 12:21:34.587
Aug 24 12:21:34.620: INFO: Namespace "e2e-limitrange-c5ttz-3029" created
Aug 24 12:21:34.620: INFO: Creating LimitRange "e2e-limitrange-c5ttz" in namespace "e2e-limitrange-c5ttz-3029"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-c5ttz" 08/24/23 12:21:34.631
Aug 24 12:21:34.639: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-c5ttz" in "limitrange-8653" namespace 08/24/23 12:21:34.639
Aug 24 12:21:34.656: INFO: LimitRange "e2e-limitrange-c5ttz" has been patched
STEP: Delete LimitRange "e2e-limitrange-c5ttz" by Collection with labelSelector: "e2e-limitrange-c5ttz=patched" 08/24/23 12:21:34.656
STEP: Confirm that the limitRange "e2e-limitrange-c5ttz" has been deleted 08/24/23 12:21:34.68
Aug 24 12:21:34.681: INFO: Requesting list of LimitRange to confirm quantity
Aug 24 12:21:34.686: INFO: Found 0 LimitRange with label "e2e-limitrange-c5ttz=patched"
Aug 24 12:21:34.686: INFO: LimitRange "e2e-limitrange-c5ttz" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-c5ttz" 08/24/23 12:21:34.686
Aug 24 12:21:34.693: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:34.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8653" for this suite. 08/24/23 12:21:34.702
STEP: Destroying namespace "e2e-limitrange-c5ttz-3029" for this suite. 08/24/23 12:21:34.712
------------------------------
â€¢ [0.182 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:34.538
    Aug 24 12:21:34.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename limitrange 08/24/23 12:21:34.54
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:34.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:34.573
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-c5ttz" in namespace "limitrange-8653" 08/24/23 12:21:34.577
    STEP: Creating another limitRange in another namespace 08/24/23 12:21:34.587
    Aug 24 12:21:34.620: INFO: Namespace "e2e-limitrange-c5ttz-3029" created
    Aug 24 12:21:34.620: INFO: Creating LimitRange "e2e-limitrange-c5ttz" in namespace "e2e-limitrange-c5ttz-3029"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-c5ttz" 08/24/23 12:21:34.631
    Aug 24 12:21:34.639: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-c5ttz" in "limitrange-8653" namespace 08/24/23 12:21:34.639
    Aug 24 12:21:34.656: INFO: LimitRange "e2e-limitrange-c5ttz" has been patched
    STEP: Delete LimitRange "e2e-limitrange-c5ttz" by Collection with labelSelector: "e2e-limitrange-c5ttz=patched" 08/24/23 12:21:34.656
    STEP: Confirm that the limitRange "e2e-limitrange-c5ttz" has been deleted 08/24/23 12:21:34.68
    Aug 24 12:21:34.681: INFO: Requesting list of LimitRange to confirm quantity
    Aug 24 12:21:34.686: INFO: Found 0 LimitRange with label "e2e-limitrange-c5ttz=patched"
    Aug 24 12:21:34.686: INFO: LimitRange "e2e-limitrange-c5ttz" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-c5ttz" 08/24/23 12:21:34.686
    Aug 24 12:21:34.693: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:34.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8653" for this suite. 08/24/23 12:21:34.702
    STEP: Destroying namespace "e2e-limitrange-c5ttz-3029" for this suite. 08/24/23 12:21:34.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:34.738
Aug 24 12:21:34.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:21:34.74
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:34.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:34.78
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/24/23 12:21:34.787
Aug 24 12:21:34.803: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6780" to be "running and ready"
Aug 24 12:21:34.813: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.445319ms
Aug 24 12:21:34.813: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:21:36.824: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.020611273s
Aug 24 12:21:36.824: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 24 12:21:36.824: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/24/23 12:21:36.83
STEP: Then the orphan pod is adopted 08/24/23 12:21:36.844
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:37.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6780" for this suite. 08/24/23 12:21:37.869
------------------------------
â€¢ [3.142 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:34.738
    Aug 24 12:21:34.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:21:34.74
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:34.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:34.78
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/24/23 12:21:34.787
    Aug 24 12:21:34.803: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6780" to be "running and ready"
    Aug 24 12:21:34.813: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.445319ms
    Aug 24 12:21:34.813: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:21:36.824: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.020611273s
    Aug 24 12:21:36.824: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 24 12:21:36.824: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/24/23 12:21:36.83
    STEP: Then the orphan pod is adopted 08/24/23 12:21:36.844
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:37.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6780" for this suite. 08/24/23 12:21:37.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:37.888
Aug 24 12:21:37.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:21:37.89
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:37.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:37.922
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/24/23 12:21:37.928
Aug 24 12:21:37.928: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 24 12:21:37.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
Aug 24 12:21:38.564: INFO: stderr: ""
Aug 24 12:21:38.564: INFO: stdout: "service/agnhost-replica created\n"
Aug 24 12:21:38.564: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 24 12:21:38.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
Aug 24 12:21:39.934: INFO: stderr: ""
Aug 24 12:21:39.934: INFO: stdout: "service/agnhost-primary created\n"
Aug 24 12:21:39.934: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 24 12:21:39.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
Aug 24 12:21:40.575: INFO: stderr: ""
Aug 24 12:21:40.575: INFO: stdout: "service/frontend created\n"
Aug 24 12:21:40.575: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 24 12:21:40.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
Aug 24 12:21:41.045: INFO: stderr: ""
Aug 24 12:21:41.045: INFO: stdout: "deployment.apps/frontend created\n"
Aug 24 12:21:41.045: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 24 12:21:41.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
Aug 24 12:21:41.585: INFO: stderr: ""
Aug 24 12:21:41.585: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 24 12:21:41.586: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 24 12:21:41.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
Aug 24 12:21:42.460: INFO: stderr: ""
Aug 24 12:21:42.460: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/24/23 12:21:42.46
Aug 24 12:21:42.460: INFO: Waiting for all frontend pods to be Running.
Aug 24 12:21:47.526: INFO: Waiting for frontend to serve content.
Aug 24 12:21:47.551: INFO: Trying to add a new entry to the guestbook.
Aug 24 12:21:47.573: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/24/23 12:21:47.597
Aug 24 12:21:47.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
Aug 24 12:21:47.755: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:21:47.755: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 12:21:47.755
Aug 24 12:21:47.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
Aug 24 12:21:47.968: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:21:47.968: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 12:21:47.968
Aug 24 12:21:47.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
Aug 24 12:21:48.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:21:48.185: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 12:21:48.186
Aug 24 12:21:48.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
Aug 24 12:21:48.329: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:21:48.329: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 12:21:48.329
Aug 24 12:21:48.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
Aug 24 12:21:48.549: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:21:48.549: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 12:21:48.549
Aug 24 12:21:48.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
Aug 24 12:21:48.782: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:21:48.782: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:48.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-820" for this suite. 08/24/23 12:21:48.792
------------------------------
â€¢ [SLOW TEST] [10.918 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:37.888
    Aug 24 12:21:37.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:21:37.89
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:37.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:37.922
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/24/23 12:21:37.928
    Aug 24 12:21:37.928: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 24 12:21:37.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
    Aug 24 12:21:38.564: INFO: stderr: ""
    Aug 24 12:21:38.564: INFO: stdout: "service/agnhost-replica created\n"
    Aug 24 12:21:38.564: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 24 12:21:38.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
    Aug 24 12:21:39.934: INFO: stderr: ""
    Aug 24 12:21:39.934: INFO: stdout: "service/agnhost-primary created\n"
    Aug 24 12:21:39.934: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 24 12:21:39.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
    Aug 24 12:21:40.575: INFO: stderr: ""
    Aug 24 12:21:40.575: INFO: stdout: "service/frontend created\n"
    Aug 24 12:21:40.575: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 24 12:21:40.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
    Aug 24 12:21:41.045: INFO: stderr: ""
    Aug 24 12:21:41.045: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 24 12:21:41.045: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 24 12:21:41.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
    Aug 24 12:21:41.585: INFO: stderr: ""
    Aug 24 12:21:41.585: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 24 12:21:41.586: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 24 12:21:41.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 create -f -'
    Aug 24 12:21:42.460: INFO: stderr: ""
    Aug 24 12:21:42.460: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/24/23 12:21:42.46
    Aug 24 12:21:42.460: INFO: Waiting for all frontend pods to be Running.
    Aug 24 12:21:47.526: INFO: Waiting for frontend to serve content.
    Aug 24 12:21:47.551: INFO: Trying to add a new entry to the guestbook.
    Aug 24 12:21:47.573: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/24/23 12:21:47.597
    Aug 24 12:21:47.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
    Aug 24 12:21:47.755: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:21:47.755: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 12:21:47.755
    Aug 24 12:21:47.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
    Aug 24 12:21:47.968: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:21:47.968: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 12:21:47.968
    Aug 24 12:21:47.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
    Aug 24 12:21:48.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:21:48.185: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 12:21:48.186
    Aug 24 12:21:48.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
    Aug 24 12:21:48.329: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:21:48.329: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 12:21:48.329
    Aug 24 12:21:48.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
    Aug 24 12:21:48.549: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:21:48.549: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 12:21:48.549
    Aug 24 12:21:48.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-820 delete --grace-period=0 --force -f -'
    Aug 24 12:21:48.782: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:21:48.782: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:48.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-820" for this suite. 08/24/23 12:21:48.792
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:48.809
Aug 24 12:21:48.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:21:48.817
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:48.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:48.882
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/24/23 12:21:48.897
Aug 24 12:21:48.916: INFO: Waiting up to 5m0s for pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3" in namespace "downward-api-9750" to be "Succeeded or Failed"
Aug 24 12:21:48.924: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.646101ms
Aug 24 12:21:50.939: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022422239s
Aug 24 12:21:52.933: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015818889s
STEP: Saw pod success 08/24/23 12:21:52.933
Aug 24 12:21:52.933: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3" satisfied condition "Succeeded or Failed"
Aug 24 12:21:52.939: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:21:52.955
Aug 24 12:21:52.974: INFO: Waiting for pod downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3 to disappear
Aug 24 12:21:52.981: INFO: Pod downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9750" for this suite. 08/24/23 12:21:52.994
------------------------------
â€¢ [4.206 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:48.809
    Aug 24 12:21:48.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:21:48.817
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:48.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:48.882
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/24/23 12:21:48.897
    Aug 24 12:21:48.916: INFO: Waiting up to 5m0s for pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3" in namespace "downward-api-9750" to be "Succeeded or Failed"
    Aug 24 12:21:48.924: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.646101ms
    Aug 24 12:21:50.939: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022422239s
    Aug 24 12:21:52.933: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015818889s
    STEP: Saw pod success 08/24/23 12:21:52.933
    Aug 24 12:21:52.933: INFO: Pod "downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3" satisfied condition "Succeeded or Failed"
    Aug 24 12:21:52.939: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:21:52.955
    Aug 24 12:21:52.974: INFO: Waiting for pod downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3 to disappear
    Aug 24 12:21:52.981: INFO: Pod downward-api-1995786d-aa1d-4932-bb72-9a57b96dd6e3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9750" for this suite. 08/24/23 12:21:52.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:53.017
Aug 24 12:21:53.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:21:53.02
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:53.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:53.058
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 24 12:21:53.088: INFO: Waiting up to 5m0s for pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580" in namespace "svcaccounts-8346" to be "running"
Aug 24 12:21:53.093: INFO: Pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567644ms
Aug 24 12:21:55.099: INFO: Pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580": Phase="Running", Reason="", readiness=true. Elapsed: 2.010804436s
Aug 24 12:21:55.099: INFO: Pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580" satisfied condition "running"
STEP: reading a file in the container 08/24/23 12:21:55.099
Aug 24 12:21:55.100: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8346 pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/24/23 12:21:55.411
Aug 24 12:21:55.412: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8346 pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/24/23 12:21:55.673
Aug 24 12:21:55.673: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8346 pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 24 12:21:55.920: INFO: Got root ca configmap in namespace "svcaccounts-8346"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:55.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8346" for this suite. 08/24/23 12:21:55.933
------------------------------
â€¢ [2.928 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:53.017
    Aug 24 12:21:53.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:21:53.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:53.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:53.058
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 24 12:21:53.088: INFO: Waiting up to 5m0s for pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580" in namespace "svcaccounts-8346" to be "running"
    Aug 24 12:21:53.093: INFO: Pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567644ms
    Aug 24 12:21:55.099: INFO: Pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580": Phase="Running", Reason="", readiness=true. Elapsed: 2.010804436s
    Aug 24 12:21:55.099: INFO: Pod "pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580" satisfied condition "running"
    STEP: reading a file in the container 08/24/23 12:21:55.099
    Aug 24 12:21:55.100: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8346 pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/24/23 12:21:55.411
    Aug 24 12:21:55.412: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8346 pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/24/23 12:21:55.673
    Aug 24 12:21:55.673: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8346 pod-service-account-4a0590c0-b2e1-4b74-a69d-4b853a9a7580 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 24 12:21:55.920: INFO: Got root ca configmap in namespace "svcaccounts-8346"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:55.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8346" for this suite. 08/24/23 12:21:55.933
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:55.946
Aug 24 12:21:55.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:21:55.948
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:55.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:55.983
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:21:55.998
Aug 24 12:21:56.016: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1487" to be "running and ready"
Aug 24 12:21:56.022: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.380547ms
Aug 24 12:21:56.022: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:21:58.030: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013856465s
Aug 24 12:21:58.030: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:22:00.032: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.015825286s
Aug 24 12:22:00.032: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 12:22:00.032: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/24/23 12:22:00.038
Aug 24 12:22:00.049: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1487" to be "running and ready"
Aug 24 12:22:00.059: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.163026ms
Aug 24 12:22:00.059: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:22:02.072: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023383779s
Aug 24 12:22:02.072: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 24 12:22:02.073: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/24/23 12:22:02.08
STEP: delete the pod with lifecycle hook 08/24/23 12:22:02.13
Aug 24 12:22:02.160: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 24 12:22:02.171: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 24 12:22:04.172: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 24 12:22:04.180: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 24 12:22:06.171: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 24 12:22:06.179: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:06.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1487" for this suite. 08/24/23 12:22:06.189
------------------------------
â€¢ [SLOW TEST] [10.253 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:55.946
    Aug 24 12:21:55.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:21:55.948
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:55.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:55.983
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:21:55.998
    Aug 24 12:21:56.016: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1487" to be "running and ready"
    Aug 24 12:21:56.022: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.380547ms
    Aug 24 12:21:56.022: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:21:58.030: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013856465s
    Aug 24 12:21:58.030: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:22:00.032: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.015825286s
    Aug 24 12:22:00.032: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 12:22:00.032: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/24/23 12:22:00.038
    Aug 24 12:22:00.049: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1487" to be "running and ready"
    Aug 24 12:22:00.059: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.163026ms
    Aug 24 12:22:00.059: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:22:02.072: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023383779s
    Aug 24 12:22:02.072: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 24 12:22:02.073: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/24/23 12:22:02.08
    STEP: delete the pod with lifecycle hook 08/24/23 12:22:02.13
    Aug 24 12:22:02.160: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 24 12:22:02.171: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 24 12:22:04.172: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 24 12:22:04.180: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 24 12:22:06.171: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 24 12:22:06.179: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:06.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1487" for this suite. 08/24/23 12:22:06.189
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:06.204
Aug 24 12:22:06.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:22:06.206
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:06.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:06.236
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-68dc8eef-7bae-4719-8bf8-ea9ca3407c66 08/24/23 12:22:06.243
STEP: Creating a pod to test consume configMaps 08/24/23 12:22:06.253
Aug 24 12:22:06.269: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6" in namespace "projected-7558" to be "Succeeded or Failed"
Aug 24 12:22:06.274: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764878ms
Aug 24 12:22:08.286: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017009128s
Aug 24 12:22:10.284: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015563587s
STEP: Saw pod success 08/24/23 12:22:10.284
Aug 24 12:22:10.285: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6" satisfied condition "Succeeded or Failed"
Aug 24 12:22:10.290: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6 container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/24/23 12:22:10.301
Aug 24 12:22:10.323: INFO: Waiting for pod pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6 to disappear
Aug 24 12:22:10.330: INFO: Pod pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:10.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7558" for this suite. 08/24/23 12:22:10.34
------------------------------
â€¢ [4.147 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:06.204
    Aug 24 12:22:06.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:22:06.206
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:06.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:06.236
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-68dc8eef-7bae-4719-8bf8-ea9ca3407c66 08/24/23 12:22:06.243
    STEP: Creating a pod to test consume configMaps 08/24/23 12:22:06.253
    Aug 24 12:22:06.269: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6" in namespace "projected-7558" to be "Succeeded or Failed"
    Aug 24 12:22:06.274: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764878ms
    Aug 24 12:22:08.286: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017009128s
    Aug 24 12:22:10.284: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015563587s
    STEP: Saw pod success 08/24/23 12:22:10.284
    Aug 24 12:22:10.285: INFO: Pod "pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6" satisfied condition "Succeeded or Failed"
    Aug 24 12:22:10.290: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:22:10.301
    Aug 24 12:22:10.323: INFO: Waiting for pod pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6 to disappear
    Aug 24 12:22:10.330: INFO: Pod pod-projected-configmaps-ccc68296-3a9a-4cbd-b752-f289203939d6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:10.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7558" for this suite. 08/24/23 12:22:10.34
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:10.353
Aug 24 12:22:10.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:22:10.356
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:10.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:10.392
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 12:22:10.397
Aug 24 12:22:10.413: INFO: Waiting up to 5m0s for pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc" in namespace "emptydir-6731" to be "Succeeded or Failed"
Aug 24 12:22:10.423: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.352029ms
Aug 24 12:22:12.432: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019247345s
Aug 24 12:22:14.432: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018707008s
STEP: Saw pod success 08/24/23 12:22:14.432
Aug 24 12:22:14.432: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc" satisfied condition "Succeeded or Failed"
Aug 24 12:22:14.438: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc container test-container: <nil>
STEP: delete the pod 08/24/23 12:22:14.45
Aug 24 12:22:14.470: INFO: Waiting for pod pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc to disappear
Aug 24 12:22:14.476: INFO: Pod pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:14.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6731" for this suite. 08/24/23 12:22:14.487
------------------------------
â€¢ [4.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:10.353
    Aug 24 12:22:10.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:22:10.356
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:10.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:10.392
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 12:22:10.397
    Aug 24 12:22:10.413: INFO: Waiting up to 5m0s for pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc" in namespace "emptydir-6731" to be "Succeeded or Failed"
    Aug 24 12:22:10.423: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.352029ms
    Aug 24 12:22:12.432: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019247345s
    Aug 24 12:22:14.432: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018707008s
    STEP: Saw pod success 08/24/23 12:22:14.432
    Aug 24 12:22:14.432: INFO: Pod "pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc" satisfied condition "Succeeded or Failed"
    Aug 24 12:22:14.438: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc container test-container: <nil>
    STEP: delete the pod 08/24/23 12:22:14.45
    Aug 24 12:22:14.470: INFO: Waiting for pod pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc to disappear
    Aug 24 12:22:14.476: INFO: Pod pod-18835aa8-59cf-4e74-8d1c-33e5ff0a5abc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:14.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6731" for this suite. 08/24/23 12:22:14.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:14.506
Aug 24 12:22:14.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:22:14.508
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:14.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:14.55
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 24 12:22:14.577: INFO: Waiting up to 2m0s for pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" in namespace "var-expansion-5845" to be "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:22:14.588: INFO: Pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.295757ms
Aug 24 12:22:16.595: INFO: Pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017208299s
Aug 24 12:22:16.595: INFO: Pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:22:16.595: INFO: Deleting pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" in namespace "var-expansion-5845"
Aug 24 12:22:16.609: INFO: Wait up to 5m0s for pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5845" for this suite. 08/24/23 12:22:18.635
------------------------------
â€¢ [4.139 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:14.506
    Aug 24 12:22:14.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:22:14.508
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:14.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:14.55
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 24 12:22:14.577: INFO: Waiting up to 2m0s for pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" in namespace "var-expansion-5845" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:22:14.588: INFO: Pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.295757ms
    Aug 24 12:22:16.595: INFO: Pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017208299s
    Aug 24 12:22:16.595: INFO: Pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:22:16.595: INFO: Deleting pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" in namespace "var-expansion-5845"
    Aug 24 12:22:16.609: INFO: Wait up to 5m0s for pod "var-expansion-d6160d00-d301-4b56-aea0-429c1dfce20a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5845" for this suite. 08/24/23 12:22:18.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:18.655
Aug 24 12:22:18.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:22:18.657
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:18.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:18.695
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:22:18.723
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:22:19.901
STEP: Deploying the webhook pod 08/24/23 12:22:19.922
STEP: Wait for the deployment to be ready 08/24/23 12:22:19.947
Aug 24 12:22:19.963: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:22:21.992
STEP: Verifying the service has paired with the endpoint 08/24/23 12:22:22.011
Aug 24 12:22:23.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/24/23 12:22:23.019
STEP: create a namespace for the webhook 08/24/23 12:22:23.099
STEP: create a configmap should be unconditionally rejected by the webhook 08/24/23 12:22:23.116
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:23.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-746" for this suite. 08/24/23 12:22:23.289
STEP: Destroying namespace "webhook-746-markers" for this suite. 08/24/23 12:22:23.303
------------------------------
â€¢ [4.665 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:18.655
    Aug 24 12:22:18.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:22:18.657
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:18.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:18.695
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:22:18.723
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:22:19.901
    STEP: Deploying the webhook pod 08/24/23 12:22:19.922
    STEP: Wait for the deployment to be ready 08/24/23 12:22:19.947
    Aug 24 12:22:19.963: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:22:21.992
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:22:22.011
    Aug 24 12:22:23.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/24/23 12:22:23.019
    STEP: create a namespace for the webhook 08/24/23 12:22:23.099
    STEP: create a configmap should be unconditionally rejected by the webhook 08/24/23 12:22:23.116
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:23.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-746" for this suite. 08/24/23 12:22:23.289
    STEP: Destroying namespace "webhook-746-markers" for this suite. 08/24/23 12:22:23.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:23.326
Aug 24 12:22:23.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename containers 08/24/23 12:22:23.328
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:23.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:23.386
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/24/23 12:22:23.396
Aug 24 12:22:23.424: INFO: Waiting up to 5m0s for pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0" in namespace "containers-643" to be "Succeeded or Failed"
Aug 24 12:22:23.460: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 35.688133ms
Aug 24 12:22:25.469: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045301172s
Aug 24 12:22:27.468: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043870592s
STEP: Saw pod success 08/24/23 12:22:27.468
Aug 24 12:22:27.468: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0" satisfied condition "Succeeded or Failed"
Aug 24 12:22:27.475: INFO: Trying to get logs from node pe9deep4seen-3 pod client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:22:27.488
Aug 24 12:22:27.512: INFO: Waiting for pod client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0 to disappear
Aug 24 12:22:27.518: INFO: Pod client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:27.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-643" for this suite. 08/24/23 12:22:27.527
------------------------------
â€¢ [4.218 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:23.326
    Aug 24 12:22:23.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename containers 08/24/23 12:22:23.328
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:23.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:23.386
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/24/23 12:22:23.396
    Aug 24 12:22:23.424: INFO: Waiting up to 5m0s for pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0" in namespace "containers-643" to be "Succeeded or Failed"
    Aug 24 12:22:23.460: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 35.688133ms
    Aug 24 12:22:25.469: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045301172s
    Aug 24 12:22:27.468: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043870592s
    STEP: Saw pod success 08/24/23 12:22:27.468
    Aug 24 12:22:27.468: INFO: Pod "client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0" satisfied condition "Succeeded or Failed"
    Aug 24 12:22:27.475: INFO: Trying to get logs from node pe9deep4seen-3 pod client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:22:27.488
    Aug 24 12:22:27.512: INFO: Waiting for pod client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0 to disappear
    Aug 24 12:22:27.518: INFO: Pod client-containers-d8f6bfda-e314-41af-b722-3e3d0c925ca0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:27.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-643" for this suite. 08/24/23 12:22:27.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:27.551
Aug 24 12:22:27.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:22:27.554
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:27.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:27.593
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-422 08/24/23 12:22:27.599
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 12:22:27.631
STEP: creating service externalsvc in namespace services-422 08/24/23 12:22:27.632
STEP: creating replication controller externalsvc in namespace services-422 08/24/23 12:22:27.661
I0824 12:22:27.678918      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-422, replica count: 2
I0824 12:22:30.732804      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/24/23 12:22:30.739
Aug 24 12:22:30.779: INFO: Creating new exec pod
Aug 24 12:22:30.798: INFO: Waiting up to 5m0s for pod "execpodrsqpl" in namespace "services-422" to be "running"
Aug 24 12:22:30.809: INFO: Pod "execpodrsqpl": Phase="Pending", Reason="", readiness=false. Elapsed: 10.240906ms
Aug 24 12:22:32.819: INFO: Pod "execpodrsqpl": Phase="Running", Reason="", readiness=true. Elapsed: 2.020851298s
Aug 24 12:22:32.820: INFO: Pod "execpodrsqpl" satisfied condition "running"
Aug 24 12:22:32.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-422 exec execpodrsqpl -- /bin/sh -x -c nslookup nodeport-service.services-422.svc.cluster.local'
Aug 24 12:22:33.243: INFO: stderr: "+ nslookup nodeport-service.services-422.svc.cluster.local\n"
Aug 24 12:22:33.243: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-422.svc.cluster.local\tcanonical name = externalsvc.services-422.svc.cluster.local.\nName:\texternalsvc.services-422.svc.cluster.local\nAddress: 10.233.44.210\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-422, will wait for the garbage collector to delete the pods 08/24/23 12:22:33.243
Aug 24 12:22:33.310: INFO: Deleting ReplicationController externalsvc took: 10.241701ms
Aug 24 12:22:33.413: INFO: Terminating ReplicationController externalsvc pods took: 102.23208ms
Aug 24 12:22:35.384: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:35.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-422" for this suite. 08/24/23 12:22:35.491
------------------------------
â€¢ [SLOW TEST] [8.016 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:27.551
    Aug 24 12:22:27.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:22:27.554
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:27.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:27.593
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-422 08/24/23 12:22:27.599
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 12:22:27.631
    STEP: creating service externalsvc in namespace services-422 08/24/23 12:22:27.632
    STEP: creating replication controller externalsvc in namespace services-422 08/24/23 12:22:27.661
    I0824 12:22:27.678918      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-422, replica count: 2
    I0824 12:22:30.732804      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/24/23 12:22:30.739
    Aug 24 12:22:30.779: INFO: Creating new exec pod
    Aug 24 12:22:30.798: INFO: Waiting up to 5m0s for pod "execpodrsqpl" in namespace "services-422" to be "running"
    Aug 24 12:22:30.809: INFO: Pod "execpodrsqpl": Phase="Pending", Reason="", readiness=false. Elapsed: 10.240906ms
    Aug 24 12:22:32.819: INFO: Pod "execpodrsqpl": Phase="Running", Reason="", readiness=true. Elapsed: 2.020851298s
    Aug 24 12:22:32.820: INFO: Pod "execpodrsqpl" satisfied condition "running"
    Aug 24 12:22:32.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-422 exec execpodrsqpl -- /bin/sh -x -c nslookup nodeport-service.services-422.svc.cluster.local'
    Aug 24 12:22:33.243: INFO: stderr: "+ nslookup nodeport-service.services-422.svc.cluster.local\n"
    Aug 24 12:22:33.243: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-422.svc.cluster.local\tcanonical name = externalsvc.services-422.svc.cluster.local.\nName:\texternalsvc.services-422.svc.cluster.local\nAddress: 10.233.44.210\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-422, will wait for the garbage collector to delete the pods 08/24/23 12:22:33.243
    Aug 24 12:22:33.310: INFO: Deleting ReplicationController externalsvc took: 10.241701ms
    Aug 24 12:22:33.413: INFO: Terminating ReplicationController externalsvc pods took: 102.23208ms
    Aug 24 12:22:35.384: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:35.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-422" for this suite. 08/24/23 12:22:35.491
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:35.57
Aug 24 12:22:35.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:22:35.575
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:35.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:35.626
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:22:35.654
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:22:37.013
STEP: Deploying the webhook pod 08/24/23 12:22:37.022
STEP: Wait for the deployment to be ready 08/24/23 12:22:37.044
Aug 24 12:22:37.062: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:22:39.119
STEP: Verifying the service has paired with the endpoint 08/24/23 12:22:39.147
Aug 24 12:22:40.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/24/23 12:22:40.157
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/24/23 12:22:40.161
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 12:22:40.161
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/24/23 12:22:40.162
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/24/23 12:22:40.166
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 12:22:40.167
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 12:22:40.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:40.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8977" for this suite. 08/24/23 12:22:40.252
STEP: Destroying namespace "webhook-8977-markers" for this suite. 08/24/23 12:22:40.275
------------------------------
â€¢ [4.730 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:35.57
    Aug 24 12:22:35.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:22:35.575
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:35.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:35.626
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:22:35.654
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:22:37.013
    STEP: Deploying the webhook pod 08/24/23 12:22:37.022
    STEP: Wait for the deployment to be ready 08/24/23 12:22:37.044
    Aug 24 12:22:37.062: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:22:39.119
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:22:39.147
    Aug 24 12:22:40.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/24/23 12:22:40.157
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/24/23 12:22:40.161
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 12:22:40.161
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/24/23 12:22:40.162
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/24/23 12:22:40.166
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 12:22:40.167
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 12:22:40.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:40.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8977" for this suite. 08/24/23 12:22:40.252
    STEP: Destroying namespace "webhook-8977-markers" for this suite. 08/24/23 12:22:40.275
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:40.303
Aug 24 12:22:40.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:22:40.308
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:40.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:40.369
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 24 12:22:40.408: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4387 to be scheduled
Aug 24 12:22:40.419: INFO: 1 pods are not scheduled: [runtimeclass-4387/test-runtimeclass-runtimeclass-4387-preconfigured-handler-2kvvr(d6f39fd1-77f5-4c72-8305-d951cc0153db)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:42.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4387" for this suite. 08/24/23 12:22:42.455
------------------------------
â€¢ [2.167 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:40.303
    Aug 24 12:22:40.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:22:40.308
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:40.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:40.369
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 24 12:22:40.408: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4387 to be scheduled
    Aug 24 12:22:40.419: INFO: 1 pods are not scheduled: [runtimeclass-4387/test-runtimeclass-runtimeclass-4387-preconfigured-handler-2kvvr(d6f39fd1-77f5-4c72-8305-d951cc0153db)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:42.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4387" for this suite. 08/24/23 12:22:42.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:42.48
Aug 24 12:22:42.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:22:42.483
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:42.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:42.519
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-938ec7f2-cc01-43fd-87a6-762acad423ee 08/24/23 12:22:42.532
STEP: Creating secret with name s-test-opt-upd-a0fe756a-500b-44f6-ad05-49b20e651f8d 08/24/23 12:22:42.544
STEP: Creating the pod 08/24/23 12:22:42.556
Aug 24 12:22:42.575: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27" in namespace "projected-7330" to be "running and ready"
Aug 24 12:22:42.589: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27": Phase="Pending", Reason="", readiness=false. Elapsed: 13.79241ms
Aug 24 12:22:42.589: INFO: The phase of Pod pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:22:44.596: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020304127s
Aug 24 12:22:44.596: INFO: The phase of Pod pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:22:46.599: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27": Phase="Running", Reason="", readiness=true. Elapsed: 4.023798649s
Aug 24 12:22:46.599: INFO: The phase of Pod pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27 is Running (Ready = true)
Aug 24 12:22:46.600: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-938ec7f2-cc01-43fd-87a6-762acad423ee 08/24/23 12:22:46.643
STEP: Updating secret s-test-opt-upd-a0fe756a-500b-44f6-ad05-49b20e651f8d 08/24/23 12:22:46.655
STEP: Creating secret with name s-test-opt-create-f747ad0c-1ee1-4f5a-a638-253222cd3ea0 08/24/23 12:22:46.668
STEP: waiting to observe update in volume 08/24/23 12:22:46.68
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:55.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7330" for this suite. 08/24/23 12:23:55.379
------------------------------
â€¢ [SLOW TEST] [72.918 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:42.48
    Aug 24 12:22:42.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:22:42.483
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:42.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:42.519
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-938ec7f2-cc01-43fd-87a6-762acad423ee 08/24/23 12:22:42.532
    STEP: Creating secret with name s-test-opt-upd-a0fe756a-500b-44f6-ad05-49b20e651f8d 08/24/23 12:22:42.544
    STEP: Creating the pod 08/24/23 12:22:42.556
    Aug 24 12:22:42.575: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27" in namespace "projected-7330" to be "running and ready"
    Aug 24 12:22:42.589: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27": Phase="Pending", Reason="", readiness=false. Elapsed: 13.79241ms
    Aug 24 12:22:42.589: INFO: The phase of Pod pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:22:44.596: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020304127s
    Aug 24 12:22:44.596: INFO: The phase of Pod pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:22:46.599: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27": Phase="Running", Reason="", readiness=true. Elapsed: 4.023798649s
    Aug 24 12:22:46.599: INFO: The phase of Pod pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27 is Running (Ready = true)
    Aug 24 12:22:46.600: INFO: Pod "pod-projected-secrets-25974e37-07a6-4a5c-9590-023d306bfd27" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-938ec7f2-cc01-43fd-87a6-762acad423ee 08/24/23 12:22:46.643
    STEP: Updating secret s-test-opt-upd-a0fe756a-500b-44f6-ad05-49b20e651f8d 08/24/23 12:22:46.655
    STEP: Creating secret with name s-test-opt-create-f747ad0c-1ee1-4f5a-a638-253222cd3ea0 08/24/23 12:22:46.668
    STEP: waiting to observe update in volume 08/24/23 12:22:46.68
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:55.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7330" for this suite. 08/24/23 12:23:55.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:55.399
Aug 24 12:23:55.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:23:55.405
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:55.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:55.454
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-0f2edd49-cd0e-4f35-b8c8-9689e2d30b0f 08/24/23 12:23:55.46
STEP: Creating a pod to test consume secrets 08/24/23 12:23:55.471
Aug 24 12:23:55.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1" in namespace "projected-1911" to be "Succeeded or Failed"
Aug 24 12:23:55.500: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.822068ms
Aug 24 12:23:57.508: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018408687s
Aug 24 12:23:59.509: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019373657s
STEP: Saw pod success 08/24/23 12:23:59.509
Aug 24 12:23:59.509: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1" satisfied condition "Succeeded or Failed"
Aug 24 12:23:59.515: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:23:59.528
Aug 24 12:23:59.554: INFO: Waiting for pod pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1 to disappear
Aug 24 12:23:59.560: INFO: Pod pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:59.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1911" for this suite. 08/24/23 12:23:59.57
------------------------------
â€¢ [4.193 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:55.399
    Aug 24 12:23:55.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:23:55.405
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:55.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:55.454
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-0f2edd49-cd0e-4f35-b8c8-9689e2d30b0f 08/24/23 12:23:55.46
    STEP: Creating a pod to test consume secrets 08/24/23 12:23:55.471
    Aug 24 12:23:55.489: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1" in namespace "projected-1911" to be "Succeeded or Failed"
    Aug 24 12:23:55.500: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.822068ms
    Aug 24 12:23:57.508: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018408687s
    Aug 24 12:23:59.509: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019373657s
    STEP: Saw pod success 08/24/23 12:23:59.509
    Aug 24 12:23:59.509: INFO: Pod "pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1" satisfied condition "Succeeded or Failed"
    Aug 24 12:23:59.515: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:23:59.528
    Aug 24 12:23:59.554: INFO: Waiting for pod pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1 to disappear
    Aug 24 12:23:59.560: INFO: Pod pod-projected-secrets-1e03aaa4-29e0-46bc-b02e-ea445fffc5e1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:59.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1911" for this suite. 08/24/23 12:23:59.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:59.597
Aug 24 12:23:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:23:59.599
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:59.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:59.691
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/24/23 12:23:59.711
STEP: watching for Pod to be ready 08/24/23 12:23:59.734
Aug 24 12:23:59.737: INFO: observed Pod pod-test in namespace pods-4062 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 24 12:23:59.743: INFO: observed Pod pod-test in namespace pods-4062 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  }]
Aug 24 12:23:59.765: INFO: observed Pod pod-test in namespace pods-4062 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  }]
Aug 24 12:24:01.337: INFO: Found Pod pod-test in namespace pods-4062 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/24/23 12:24:01.342
STEP: getting the Pod and ensuring that it's patched 08/24/23 12:24:01.367
STEP: replacing the Pod's status Ready condition to False 08/24/23 12:24:01.375
STEP: check the Pod again to ensure its Ready conditions are False 08/24/23 12:24:01.396
STEP: deleting the Pod via a Collection with a LabelSelector 08/24/23 12:24:01.397
STEP: watching for the Pod to be deleted 08/24/23 12:24:01.414
Aug 24 12:24:01.418: INFO: observed event type MODIFIED
Aug 24 12:24:03.358: INFO: observed event type MODIFIED
Aug 24 12:24:04.371: INFO: observed event type MODIFIED
Aug 24 12:24:04.384: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:24:04.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4062" for this suite. 08/24/23 12:24:04.413
------------------------------
â€¢ [4.826 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:59.597
    Aug 24 12:23:59.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:23:59.599
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:59.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:59.691
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/24/23 12:23:59.711
    STEP: watching for Pod to be ready 08/24/23 12:23:59.734
    Aug 24 12:23:59.737: INFO: observed Pod pod-test in namespace pods-4062 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 24 12:23:59.743: INFO: observed Pod pod-test in namespace pods-4062 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  }]
    Aug 24 12:23:59.765: INFO: observed Pod pod-test in namespace pods-4062 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  }]
    Aug 24 12:24:01.337: INFO: Found Pod pod-test in namespace pods-4062 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:23:59 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/24/23 12:24:01.342
    STEP: getting the Pod and ensuring that it's patched 08/24/23 12:24:01.367
    STEP: replacing the Pod's status Ready condition to False 08/24/23 12:24:01.375
    STEP: check the Pod again to ensure its Ready conditions are False 08/24/23 12:24:01.396
    STEP: deleting the Pod via a Collection with a LabelSelector 08/24/23 12:24:01.397
    STEP: watching for the Pod to be deleted 08/24/23 12:24:01.414
    Aug 24 12:24:01.418: INFO: observed event type MODIFIED
    Aug 24 12:24:03.358: INFO: observed event type MODIFIED
    Aug 24 12:24:04.371: INFO: observed event type MODIFIED
    Aug 24 12:24:04.384: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:24:04.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4062" for this suite. 08/24/23 12:24:04.413
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:24:04.424
Aug 24 12:24:04.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:24:04.428
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:24:04.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:24:04.463
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 24 12:24:04.487: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 24 12:24:09.499: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:24:09.499
Aug 24 12:24:09.500: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 24 12:24:11.508: INFO: Creating deployment "test-rollover-deployment"
Aug 24 12:24:11.536: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 24 12:24:13.553: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 24 12:24:13.565: INFO: Ensure that both replica sets have 1 created replica
Aug 24 12:24:13.576: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 24 12:24:13.594: INFO: Updating deployment test-rollover-deployment
Aug 24 12:24:13.594: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 24 12:24:15.611: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 24 12:24:15.623: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 24 12:24:15.635: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:24:15.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:24:17.654: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:24:17.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:24:19.654: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:24:19.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:24:21.650: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:24:21.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:24:23.652: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:24:23.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:24:25.655: INFO: 
Aug 24 12:24:25.655: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:24:25.675: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6194  3a253bf1-2f34-467e-804e-d9eda488d067 19583 2 2023-08-24 12:24:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00429eba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:24:11 +0000 UTC,LastTransitionTime:2023-08-24 12:24:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-24 12:24:25 +0000 UTC,LastTransitionTime:2023-08-24 12:24:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 12:24:25.685: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6194  082d9606-3e45-4f9d-85f1-35af12302a59 19572 2 2023-08-24 12:24:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3a253bf1-2f34-467e-804e-d9eda488d067 0xc00708ce17 0xc00708ce18}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a253bf1-2f34-467e-804e-d9eda488d067\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708cec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:24:25.685: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 24 12:24:25.685: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6194  02ba96ae-0477-4111-8b49-8e766acad8ef 19582 2 2023-08-24 12:24:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3a253bf1-2f34-467e-804e-d9eda488d067 0xc00708cce7 0xc00708cce8}] [] [{e2e.test Update apps/v1 2023-08-24 12:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a253bf1-2f34-467e-804e-d9eda488d067\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00708cda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:24:25.685: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6194  4ed4bcfe-839a-44e8-8d3a-e7632da07a96 19532 2 2023-08-24 12:24:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3a253bf1-2f34-467e-804e-d9eda488d067 0xc00708cf47 0xc00708cf48}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a253bf1-2f34-467e-804e-d9eda488d067\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708cff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:24:25.697: INFO: Pod "test-rollover-deployment-6c6df9974f-xvgz8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-xvgz8 test-rollover-deployment-6c6df9974f- deployment-6194  7c44eb38-db0e-4a1c-bdd9-39b740feee3a 19545 0 2023-08-24 12:24:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 082d9606-3e45-4f9d-85f1-35af12302a59 0xc00708d567 0xc00708d568}] [] [{kube-controller-manager Update v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"082d9606-3e45-4f9d-85f1-35af12302a59\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:24:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qvw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qvw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.125,StartTime:2023-08-24 12:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:24:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://b60a484e7ec4b92a1b8df72bd4bfe8273c631898b5e9f0a796b442d913977964,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:24:25.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6194" for this suite. 08/24/23 12:24:25.709
------------------------------
â€¢ [SLOW TEST] [21.307 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:24:04.424
    Aug 24 12:24:04.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:24:04.428
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:24:04.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:24:04.463
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 24 12:24:04.487: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 24 12:24:09.499: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:24:09.499
    Aug 24 12:24:09.500: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 24 12:24:11.508: INFO: Creating deployment "test-rollover-deployment"
    Aug 24 12:24:11.536: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 24 12:24:13.553: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 24 12:24:13.565: INFO: Ensure that both replica sets have 1 created replica
    Aug 24 12:24:13.576: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 24 12:24:13.594: INFO: Updating deployment test-rollover-deployment
    Aug 24 12:24:13.594: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 24 12:24:15.611: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 24 12:24:15.623: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 24 12:24:15.635: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:24:15.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:24:17.654: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:24:17.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:24:19.654: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:24:19.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:24:21.650: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:24:21.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:24:23.652: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:24:23.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:24:25.655: INFO: 
    Aug 24 12:24:25.655: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:24:25.675: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6194  3a253bf1-2f34-467e-804e-d9eda488d067 19583 2 2023-08-24 12:24:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00429eba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:24:11 +0000 UTC,LastTransitionTime:2023-08-24 12:24:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-24 12:24:25 +0000 UTC,LastTransitionTime:2023-08-24 12:24:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 12:24:25.685: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6194  082d9606-3e45-4f9d-85f1-35af12302a59 19572 2 2023-08-24 12:24:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3a253bf1-2f34-467e-804e-d9eda488d067 0xc00708ce17 0xc00708ce18}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a253bf1-2f34-467e-804e-d9eda488d067\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708cec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:24:25.685: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 24 12:24:25.685: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6194  02ba96ae-0477-4111-8b49-8e766acad8ef 19582 2 2023-08-24 12:24:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3a253bf1-2f34-467e-804e-d9eda488d067 0xc00708cce7 0xc00708cce8}] [] [{e2e.test Update apps/v1 2023-08-24 12:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a253bf1-2f34-467e-804e-d9eda488d067\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00708cda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:24:25.685: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6194  4ed4bcfe-839a-44e8-8d3a-e7632da07a96 19532 2 2023-08-24 12:24:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3a253bf1-2f34-467e-804e-d9eda488d067 0xc00708cf47 0xc00708cf48}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a253bf1-2f34-467e-804e-d9eda488d067\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00708cff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:24:25.697: INFO: Pod "test-rollover-deployment-6c6df9974f-xvgz8" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-xvgz8 test-rollover-deployment-6c6df9974f- deployment-6194  7c44eb38-db0e-4a1c-bdd9-39b740feee3a 19545 0 2023-08-24 12:24:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 082d9606-3e45-4f9d-85f1-35af12302a59 0xc00708d567 0xc00708d568}] [] [{kube-controller-manager Update v1 2023-08-24 12:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"082d9606-3e45-4f9d-85f1-35af12302a59\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:24:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qvw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qvw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:24:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.125,StartTime:2023-08-24 12:24:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:24:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://b60a484e7ec4b92a1b8df72bd4bfe8273c631898b5e9f0a796b442d913977964,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:24:25.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6194" for this suite. 08/24/23 12:24:25.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:24:25.736
Aug 24 12:24:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 12:24:25.744
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:24:25.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:24:25.783
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1231 08/24/23 12:24:25.789
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/24/23 12:24:25.812
STEP: Creating stateful set ss in namespace statefulset-1231 08/24/23 12:24:25.816
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1231 08/24/23 12:24:25.829
Aug 24 12:24:25.839: INFO: Found 0 stateful pods, waiting for 1
Aug 24 12:24:35.849: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/24/23 12:24:35.849
Aug 24 12:24:35.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:24:36.123: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:24:36.123: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:24:36.123: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:24:36.129: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 24 12:24:46.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:24:46.141: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:24:46.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999233s
Aug 24 12:24:47.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992517991s
Aug 24 12:24:48.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985141762s
Aug 24 12:24:49.212: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976339295s
Aug 24 12:24:50.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967712079s
Aug 24 12:24:51.228: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960407188s
Aug 24 12:24:52.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951556468s
Aug 24 12:24:53.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943120202s
Aug 24 12:24:54.252: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.934961545s
Aug 24 12:24:55.261: INFO: Verifying statefulset ss doesn't scale past 1 for another 926.827487ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1231 08/24/23 12:24:56.261
Aug 24 12:24:56.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:24:56.528: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 12:24:56.528: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:24:56.528: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:24:56.538: INFO: Found 1 stateful pods, waiting for 3
Aug 24 12:25:06.550: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:25:06.550: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:25:06.550: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/24/23 12:25:06.55
STEP: Scale down will halt with unhealthy stateful pod 08/24/23 12:25:06.55
Aug 24 12:25:06.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:25:06.848: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:25:06.848: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:25:06.848: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:25:06.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:25:07.182: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:25:07.182: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:25:07.182: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:25:07.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:25:07.497: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:25:07.497: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:25:07.497: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:25:07.497: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:25:07.502: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 24 12:25:17.519: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:25:17.520: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:25:17.520: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:25:17.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999758s
Aug 24 12:25:18.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991242738s
Aug 24 12:25:19.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982446647s
Aug 24 12:25:20.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973604151s
Aug 24 12:25:21.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963957558s
Aug 24 12:25:22.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952667003s
Aug 24 12:25:23.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943546508s
Aug 24 12:25:24.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.934191488s
Aug 24 12:25:25.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921650837s
Aug 24 12:25:26.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.901186ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1231 08/24/23 12:25:27.64
Aug 24 12:25:27.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:25:27.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 12:25:27.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:25:27.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:25:27.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:25:28.335: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 12:25:28.335: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:25:28.335: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:25:28.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:25:28.637: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 12:25:28.637: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:25:28.637: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:25:28.637: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/24/23 12:25:38.677
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:25:38.678: INFO: Deleting all statefulset in ns statefulset-1231
Aug 24 12:25:38.686: INFO: Scaling statefulset ss to 0
Aug 24 12:25:38.711: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:25:38.717: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:25:38.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1231" for this suite. 08/24/23 12:25:38.762
------------------------------
â€¢ [SLOW TEST] [73.043 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:24:25.736
    Aug 24 12:24:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:24:25.744
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:24:25.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:24:25.783
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1231 08/24/23 12:24:25.789
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/24/23 12:24:25.812
    STEP: Creating stateful set ss in namespace statefulset-1231 08/24/23 12:24:25.816
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1231 08/24/23 12:24:25.829
    Aug 24 12:24:25.839: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 12:24:35.849: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/24/23 12:24:35.849
    Aug 24 12:24:35.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:24:36.123: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:24:36.123: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:24:36.123: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:24:36.129: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 24 12:24:46.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:24:46.141: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:24:46.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999233s
    Aug 24 12:24:47.194: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992517991s
    Aug 24 12:24:48.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985141762s
    Aug 24 12:24:49.212: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976339295s
    Aug 24 12:24:50.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967712079s
    Aug 24 12:24:51.228: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.960407188s
    Aug 24 12:24:52.235: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.951556468s
    Aug 24 12:24:53.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943120202s
    Aug 24 12:24:54.252: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.934961545s
    Aug 24 12:24:55.261: INFO: Verifying statefulset ss doesn't scale past 1 for another 926.827487ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1231 08/24/23 12:24:56.261
    Aug 24 12:24:56.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:24:56.528: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 12:24:56.528: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:24:56.528: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:24:56.538: INFO: Found 1 stateful pods, waiting for 3
    Aug 24 12:25:06.550: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:25:06.550: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:25:06.550: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/24/23 12:25:06.55
    STEP: Scale down will halt with unhealthy stateful pod 08/24/23 12:25:06.55
    Aug 24 12:25:06.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:25:06.848: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:25:06.848: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:25:06.848: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:25:06.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:25:07.182: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:25:07.182: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:25:07.182: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:25:07.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:25:07.497: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:25:07.497: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:25:07.497: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:25:07.497: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:25:07.502: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 24 12:25:17.519: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:25:17.520: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:25:17.520: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:25:17.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999758s
    Aug 24 12:25:18.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991242738s
    Aug 24 12:25:19.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982446647s
    Aug 24 12:25:20.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.973604151s
    Aug 24 12:25:21.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.963957558s
    Aug 24 12:25:22.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952667003s
    Aug 24 12:25:23.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943546508s
    Aug 24 12:25:24.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.934191488s
    Aug 24 12:25:25.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921650837s
    Aug 24 12:25:26.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.901186ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1231 08/24/23 12:25:27.64
    Aug 24 12:25:27.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:25:27.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 12:25:27.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:25:27.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:25:27.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:25:28.335: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 12:25:28.335: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:25:28.335: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:25:28.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-1231 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:25:28.637: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 12:25:28.637: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:25:28.637: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:25:28.637: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/24/23 12:25:38.677
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:25:38.678: INFO: Deleting all statefulset in ns statefulset-1231
    Aug 24 12:25:38.686: INFO: Scaling statefulset ss to 0
    Aug 24 12:25:38.711: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:25:38.717: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:25:38.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1231" for this suite. 08/24/23 12:25:38.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:25:38.781
Aug 24 12:25:38.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:25:38.787
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:25:38.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:25:38.824
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:25:38.868
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:25:38.88
Aug 24 12:25:38.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:25:38.894: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:25:39.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:25:39.923: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:25:40.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:25:40.911: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:25:41.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:25:41.918: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:25:42.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:25:42.914: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 08/24/23 12:25:42.92
STEP: DeleteCollection of the DaemonSets 08/24/23 12:25:42.931
STEP: Verify that ReplicaSets have been deleted 08/24/23 12:25:42.95
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Aug 24 12:25:42.982: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20038"},"items":null}

Aug 24 12:25:42.992: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20038"},"items":[{"metadata":{"name":"daemon-set-g8l88","generateName":"daemon-set-","namespace":"daemonsets-7867","uid":"5ed3c1a6-1562-4070-a8f5-1f914e667434","resourceVersion":"20024","creationTimestamp":"2023-08-24T12:25:38Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"49126391-02e5-4f9c-8e39-702d32697287","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49126391-02e5-4f9c-8e39-702d32697287\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-778vn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-778vn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pe9deep4seen-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pe9deep4seen-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"}],"hostIP":"192.168.121.130","podIP":"10.233.66.195","podIPs":[{"ip":"10.233.66.195"}],"startTime":"2023-08-24T12:25:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:25:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://0c4a0c4c1f127a0d7634fdb4b6ef2f1186bc747948c1ff5a3d16dfc7b2632467","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q7v59","generateName":"daemon-set-","namespace":"daemonsets-7867","uid":"2c1aa335-92f1-4b11-8331-18243a71c598","resourceVersion":"20034","creationTimestamp":"2023-08-24T12:25:38Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"49126391-02e5-4f9c-8e39-702d32697287","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49126391-02e5-4f9c-8e39-702d32697287\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dhqf6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dhqf6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pe9deep4seen-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pe9deep4seen-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:42Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:42Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"}],"hostIP":"192.168.121.127","podIP":"10.233.64.16","podIPs":[{"ip":"10.233.64.16"}],"startTime":"2023-08-24T12:25:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:25:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://c40cc37da81419ddfd4e9b2eed53a688e3611213015129c1051e7eed414a1d69","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x86rv","generateName":"daemon-set-","namespace":"daemonsets-7867","uid":"055e48e7-5329-45fd-b77d-03f2f0ab2b9c","resourceVersion":"20026","creationTimestamp":"2023-08-24T12:25:38Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"49126391-02e5-4f9c-8e39-702d32697287","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49126391-02e5-4f9c-8e39-702d32697287\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-472zc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-472zc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pe9deep4seen-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pe9deep4seen-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"}],"hostIP":"192.168.121.111","podIP":"10.233.65.43","podIPs":[{"ip":"10.233.65.43"}],"startTime":"2023-08-24T12:25:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:25:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ad912fa2ca3caa2e15573019d661489674f7690fbc5c880bdd3668079a0986f5","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:25:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7867" for this suite. 08/24/23 12:25:43.118
------------------------------
â€¢ [4.355 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:25:38.781
    Aug 24 12:25:38.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:25:38.787
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:25:38.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:25:38.824
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:25:38.868
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:25:38.88
    Aug 24 12:25:38.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:25:38.894: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:25:39.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:25:39.923: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:25:40.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:25:40.911: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:25:41.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:25:41.918: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:25:42.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:25:42.914: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 08/24/23 12:25:42.92
    STEP: DeleteCollection of the DaemonSets 08/24/23 12:25:42.931
    STEP: Verify that ReplicaSets have been deleted 08/24/23 12:25:42.95
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Aug 24 12:25:42.982: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20038"},"items":null}

    Aug 24 12:25:42.992: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20038"},"items":[{"metadata":{"name":"daemon-set-g8l88","generateName":"daemon-set-","namespace":"daemonsets-7867","uid":"5ed3c1a6-1562-4070-a8f5-1f914e667434","resourceVersion":"20024","creationTimestamp":"2023-08-24T12:25:38Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"49126391-02e5-4f9c-8e39-702d32697287","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49126391-02e5-4f9c-8e39-702d32697287\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-778vn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-778vn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pe9deep4seen-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pe9deep4seen-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"}],"hostIP":"192.168.121.130","podIP":"10.233.66.195","podIPs":[{"ip":"10.233.66.195"}],"startTime":"2023-08-24T12:25:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:25:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://0c4a0c4c1f127a0d7634fdb4b6ef2f1186bc747948c1ff5a3d16dfc7b2632467","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q7v59","generateName":"daemon-set-","namespace":"daemonsets-7867","uid":"2c1aa335-92f1-4b11-8331-18243a71c598","resourceVersion":"20034","creationTimestamp":"2023-08-24T12:25:38Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"49126391-02e5-4f9c-8e39-702d32697287","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49126391-02e5-4f9c-8e39-702d32697287\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dhqf6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dhqf6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pe9deep4seen-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pe9deep4seen-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:42Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:42Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"}],"hostIP":"192.168.121.127","podIP":"10.233.64.16","podIPs":[{"ip":"10.233.64.16"}],"startTime":"2023-08-24T12:25:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:25:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://c40cc37da81419ddfd4e9b2eed53a688e3611213015129c1051e7eed414a1d69","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x86rv","generateName":"daemon-set-","namespace":"daemonsets-7867","uid":"055e48e7-5329-45fd-b77d-03f2f0ab2b9c","resourceVersion":"20026","creationTimestamp":"2023-08-24T12:25:38Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"49126391-02e5-4f9c-8e39-702d32697287","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49126391-02e5-4f9c-8e39-702d32697287\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:25:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-472zc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-472zc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"pe9deep4seen-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["pe9deep4seen-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:25:38Z"}],"hostIP":"192.168.121.111","podIP":"10.233.65.43","podIPs":[{"ip":"10.233.65.43"}],"startTime":"2023-08-24T12:25:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:25:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ad912fa2ca3caa2e15573019d661489674f7690fbc5c880bdd3668079a0986f5","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:25:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7867" for this suite. 08/24/23 12:25:43.118
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:25:43.139
Aug 24 12:25:43.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 12:25:43.143
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:25:43.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:25:43.195
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1365 08/24/23 12:25:43.204
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-1365 08/24/23 12:25:43.223
Aug 24 12:25:43.256: INFO: Found 0 stateful pods, waiting for 1
Aug 24 12:25:53.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/24/23 12:25:53.278
STEP: updating a scale subresource 08/24/23 12:25:53.284
STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 12:25:53.295
STEP: Patch a scale subresource 08/24/23 12:25:53.304
STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 12:25:53.32
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:25:53.327: INFO: Deleting all statefulset in ns statefulset-1365
Aug 24 12:25:53.343: INFO: Scaling statefulset ss to 0
Aug 24 12:26:03.442: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:26:03.450: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:26:03.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1365" for this suite. 08/24/23 12:26:03.49
------------------------------
â€¢ [SLOW TEST] [20.363 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:25:43.139
    Aug 24 12:25:43.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:25:43.143
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:25:43.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:25:43.195
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1365 08/24/23 12:25:43.204
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-1365 08/24/23 12:25:43.223
    Aug 24 12:25:43.256: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 12:25:53.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/24/23 12:25:53.278
    STEP: updating a scale subresource 08/24/23 12:25:53.284
    STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 12:25:53.295
    STEP: Patch a scale subresource 08/24/23 12:25:53.304
    STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 12:25:53.32
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:25:53.327: INFO: Deleting all statefulset in ns statefulset-1365
    Aug 24 12:25:53.343: INFO: Scaling statefulset ss to 0
    Aug 24 12:26:03.442: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:26:03.450: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:26:03.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1365" for this suite. 08/24/23 12:26:03.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:26:03.512
Aug 24 12:26:03.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 12:26:03.514
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:03.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:03.553
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 24 12:26:03.655: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e6987ead-b79a-4cb2-9887-aa11a57d82b6", Controller:(*bool)(0xc004b5f636), BlockOwnerDeletion:(*bool)(0xc004b5f637)}}
Aug 24 12:26:03.676: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b2429a34-9311-4b94-8f72-a96e802fab10", Controller:(*bool)(0xc004b5f8aa), BlockOwnerDeletion:(*bool)(0xc004b5f8ab)}}
Aug 24 12:26:03.690: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a6d24715-ca3a-451a-83cf-424c9b14515c", Controller:(*bool)(0xc0050a6446), BlockOwnerDeletion:(*bool)(0xc0050a6447)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 12:26:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4499" for this suite. 08/24/23 12:26:08.726
------------------------------
â€¢ [SLOW TEST] [5.226 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:26:03.512
    Aug 24 12:26:03.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 12:26:03.514
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:03.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:03.553
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 24 12:26:03.655: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e6987ead-b79a-4cb2-9887-aa11a57d82b6", Controller:(*bool)(0xc004b5f636), BlockOwnerDeletion:(*bool)(0xc004b5f637)}}
    Aug 24 12:26:03.676: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b2429a34-9311-4b94-8f72-a96e802fab10", Controller:(*bool)(0xc004b5f8aa), BlockOwnerDeletion:(*bool)(0xc004b5f8ab)}}
    Aug 24 12:26:03.690: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a6d24715-ca3a-451a-83cf-424c9b14515c", Controller:(*bool)(0xc0050a6446), BlockOwnerDeletion:(*bool)(0xc0050a6447)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:26:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4499" for this suite. 08/24/23 12:26:08.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:26:08.74
Aug 24 12:26:08.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename server-version 08/24/23 12:26:08.743
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:08.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:08.781
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/24/23 12:26:08.785
STEP: Confirm major version 08/24/23 12:26:08.786
Aug 24 12:26:08.787: INFO: Major version: 1
STEP: Confirm minor version 08/24/23 12:26:08.787
Aug 24 12:26:08.787: INFO: cleanMinorVersion: 26
Aug 24 12:26:08.787: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 24 12:26:08.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-6323" for this suite. 08/24/23 12:26:08.796
------------------------------
â€¢ [0.069 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:26:08.74
    Aug 24 12:26:08.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename server-version 08/24/23 12:26:08.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:08.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:08.781
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/24/23 12:26:08.785
    STEP: Confirm major version 08/24/23 12:26:08.786
    Aug 24 12:26:08.787: INFO: Major version: 1
    STEP: Confirm minor version 08/24/23 12:26:08.787
    Aug 24 12:26:08.787: INFO: cleanMinorVersion: 26
    Aug 24 12:26:08.787: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:26:08.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-6323" for this suite. 08/24/23 12:26:08.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:26:08.811
Aug 24 12:26:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:26:08.814
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:08.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:08.85
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-a7a01ee3-e5fc-4b22-8f0b-36d967a24c6b 08/24/23 12:26:08.853
STEP: Creating a pod to test consume configMaps 08/24/23 12:26:08.861
Aug 24 12:26:08.875: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1" in namespace "projected-7089" to be "Succeeded or Failed"
Aug 24 12:26:08.881: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.636592ms
Aug 24 12:26:10.890: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01542597s
Aug 24 12:26:12.890: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015327567s
Aug 24 12:26:14.891: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016604244s
STEP: Saw pod success 08/24/23 12:26:14.892
Aug 24 12:26:14.892: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1" satisfied condition "Succeeded or Failed"
Aug 24 12:26:14.897: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:26:14.925
Aug 24 12:26:14.947: INFO: Waiting for pod pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1 to disappear
Aug 24 12:26:14.952: INFO: Pod pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:26:14.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7089" for this suite. 08/24/23 12:26:14.96
------------------------------
â€¢ [SLOW TEST] [6.162 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:26:08.811
    Aug 24 12:26:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:26:08.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:08.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:08.85
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-a7a01ee3-e5fc-4b22-8f0b-36d967a24c6b 08/24/23 12:26:08.853
    STEP: Creating a pod to test consume configMaps 08/24/23 12:26:08.861
    Aug 24 12:26:08.875: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1" in namespace "projected-7089" to be "Succeeded or Failed"
    Aug 24 12:26:08.881: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.636592ms
    Aug 24 12:26:10.890: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01542597s
    Aug 24 12:26:12.890: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015327567s
    Aug 24 12:26:14.891: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016604244s
    STEP: Saw pod success 08/24/23 12:26:14.892
    Aug 24 12:26:14.892: INFO: Pod "pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1" satisfied condition "Succeeded or Failed"
    Aug 24 12:26:14.897: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:26:14.925
    Aug 24 12:26:14.947: INFO: Waiting for pod pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1 to disappear
    Aug 24 12:26:14.952: INFO: Pod pod-projected-configmaps-1084ffcb-adfb-45c2-87a7-b3ecaf8d47c1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:26:14.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7089" for this suite. 08/24/23 12:26:14.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:26:14.978
Aug 24 12:26:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:26:14.98
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:15.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:15.023
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 24 12:26:15.084: INFO: Waiting up to 5m0s for pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b" in namespace "pods-2016" to be "running and ready"
Aug 24 12:26:15.095: INFO: Pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.549729ms
Aug 24 12:26:15.095: INFO: The phase of Pod server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:26:17.104: INFO: Pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b": Phase="Running", Reason="", readiness=true. Elapsed: 2.020221792s
Aug 24 12:26:17.104: INFO: The phase of Pod server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b is Running (Ready = true)
Aug 24 12:26:17.104: INFO: Pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b" satisfied condition "running and ready"
Aug 24 12:26:17.146: INFO: Waiting up to 5m0s for pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8" in namespace "pods-2016" to be "Succeeded or Failed"
Aug 24 12:26:17.154: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.73876ms
Aug 24 12:26:19.160: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014470863s
Aug 24 12:26:21.161: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014864123s
STEP: Saw pod success 08/24/23 12:26:21.161
Aug 24 12:26:21.162: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8" satisfied condition "Succeeded or Failed"
Aug 24 12:26:21.168: INFO: Trying to get logs from node pe9deep4seen-3 pod client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8 container env3cont: <nil>
STEP: delete the pod 08/24/23 12:26:21.18
Aug 24 12:26:21.198: INFO: Waiting for pod client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8 to disappear
Aug 24 12:26:21.203: INFO: Pod client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:26:21.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2016" for this suite. 08/24/23 12:26:21.211
------------------------------
â€¢ [SLOW TEST] [6.244 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:26:14.978
    Aug 24 12:26:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:26:14.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:15.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:15.023
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 24 12:26:15.084: INFO: Waiting up to 5m0s for pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b" in namespace "pods-2016" to be "running and ready"
    Aug 24 12:26:15.095: INFO: Pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.549729ms
    Aug 24 12:26:15.095: INFO: The phase of Pod server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:26:17.104: INFO: Pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b": Phase="Running", Reason="", readiness=true. Elapsed: 2.020221792s
    Aug 24 12:26:17.104: INFO: The phase of Pod server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b is Running (Ready = true)
    Aug 24 12:26:17.104: INFO: Pod "server-envvars-cef2a872-5945-4e63-9889-a6846f7be56b" satisfied condition "running and ready"
    Aug 24 12:26:17.146: INFO: Waiting up to 5m0s for pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8" in namespace "pods-2016" to be "Succeeded or Failed"
    Aug 24 12:26:17.154: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.73876ms
    Aug 24 12:26:19.160: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014470863s
    Aug 24 12:26:21.161: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014864123s
    STEP: Saw pod success 08/24/23 12:26:21.161
    Aug 24 12:26:21.162: INFO: Pod "client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8" satisfied condition "Succeeded or Failed"
    Aug 24 12:26:21.168: INFO: Trying to get logs from node pe9deep4seen-3 pod client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8 container env3cont: <nil>
    STEP: delete the pod 08/24/23 12:26:21.18
    Aug 24 12:26:21.198: INFO: Waiting for pod client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8 to disappear
    Aug 24 12:26:21.203: INFO: Pod client-envvars-d706041e-3eb4-438d-b509-92843f3f8fc8 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:26:21.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2016" for this suite. 08/24/23 12:26:21.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:26:21.226
Aug 24 12:26:21.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 12:26:21.229
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:21.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:21.265
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2689 08/24/23 12:26:21.271
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-2689 08/24/23 12:26:21.284
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2689 08/24/23 12:26:21.301
Aug 24 12:26:21.308: INFO: Found 0 stateful pods, waiting for 1
Aug 24 12:26:31.328: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/24/23 12:26:31.329
Aug 24 12:26:31.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:26:31.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:26:31.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:26:31.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:26:31.690: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 24 12:26:41.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:26:41.712: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:26:41.751: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 24 12:26:41.751: INFO: ss-0  pe9deep4seen-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  }]
Aug 24 12:26:41.752: INFO: 
Aug 24 12:26:41.752: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 24 12:26:42.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985040937s
Aug 24 12:26:43.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97447662s
Aug 24 12:26:44.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965122027s
Aug 24 12:26:45.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956670854s
Aug 24 12:26:46.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946226078s
Aug 24 12:26:47.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936287521s
Aug 24 12:26:48.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.925969662s
Aug 24 12:26:49.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.91662521s
Aug 24 12:26:50.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 907.488408ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2689 08/24/23 12:26:51.842
Aug 24 12:26:51.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:26:52.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 12:26:52.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:26:52.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:26:52.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:26:52.397: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 24 12:26:52.397: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:26:52.397: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:26:52.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:26:52.700: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 24 12:26:52.700: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:26:52.700: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:26:52.708: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 24 12:27:02.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:27:02.720: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:27:02.720: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/24/23 12:27:02.72
Aug 24 12:27:02.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:27:03.089: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:27:03.089: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:27:03.089: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:27:03.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:27:03.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:27:03.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:27:03.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:27:03.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:27:03.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:27:03.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:27:03.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:27:03.751: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:27:03.757: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 24 12:27:13.775: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:27:13.776: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:27:13.776: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:27:13.806: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 24 12:27:13.806: INFO: ss-0  pe9deep4seen-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  }]
Aug 24 12:27:13.807: INFO: ss-1  pe9deep4seen-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  }]
Aug 24 12:27:13.807: INFO: ss-2  pe9deep4seen-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  }]
Aug 24 12:27:13.807: INFO: 
Aug 24 12:27:13.807: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 24 12:27:14.813: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 24 12:27:14.814: INFO: ss-0  pe9deep4seen-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  }]
Aug 24 12:27:14.814: INFO: ss-1  pe9deep4seen-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  }]
Aug 24 12:27:14.814: INFO: 
Aug 24 12:27:14.814: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 24 12:27:15.822: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.983616254s
Aug 24 12:27:16.828: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975798193s
Aug 24 12:27:17.836: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969662968s
Aug 24 12:27:18.842: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.961710368s
Aug 24 12:27:19.849: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.955257273s
Aug 24 12:27:20.856: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.94885905s
Aug 24 12:27:21.862: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.941384868s
Aug 24 12:27:22.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 935.945647ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2689 08/24/23 12:27:23.87
Aug 24 12:27:23.878: INFO: Scaling statefulset ss to 0
Aug 24 12:27:23.898: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:27:23.904: INFO: Deleting all statefulset in ns statefulset-2689
Aug 24 12:27:23.909: INFO: Scaling statefulset ss to 0
Aug 24 12:27:23.928: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:27:23.932: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:27:23.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2689" for this suite. 08/24/23 12:27:23.97
------------------------------
â€¢ [SLOW TEST] [62.758 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:26:21.226
    Aug 24 12:26:21.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:26:21.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:26:21.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:26:21.265
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2689 08/24/23 12:26:21.271
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-2689 08/24/23 12:26:21.284
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2689 08/24/23 12:26:21.301
    Aug 24 12:26:21.308: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 12:26:31.328: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/24/23 12:26:31.329
    Aug 24 12:26:31.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:26:31.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:26:31.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:26:31.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:26:31.690: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 24 12:26:41.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:26:41.712: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:26:41.751: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 24 12:26:41.751: INFO: ss-0  pe9deep4seen-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  }]
    Aug 24 12:26:41.752: INFO: 
    Aug 24 12:26:41.752: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 24 12:26:42.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985040937s
    Aug 24 12:26:43.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97447662s
    Aug 24 12:26:44.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965122027s
    Aug 24 12:26:45.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956670854s
    Aug 24 12:26:46.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946226078s
    Aug 24 12:26:47.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936287521s
    Aug 24 12:26:48.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.925969662s
    Aug 24 12:26:49.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.91662521s
    Aug 24 12:26:50.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 907.488408ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2689 08/24/23 12:26:51.842
    Aug 24 12:26:51.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:26:52.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 12:26:52.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:26:52.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:26:52.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:26:52.397: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 24 12:26:52.397: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:26:52.397: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:26:52.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:26:52.700: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 24 12:26:52.700: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:26:52.700: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:26:52.708: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 24 12:27:02.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:27:02.720: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:27:02.720: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/24/23 12:27:02.72
    Aug 24 12:27:02.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:27:03.089: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:27:03.089: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:27:03.089: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:27:03.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:27:03.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:27:03.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:27:03.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:27:03.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-2689 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:27:03.751: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:27:03.751: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:27:03.751: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:27:03.751: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:27:03.757: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 24 12:27:13.775: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:27:13.776: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:27:13.776: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:27:13.806: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 24 12:27:13.806: INFO: ss-0  pe9deep4seen-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  }]
    Aug 24 12:27:13.807: INFO: ss-1  pe9deep4seen-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  }]
    Aug 24 12:27:13.807: INFO: ss-2  pe9deep4seen-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  }]
    Aug 24 12:27:13.807: INFO: 
    Aug 24 12:27:13.807: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 24 12:27:14.813: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 24 12:27:14.814: INFO: ss-0  pe9deep4seen-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:21 +0000 UTC  }]
    Aug 24 12:27:14.814: INFO: ss-1  pe9deep4seen-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:26:41 +0000 UTC  }]
    Aug 24 12:27:14.814: INFO: 
    Aug 24 12:27:14.814: INFO: StatefulSet ss has not reached scale 0, at 2
    Aug 24 12:27:15.822: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.983616254s
    Aug 24 12:27:16.828: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975798193s
    Aug 24 12:27:17.836: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969662968s
    Aug 24 12:27:18.842: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.961710368s
    Aug 24 12:27:19.849: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.955257273s
    Aug 24 12:27:20.856: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.94885905s
    Aug 24 12:27:21.862: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.941384868s
    Aug 24 12:27:22.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 935.945647ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2689 08/24/23 12:27:23.87
    Aug 24 12:27:23.878: INFO: Scaling statefulset ss to 0
    Aug 24 12:27:23.898: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:27:23.904: INFO: Deleting all statefulset in ns statefulset-2689
    Aug 24 12:27:23.909: INFO: Scaling statefulset ss to 0
    Aug 24 12:27:23.928: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:27:23.932: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:27:23.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2689" for this suite. 08/24/23 12:27:23.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:27:23.995
Aug 24 12:27:23.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:27:23.998
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:24.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:24.036
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7870 08/24/23 12:27:24.04
STEP: creating service affinity-clusterip in namespace services-7870 08/24/23 12:27:24.041
STEP: creating replication controller affinity-clusterip in namespace services-7870 08/24/23 12:27:24.069
I0824 12:27:24.101778      14 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7870, replica count: 3
I0824 12:27:27.155872      14 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:27:27.174: INFO: Creating new exec pod
Aug 24 12:27:27.197: INFO: Waiting up to 5m0s for pod "execpod-affinity9xqkr" in namespace "services-7870" to be "running"
Aug 24 12:27:27.207: INFO: Pod "execpod-affinity9xqkr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305999ms
Aug 24 12:27:29.215: INFO: Pod "execpod-affinity9xqkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.017902207s
Aug 24 12:27:29.215: INFO: Pod "execpod-affinity9xqkr" satisfied condition "running"
Aug 24 12:27:30.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7870 exec execpod-affinity9xqkr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 24 12:27:30.567: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 24 12:27:30.567: INFO: stdout: ""
Aug 24 12:27:30.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7870 exec execpod-affinity9xqkr -- /bin/sh -x -c nc -v -z -w 2 10.233.43.126 80'
Aug 24 12:27:30.839: INFO: stderr: "+ nc -v -z -w 2 10.233.43.126 80\nConnection to 10.233.43.126 80 port [tcp/http] succeeded!\n"
Aug 24 12:27:30.839: INFO: stdout: ""
Aug 24 12:27:30.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7870 exec execpod-affinity9xqkr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.126:80/ ; done'
Aug 24 12:27:31.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n"
Aug 24 12:27:31.341: INFO: stdout: "\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv"
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
Aug 24 12:27:31.341: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7870, will wait for the garbage collector to delete the pods 08/24/23 12:27:31.36
Aug 24 12:27:31.446: INFO: Deleting ReplicationController affinity-clusterip took: 18.530844ms
Aug 24 12:27:31.547: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.200372ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:27:33.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7870" for this suite. 08/24/23 12:27:33.616
------------------------------
â€¢ [SLOW TEST] [9.638 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:27:23.995
    Aug 24 12:27:23.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:27:23.998
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:24.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:24.036
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7870 08/24/23 12:27:24.04
    STEP: creating service affinity-clusterip in namespace services-7870 08/24/23 12:27:24.041
    STEP: creating replication controller affinity-clusterip in namespace services-7870 08/24/23 12:27:24.069
    I0824 12:27:24.101778      14 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7870, replica count: 3
    I0824 12:27:27.155872      14 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:27:27.174: INFO: Creating new exec pod
    Aug 24 12:27:27.197: INFO: Waiting up to 5m0s for pod "execpod-affinity9xqkr" in namespace "services-7870" to be "running"
    Aug 24 12:27:27.207: INFO: Pod "execpod-affinity9xqkr": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305999ms
    Aug 24 12:27:29.215: INFO: Pod "execpod-affinity9xqkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.017902207s
    Aug 24 12:27:29.215: INFO: Pod "execpod-affinity9xqkr" satisfied condition "running"
    Aug 24 12:27:30.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7870 exec execpod-affinity9xqkr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 24 12:27:30.567: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 24 12:27:30.567: INFO: stdout: ""
    Aug 24 12:27:30.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7870 exec execpod-affinity9xqkr -- /bin/sh -x -c nc -v -z -w 2 10.233.43.126 80'
    Aug 24 12:27:30.839: INFO: stderr: "+ nc -v -z -w 2 10.233.43.126 80\nConnection to 10.233.43.126 80 port [tcp/http] succeeded!\n"
    Aug 24 12:27:30.839: INFO: stdout: ""
    Aug 24 12:27:30.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7870 exec execpod-affinity9xqkr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.126:80/ ; done'
    Aug 24 12:27:31.341: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.126:80/\n"
    Aug 24 12:27:31.341: INFO: stdout: "\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv\naffinity-clusterip-k94xv"
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Received response from host: affinity-clusterip-k94xv
    Aug 24 12:27:31.341: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7870, will wait for the garbage collector to delete the pods 08/24/23 12:27:31.36
    Aug 24 12:27:31.446: INFO: Deleting ReplicationController affinity-clusterip took: 18.530844ms
    Aug 24 12:27:31.547: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.200372ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:27:33.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7870" for this suite. 08/24/23 12:27:33.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:27:33.643
Aug 24 12:27:33.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:27:33.645
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:33.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:33.687
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/24/23 12:27:33.695
Aug 24 12:27:33.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 24 12:27:33.853: INFO: stderr: ""
Aug 24 12:27:33.853: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/24/23 12:27:33.854
Aug 24 12:27:33.854: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 24 12:27:33.854: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9675" to be "running and ready, or succeeded"
Aug 24 12:27:33.861: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.967357ms
Aug 24 12:27:33.862: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pe9deep4seen-3' to be 'Running' but was 'Pending'
Aug 24 12:27:35.870: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015848454s
Aug 24 12:27:35.870: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 24 12:27:35.870: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/24/23 12:27:35.87
Aug 24 12:27:35.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator'
Aug 24 12:27:36.017: INFO: stderr: ""
Aug 24 12:27:36.017: INFO: stdout: "I0824 12:27:34.930978       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/znv4 210\nI0824 12:27:35.131199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/8sbt 544\nI0824 12:27:35.331694       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/svw 480\nI0824 12:27:35.531185       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/bgxt 257\nI0824 12:27:35.731614       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zj8k 229\nI0824 12:27:35.932177       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lzpd 541\n"
STEP: limiting log lines 08/24/23 12:27:36.017
Aug 24 12:27:36.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --tail=1'
Aug 24 12:27:36.172: INFO: stderr: ""
Aug 24 12:27:36.172: INFO: stdout: "I0824 12:27:36.131698       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5lr 231\n"
Aug 24 12:27:36.172: INFO: got output "I0824 12:27:36.131698       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5lr 231\n"
STEP: limiting log bytes 08/24/23 12:27:36.172
Aug 24 12:27:36.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --limit-bytes=1'
Aug 24 12:27:36.331: INFO: stderr: ""
Aug 24 12:27:36.331: INFO: stdout: "I"
Aug 24 12:27:36.331: INFO: got output "I"
STEP: exposing timestamps 08/24/23 12:27:36.331
Aug 24 12:27:36.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 24 12:27:36.499: INFO: stderr: ""
Aug 24 12:27:36.499: INFO: stdout: "2023-08-24T12:27:36.331014638Z I0824 12:27:36.330868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pcl 583\n"
Aug 24 12:27:36.499: INFO: got output "2023-08-24T12:27:36.331014638Z I0824 12:27:36.330868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pcl 583\n"
STEP: restricting to a time range 08/24/23 12:27:36.499
Aug 24 12:27:39.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --since=1s'
Aug 24 12:27:39.171: INFO: stderr: ""
Aug 24 12:27:39.171: INFO: stdout: "I0824 12:27:38.331636       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/4pc 348\nI0824 12:27:38.530935       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/6cn2 210\nI0824 12:27:38.731376       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/p57k 514\nI0824 12:27:38.931626       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5t4 448\nI0824 12:27:39.131113       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jp7 327\n"
Aug 24 12:27:39.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --since=24h'
Aug 24 12:27:39.357: INFO: stderr: ""
Aug 24 12:27:39.357: INFO: stdout: "I0824 12:27:34.930978       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/znv4 210\nI0824 12:27:35.131199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/8sbt 544\nI0824 12:27:35.331694       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/svw 480\nI0824 12:27:35.531185       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/bgxt 257\nI0824 12:27:35.731614       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zj8k 229\nI0824 12:27:35.932177       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lzpd 541\nI0824 12:27:36.131698       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5lr 231\nI0824 12:27:36.330868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pcl 583\nI0824 12:27:36.531347       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/nl7k 500\nI0824 12:27:36.730834       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/pqc 446\nI0824 12:27:36.931352       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/k8tf 366\nI0824 12:27:37.130756       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/hs5 405\nI0824 12:27:37.331332       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/2th 310\nI0824 12:27:37.531642       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/xw8 209\nI0824 12:27:37.731132       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/s62 555\nI0824 12:27:37.931919       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xbsm 523\nI0824 12:27:38.131114       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/wb4 412\nI0824 12:27:38.331636       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/4pc 348\nI0824 12:27:38.530935       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/6cn2 210\nI0824 12:27:38.731376       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/p57k 514\nI0824 12:27:38.931626       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5t4 448\nI0824 12:27:39.131113       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jp7 327\nI0824 12:27:39.331322       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/mcjq 305\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 24 12:27:39.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 delete pod logs-generator'
Aug 24 12:27:40.273: INFO: stderr: ""
Aug 24 12:27:40.273: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:27:40.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9675" for this suite. 08/24/23 12:27:40.287
------------------------------
â€¢ [SLOW TEST] [6.658 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:27:33.643
    Aug 24 12:27:33.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:27:33.645
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:33.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:33.687
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/24/23 12:27:33.695
    Aug 24 12:27:33.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 24 12:27:33.853: INFO: stderr: ""
    Aug 24 12:27:33.853: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/24/23 12:27:33.854
    Aug 24 12:27:33.854: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 24 12:27:33.854: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9675" to be "running and ready, or succeeded"
    Aug 24 12:27:33.861: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.967357ms
    Aug 24 12:27:33.862: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'pe9deep4seen-3' to be 'Running' but was 'Pending'
    Aug 24 12:27:35.870: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.015848454s
    Aug 24 12:27:35.870: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 24 12:27:35.870: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/24/23 12:27:35.87
    Aug 24 12:27:35.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator'
    Aug 24 12:27:36.017: INFO: stderr: ""
    Aug 24 12:27:36.017: INFO: stdout: "I0824 12:27:34.930978       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/znv4 210\nI0824 12:27:35.131199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/8sbt 544\nI0824 12:27:35.331694       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/svw 480\nI0824 12:27:35.531185       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/bgxt 257\nI0824 12:27:35.731614       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zj8k 229\nI0824 12:27:35.932177       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lzpd 541\n"
    STEP: limiting log lines 08/24/23 12:27:36.017
    Aug 24 12:27:36.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --tail=1'
    Aug 24 12:27:36.172: INFO: stderr: ""
    Aug 24 12:27:36.172: INFO: stdout: "I0824 12:27:36.131698       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5lr 231\n"
    Aug 24 12:27:36.172: INFO: got output "I0824 12:27:36.131698       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5lr 231\n"
    STEP: limiting log bytes 08/24/23 12:27:36.172
    Aug 24 12:27:36.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --limit-bytes=1'
    Aug 24 12:27:36.331: INFO: stderr: ""
    Aug 24 12:27:36.331: INFO: stdout: "I"
    Aug 24 12:27:36.331: INFO: got output "I"
    STEP: exposing timestamps 08/24/23 12:27:36.331
    Aug 24 12:27:36.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 24 12:27:36.499: INFO: stderr: ""
    Aug 24 12:27:36.499: INFO: stdout: "2023-08-24T12:27:36.331014638Z I0824 12:27:36.330868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pcl 583\n"
    Aug 24 12:27:36.499: INFO: got output "2023-08-24T12:27:36.331014638Z I0824 12:27:36.330868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pcl 583\n"
    STEP: restricting to a time range 08/24/23 12:27:36.499
    Aug 24 12:27:39.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --since=1s'
    Aug 24 12:27:39.171: INFO: stderr: ""
    Aug 24 12:27:39.171: INFO: stdout: "I0824 12:27:38.331636       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/4pc 348\nI0824 12:27:38.530935       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/6cn2 210\nI0824 12:27:38.731376       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/p57k 514\nI0824 12:27:38.931626       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5t4 448\nI0824 12:27:39.131113       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jp7 327\n"
    Aug 24 12:27:39.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 logs logs-generator logs-generator --since=24h'
    Aug 24 12:27:39.357: INFO: stderr: ""
    Aug 24 12:27:39.357: INFO: stdout: "I0824 12:27:34.930978       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/znv4 210\nI0824 12:27:35.131199       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/8sbt 544\nI0824 12:27:35.331694       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/svw 480\nI0824 12:27:35.531185       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/bgxt 257\nI0824 12:27:35.731614       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/zj8k 229\nI0824 12:27:35.932177       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lzpd 541\nI0824 12:27:36.131698       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5lr 231\nI0824 12:27:36.330868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pcl 583\nI0824 12:27:36.531347       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/nl7k 500\nI0824 12:27:36.730834       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/pqc 446\nI0824 12:27:36.931352       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/k8tf 366\nI0824 12:27:37.130756       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/hs5 405\nI0824 12:27:37.331332       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/2th 310\nI0824 12:27:37.531642       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/xw8 209\nI0824 12:27:37.731132       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/s62 555\nI0824 12:27:37.931919       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xbsm 523\nI0824 12:27:38.131114       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/wb4 412\nI0824 12:27:38.331636       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/4pc 348\nI0824 12:27:38.530935       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/6cn2 210\nI0824 12:27:38.731376       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/p57k 514\nI0824 12:27:38.931626       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5t4 448\nI0824 12:27:39.131113       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/jp7 327\nI0824 12:27:39.331322       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/mcjq 305\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 24 12:27:39.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9675 delete pod logs-generator'
    Aug 24 12:27:40.273: INFO: stderr: ""
    Aug 24 12:27:40.273: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:27:40.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9675" for this suite. 08/24/23 12:27:40.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:27:40.303
Aug 24 12:27:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:27:40.305
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:40.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:40.344
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 24 12:27:40.349: INFO: Creating simple deployment test-new-deployment
Aug 24 12:27:40.385: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/24/23 12:27:42.411
STEP: updating a scale subresource 08/24/23 12:27:42.416
STEP: verifying the deployment Spec.Replicas was modified 08/24/23 12:27:42.428
STEP: Patch a scale subresource 08/24/23 12:27:42.435
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:27:42.464: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4918  acb71e51-b7e0-4ba6-a924-3a0767ad3607 20995 3 2023-08-24 12:27:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-24 12:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d79c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:27:42 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-24 12:27:42 +0000 UTC,LastTransitionTime:2023-08-24 12:27:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 12:27:42.470: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4918  e08390d1-409f-4ad4-8639-40e8d9a6aeea 20994 2 2023-08-24 12:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment acb71e51-b7e0-4ba6-a924-3a0767ad3607 0xc001ed2ba7 0xc001ed2ba8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acb71e51-b7e0-4ba6-a924-3a0767ad3607\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ed2c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:27:42.509: INFO: Pod "test-new-deployment-7f5969cbc7-dsmqf" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dsmqf test-new-deployment-7f5969cbc7- deployment-4918  f40c9770-97f5-4e7f-9011-b2d3ad936ff7 20990 0 2023-08-24 12:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e08390d1-409f-4ad4-8639-40e8d9a6aeea 0xc00028ea97 0xc00028ea98}] [] [{kube-controller-manager Update v1 2023-08-24 12:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e08390d1-409f-4ad4-8639-40e8d9a6aeea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rjbvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rjbvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.88,StartTime:2023-08-24 12:27:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:27:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://918fecbcb2c2f6f7ac9698deb7fe0b4cb9558516906641466086e37f16bbd737,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:27:42.510: INFO: Pod "test-new-deployment-7f5969cbc7-ghvk6" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-ghvk6 test-new-deployment-7f5969cbc7- deployment-4918  69d73c7f-f2e1-4fa0-8e85-20d46958b5f2 20999 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e08390d1-409f-4ad4-8639-40e8d9a6aeea 0xc00028f287 0xc00028f288}] [] [{kube-controller-manager Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e08390d1-409f-4ad4-8639-40e8d9a6aeea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfbb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfbb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:27:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4918" for this suite. 08/24/23 12:27:42.532
------------------------------
â€¢ [2.239 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:27:40.303
    Aug 24 12:27:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:27:40.305
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:40.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:40.344
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 24 12:27:40.349: INFO: Creating simple deployment test-new-deployment
    Aug 24 12:27:40.385: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/24/23 12:27:42.411
    STEP: updating a scale subresource 08/24/23 12:27:42.416
    STEP: verifying the deployment Spec.Replicas was modified 08/24/23 12:27:42.428
    STEP: Patch a scale subresource 08/24/23 12:27:42.435
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:27:42.464: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4918  acb71e51-b7e0-4ba6-a924-3a0767ad3607 20995 3 2023-08-24 12:27:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-24 12:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d79c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:27:42 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-24 12:27:42 +0000 UTC,LastTransitionTime:2023-08-24 12:27:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 12:27:42.470: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4918  e08390d1-409f-4ad4-8639-40e8d9a6aeea 20994 2 2023-08-24 12:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment acb71e51-b7e0-4ba6-a924-3a0767ad3607 0xc001ed2ba7 0xc001ed2ba8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acb71e51-b7e0-4ba6-a924-3a0767ad3607\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ed2c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:27:42.509: INFO: Pod "test-new-deployment-7f5969cbc7-dsmqf" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-dsmqf test-new-deployment-7f5969cbc7- deployment-4918  f40c9770-97f5-4e7f-9011-b2d3ad936ff7 20990 0 2023-08-24 12:27:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e08390d1-409f-4ad4-8639-40e8d9a6aeea 0xc00028ea97 0xc00028ea98}] [] [{kube-controller-manager Update v1 2023-08-24 12:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e08390d1-409f-4ad4-8639-40e8d9a6aeea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rjbvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rjbvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.88,StartTime:2023-08-24 12:27:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:27:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://918fecbcb2c2f6f7ac9698deb7fe0b4cb9558516906641466086e37f16bbd737,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:27:42.510: INFO: Pod "test-new-deployment-7f5969cbc7-ghvk6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-ghvk6 test-new-deployment-7f5969cbc7- deployment-4918  69d73c7f-f2e1-4fa0-8e85-20d46958b5f2 20999 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e08390d1-409f-4ad4-8639-40e8d9a6aeea 0xc00028f287 0xc00028f288}] [] [{kube-controller-manager Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e08390d1-409f-4ad4-8639-40e8d9a6aeea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xfbb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xfbb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:27:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4918" for this suite. 08/24/23 12:27:42.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:27:42.558
Aug 24 12:27:42.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename watch 08/24/23 12:27:42.561
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:42.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:42.607
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/24/23 12:27:42.614
STEP: creating a new configmap 08/24/23 12:27:42.617
STEP: modifying the configmap once 08/24/23 12:27:42.629
STEP: closing the watch once it receives two notifications 08/24/23 12:27:42.644
Aug 24 12:27:42.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21017 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:27:42.645: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21018 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/24/23 12:27:42.646
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/24/23 12:27:42.669
STEP: deleting the configmap 08/24/23 12:27:42.672
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/24/23 12:27:42.684
Aug 24 12:27:42.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21019 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:27:42.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21020 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:27:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1076" for this suite. 08/24/23 12:27:42.693
------------------------------
â€¢ [0.151 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:27:42.558
    Aug 24 12:27:42.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename watch 08/24/23 12:27:42.561
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:42.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:42.607
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/24/23 12:27:42.614
    STEP: creating a new configmap 08/24/23 12:27:42.617
    STEP: modifying the configmap once 08/24/23 12:27:42.629
    STEP: closing the watch once it receives two notifications 08/24/23 12:27:42.644
    Aug 24 12:27:42.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21017 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:27:42.645: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21018 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/24/23 12:27:42.646
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/24/23 12:27:42.669
    STEP: deleting the configmap 08/24/23 12:27:42.672
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/24/23 12:27:42.684
    Aug 24 12:27:42.685: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21019 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:27:42.685: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1076  ec9e324a-5585-4a1a-a813-0dc785b93a76 21020 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:27:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1076" for this suite. 08/24/23 12:27:42.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:27:42.712
Aug 24 12:27:42.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:27:42.714
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:42.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:42.746
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/24/23 12:27:42.772
Aug 24 12:27:42.772: INFO: Creating simple deployment test-deployment-4skvs
Aug 24 12:27:42.794: INFO: deployment "test-deployment-4skvs" doesn't have the required revision set
STEP: Getting /status 08/24/23 12:27:44.818
Aug 24 12:27:44.826: INFO: Deployment test-deployment-4skvs has Conditions: [{Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/24/23 12:27:44.826
Aug 24 12:27:44.848: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 27, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 27, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 27, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 27, 42, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4skvs-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/24/23 12:27:44.849
Aug 24 12:27:44.853: INFO: Observed &Deployment event: ADDED
Aug 24 12:27:44.853: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
Aug 24 12:27:44.853: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.853: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
Aug 24 12:27:44.854: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 12:27:44.854: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.854: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 12:27:44.854: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4skvs-54bc444df" is progressing.}
Aug 24 12:27:44.855: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.855: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 12:27:44.855: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
Aug 24 12:27:44.855: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.856: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 12:27:44.856: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
Aug 24 12:27:44.856: INFO: Found Deployment test-deployment-4skvs in namespace deployment-7218 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 12:27:44.856: INFO: Deployment test-deployment-4skvs has an updated status
STEP: patching the Statefulset Status 08/24/23 12:27:44.856
Aug 24 12:27:44.856: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 24 12:27:44.869: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/24/23 12:27:44.869
Aug 24 12:27:44.873: INFO: Observed &Deployment event: ADDED
Aug 24 12:27:44.874: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
Aug 24 12:27:44.874: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.874: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
Aug 24 12:27:44.874: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 12:27:44.874: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.875: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 12:27:44.875: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4skvs-54bc444df" is progressing.}
Aug 24 12:27:44.875: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.875: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
Aug 24 12:27:44.876: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 12:27:44.877: INFO: Observed &Deployment event: MODIFIED
Aug 24 12:27:44.877: INFO: Found deployment test-deployment-4skvs in namespace deployment-7218 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 24 12:27:44.877: INFO: Deployment test-deployment-4skvs has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:27:44.889: INFO: Deployment "test-deployment-4skvs":
&Deployment{ObjectMeta:{test-deployment-4skvs  deployment-7218  f6ba29ac-5823-408c-bc41-89372e6452e5 21053 1 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a73d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-4skvs-54bc444df",LastUpdateTime:2023-08-24 12:27:44 +0000 UTC,LastTransitionTime:2023-08-24 12:27:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 12:27:44.896: INFO: New ReplicaSet "test-deployment-4skvs-54bc444df" of Deployment "test-deployment-4skvs":
&ReplicaSet{ObjectMeta:{test-deployment-4skvs-54bc444df  deployment-7218  d849b5e1-12cb-486c-876c-146e16b80a09 21049 1 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4skvs f6ba29ac-5823-408c-bc41-89372e6452e5 0xc004b5e880 0xc004b5e881}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6ba29ac-5823-408c-bc41-89372e6452e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b5e928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:27:44.907: INFO: Pod "test-deployment-4skvs-54bc444df-hlclx" is available:
&Pod{ObjectMeta:{test-deployment-4skvs-54bc444df-hlclx test-deployment-4skvs-54bc444df- deployment-7218  50c5f5b0-a375-420d-a63c-9fcdee59d57e 21048 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-4skvs-54bc444df d849b5e1-12cb-486c-876c-146e16b80a09 0xc004b5ecf0 0xc004b5ecf1}] [] [{kube-controller-manager Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d849b5e1-12cb-486c-876c-146e16b80a09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bblgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bblgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.19,StartTime:2023-08-24 12:27:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:27:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4f94a4347c3752ae3e32f6fbeea00b396ca95bd92de842b23b5800e386586382,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:27:44.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7218" for this suite. 08/24/23 12:27:44.918
------------------------------
â€¢ [2.218 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:27:42.712
    Aug 24 12:27:42.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:27:42.714
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:42.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:42.746
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/24/23 12:27:42.772
    Aug 24 12:27:42.772: INFO: Creating simple deployment test-deployment-4skvs
    Aug 24 12:27:42.794: INFO: deployment "test-deployment-4skvs" doesn't have the required revision set
    STEP: Getting /status 08/24/23 12:27:44.818
    Aug 24 12:27:44.826: INFO: Deployment test-deployment-4skvs has Conditions: [{Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/24/23 12:27:44.826
    Aug 24 12:27:44.848: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 27, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 27, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 27, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 27, 42, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-4skvs-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/24/23 12:27:44.849
    Aug 24 12:27:44.853: INFO: Observed &Deployment event: ADDED
    Aug 24 12:27:44.853: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
    Aug 24 12:27:44.853: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.853: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
    Aug 24 12:27:44.854: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 12:27:44.854: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.854: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 12:27:44.854: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4skvs-54bc444df" is progressing.}
    Aug 24 12:27:44.855: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.855: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 12:27:44.855: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
    Aug 24 12:27:44.855: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.856: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 12:27:44.856: INFO: Observed Deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
    Aug 24 12:27:44.856: INFO: Found Deployment test-deployment-4skvs in namespace deployment-7218 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 12:27:44.856: INFO: Deployment test-deployment-4skvs has an updated status
    STEP: patching the Statefulset Status 08/24/23 12:27:44.856
    Aug 24 12:27:44.856: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 24 12:27:44.869: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/24/23 12:27:44.869
    Aug 24 12:27:44.873: INFO: Observed &Deployment event: ADDED
    Aug 24 12:27:44.874: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
    Aug 24 12:27:44.874: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.874: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-4skvs-54bc444df"}
    Aug 24 12:27:44.874: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 12:27:44.874: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.875: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 12:27:44.875: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:42 +0000 UTC 2023-08-24 12:27:42 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-4skvs-54bc444df" is progressing.}
    Aug 24 12:27:44.875: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.875: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
    Aug 24 12:27:44.876: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:27:44 +0000 UTC 2023-08-24 12:27:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-4skvs-54bc444df" has successfully progressed.}
    Aug 24 12:27:44.876: INFO: Observed deployment test-deployment-4skvs in namespace deployment-7218 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 12:27:44.877: INFO: Observed &Deployment event: MODIFIED
    Aug 24 12:27:44.877: INFO: Found deployment test-deployment-4skvs in namespace deployment-7218 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 24 12:27:44.877: INFO: Deployment test-deployment-4skvs has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:27:44.889: INFO: Deployment "test-deployment-4skvs":
    &Deployment{ObjectMeta:{test-deployment-4skvs  deployment-7218  f6ba29ac-5823-408c-bc41-89372e6452e5 21053 1 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050a73d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-4skvs-54bc444df",LastUpdateTime:2023-08-24 12:27:44 +0000 UTC,LastTransitionTime:2023-08-24 12:27:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 12:27:44.896: INFO: New ReplicaSet "test-deployment-4skvs-54bc444df" of Deployment "test-deployment-4skvs":
    &ReplicaSet{ObjectMeta:{test-deployment-4skvs-54bc444df  deployment-7218  d849b5e1-12cb-486c-876c-146e16b80a09 21049 1 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-4skvs f6ba29ac-5823-408c-bc41-89372e6452e5 0xc004b5e880 0xc004b5e881}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6ba29ac-5823-408c-bc41-89372e6452e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b5e928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:27:44.907: INFO: Pod "test-deployment-4skvs-54bc444df-hlclx" is available:
    &Pod{ObjectMeta:{test-deployment-4skvs-54bc444df-hlclx test-deployment-4skvs-54bc444df- deployment-7218  50c5f5b0-a375-420d-a63c-9fcdee59d57e 21048 0 2023-08-24 12:27:42 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-4skvs-54bc444df d849b5e1-12cb-486c-876c-146e16b80a09 0xc004b5ecf0 0xc004b5ecf1}] [] [{kube-controller-manager Update v1 2023-08-24 12:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d849b5e1-12cb-486c-876c-146e16b80a09\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:27:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bblgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bblgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:27:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.19,StartTime:2023-08-24 12:27:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:27:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4f94a4347c3752ae3e32f6fbeea00b396ca95bd92de842b23b5800e386586382,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:27:44.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7218" for this suite. 08/24/23 12:27:44.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:27:44.945
Aug 24 12:27:44.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename cronjob 08/24/23 12:27:44.947
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:44.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:44.981
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/24/23 12:27:44.987
STEP: Ensuring a job is scheduled 08/24/23 12:27:44.998
STEP: Ensuring exactly one is scheduled 08/24/23 12:28:01.007
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 12:28:01.015
STEP: Ensuring no more jobs are scheduled 08/24/23 12:28:01.023
STEP: Removing cronjob 08/24/23 12:33:01.037
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6531" for this suite. 08/24/23 12:33:01.067
------------------------------
â€¢ [SLOW TEST] [316.134 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:27:44.945
    Aug 24 12:27:44.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename cronjob 08/24/23 12:27:44.947
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:27:44.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:27:44.981
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/24/23 12:27:44.987
    STEP: Ensuring a job is scheduled 08/24/23 12:27:44.998
    STEP: Ensuring exactly one is scheduled 08/24/23 12:28:01.007
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 12:28:01.015
    STEP: Ensuring no more jobs are scheduled 08/24/23 12:28:01.023
    STEP: Removing cronjob 08/24/23 12:33:01.037
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6531" for this suite. 08/24/23 12:33:01.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:01.095
Aug 24 12:33:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename endpointslice 08/24/23 12:33:01.098
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.154
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 24 12:33:01.206: INFO: Endpoints addresses: [192.168.121.111 192.168.121.127] , ports: [6443]
Aug 24 12:33:01.206: INFO: EndpointSlices addresses: [192.168.121.111 192.168.121.127] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:01.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9154" for this suite. 08/24/23 12:33:01.216
------------------------------
â€¢ [0.143 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:01.095
    Aug 24 12:33:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename endpointslice 08/24/23 12:33:01.098
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.154
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 24 12:33:01.206: INFO: Endpoints addresses: [192.168.121.111 192.168.121.127] , ports: [6443]
    Aug 24 12:33:01.206: INFO: EndpointSlices addresses: [192.168.121.111 192.168.121.127] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:01.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9154" for this suite. 08/24/23 12:33:01.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:01.242
Aug 24 12:33:01.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:33:01.245
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.283
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 24 12:33:01.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5078 version'
Aug 24 12:33:01.468: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 24 12:33:01.468: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:01.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5078" for this suite. 08/24/23 12:33:01.476
------------------------------
â€¢ [0.247 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:01.242
    Aug 24 12:33:01.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:33:01.245
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.283
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 24 12:33:01.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5078 version'
    Aug 24 12:33:01.468: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 24 12:33:01.468: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:01.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5078" for this suite. 08/24/23 12:33:01.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:01.49
Aug 24 12:33:01.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:33:01.491
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.521
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/24/23 12:33:01.525
Aug 24 12:33:01.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9418 api-versions'
Aug 24 12:33:01.736: INFO: stderr: ""
Aug 24 12:33:01.736: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9418" for this suite. 08/24/23 12:33:01.749
------------------------------
â€¢ [0.275 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:01.49
    Aug 24 12:33:01.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:33:01.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.521
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/24/23 12:33:01.525
    Aug 24 12:33:01.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-9418 api-versions'
    Aug 24 12:33:01.736: INFO: stderr: ""
    Aug 24 12:33:01.736: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9418" for this suite. 08/24/23 12:33:01.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:01.768
Aug 24 12:33:01.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:33:01.769
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.804
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/24/23 12:33:18.816
STEP: Creating a ResourceQuota 08/24/23 12:33:23.823
STEP: Ensuring resource quota status is calculated 08/24/23 12:33:23.832
STEP: Creating a ConfigMap 08/24/23 12:33:25.842
STEP: Ensuring resource quota status captures configMap creation 08/24/23 12:33:25.863
STEP: Deleting a ConfigMap 08/24/23 12:33:27.87
STEP: Ensuring resource quota status released usage 08/24/23 12:33:27.879
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:29.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3442" for this suite. 08/24/23 12:33:29.898
------------------------------
â€¢ [SLOW TEST] [28.143 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:01.768
    Aug 24 12:33:01.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:33:01.769
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:01.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:01.804
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/24/23 12:33:18.816
    STEP: Creating a ResourceQuota 08/24/23 12:33:23.823
    STEP: Ensuring resource quota status is calculated 08/24/23 12:33:23.832
    STEP: Creating a ConfigMap 08/24/23 12:33:25.842
    STEP: Ensuring resource quota status captures configMap creation 08/24/23 12:33:25.863
    STEP: Deleting a ConfigMap 08/24/23 12:33:27.87
    STEP: Ensuring resource quota status released usage 08/24/23 12:33:27.879
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:29.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3442" for this suite. 08/24/23 12:33:29.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:29.918
Aug 24 12:33:29.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:33:29.921
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:29.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:29.964
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:30.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3716" for this suite. 08/24/23 12:33:30.073
------------------------------
â€¢ [0.178 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:29.918
    Aug 24 12:33:29.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:33:29.921
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:29.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:29.964
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:30.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3716" for this suite. 08/24/23 12:33:30.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:30.097
Aug 24 12:33:30.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:33:30.1
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:30.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:30.191
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1408 08/24/23 12:33:30.198
STEP: creating service affinity-clusterip-transition in namespace services-1408 08/24/23 12:33:30.198
STEP: creating replication controller affinity-clusterip-transition in namespace services-1408 08/24/23 12:33:30.223
I0824 12:33:30.244093      14 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1408, replica count: 3
I0824 12:33:33.306571      14 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:33:33.322: INFO: Creating new exec pod
Aug 24 12:33:33.354: INFO: Waiting up to 5m0s for pod "execpod-affinitykzgxf" in namespace "services-1408" to be "running"
Aug 24 12:33:33.376: INFO: Pod "execpod-affinitykzgxf": Phase="Pending", Reason="", readiness=false. Elapsed: 21.510139ms
Aug 24 12:33:35.386: INFO: Pod "execpod-affinitykzgxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.032392737s
Aug 24 12:33:35.387: INFO: Pod "execpod-affinitykzgxf" satisfied condition "running"
Aug 24 12:33:36.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 24 12:33:36.699: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 24 12:33:36.699: INFO: stdout: ""
Aug 24 12:33:36.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c nc -v -z -w 2 10.233.60.140 80'
Aug 24 12:33:36.974: INFO: stderr: "+ nc -v -z -w 2 10.233.60.140 80\nConnection to 10.233.60.140 80 port [tcp/http] succeeded!\n"
Aug 24 12:33:36.974: INFO: stdout: ""
Aug 24 12:33:36.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.140:80/ ; done'
Aug 24 12:33:37.460: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n"
Aug 24 12:33:37.460: INFO: stdout: "\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-bbjv5\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-bbjv5\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv"
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bbjv5
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bbjv5
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
Aug 24 12:33:37.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.140:80/ ; done'
Aug 24 12:33:37.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n"
Aug 24 12:33:37.964: INFO: stdout: "\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9"
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
Aug 24 12:33:37.964: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1408, will wait for the garbage collector to delete the pods 08/24/23 12:33:37.986
Aug 24 12:33:38.056: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.376733ms
Aug 24 12:33:38.157: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.024964ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:40.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1408" for this suite. 08/24/23 12:33:40.511
------------------------------
â€¢ [SLOW TEST] [10.447 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:30.097
    Aug 24 12:33:30.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:33:30.1
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:30.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:30.191
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1408 08/24/23 12:33:30.198
    STEP: creating service affinity-clusterip-transition in namespace services-1408 08/24/23 12:33:30.198
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1408 08/24/23 12:33:30.223
    I0824 12:33:30.244093      14 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1408, replica count: 3
    I0824 12:33:33.306571      14 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:33:33.322: INFO: Creating new exec pod
    Aug 24 12:33:33.354: INFO: Waiting up to 5m0s for pod "execpod-affinitykzgxf" in namespace "services-1408" to be "running"
    Aug 24 12:33:33.376: INFO: Pod "execpod-affinitykzgxf": Phase="Pending", Reason="", readiness=false. Elapsed: 21.510139ms
    Aug 24 12:33:35.386: INFO: Pod "execpod-affinitykzgxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.032392737s
    Aug 24 12:33:35.387: INFO: Pod "execpod-affinitykzgxf" satisfied condition "running"
    Aug 24 12:33:36.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 24 12:33:36.699: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 24 12:33:36.699: INFO: stdout: ""
    Aug 24 12:33:36.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c nc -v -z -w 2 10.233.60.140 80'
    Aug 24 12:33:36.974: INFO: stderr: "+ nc -v -z -w 2 10.233.60.140 80\nConnection to 10.233.60.140 80 port [tcp/http] succeeded!\n"
    Aug 24 12:33:36.974: INFO: stdout: ""
    Aug 24 12:33:36.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.140:80/ ; done'
    Aug 24 12:33:37.460: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n"
    Aug 24 12:33:37.460: INFO: stdout: "\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-bbjv5\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-bbjv5\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv\naffinity-clusterip-transition-bnflv"
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bbjv5
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bbjv5
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.460: INFO: Received response from host: affinity-clusterip-transition-bnflv
    Aug 24 12:33:37.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1408 exec execpod-affinitykzgxf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.140:80/ ; done'
    Aug 24 12:33:37.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.140:80/\n"
    Aug 24 12:33:37.964: INFO: stdout: "\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9\naffinity-clusterip-transition-srsb9"
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Received response from host: affinity-clusterip-transition-srsb9
    Aug 24 12:33:37.964: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1408, will wait for the garbage collector to delete the pods 08/24/23 12:33:37.986
    Aug 24 12:33:38.056: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.376733ms
    Aug 24 12:33:38.157: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.024964ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:40.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1408" for this suite. 08/24/23 12:33:40.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:40.553
Aug 24 12:33:40.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:33:40.556
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:40.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:40.599
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/24/23 12:33:40.605
Aug 24 12:33:40.619: INFO: Waiting up to 5m0s for pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f" in namespace "var-expansion-8900" to be "Succeeded or Failed"
Aug 24 12:33:40.626: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.278822ms
Aug 24 12:33:42.634: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014324623s
Aug 24 12:33:44.634: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014408255s
STEP: Saw pod success 08/24/23 12:33:44.634
Aug 24 12:33:44.635: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f" satisfied condition "Succeeded or Failed"
Aug 24 12:33:44.640: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:33:44.672
Aug 24 12:33:44.691: INFO: Waiting for pod var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f to disappear
Aug 24 12:33:44.698: INFO: Pod var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:44.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8900" for this suite. 08/24/23 12:33:44.713
------------------------------
â€¢ [4.173 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:40.553
    Aug 24 12:33:40.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:33:40.556
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:40.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:40.599
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/24/23 12:33:40.605
    Aug 24 12:33:40.619: INFO: Waiting up to 5m0s for pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f" in namespace "var-expansion-8900" to be "Succeeded or Failed"
    Aug 24 12:33:40.626: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.278822ms
    Aug 24 12:33:42.634: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014324623s
    Aug 24 12:33:44.634: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014408255s
    STEP: Saw pod success 08/24/23 12:33:44.634
    Aug 24 12:33:44.635: INFO: Pod "var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f" satisfied condition "Succeeded or Failed"
    Aug 24 12:33:44.640: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:33:44.672
    Aug 24 12:33:44.691: INFO: Waiting for pod var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f to disappear
    Aug 24 12:33:44.698: INFO: Pod var-expansion-a32bdd95-0af5-4977-8323-11e3e46f9e2f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:44.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8900" for this suite. 08/24/23 12:33:44.713
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:44.726
Aug 24 12:33:44.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:33:44.729
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:44.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:44.761
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:33:44.799
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:33:45.896
STEP: Deploying the webhook pod 08/24/23 12:33:45.913
STEP: Wait for the deployment to be ready 08/24/23 12:33:45.935
Aug 24 12:33:45.950: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 12:33:47.971
STEP: Verifying the service has paired with the endpoint 08/24/23 12:33:47.991
Aug 24 12:33:48.993: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/24/23 12:33:49.117
STEP: Creating a configMap that should be mutated 08/24/23 12:33:49.148
STEP: Deleting the collection of validation webhooks 08/24/23 12:33:49.203
STEP: Creating a configMap that should not be mutated 08/24/23 12:33:49.304
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7320" for this suite. 08/24/23 12:33:49.424
STEP: Destroying namespace "webhook-7320-markers" for this suite. 08/24/23 12:33:49.437
------------------------------
â€¢ [4.721 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:44.726
    Aug 24 12:33:44.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:33:44.729
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:44.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:44.761
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:33:44.799
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:33:45.896
    STEP: Deploying the webhook pod 08/24/23 12:33:45.913
    STEP: Wait for the deployment to be ready 08/24/23 12:33:45.935
    Aug 24 12:33:45.950: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 12:33:47.971
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:33:47.991
    Aug 24 12:33:48.993: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/24/23 12:33:49.117
    STEP: Creating a configMap that should be mutated 08/24/23 12:33:49.148
    STEP: Deleting the collection of validation webhooks 08/24/23 12:33:49.203
    STEP: Creating a configMap that should not be mutated 08/24/23 12:33:49.304
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:49.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7320" for this suite. 08/24/23 12:33:49.424
    STEP: Destroying namespace "webhook-7320-markers" for this suite. 08/24/23 12:33:49.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:49.46
Aug 24 12:33:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:33:49.464
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:49.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:49.499
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 12:33:49.507
Aug 24 12:33:49.522: INFO: Waiting up to 5m0s for pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b" in namespace "emptydir-2055" to be "Succeeded or Failed"
Aug 24 12:33:49.526: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425296ms
Aug 24 12:33:51.535: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013135886s
Aug 24 12:33:53.535: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012679947s
STEP: Saw pod success 08/24/23 12:33:53.535
Aug 24 12:33:53.536: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b" satisfied condition "Succeeded or Failed"
Aug 24 12:33:53.543: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b container test-container: <nil>
STEP: delete the pod 08/24/23 12:33:53.555
Aug 24 12:33:53.575: INFO: Waiting for pod pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b to disappear
Aug 24 12:33:53.581: INFO: Pod pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:53.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2055" for this suite. 08/24/23 12:33:53.594
------------------------------
â€¢ [4.144 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:49.46
    Aug 24 12:33:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:33:49.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:49.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:49.499
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 12:33:49.507
    Aug 24 12:33:49.522: INFO: Waiting up to 5m0s for pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b" in namespace "emptydir-2055" to be "Succeeded or Failed"
    Aug 24 12:33:49.526: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425296ms
    Aug 24 12:33:51.535: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013135886s
    Aug 24 12:33:53.535: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012679947s
    STEP: Saw pod success 08/24/23 12:33:53.535
    Aug 24 12:33:53.536: INFO: Pod "pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b" satisfied condition "Succeeded or Failed"
    Aug 24 12:33:53.543: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b container test-container: <nil>
    STEP: delete the pod 08/24/23 12:33:53.555
    Aug 24 12:33:53.575: INFO: Waiting for pod pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b to disappear
    Aug 24 12:33:53.581: INFO: Pod pod-17d91193-563b-45f7-9cfc-bfa4ca5f5e4b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:53.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2055" for this suite. 08/24/23 12:33:53.594
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:53.609
Aug 24 12:33:53.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:33:53.613
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:53.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:53.648
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1289/configmap-test-6dc1437d-e477-4a57-bd89-458ca7400e8a 08/24/23 12:33:53.653
STEP: Creating a pod to test consume configMaps 08/24/23 12:33:53.66
Aug 24 12:33:53.674: INFO: Waiting up to 5m0s for pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8" in namespace "configmap-1289" to be "Succeeded or Failed"
Aug 24 12:33:53.681: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.840054ms
Aug 24 12:33:55.692: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017881268s
Aug 24 12:33:57.688: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013516739s
STEP: Saw pod success 08/24/23 12:33:57.688
Aug 24 12:33:57.688: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8" satisfied condition "Succeeded or Failed"
Aug 24 12:33:57.697: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8 container env-test: <nil>
STEP: delete the pod 08/24/23 12:33:57.711
Aug 24 12:33:57.729: INFO: Waiting for pod pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8 to disappear
Aug 24 12:33:57.735: INFO: Pod pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:33:57.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1289" for this suite. 08/24/23 12:33:57.744
------------------------------
â€¢ [4.147 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:53.609
    Aug 24 12:33:53.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:33:53.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:53.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:53.648
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1289/configmap-test-6dc1437d-e477-4a57-bd89-458ca7400e8a 08/24/23 12:33:53.653
    STEP: Creating a pod to test consume configMaps 08/24/23 12:33:53.66
    Aug 24 12:33:53.674: INFO: Waiting up to 5m0s for pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8" in namespace "configmap-1289" to be "Succeeded or Failed"
    Aug 24 12:33:53.681: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.840054ms
    Aug 24 12:33:55.692: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017881268s
    Aug 24 12:33:57.688: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013516739s
    STEP: Saw pod success 08/24/23 12:33:57.688
    Aug 24 12:33:57.688: INFO: Pod "pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8" satisfied condition "Succeeded or Failed"
    Aug 24 12:33:57.697: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8 container env-test: <nil>
    STEP: delete the pod 08/24/23 12:33:57.711
    Aug 24 12:33:57.729: INFO: Waiting for pod pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8 to disappear
    Aug 24 12:33:57.735: INFO: Pod pod-configmaps-0968bb4f-390b-43bb-805d-6d32368bb8e8 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:33:57.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1289" for this suite. 08/24/23 12:33:57.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:33:57.771
Aug 24 12:33:57.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:33:57.773
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:57.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:57.805
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-c72672b6-e67d-4a2a-9043-66f32eb370ba 08/24/23 12:33:57.811
STEP: Creating a pod to test consume secrets 08/24/23 12:33:57.82
Aug 24 12:33:57.836: INFO: Waiting up to 5m0s for pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f" in namespace "secrets-8130" to be "Succeeded or Failed"
Aug 24 12:33:57.846: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.705174ms
Aug 24 12:33:59.852: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016038036s
Aug 24 12:34:01.851: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015319542s
STEP: Saw pod success 08/24/23 12:34:01.852
Aug 24 12:34:01.852: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f" satisfied condition "Succeeded or Failed"
Aug 24 12:34:01.858: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:34:01.871
Aug 24 12:34:01.889: INFO: Waiting for pod pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f to disappear
Aug 24 12:34:01.894: INFO: Pod pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:34:01.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8130" for this suite. 08/24/23 12:34:01.902
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:33:57.771
    Aug 24 12:33:57.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:33:57.773
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:33:57.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:33:57.805
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-c72672b6-e67d-4a2a-9043-66f32eb370ba 08/24/23 12:33:57.811
    STEP: Creating a pod to test consume secrets 08/24/23 12:33:57.82
    Aug 24 12:33:57.836: INFO: Waiting up to 5m0s for pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f" in namespace "secrets-8130" to be "Succeeded or Failed"
    Aug 24 12:33:57.846: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.705174ms
    Aug 24 12:33:59.852: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016038036s
    Aug 24 12:34:01.851: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015319542s
    STEP: Saw pod success 08/24/23 12:34:01.852
    Aug 24 12:34:01.852: INFO: Pod "pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f" satisfied condition "Succeeded or Failed"
    Aug 24 12:34:01.858: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:34:01.871
    Aug 24 12:34:01.889: INFO: Waiting for pod pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f to disappear
    Aug 24 12:34:01.894: INFO: Pod pod-secrets-048bb86b-93b2-4407-88b6-d28af02d2b9f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:34:01.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8130" for this suite. 08/24/23 12:34:01.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:34:01.925
Aug 24 12:34:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:34:01.927
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:34:01.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:34:01.968
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 24 12:34:01.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 12:34:04.657
Aug 24 12:34:04.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 create -f -'
Aug 24 12:34:06.100: INFO: stderr: ""
Aug 24 12:34:06.100: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 24 12:34:06.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 delete e2e-test-crd-publish-openapi-4804-crds test-cr'
Aug 24 12:34:06.260: INFO: stderr: ""
Aug 24 12:34:06.260: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 24 12:34:06.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 apply -f -'
Aug 24 12:34:06.668: INFO: stderr: ""
Aug 24 12:34:06.668: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 24 12:34:06.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 delete e2e-test-crd-publish-openapi-4804-crds test-cr'
Aug 24 12:34:06.816: INFO: stderr: ""
Aug 24 12:34:06.816: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/24/23 12:34:06.816
Aug 24 12:34:06.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 explain e2e-test-crd-publish-openapi-4804-crds'
Aug 24 12:34:08.183: INFO: stderr: ""
Aug 24 12:34:08.183: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4804-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:34:10.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8763" for this suite. 08/24/23 12:34:10.685
------------------------------
â€¢ [SLOW TEST] [8.771 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:34:01.925
    Aug 24 12:34:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:34:01.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:34:01.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:34:01.968
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 24 12:34:01.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 12:34:04.657
    Aug 24 12:34:04.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 create -f -'
    Aug 24 12:34:06.100: INFO: stderr: ""
    Aug 24 12:34:06.100: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 24 12:34:06.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 delete e2e-test-crd-publish-openapi-4804-crds test-cr'
    Aug 24 12:34:06.260: INFO: stderr: ""
    Aug 24 12:34:06.260: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 24 12:34:06.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 apply -f -'
    Aug 24 12:34:06.668: INFO: stderr: ""
    Aug 24 12:34:06.668: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 24 12:34:06.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 --namespace=crd-publish-openapi-8763 delete e2e-test-crd-publish-openapi-4804-crds test-cr'
    Aug 24 12:34:06.816: INFO: stderr: ""
    Aug 24 12:34:06.816: INFO: stdout: "e2e-test-crd-publish-openapi-4804-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/24/23 12:34:06.816
    Aug 24 12:34:06.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8763 explain e2e-test-crd-publish-openapi-4804-crds'
    Aug 24 12:34:08.183: INFO: stderr: ""
    Aug 24 12:34:08.183: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4804-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:34:10.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8763" for this suite. 08/24/23 12:34:10.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:34:10.71
Aug 24 12:34:10.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-pred 08/24/23 12:34:10.715
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:34:10.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:34:10.75
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 12:34:10.755: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 12:34:10.770: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 12:34:10.777: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
Aug 24 12:34:10.788: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.789: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:34:10.789: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.789: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:34:10.789: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.789: INFO: 	Container coredns ready: true, restart count 0
Aug 24 12:34:10.789: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.790: INFO: 	Container coredns ready: true, restart count 0
Aug 24 12:34:10.790: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.790: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 12:34:10.790: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.790: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 12:34:10.790: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.790: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 12:34:10.791: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.791: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:34:10.791: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.791: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 12:34:10.791: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 12:34:10.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:34:10.791: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:34:10.792: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
Aug 24 12:34:10.809: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:34:10.809: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 24 12:34:10.809: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:34:10.809: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 12:34:10.809: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 12:34:10.809: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 12:34:10.809: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.809: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:34:10.809: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.810: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 12:34:10.810: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 12:34:10.810: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:34:10.810: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:34:10.810: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
Aug 24 12:34:10.825: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.825: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:34:10.825: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.825: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:34:10.825: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.825: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:34:10.825: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
Aug 24 12:34:10.825: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 12:34:10.825: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 12:34:10.825: INFO: 	Container e2e ready: true, restart count 0
Aug 24 12:34:10.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:34:10.825: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 12:34:10.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:34:10.825: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 12:34:10.826
Aug 24 12:34:10.840: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3134" to be "running"
Aug 24 12:34:10.856: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 16.315186ms
Aug 24 12:34:12.864: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023854803s
Aug 24 12:34:12.864: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 12:34:12.869
STEP: Trying to apply a random label on the found node. 08/24/23 12:34:12.896
STEP: verifying the node has the label kubernetes.io/e2e-8ef54f1b-edb3-4554-9dfa-c5fd4f2f3046 95 08/24/23 12:34:12.919
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/24/23 12:34:12.928
Aug 24 12:34:12.951: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3134" to be "not pending"
Aug 24 12:34:12.972: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.160199ms
Aug 24 12:34:14.978: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.025701785s
Aug 24 12:34:14.978: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.130 on the node which pod4 resides and expect not scheduled 08/24/23 12:34:14.978
Aug 24 12:34:14.991: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3134" to be "not pending"
Aug 24 12:34:14.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.397528ms
Aug 24 12:34:17.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016187065s
Aug 24 12:34:19.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016568496s
Aug 24 12:34:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013403989s
Aug 24 12:34:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013528132s
Aug 24 12:34:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012705109s
Aug 24 12:34:27.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044146317s
Aug 24 12:34:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013334662s
Aug 24 12:34:31.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042684124s
Aug 24 12:34:33.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016794325s
Aug 24 12:34:35.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020865805s
Aug 24 12:34:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013676929s
Aug 24 12:34:39.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019658769s
Aug 24 12:34:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012651478s
Aug 24 12:34:43.042: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.051525807s
Aug 24 12:34:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013439678s
Aug 24 12:34:47.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0139782s
Aug 24 12:34:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012498397s
Aug 24 12:34:51.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015442515s
Aug 24 12:34:53.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013344845s
Aug 24 12:34:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013093367s
Aug 24 12:34:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012613544s
Aug 24 12:34:59.042: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.051069512s
Aug 24 12:35:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012773398s
Aug 24 12:35:03.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017858851s
Aug 24 12:35:05.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015108405s
Aug 24 12:35:07.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014003968s
Aug 24 12:35:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.013086127s
Aug 24 12:35:11.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015330838s
Aug 24 12:35:13.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017919634s
Aug 24 12:35:15.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014412179s
Aug 24 12:35:17.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014244027s
Aug 24 12:35:19.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017111816s
Aug 24 12:35:21.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.016786902s
Aug 24 12:35:23.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01406122s
Aug 24 12:35:25.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.015425437s
Aug 24 12:35:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012307705s
Aug 24 12:35:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01346553s
Aug 24 12:35:31.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02050172s
Aug 24 12:35:33.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016939847s
Aug 24 12:35:35.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.021296425s
Aug 24 12:35:37.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017135965s
Aug 24 12:35:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012871283s
Aug 24 12:35:41.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01456168s
Aug 24 12:35:43.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014558129s
Aug 24 12:35:45.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014685166s
Aug 24 12:35:47.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016356525s
Aug 24 12:35:49.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01429223s
Aug 24 12:35:51.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02038706s
Aug 24 12:35:53.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015931002s
Aug 24 12:35:55.051: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.060477899s
Aug 24 12:35:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013418063s
Aug 24 12:35:59.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.044322s
Aug 24 12:36:01.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015218911s
Aug 24 12:36:03.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.020676964s
Aug 24 12:36:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01245747s
Aug 24 12:36:07.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015057905s
Aug 24 12:36:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013011806s
Aug 24 12:36:11.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020167171s
Aug 24 12:36:13.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.015253157s
Aug 24 12:36:15.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019478707s
Aug 24 12:36:17.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.014282576s
Aug 24 12:36:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013106138s
Aug 24 12:36:21.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013897819s
Aug 24 12:36:23.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.016215212s
Aug 24 12:36:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.013073119s
Aug 24 12:36:27.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012559931s
Aug 24 12:36:29.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.015658704s
Aug 24 12:36:31.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017172631s
Aug 24 12:36:33.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.017891698s
Aug 24 12:36:35.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01378755s
Aug 24 12:36:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.014264199s
Aug 24 12:36:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.013571669s
Aug 24 12:36:41.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.013553699s
Aug 24 12:36:43.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.01384054s
Aug 24 12:36:45.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.014759394s
Aug 24 12:36:47.037: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.046019365s
Aug 24 12:36:49.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015881022s
Aug 24 12:36:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.063493696s
Aug 24 12:36:53.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.017693968s
Aug 24 12:36:55.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.023890743s
Aug 24 12:36:57.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.01432793s
Aug 24 12:36:59.045: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.054074881s
Aug 24 12:37:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012937132s
Aug 24 12:37:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.013433547s
Aug 24 12:37:05.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.01407981s
Aug 24 12:37:07.039: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.048368795s
Aug 24 12:37:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013297032s
Aug 24 12:37:11.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.041500755s
Aug 24 12:37:13.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.016303414s
Aug 24 12:37:15.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.040316818s
Aug 24 12:37:17.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01329445s
Aug 24 12:37:19.042: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.050683114s
Aug 24 12:37:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012712154s
Aug 24 12:37:23.043: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.051893617s
Aug 24 12:37:25.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.014539184s
Aug 24 12:37:27.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.014006634s
Aug 24 12:37:29.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.014447159s
Aug 24 12:37:31.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.014168908s
Aug 24 12:37:33.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.013435872s
Aug 24 12:37:35.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.01477666s
Aug 24 12:37:37.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.015192492s
Aug 24 12:37:39.014: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.023168514s
Aug 24 12:37:41.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.015026379s
Aug 24 12:37:43.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.014131972s
Aug 24 12:37:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.01333294s
Aug 24 12:37:47.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.014482923s
Aug 24 12:37:49.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.01609057s
Aug 24 12:37:51.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.016148694s
Aug 24 12:37:53.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.015743263s
Aug 24 12:37:55.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.013744124s
Aug 24 12:37:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.01301103s
Aug 24 12:37:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012241039s
Aug 24 12:38:01.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.015568152s
Aug 24 12:38:03.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.017184119s
Aug 24 12:38:05.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.013847198s
Aug 24 12:38:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.012842924s
Aug 24 12:38:09.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.014022261s
Aug 24 12:38:11.047: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.05563053s
Aug 24 12:38:13.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.015623238s
Aug 24 12:38:15.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.04293272s
Aug 24 12:38:17.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015209321s
Aug 24 12:38:19.038: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.046986422s
Aug 24 12:38:21.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013864658s
Aug 24 12:38:23.041: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.050525387s
Aug 24 12:38:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.013378497s
Aug 24 12:38:27.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014328822s
Aug 24 12:38:29.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013918314s
Aug 24 12:38:31.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016417539s
Aug 24 12:38:33.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.015220934s
Aug 24 12:38:35.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.015668812s
Aug 24 12:38:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.014192325s
Aug 24 12:38:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.014114335s
Aug 24 12:38:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013525295s
Aug 24 12:38:43.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.014314154s
Aug 24 12:38:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013017388s
Aug 24 12:38:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013251854s
Aug 24 12:38:49.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.016014154s
Aug 24 12:38:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013488897s
Aug 24 12:38:53.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.013395367s
Aug 24 12:38:55.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.015416814s
Aug 24 12:38:57.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013945027s
Aug 24 12:38:59.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.018114515s
Aug 24 12:39:01.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016068089s
Aug 24 12:39:03.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018169905s
Aug 24 12:39:05.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.014910916s
Aug 24 12:39:07.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.019050851s
Aug 24 12:39:09.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.015182266s
Aug 24 12:39:11.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.015478526s
Aug 24 12:39:13.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.014822246s
Aug 24 12:39:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012763705s
Aug 24 12:39:15.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.018665829s
STEP: removing the label kubernetes.io/e2e-8ef54f1b-edb3-4554-9dfa-c5fd4f2f3046 off the node pe9deep4seen-3 08/24/23 12:39:15.01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8ef54f1b-edb3-4554-9dfa-c5fd4f2f3046 08/24/23 12:39:15.058
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:15.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3134" for this suite. 08/24/23 12:39:15.09
------------------------------
â€¢ [SLOW TEST] [304.408 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:34:10.71
    Aug 24 12:34:10.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-pred 08/24/23 12:34:10.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:34:10.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:34:10.75
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 12:34:10.755: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 12:34:10.770: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 12:34:10.777: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
    Aug 24 12:34:10.788: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.789: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:34:10.789: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.789: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:34:10.789: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.789: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 12:34:10.789: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.790: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 12:34:10.790: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.790: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 12:34:10.790: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.790: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 12:34:10.790: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.790: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 12:34:10.791: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.791: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:34:10.791: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.791: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 12:34:10.791: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 12:34:10.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:34:10.791: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:34:10.792: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
    Aug 24 12:34:10.809: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.809: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:34:10.809: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.810: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 12:34:10.810: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 12:34:10.810: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:34:10.810: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:34:10.810: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
    Aug 24 12:34:10.825: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.825: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.825: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.825: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
    Aug 24 12:34:10.825: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 12:34:10.825: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 12:34:10.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:34:10.825: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 12:34:10.826
    Aug 24 12:34:10.840: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3134" to be "running"
    Aug 24 12:34:10.856: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 16.315186ms
    Aug 24 12:34:12.864: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023854803s
    Aug 24 12:34:12.864: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 12:34:12.869
    STEP: Trying to apply a random label on the found node. 08/24/23 12:34:12.896
    STEP: verifying the node has the label kubernetes.io/e2e-8ef54f1b-edb3-4554-9dfa-c5fd4f2f3046 95 08/24/23 12:34:12.919
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/24/23 12:34:12.928
    Aug 24 12:34:12.951: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3134" to be "not pending"
    Aug 24 12:34:12.972: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.160199ms
    Aug 24 12:34:14.978: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.025701785s
    Aug 24 12:34:14.978: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.130 on the node which pod4 resides and expect not scheduled 08/24/23 12:34:14.978
    Aug 24 12:34:14.991: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3134" to be "not pending"
    Aug 24 12:34:14.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.397528ms
    Aug 24 12:34:17.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016187065s
    Aug 24 12:34:19.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016568496s
    Aug 24 12:34:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013403989s
    Aug 24 12:34:23.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013528132s
    Aug 24 12:34:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012705109s
    Aug 24 12:34:27.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.044146317s
    Aug 24 12:34:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013334662s
    Aug 24 12:34:31.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042684124s
    Aug 24 12:34:33.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016794325s
    Aug 24 12:34:35.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020865805s
    Aug 24 12:34:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013676929s
    Aug 24 12:34:39.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019658769s
    Aug 24 12:34:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012651478s
    Aug 24 12:34:43.042: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.051525807s
    Aug 24 12:34:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013439678s
    Aug 24 12:34:47.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0139782s
    Aug 24 12:34:49.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012498397s
    Aug 24 12:34:51.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015442515s
    Aug 24 12:34:53.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013344845s
    Aug 24 12:34:55.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013093367s
    Aug 24 12:34:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012613544s
    Aug 24 12:34:59.042: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.051069512s
    Aug 24 12:35:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012773398s
    Aug 24 12:35:03.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017858851s
    Aug 24 12:35:05.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015108405s
    Aug 24 12:35:07.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014003968s
    Aug 24 12:35:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.013086127s
    Aug 24 12:35:11.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015330838s
    Aug 24 12:35:13.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.017919634s
    Aug 24 12:35:15.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014412179s
    Aug 24 12:35:17.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014244027s
    Aug 24 12:35:19.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017111816s
    Aug 24 12:35:21.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.016786902s
    Aug 24 12:35:23.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01406122s
    Aug 24 12:35:25.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.015425437s
    Aug 24 12:35:27.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012307705s
    Aug 24 12:35:29.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01346553s
    Aug 24 12:35:31.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02050172s
    Aug 24 12:35:33.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016939847s
    Aug 24 12:35:35.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.021296425s
    Aug 24 12:35:37.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017135965s
    Aug 24 12:35:39.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012871283s
    Aug 24 12:35:41.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01456168s
    Aug 24 12:35:43.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014558129s
    Aug 24 12:35:45.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014685166s
    Aug 24 12:35:47.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016356525s
    Aug 24 12:35:49.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01429223s
    Aug 24 12:35:51.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.02038706s
    Aug 24 12:35:53.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.015931002s
    Aug 24 12:35:55.051: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.060477899s
    Aug 24 12:35:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013418063s
    Aug 24 12:35:59.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.044322s
    Aug 24 12:36:01.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015218911s
    Aug 24 12:36:03.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.020676964s
    Aug 24 12:36:05.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01245747s
    Aug 24 12:36:07.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015057905s
    Aug 24 12:36:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013011806s
    Aug 24 12:36:11.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020167171s
    Aug 24 12:36:13.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.015253157s
    Aug 24 12:36:15.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019478707s
    Aug 24 12:36:17.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.014282576s
    Aug 24 12:36:19.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013106138s
    Aug 24 12:36:21.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.013897819s
    Aug 24 12:36:23.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.016215212s
    Aug 24 12:36:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.013073119s
    Aug 24 12:36:27.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012559931s
    Aug 24 12:36:29.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.015658704s
    Aug 24 12:36:31.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017172631s
    Aug 24 12:36:33.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.017891698s
    Aug 24 12:36:35.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01378755s
    Aug 24 12:36:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.014264199s
    Aug 24 12:36:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.013571669s
    Aug 24 12:36:41.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.013553699s
    Aug 24 12:36:43.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.01384054s
    Aug 24 12:36:45.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.014759394s
    Aug 24 12:36:47.037: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.046019365s
    Aug 24 12:36:49.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015881022s
    Aug 24 12:36:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.063493696s
    Aug 24 12:36:53.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.017693968s
    Aug 24 12:36:55.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.023890743s
    Aug 24 12:36:57.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.01432793s
    Aug 24 12:36:59.045: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.054074881s
    Aug 24 12:37:01.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.012937132s
    Aug 24 12:37:03.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.013433547s
    Aug 24 12:37:05.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.01407981s
    Aug 24 12:37:07.039: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.048368795s
    Aug 24 12:37:09.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.013297032s
    Aug 24 12:37:11.032: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.041500755s
    Aug 24 12:37:13.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.016303414s
    Aug 24 12:37:15.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.040316818s
    Aug 24 12:37:17.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01329445s
    Aug 24 12:37:19.042: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.050683114s
    Aug 24 12:37:21.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012712154s
    Aug 24 12:37:23.043: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.051893617s
    Aug 24 12:37:25.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.014539184s
    Aug 24 12:37:27.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.014006634s
    Aug 24 12:37:29.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.014447159s
    Aug 24 12:37:31.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.014168908s
    Aug 24 12:37:33.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.013435872s
    Aug 24 12:37:35.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.01477666s
    Aug 24 12:37:37.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.015192492s
    Aug 24 12:37:39.014: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.023168514s
    Aug 24 12:37:41.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.015026379s
    Aug 24 12:37:43.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.014131972s
    Aug 24 12:37:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.01333294s
    Aug 24 12:37:47.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.014482923s
    Aug 24 12:37:49.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.01609057s
    Aug 24 12:37:51.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.016148694s
    Aug 24 12:37:53.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.015743263s
    Aug 24 12:37:55.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.013744124s
    Aug 24 12:37:57.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.01301103s
    Aug 24 12:37:59.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012241039s
    Aug 24 12:38:01.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.015568152s
    Aug 24 12:38:03.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.017184119s
    Aug 24 12:38:05.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.013847198s
    Aug 24 12:38:07.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.012842924s
    Aug 24 12:38:09.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.014022261s
    Aug 24 12:38:11.047: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.05563053s
    Aug 24 12:38:13.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.015623238s
    Aug 24 12:38:15.034: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.04293272s
    Aug 24 12:38:17.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015209321s
    Aug 24 12:38:19.038: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.046986422s
    Aug 24 12:38:21.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.013864658s
    Aug 24 12:38:23.041: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.050525387s
    Aug 24 12:38:25.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.013378497s
    Aug 24 12:38:27.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014328822s
    Aug 24 12:38:29.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013918314s
    Aug 24 12:38:31.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016417539s
    Aug 24 12:38:33.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.015220934s
    Aug 24 12:38:35.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.015668812s
    Aug 24 12:38:37.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.014192325s
    Aug 24 12:38:39.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.014114335s
    Aug 24 12:38:41.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.013525295s
    Aug 24 12:38:43.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.014314154s
    Aug 24 12:38:45.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013017388s
    Aug 24 12:38:47.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013251854s
    Aug 24 12:38:49.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.016014154s
    Aug 24 12:38:51.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.013488897s
    Aug 24 12:38:53.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.013395367s
    Aug 24 12:38:55.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.015416814s
    Aug 24 12:38:57.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013945027s
    Aug 24 12:38:59.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.018114515s
    Aug 24 12:39:01.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016068089s
    Aug 24 12:39:03.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018169905s
    Aug 24 12:39:05.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.014910916s
    Aug 24 12:39:07.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.019050851s
    Aug 24 12:39:09.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.015182266s
    Aug 24 12:39:11.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.015478526s
    Aug 24 12:39:13.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.014822246s
    Aug 24 12:39:15.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012763705s
    Aug 24 12:39:15.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.018665829s
    STEP: removing the label kubernetes.io/e2e-8ef54f1b-edb3-4554-9dfa-c5fd4f2f3046 off the node pe9deep4seen-3 08/24/23 12:39:15.01
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-8ef54f1b-edb3-4554-9dfa-c5fd4f2f3046 08/24/23 12:39:15.058
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:15.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3134" for this suite. 08/24/23 12:39:15.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:15.127
Aug 24 12:39:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-runtime 08/24/23 12:39:15.131
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:15.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:15.168
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/24/23 12:39:15.176
STEP: wait for the container to reach Succeeded 08/24/23 12:39:15.202
STEP: get the container status 08/24/23 12:39:19.255
STEP: the container should be terminated 08/24/23 12:39:19.261
STEP: the termination message should be set 08/24/23 12:39:19.261
Aug 24 12:39:19.262: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/24/23 12:39:19.262
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:19.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-194" for this suite. 08/24/23 12:39:19.303
------------------------------
â€¢ [4.190 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:15.127
    Aug 24 12:39:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-runtime 08/24/23 12:39:15.131
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:15.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:15.168
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/24/23 12:39:15.176
    STEP: wait for the container to reach Succeeded 08/24/23 12:39:15.202
    STEP: get the container status 08/24/23 12:39:19.255
    STEP: the container should be terminated 08/24/23 12:39:19.261
    STEP: the termination message should be set 08/24/23 12:39:19.261
    Aug 24 12:39:19.262: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/24/23 12:39:19.262
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:19.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-194" for this suite. 08/24/23 12:39:19.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:19.318
Aug 24 12:39:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 12:39:19.32
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:19.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:19.354
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 24 12:39:19.378: INFO: Waiting up to 5m0s for pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a" in namespace "container-probe-5187" to be "running and ready"
Aug 24 12:39:19.385: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.438705ms
Aug 24 12:39:19.385: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:39:21.396: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 2.017909529s
Aug 24 12:39:21.396: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:23.422: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 4.043565869s
Aug 24 12:39:23.422: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:25.396: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 6.017722387s
Aug 24 12:39:25.396: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:27.429: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 8.05116671s
Aug 24 12:39:27.429: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:29.394: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 10.015561442s
Aug 24 12:39:29.394: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:31.398: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 12.02025761s
Aug 24 12:39:31.399: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:33.392: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 14.013929884s
Aug 24 12:39:33.392: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:35.435: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 16.056316339s
Aug 24 12:39:35.435: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:37.393: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 18.014555018s
Aug 24 12:39:37.393: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:39.395: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 20.016496024s
Aug 24 12:39:39.395: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
Aug 24 12:39:41.394: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=true. Elapsed: 22.015399839s
Aug 24 12:39:41.394: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = true)
Aug 24 12:39:41.394: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a" satisfied condition "running and ready"
Aug 24 12:39:41.399: INFO: Container started at 2023-08-24 12:39:20 +0000 UTC, pod became ready at 2023-08-24 12:39:39 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:41.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5187" for this suite. 08/24/23 12:39:41.409
------------------------------
â€¢ [SLOW TEST] [22.104 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:19.318
    Aug 24 12:39:19.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 12:39:19.32
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:19.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:19.354
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 24 12:39:19.378: INFO: Waiting up to 5m0s for pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a" in namespace "container-probe-5187" to be "running and ready"
    Aug 24 12:39:19.385: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.438705ms
    Aug 24 12:39:19.385: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:39:21.396: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 2.017909529s
    Aug 24 12:39:21.396: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:23.422: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 4.043565869s
    Aug 24 12:39:23.422: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:25.396: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 6.017722387s
    Aug 24 12:39:25.396: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:27.429: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 8.05116671s
    Aug 24 12:39:27.429: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:29.394: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 10.015561442s
    Aug 24 12:39:29.394: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:31.398: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 12.02025761s
    Aug 24 12:39:31.399: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:33.392: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 14.013929884s
    Aug 24 12:39:33.392: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:35.435: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 16.056316339s
    Aug 24 12:39:35.435: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:37.393: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 18.014555018s
    Aug 24 12:39:37.393: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:39.395: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=false. Elapsed: 20.016496024s
    Aug 24 12:39:39.395: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = false)
    Aug 24 12:39:41.394: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a": Phase="Running", Reason="", readiness=true. Elapsed: 22.015399839s
    Aug 24 12:39:41.394: INFO: The phase of Pod test-webserver-db51fe43-5c11-489b-b477-416259bcc39a is Running (Ready = true)
    Aug 24 12:39:41.394: INFO: Pod "test-webserver-db51fe43-5c11-489b-b477-416259bcc39a" satisfied condition "running and ready"
    Aug 24 12:39:41.399: INFO: Container started at 2023-08-24 12:39:20 +0000 UTC, pod became ready at 2023-08-24 12:39:39 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:41.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5187" for this suite. 08/24/23 12:39:41.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:41.425
Aug 24 12:39:41.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:39:41.427
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:41.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:41.467
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:39:41.472
Aug 24 12:39:41.488: INFO: Waiting up to 5m0s for pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9" in namespace "emptydir-4340" to be "Succeeded or Failed"
Aug 24 12:39:41.494: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837753ms
Aug 24 12:39:43.502: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013960148s
Aug 24 12:39:45.501: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Running", Reason="", readiness=false. Elapsed: 4.013135634s
Aug 24 12:39:47.505: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016923904s
STEP: Saw pod success 08/24/23 12:39:47.505
Aug 24 12:39:47.507: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9" satisfied condition "Succeeded or Failed"
Aug 24 12:39:47.515: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-7101fd93-b351-4589-be1b-a153ea7297e9 container test-container: <nil>
STEP: delete the pod 08/24/23 12:39:47.572
Aug 24 12:39:47.610: INFO: Waiting for pod pod-7101fd93-b351-4589-be1b-a153ea7297e9 to disappear
Aug 24 12:39:47.619: INFO: Pod pod-7101fd93-b351-4589-be1b-a153ea7297e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:47.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4340" for this suite. 08/24/23 12:39:47.644
------------------------------
â€¢ [SLOW TEST] [6.233 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:41.425
    Aug 24 12:39:41.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:39:41.427
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:41.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:41.467
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:39:41.472
    Aug 24 12:39:41.488: INFO: Waiting up to 5m0s for pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9" in namespace "emptydir-4340" to be "Succeeded or Failed"
    Aug 24 12:39:41.494: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837753ms
    Aug 24 12:39:43.502: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013960148s
    Aug 24 12:39:45.501: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Running", Reason="", readiness=false. Elapsed: 4.013135634s
    Aug 24 12:39:47.505: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016923904s
    STEP: Saw pod success 08/24/23 12:39:47.505
    Aug 24 12:39:47.507: INFO: Pod "pod-7101fd93-b351-4589-be1b-a153ea7297e9" satisfied condition "Succeeded or Failed"
    Aug 24 12:39:47.515: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-7101fd93-b351-4589-be1b-a153ea7297e9 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:39:47.572
    Aug 24 12:39:47.610: INFO: Waiting for pod pod-7101fd93-b351-4589-be1b-a153ea7297e9 to disappear
    Aug 24 12:39:47.619: INFO: Pod pod-7101fd93-b351-4589-be1b-a153ea7297e9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:47.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4340" for this suite. 08/24/23 12:39:47.644
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:47.662
Aug 24 12:39:47.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:39:47.664
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:47.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:47.714
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-166/configmap-test-998f3ce1-1ddc-40bb-a08b-b0d7535540ea 08/24/23 12:39:47.72
STEP: Creating a pod to test consume configMaps 08/24/23 12:39:47.731
Aug 24 12:39:47.760: INFO: Waiting up to 5m0s for pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce" in namespace "configmap-166" to be "Succeeded or Failed"
Aug 24 12:39:47.768: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.501361ms
Aug 24 12:39:49.779: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019036009s
Aug 24 12:39:51.775: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015060946s
STEP: Saw pod success 08/24/23 12:39:51.775
Aug 24 12:39:51.776: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce" satisfied condition "Succeeded or Failed"
Aug 24 12:39:51.781: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce container env-test: <nil>
STEP: delete the pod 08/24/23 12:39:51.795
Aug 24 12:39:51.823: INFO: Waiting for pod pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce to disappear
Aug 24 12:39:51.829: INFO: Pod pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:51.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-166" for this suite. 08/24/23 12:39:51.839
------------------------------
â€¢ [4.189 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:47.662
    Aug 24 12:39:47.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:39:47.664
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:47.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:47.714
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-166/configmap-test-998f3ce1-1ddc-40bb-a08b-b0d7535540ea 08/24/23 12:39:47.72
    STEP: Creating a pod to test consume configMaps 08/24/23 12:39:47.731
    Aug 24 12:39:47.760: INFO: Waiting up to 5m0s for pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce" in namespace "configmap-166" to be "Succeeded or Failed"
    Aug 24 12:39:47.768: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.501361ms
    Aug 24 12:39:49.779: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019036009s
    Aug 24 12:39:51.775: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015060946s
    STEP: Saw pod success 08/24/23 12:39:51.775
    Aug 24 12:39:51.776: INFO: Pod "pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce" satisfied condition "Succeeded or Failed"
    Aug 24 12:39:51.781: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce container env-test: <nil>
    STEP: delete the pod 08/24/23 12:39:51.795
    Aug 24 12:39:51.823: INFO: Waiting for pod pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce to disappear
    Aug 24 12:39:51.829: INFO: Pod pod-configmaps-9073c0b1-ee61-437f-9fc2-ab72cd1f05ce no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:51.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-166" for this suite. 08/24/23 12:39:51.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:51.859
Aug 24 12:39:51.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sysctl 08/24/23 12:39:51.862
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:51.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:51.897
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/24/23 12:39:51.905
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:51.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7738" for this suite. 08/24/23 12:39:51.923
------------------------------
â€¢ [0.079 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:51.859
    Aug 24 12:39:51.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sysctl 08/24/23 12:39:51.862
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:51.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:51.897
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/24/23 12:39:51.905
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:51.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7738" for this suite. 08/24/23 12:39:51.923
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:51.94
Aug 24 12:39:51.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:39:51.944
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:51.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:51.987
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-a56f664d-a517-44a1-bc01-21ebe7255d1b 08/24/23 12:39:51.992
STEP: Creating a pod to test consume configMaps 08/24/23 12:39:52.001
Aug 24 12:39:52.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969" in namespace "configmap-560" to be "Succeeded or Failed"
Aug 24 12:39:52.023: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78597ms
Aug 24 12:39:54.032: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013728179s
Aug 24 12:39:56.031: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013379716s
STEP: Saw pod success 08/24/23 12:39:56.032
Aug 24 12:39:56.032: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969" satisfied condition "Succeeded or Failed"
Aug 24 12:39:56.037: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:39:56.048
Aug 24 12:39:56.066: INFO: Waiting for pod pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969 to disappear
Aug 24 12:39:56.071: INFO: Pod pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:56.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-560" for this suite. 08/24/23 12:39:56.089
------------------------------
â€¢ [4.162 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:51.94
    Aug 24 12:39:51.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:39:51.944
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:51.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:51.987
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-a56f664d-a517-44a1-bc01-21ebe7255d1b 08/24/23 12:39:51.992
    STEP: Creating a pod to test consume configMaps 08/24/23 12:39:52.001
    Aug 24 12:39:52.018: INFO: Waiting up to 5m0s for pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969" in namespace "configmap-560" to be "Succeeded or Failed"
    Aug 24 12:39:52.023: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78597ms
    Aug 24 12:39:54.032: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013728179s
    Aug 24 12:39:56.031: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013379716s
    STEP: Saw pod success 08/24/23 12:39:56.032
    Aug 24 12:39:56.032: INFO: Pod "pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969" satisfied condition "Succeeded or Failed"
    Aug 24 12:39:56.037: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:39:56.048
    Aug 24 12:39:56.066: INFO: Waiting for pod pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969 to disappear
    Aug 24 12:39:56.071: INFO: Pod pod-configmaps-69a6a97e-b8cb-4dad-8901-f46423289969 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:56.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-560" for this suite. 08/24/23 12:39:56.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:56.108
Aug 24 12:39:56.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename events 08/24/23 12:39:56.11
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:56.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:56.148
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/24/23 12:39:56.154
STEP: listing events in all namespaces 08/24/23 12:39:56.171
STEP: listing events in test namespace 08/24/23 12:39:56.181
STEP: listing events with field selection filtering on source 08/24/23 12:39:56.189
STEP: listing events with field selection filtering on reportingController 08/24/23 12:39:56.199
STEP: getting the test event 08/24/23 12:39:56.204
STEP: patching the test event 08/24/23 12:39:56.209
STEP: getting the test event 08/24/23 12:39:56.227
STEP: updating the test event 08/24/23 12:39:56.233
STEP: getting the test event 08/24/23 12:39:56.246
STEP: deleting the test event 08/24/23 12:39:56.253
STEP: listing events in all namespaces 08/24/23 12:39:56.27
STEP: listing events in test namespace 08/24/23 12:39:56.278
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:56.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4965" for this suite. 08/24/23 12:39:56.294
------------------------------
â€¢ [0.198 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:56.108
    Aug 24 12:39:56.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename events 08/24/23 12:39:56.11
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:56.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:56.148
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/24/23 12:39:56.154
    STEP: listing events in all namespaces 08/24/23 12:39:56.171
    STEP: listing events in test namespace 08/24/23 12:39:56.181
    STEP: listing events with field selection filtering on source 08/24/23 12:39:56.189
    STEP: listing events with field selection filtering on reportingController 08/24/23 12:39:56.199
    STEP: getting the test event 08/24/23 12:39:56.204
    STEP: patching the test event 08/24/23 12:39:56.209
    STEP: getting the test event 08/24/23 12:39:56.227
    STEP: updating the test event 08/24/23 12:39:56.233
    STEP: getting the test event 08/24/23 12:39:56.246
    STEP: deleting the test event 08/24/23 12:39:56.253
    STEP: listing events in all namespaces 08/24/23 12:39:56.27
    STEP: listing events in test namespace 08/24/23 12:39:56.278
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:56.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4965" for this suite. 08/24/23 12:39:56.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:56.307
Aug 24 12:39:56.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:39:56.309
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:56.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:56.349
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 24 12:39:56.374: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3558 to be scheduled
Aug 24 12:39:56.381: INFO: 1 pods are not scheduled: [runtimeclass-3558/test-runtimeclass-runtimeclass-3558-preconfigured-handler-7j76d(9e546da6-9518-4a99-87f8-b962a2977938)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 12:39:58.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3558" for this suite. 08/24/23 12:39:58.412
------------------------------
â€¢ [2.117 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:56.307
    Aug 24 12:39:56.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:39:56.309
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:56.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:56.349
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 24 12:39:56.374: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3558 to be scheduled
    Aug 24 12:39:56.381: INFO: 1 pods are not scheduled: [runtimeclass-3558/test-runtimeclass-runtimeclass-3558-preconfigured-handler-7j76d(9e546da6-9518-4a99-87f8-b962a2977938)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:39:58.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3558" for this suite. 08/24/23 12:39:58.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:39:58.432
Aug 24 12:39:58.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:39:58.436
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:58.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:58.476
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/24/23 12:39:58.482
STEP: Counting existing ResourceQuota 08/24/23 12:40:03.49
STEP: Creating a ResourceQuota 08/24/23 12:40:08.498
STEP: Ensuring resource quota status is calculated 08/24/23 12:40:08.508
STEP: Creating a Secret 08/24/23 12:40:10.517
STEP: Ensuring resource quota status captures secret creation 08/24/23 12:40:10.549
STEP: Deleting a secret 08/24/23 12:40:12.556
STEP: Ensuring resource quota status released usage 08/24/23 12:40:12.567
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:40:14.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7362" for this suite. 08/24/23 12:40:14.582
------------------------------
â€¢ [SLOW TEST] [16.161 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:39:58.432
    Aug 24 12:39:58.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:39:58.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:39:58.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:39:58.476
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/24/23 12:39:58.482
    STEP: Counting existing ResourceQuota 08/24/23 12:40:03.49
    STEP: Creating a ResourceQuota 08/24/23 12:40:08.498
    STEP: Ensuring resource quota status is calculated 08/24/23 12:40:08.508
    STEP: Creating a Secret 08/24/23 12:40:10.517
    STEP: Ensuring resource quota status captures secret creation 08/24/23 12:40:10.549
    STEP: Deleting a secret 08/24/23 12:40:12.556
    STEP: Ensuring resource quota status released usage 08/24/23 12:40:12.567
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:40:14.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7362" for this suite. 08/24/23 12:40:14.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:40:14.599
Aug 24 12:40:14.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:40:14.601
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:40:14.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:40:14.638
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 24 12:40:14.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 create -f -'
Aug 24 12:40:15.846: INFO: stderr: ""
Aug 24 12:40:15.846: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 24 12:40:15.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 create -f -'
Aug 24 12:40:16.367: INFO: stderr: ""
Aug 24 12:40:16.367: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/24/23 12:40:16.367
Aug 24 12:40:17.377: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:40:17.377: INFO: Found 0 / 1
Aug 24 12:40:18.376: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:40:18.376: INFO: Found 1 / 1
Aug 24 12:40:18.376: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 24 12:40:18.383: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:40:18.383: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 24 12:40:18.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe pod agnhost-primary-t2225'
Aug 24 12:40:18.560: INFO: stderr: ""
Aug 24 12:40:18.560: INFO: stdout: "Name:             agnhost-primary-t2225\nNamespace:        kubectl-5355\nPriority:         0\nService Account:  default\nNode:             pe9deep4seen-3/192.168.121.130\nStart Time:       Thu, 24 Aug 2023 12:40:15 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.147\nIPs:\n  IP:           10.233.66.147\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://f8ed2548f861d05aa4a5a7983549b4ae1a79639e7b7b85fe8b08c6f77416549f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Aug 2023 12:40:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d48tw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-d48tw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-5355/agnhost-primary-t2225 to pe9deep4seen-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Aug 24 12:40:18.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe rc agnhost-primary'
Aug 24 12:40:18.725: INFO: stderr: ""
Aug 24 12:40:18.725: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5355\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-t2225\n"
Aug 24 12:40:18.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe service agnhost-primary'
Aug 24 12:40:18.907: INFO: stderr: ""
Aug 24 12:40:18.907: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5355\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.41.156\nIPs:               10.233.41.156\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.147:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 24 12:40:18.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe node pe9deep4seen-1'
Aug 24 12:40:19.165: INFO: stderr: ""
Aug 24 12:40:19.166: INFO: stdout: "Name:               pe9deep4seen-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pe9deep4seen-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Aug 2023 11:21:05 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pe9deep4seen-1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Aug 2023 12:40:16 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Aug 2023 11:24:04 +0000   Thu, 24 Aug 2023 11:24:04 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:20:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:20:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:20:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:25:00 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.127\n  Hostname:    pe9deep4seen-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123904Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3274240Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 37a86d9f1ef842faaad122c27e6df1d5\n  System UUID:                37a86d9f-1ef8-42fa-aad1-22c27e6df1d5\n  Boot ID:                    c5ba4714-e129-4247-86e3-50c9d0a0b6ad\n  Kernel Version:             6.2.0-26-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.26.4\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-node-init-wqpdx                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         77m\n  kube-system                 cilium-wpzgb                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         77m\n  kube-system                 coredns-787d4945fb-8jnm5                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     76m\n  kube-system                 coredns-787d4945fb-d76z6                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     76m\n  kube-system                 kube-addon-manager-pe9deep4seen-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         77m\n  kube-system                 kube-apiserver-pe9deep4seen-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-controller-manager-pe9deep4seen-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-proxy-nr5bs                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                 kube-scheduler-pe9deep4seen-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         79m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                955m (59%)   0 (0%)\n  memory             390Mi (12%)  340Mi (10%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Aug 24 12:40:19.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe namespace kubectl-5355'
Aug 24 12:40:19.335: INFO: stderr: ""
Aug 24 12:40:19.335: INFO: stdout: "Name:         kubectl-5355\nLabels:       e2e-framework=kubectl\n              e2e-run=e37f2036-3a54-4653-ada1-c01489d8d1f1\n              kubernetes.io/metadata.name=kubectl-5355\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:40:19.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5355" for this suite. 08/24/23 12:40:19.345
------------------------------
â€¢ [4.770 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:40:14.599
    Aug 24 12:40:14.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:40:14.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:40:14.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:40:14.638
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 24 12:40:14.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 create -f -'
    Aug 24 12:40:15.846: INFO: stderr: ""
    Aug 24 12:40:15.846: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 24 12:40:15.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 create -f -'
    Aug 24 12:40:16.367: INFO: stderr: ""
    Aug 24 12:40:16.367: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/24/23 12:40:16.367
    Aug 24 12:40:17.377: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:40:17.377: INFO: Found 0 / 1
    Aug 24 12:40:18.376: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:40:18.376: INFO: Found 1 / 1
    Aug 24 12:40:18.376: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 24 12:40:18.383: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:40:18.383: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 24 12:40:18.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe pod agnhost-primary-t2225'
    Aug 24 12:40:18.560: INFO: stderr: ""
    Aug 24 12:40:18.560: INFO: stdout: "Name:             agnhost-primary-t2225\nNamespace:        kubectl-5355\nPriority:         0\nService Account:  default\nNode:             pe9deep4seen-3/192.168.121.130\nStart Time:       Thu, 24 Aug 2023 12:40:15 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.147\nIPs:\n  IP:           10.233.66.147\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://f8ed2548f861d05aa4a5a7983549b4ae1a79639e7b7b85fe8b08c6f77416549f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Aug 2023 12:40:16 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d48tw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-d48tw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-5355/agnhost-primary-t2225 to pe9deep4seen-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Aug 24 12:40:18.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe rc agnhost-primary'
    Aug 24 12:40:18.725: INFO: stderr: ""
    Aug 24 12:40:18.725: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5355\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-t2225\n"
    Aug 24 12:40:18.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe service agnhost-primary'
    Aug 24 12:40:18.907: INFO: stderr: ""
    Aug 24 12:40:18.907: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5355\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.41.156\nIPs:               10.233.41.156\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.147:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 24 12:40:18.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe node pe9deep4seen-1'
    Aug 24 12:40:19.165: INFO: stderr: ""
    Aug 24 12:40:19.166: INFO: stdout: "Name:               pe9deep4seen-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=pe9deep4seen-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Aug 2023 11:21:05 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  pe9deep4seen-1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Aug 2023 12:40:16 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Aug 2023 11:24:04 +0000   Thu, 24 Aug 2023 11:24:04 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:20:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:20:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:20:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Aug 2023 12:39:34 +0000   Thu, 24 Aug 2023 11:25:00 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.127\n  Hostname:    pe9deep4seen-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123904Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3274240Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 37a86d9f1ef842faaad122c27e6df1d5\n  System UUID:                37a86d9f-1ef8-42fa-aad1-22c27e6df1d5\n  Boot ID:                    c5ba4714-e129-4247-86e3-50c9d0a0b6ad\n  Kernel Version:             6.2.0-26-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.26.4\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-node-init-wqpdx                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         77m\n  kube-system                 cilium-wpzgb                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         77m\n  kube-system                 coredns-787d4945fb-8jnm5                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     76m\n  kube-system                 coredns-787d4945fb-d76z6                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     76m\n  kube-system                 kube-addon-manager-pe9deep4seen-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         77m\n  kube-system                 kube-apiserver-pe9deep4seen-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-controller-manager-pe9deep4seen-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-proxy-nr5bs                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                 kube-scheduler-pe9deep4seen-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         79m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                955m (59%)   0 (0%)\n  memory             390Mi (12%)  340Mi (10%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
    Aug 24 12:40:19.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5355 describe namespace kubectl-5355'
    Aug 24 12:40:19.335: INFO: stderr: ""
    Aug 24 12:40:19.335: INFO: stdout: "Name:         kubectl-5355\nLabels:       e2e-framework=kubectl\n              e2e-run=e37f2036-3a54-4653-ada1-c01489d8d1f1\n              kubernetes.io/metadata.name=kubectl-5355\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:40:19.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5355" for this suite. 08/24/23 12:40:19.345
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:40:19.37
Aug 24 12:40:19.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:40:19.376
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:40:19.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:40:19.431
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 12:40:19.469: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 12:41:19.529: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:19.536
Aug 24 12:41:19.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 12:41:19.538
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:19.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:19.604
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 24 12:41:19.641: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 24 12:41:19.650: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:19.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1328" for this suite. 08/24/23 12:41:19.821
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6369" for this suite. 08/24/23 12:41:19.835
------------------------------
â€¢ [SLOW TEST] [60.478 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:40:19.37
    Aug 24 12:40:19.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:40:19.376
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:40:19.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:40:19.431
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 12:40:19.469: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 12:41:19.529: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:19.536
    Aug 24 12:41:19.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 12:41:19.538
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:19.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:19.604
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 24 12:41:19.641: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 24 12:41:19.650: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:19.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1328" for this suite. 08/24/23 12:41:19.821
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6369" for this suite. 08/24/23 12:41:19.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:19.856
Aug 24 12:41:19.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:41:19.859
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:19.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:19.897
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-80b8c09a-cf0f-4301-8841-31061bb5b896 08/24/23 12:41:19.913
STEP: Creating the pod 08/24/23 12:41:19.922
Aug 24 12:41:19.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d" in namespace "configmap-5990" to be "running and ready"
Aug 24 12:41:19.950: INFO: Pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.778726ms
Aug 24 12:41:19.950: INFO: The phase of Pod pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:41:21.958: INFO: Pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d": Phase="Running", Reason="", readiness=true. Elapsed: 2.015880557s
Aug 24 12:41:21.958: INFO: The phase of Pod pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d is Running (Ready = true)
Aug 24 12:41:21.958: INFO: Pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-80b8c09a-cf0f-4301-8841-31061bb5b896 08/24/23 12:41:21.977
STEP: waiting to observe update in volume 08/24/23 12:41:21.987
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:24.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5990" for this suite. 08/24/23 12:41:24.02
------------------------------
â€¢ [4.174 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:19.856
    Aug 24 12:41:19.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:41:19.859
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:19.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:19.897
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-80b8c09a-cf0f-4301-8841-31061bb5b896 08/24/23 12:41:19.913
    STEP: Creating the pod 08/24/23 12:41:19.922
    Aug 24 12:41:19.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d" in namespace "configmap-5990" to be "running and ready"
    Aug 24 12:41:19.950: INFO: Pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.778726ms
    Aug 24 12:41:19.950: INFO: The phase of Pod pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:41:21.958: INFO: Pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d": Phase="Running", Reason="", readiness=true. Elapsed: 2.015880557s
    Aug 24 12:41:21.958: INFO: The phase of Pod pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d is Running (Ready = true)
    Aug 24 12:41:21.958: INFO: Pod "pod-configmaps-26e7a79b-d13b-4b20-bdf9-ca027a83977d" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-80b8c09a-cf0f-4301-8841-31061bb5b896 08/24/23 12:41:21.977
    STEP: waiting to observe update in volume 08/24/23 12:41:21.987
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:24.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5990" for this suite. 08/24/23 12:41:24.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:24.032
Aug 24 12:41:24.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:41:24.034
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:24.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:24.074
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/24/23 12:41:24.081
Aug 24 12:41:24.101: INFO: Waiting up to 5m0s for pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5" in namespace "downward-api-5476" to be "Succeeded or Failed"
Aug 24 12:41:24.107: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.439638ms
Aug 24 12:41:26.121: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019913911s
Aug 24 12:41:28.118: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017262184s
STEP: Saw pod success 08/24/23 12:41:28.118
Aug 24 12:41:28.118: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5" satisfied condition "Succeeded or Failed"
Aug 24 12:41:28.123: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:41:28.137
Aug 24 12:41:28.158: INFO: Waiting for pod downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5 to disappear
Aug 24 12:41:28.164: INFO: Pod downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:28.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5476" for this suite. 08/24/23 12:41:28.175
------------------------------
â€¢ [4.157 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:24.032
    Aug 24 12:41:24.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:41:24.034
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:24.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:24.074
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/24/23 12:41:24.081
    Aug 24 12:41:24.101: INFO: Waiting up to 5m0s for pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5" in namespace "downward-api-5476" to be "Succeeded or Failed"
    Aug 24 12:41:24.107: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.439638ms
    Aug 24 12:41:26.121: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019913911s
    Aug 24 12:41:28.118: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017262184s
    STEP: Saw pod success 08/24/23 12:41:28.118
    Aug 24 12:41:28.118: INFO: Pod "downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5" satisfied condition "Succeeded or Failed"
    Aug 24 12:41:28.123: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:41:28.137
    Aug 24 12:41:28.158: INFO: Waiting for pod downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5 to disappear
    Aug 24 12:41:28.164: INFO: Pod downward-api-876f3d5a-acd5-4768-b086-a8976b0e91a5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:28.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5476" for this suite. 08/24/23 12:41:28.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:28.196
Aug 24 12:41:28.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:41:28.197
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:28.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:28.231
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:41:28.268
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:41:28.278
Aug 24 12:41:28.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:41:28.291: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:41:29.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:41:29.320: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:41:30.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:41:30.309: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/24/23 12:41:30.316
Aug 24 12:41:30.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:41:30.358: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
Aug 24 12:41:31.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:41:31.378: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
Aug 24 12:41:32.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:41:32.383: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
Aug 24 12:41:33.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:41:33.381: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
Aug 24 12:41:34.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:41:34.383: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
Aug 24 12:41:35.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:41:35.375: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:41:35.382
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6175, will wait for the garbage collector to delete the pods 08/24/23 12:41:35.383
Aug 24 12:41:35.478: INFO: Deleting DaemonSet.extensions daemon-set took: 39.074686ms
Aug 24 12:41:35.679: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.937044ms
Aug 24 12:41:37.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:41:37.786: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:41:37.791: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23889"},"items":null}

Aug 24 12:41:37.798: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23890"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:37.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6175" for this suite. 08/24/23 12:41:37.837
------------------------------
â€¢ [SLOW TEST] [9.654 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:28.196
    Aug 24 12:41:28.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:41:28.197
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:28.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:28.231
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:41:28.268
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:41:28.278
    Aug 24 12:41:28.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:41:28.291: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:41:29.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:41:29.320: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:41:30.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:41:30.309: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/24/23 12:41:30.316
    Aug 24 12:41:30.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:41:30.358: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
    Aug 24 12:41:31.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:41:31.378: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
    Aug 24 12:41:32.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:41:32.383: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
    Aug 24 12:41:33.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:41:33.381: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
    Aug 24 12:41:34.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:41:34.383: INFO: Node pe9deep4seen-2 is running 0 daemon pod, expected 1
    Aug 24 12:41:35.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:41:35.375: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:41:35.382
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6175, will wait for the garbage collector to delete the pods 08/24/23 12:41:35.383
    Aug 24 12:41:35.478: INFO: Deleting DaemonSet.extensions daemon-set took: 39.074686ms
    Aug 24 12:41:35.679: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.937044ms
    Aug 24 12:41:37.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:41:37.786: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:41:37.791: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23889"},"items":null}

    Aug 24 12:41:37.798: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23890"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:37.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6175" for this suite. 08/24/23 12:41:37.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:37.862
Aug 24 12:41:37.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename security-context 08/24/23 12:41:37.864
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:37.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:37.905
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 12:41:37.91
Aug 24 12:41:37.926: INFO: Waiting up to 5m0s for pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb" in namespace "security-context-4176" to be "Succeeded or Failed"
Aug 24 12:41:37.933: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273115ms
Aug 24 12:41:39.943: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01674136s
Aug 24 12:41:41.940: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013219589s
STEP: Saw pod success 08/24/23 12:41:41.94
Aug 24 12:41:41.940: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb" satisfied condition "Succeeded or Failed"
Aug 24 12:41:41.946: INFO: Trying to get logs from node pe9deep4seen-3 pod security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb container test-container: <nil>
STEP: delete the pod 08/24/23 12:41:41.959
Aug 24 12:41:41.981: INFO: Waiting for pod security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb to disappear
Aug 24 12:41:41.986: INFO: Pod security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:41.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4176" for this suite. 08/24/23 12:41:41.996
------------------------------
â€¢ [4.145 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:37.862
    Aug 24 12:41:37.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename security-context 08/24/23 12:41:37.864
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:37.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:37.905
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 12:41:37.91
    Aug 24 12:41:37.926: INFO: Waiting up to 5m0s for pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb" in namespace "security-context-4176" to be "Succeeded or Failed"
    Aug 24 12:41:37.933: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273115ms
    Aug 24 12:41:39.943: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01674136s
    Aug 24 12:41:41.940: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013219589s
    STEP: Saw pod success 08/24/23 12:41:41.94
    Aug 24 12:41:41.940: INFO: Pod "security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb" satisfied condition "Succeeded or Failed"
    Aug 24 12:41:41.946: INFO: Trying to get logs from node pe9deep4seen-3 pod security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb container test-container: <nil>
    STEP: delete the pod 08/24/23 12:41:41.959
    Aug 24 12:41:41.981: INFO: Waiting for pod security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb to disappear
    Aug 24 12:41:41.986: INFO: Pod security-context-7b0e1685-ce31-4540-82f6-cb179ee64adb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:41.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4176" for this suite. 08/24/23 12:41:41.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:42.008
Aug 24 12:41:42.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:41:42.01
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:42.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:42.044
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 12:41:42.051
Aug 24 12:41:42.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-308 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 24 12:41:42.272: INFO: stderr: ""
Aug 24 12:41:42.272: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 12:41:42.272
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 24 12:41:42.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-308 delete pods e2e-test-httpd-pod'
Aug 24 12:41:44.997: INFO: stderr: ""
Aug 24 12:41:44.997: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:41:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-308" for this suite. 08/24/23 12:41:45.007
------------------------------
â€¢ [3.010 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:42.008
    Aug 24 12:41:42.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:41:42.01
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:42.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:42.044
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 12:41:42.051
    Aug 24 12:41:42.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-308 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 24 12:41:42.272: INFO: stderr: ""
    Aug 24 12:41:42.272: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 12:41:42.272
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 24 12:41:42.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-308 delete pods e2e-test-httpd-pod'
    Aug 24 12:41:44.997: INFO: stderr: ""
    Aug 24 12:41:44.997: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:41:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-308" for this suite. 08/24/23 12:41:45.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:41:45.019
Aug 24 12:41:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename subpath 08/24/23 12:41:45.023
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:45.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:45.065
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 12:41:45.072
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-fhmm 08/24/23 12:41:45.095
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 12:41:45.095
Aug 24 12:41:45.120: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fhmm" in namespace "subpath-874" to be "Succeeded or Failed"
Aug 24 12:41:45.125: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.2606ms
Aug 24 12:41:47.132: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012737273s
Aug 24 12:41:49.134: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 4.013981102s
Aug 24 12:41:51.137: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 6.017226553s
Aug 24 12:41:53.136: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 8.01629814s
Aug 24 12:41:55.132: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 10.012414589s
Aug 24 12:41:57.133: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.013582052s
Aug 24 12:41:59.134: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 14.013899791s
Aug 24 12:42:01.133: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 16.013755897s
Aug 24 12:42:03.143: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 18.023488648s
Aug 24 12:42:05.134: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 20.014015763s
Aug 24 12:42:07.135: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=false. Elapsed: 22.015438786s
Aug 24 12:42:09.132: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012596731s
STEP: Saw pod success 08/24/23 12:42:09.132
Aug 24 12:42:09.133: INFO: Pod "pod-subpath-test-configmap-fhmm" satisfied condition "Succeeded or Failed"
Aug 24 12:42:09.138: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-configmap-fhmm container test-container-subpath-configmap-fhmm: <nil>
STEP: delete the pod 08/24/23 12:42:09.154
Aug 24 12:42:09.180: INFO: Waiting for pod pod-subpath-test-configmap-fhmm to disappear
Aug 24 12:42:09.186: INFO: Pod pod-subpath-test-configmap-fhmm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fhmm 08/24/23 12:42:09.186
Aug 24 12:42:09.187: INFO: Deleting pod "pod-subpath-test-configmap-fhmm" in namespace "subpath-874"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 12:42:09.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-874" for this suite. 08/24/23 12:42:09.208
------------------------------
â€¢ [SLOW TEST] [24.201 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:41:45.019
    Aug 24 12:41:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename subpath 08/24/23 12:41:45.023
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:41:45.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:41:45.065
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 12:41:45.072
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-fhmm 08/24/23 12:41:45.095
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 12:41:45.095
    Aug 24 12:41:45.120: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fhmm" in namespace "subpath-874" to be "Succeeded or Failed"
    Aug 24 12:41:45.125: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.2606ms
    Aug 24 12:41:47.132: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.012737273s
    Aug 24 12:41:49.134: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 4.013981102s
    Aug 24 12:41:51.137: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 6.017226553s
    Aug 24 12:41:53.136: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 8.01629814s
    Aug 24 12:41:55.132: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 10.012414589s
    Aug 24 12:41:57.133: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.013582052s
    Aug 24 12:41:59.134: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 14.013899791s
    Aug 24 12:42:01.133: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 16.013755897s
    Aug 24 12:42:03.143: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 18.023488648s
    Aug 24 12:42:05.134: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=true. Elapsed: 20.014015763s
    Aug 24 12:42:07.135: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Running", Reason="", readiness=false. Elapsed: 22.015438786s
    Aug 24 12:42:09.132: INFO: Pod "pod-subpath-test-configmap-fhmm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012596731s
    STEP: Saw pod success 08/24/23 12:42:09.132
    Aug 24 12:42:09.133: INFO: Pod "pod-subpath-test-configmap-fhmm" satisfied condition "Succeeded or Failed"
    Aug 24 12:42:09.138: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-subpath-test-configmap-fhmm container test-container-subpath-configmap-fhmm: <nil>
    STEP: delete the pod 08/24/23 12:42:09.154
    Aug 24 12:42:09.180: INFO: Waiting for pod pod-subpath-test-configmap-fhmm to disappear
    Aug 24 12:42:09.186: INFO: Pod pod-subpath-test-configmap-fhmm no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-fhmm 08/24/23 12:42:09.186
    Aug 24 12:42:09.187: INFO: Deleting pod "pod-subpath-test-configmap-fhmm" in namespace "subpath-874"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:42:09.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-874" for this suite. 08/24/23 12:42:09.208
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:42:09.223
Aug 24 12:42:09.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 12:42:09.226
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:09.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:09.267
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 24 12:42:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:42:12.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3153" for this suite. 08/24/23 12:42:12.82
------------------------------
â€¢ [3.615 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:42:09.223
    Aug 24 12:42:09.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 12:42:09.226
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:09.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:09.267
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 24 12:42:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:42:12.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3153" for this suite. 08/24/23 12:42:12.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:42:12.851
Aug 24 12:42:12.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename namespaces 08/24/23 12:42:12.853
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:12.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:12.898
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/24/23 12:42:12.904
Aug 24 12:42:12.910: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/24/23 12:42:12.911
Aug 24 12:42:12.923: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/24/23 12:42:12.923
Aug 24 12:42:12.941: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:42:12.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4061" for this suite. 08/24/23 12:42:12.949
------------------------------
â€¢ [0.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:42:12.851
    Aug 24 12:42:12.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:42:12.853
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:12.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:12.898
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/24/23 12:42:12.904
    Aug 24 12:42:12.910: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/24/23 12:42:12.911
    Aug 24 12:42:12.923: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/24/23 12:42:12.923
    Aug 24 12:42:12.941: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:42:12.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4061" for this suite. 08/24/23 12:42:12.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:42:12.971
Aug 24 12:42:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:42:12.974
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:13.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:13.018
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/24/23 12:42:13.023
Aug 24 12:42:13.037: INFO: Waiting up to 5m0s for pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4" in namespace "projected-6134" to be "running and ready"
Aug 24 12:42:13.047: INFO: Pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468582ms
Aug 24 12:42:13.047: INFO: The phase of Pod annotationupdate39783633-939d-4294-837f-0ecda8ac45a4 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:42:15.056: INFO: Pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018654512s
Aug 24 12:42:15.056: INFO: The phase of Pod annotationupdate39783633-939d-4294-837f-0ecda8ac45a4 is Running (Ready = true)
Aug 24 12:42:15.056: INFO: Pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4" satisfied condition "running and ready"
Aug 24 12:42:15.609: INFO: Successfully updated pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:42:19.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6134" for this suite. 08/24/23 12:42:19.668
------------------------------
â€¢ [SLOW TEST] [6.712 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:42:12.971
    Aug 24 12:42:12.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:42:12.974
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:13.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:13.018
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/24/23 12:42:13.023
    Aug 24 12:42:13.037: INFO: Waiting up to 5m0s for pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4" in namespace "projected-6134" to be "running and ready"
    Aug 24 12:42:13.047: INFO: Pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468582ms
    Aug 24 12:42:13.047: INFO: The phase of Pod annotationupdate39783633-939d-4294-837f-0ecda8ac45a4 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:42:15.056: INFO: Pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018654512s
    Aug 24 12:42:15.056: INFO: The phase of Pod annotationupdate39783633-939d-4294-837f-0ecda8ac45a4 is Running (Ready = true)
    Aug 24 12:42:15.056: INFO: Pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4" satisfied condition "running and ready"
    Aug 24 12:42:15.609: INFO: Successfully updated pod "annotationupdate39783633-939d-4294-837f-0ecda8ac45a4"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:42:19.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6134" for this suite. 08/24/23 12:42:19.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:42:19.693
Aug 24 12:42:19.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 12:42:19.696
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:19.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:19.743
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/24/23 12:42:19.754
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-428;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-428;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +notcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_tcp@PTR;sleep 1; done
 08/24/23 12:42:19.802
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-428;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-428;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +notcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_tcp@PTR;sleep 1; done
 08/24/23 12:42:19.802
STEP: creating a pod to probe DNS 08/24/23 12:42:19.803
STEP: submitting the pod to kubernetes 08/24/23 12:42:19.804
Aug 24 12:42:19.839: INFO: Waiting up to 15m0s for pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1" in namespace "dns-428" to be "running"
Aug 24 12:42:19.848: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273022ms
Aug 24 12:42:21.858: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018682047s
Aug 24 12:42:23.857: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1": Phase="Running", Reason="", readiness=true. Elapsed: 4.018037921s
Aug 24 12:42:23.857: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:42:23.857
STEP: looking for the results for each expected name from probers 08/24/23 12:42:23.864
Aug 24 12:42:23.873: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.879: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.887: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.893: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.900: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.905: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.910: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.916: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.944: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.950: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.955: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.960: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.965: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.970: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.974: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:23.980: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:24.003: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

Aug 24 12:42:29.014: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.026: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.033: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.060: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.074: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.082: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.116: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.124: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.130: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.137: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.150: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.155: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.161: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.170: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:29.209: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

Aug 24 12:42:34.017: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.025: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.037: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.058: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.066: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.111: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.122: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.178: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.194: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.202: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.217: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:34.245: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

Aug 24 12:42:39.011: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.017: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.056: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.086: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.096: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.124: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.130: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.137: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.144: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.151: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.158: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.164: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.171: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:39.197: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

Aug 24 12:42:44.016: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.024: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.048: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.054: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.061: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.072: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.112: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.119: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.128: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.152: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.160: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.167: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.174: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:44.208: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

Aug 24 12:42:49.013: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.019: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.024: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.043: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.054: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.062: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.071: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.107: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.112: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.118: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.124: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.129: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.144: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.150: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
Aug 24 12:42:49.176: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

Aug 24 12:42:54.235: INFO: DNS probes using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 succeeded

STEP: deleting the pod 08/24/23 12:42:54.235
STEP: deleting the test service 08/24/23 12:42:54.302
STEP: deleting the test headless service 08/24/23 12:42:54.348
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:42:54.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-428" for this suite. 08/24/23 12:42:54.403
------------------------------
â€¢ [SLOW TEST] [34.730 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:42:19.693
    Aug 24 12:42:19.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 12:42:19.696
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:19.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:19.743
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/24/23 12:42:19.754
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-428;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-428;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +notcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_tcp@PTR;sleep 1; done
     08/24/23 12:42:19.802
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-428;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-428;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-428.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-428.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-428.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-428.svc;check="$$(dig +notcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_udp@PTR;check="$$(dig +tcp +noall +answer +search 42.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.42_tcp@PTR;sleep 1; done
     08/24/23 12:42:19.802
    STEP: creating a pod to probe DNS 08/24/23 12:42:19.803
    STEP: submitting the pod to kubernetes 08/24/23 12:42:19.804
    Aug 24 12:42:19.839: INFO: Waiting up to 15m0s for pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1" in namespace "dns-428" to be "running"
    Aug 24 12:42:19.848: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.273022ms
    Aug 24 12:42:21.858: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018682047s
    Aug 24 12:42:23.857: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1": Phase="Running", Reason="", readiness=true. Elapsed: 4.018037921s
    Aug 24 12:42:23.857: INFO: Pod "dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:42:23.857
    STEP: looking for the results for each expected name from probers 08/24/23 12:42:23.864
    Aug 24 12:42:23.873: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.879: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.887: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.893: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.900: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.905: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.910: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.916: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.944: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.950: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.955: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.960: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.965: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.970: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.974: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:23.980: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:24.003: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

    Aug 24 12:42:29.014: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.026: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.033: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.045: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.060: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.074: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.082: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.116: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.124: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.130: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.137: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.150: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.155: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.161: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.170: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:29.209: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

    Aug 24 12:42:34.017: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.025: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.037: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.058: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.066: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.111: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.122: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.178: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.188: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.194: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.202: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.217: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:34.245: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

    Aug 24 12:42:39.011: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.017: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.056: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.063: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.086: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.096: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.124: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.130: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.137: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.144: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.151: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.158: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.164: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.171: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:39.197: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

    Aug 24 12:42:44.016: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.024: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.038: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.048: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.054: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.061: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.072: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.112: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.119: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.128: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.152: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.160: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.167: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.174: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:44.208: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

    Aug 24 12:42:49.013: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.019: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.024: INFO: Unable to read wheezy_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.034: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.043: INFO: Unable to read wheezy_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.054: INFO: Unable to read wheezy_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.062: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.071: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.107: INFO: Unable to read jessie_udp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.112: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.118: INFO: Unable to read jessie_udp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.124: INFO: Unable to read jessie_tcp@dns-test-service.dns-428 from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.129: INFO: Unable to read jessie_udp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.144: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.150: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-428.svc from pod dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1: the server could not find the requested resource (get pods dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1)
    Aug 24 12:42:49.176: INFO: Lookups using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-428 wheezy_tcp@dns-test-service.dns-428 wheezy_udp@dns-test-service.dns-428.svc wheezy_tcp@dns-test-service.dns-428.svc wheezy_udp@_http._tcp.dns-test-service.dns-428.svc wheezy_tcp@_http._tcp.dns-test-service.dns-428.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-428 jessie_tcp@dns-test-service.dns-428 jessie_udp@dns-test-service.dns-428.svc jessie_tcp@dns-test-service.dns-428.svc jessie_udp@_http._tcp.dns-test-service.dns-428.svc jessie_tcp@_http._tcp.dns-test-service.dns-428.svc]

    Aug 24 12:42:54.235: INFO: DNS probes using dns-428/dns-test-e0af4161-a30e-40ee-8d39-f9fda4abd5e1 succeeded

    STEP: deleting the pod 08/24/23 12:42:54.235
    STEP: deleting the test service 08/24/23 12:42:54.302
    STEP: deleting the test headless service 08/24/23 12:42:54.348
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:42:54.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-428" for this suite. 08/24/23 12:42:54.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:42:54.437
Aug 24 12:42:54.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-pred 08/24/23 12:42:54.444
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:54.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:54.488
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 12:42:54.496: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 12:42:54.524: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 12:42:54.532: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
Aug 24 12:42:54.565: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.565: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:42:54.565: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.565: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:42:54.565: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.565: INFO: 	Container coredns ready: true, restart count 0
Aug 24 12:42:54.565: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.565: INFO: 	Container coredns ready: true, restart count 0
Aug 24 12:42:54.565: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.565: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 12:42:54.566: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.566: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 12:42:54.566: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.566: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 12:42:54.566: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.566: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:42:54.566: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.566: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 12:42:54.566: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 12:42:54.566: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:42:54.566: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:42:54.566: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
Aug 24 12:42:54.586: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.587: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:42:54.587: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.587: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 24 12:42:54.587: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.587: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:42:54.587: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.587: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 12:42:54.587: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.587: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 12:42:54.588: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.588: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 12:42:54.588: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.588: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:42:54.588: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.588: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 12:42:54.588: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 12:42:54.588: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:42:54.588: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:42:54.588: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
Aug 24 12:42:54.608: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.609: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:42:54.610: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.610: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:42:54.610: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.610: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:42:54.610: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
Aug 24 12:42:54.611: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 12:42:54.611: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 12:42:54.611: INFO: 	Container e2e ready: true, restart count 0
Aug 24 12:42:54.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:42:54.611: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 12:42:54.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:42:54.611: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node pe9deep4seen-1 08/24/23 12:42:54.68
STEP: verifying the node has the label node pe9deep4seen-2 08/24/23 12:42:54.723
STEP: verifying the node has the label node pe9deep4seen-3 08/24/23 12:42:54.748
Aug 24 12:42:54.806: INFO: Pod cilium-node-init-95cbk requesting resource cpu=100m on Node pe9deep4seen-2
Aug 24 12:42:54.806: INFO: Pod cilium-node-init-pdcw9 requesting resource cpu=100m on Node pe9deep4seen-3
Aug 24 12:42:54.806: INFO: Pod cilium-node-init-wqpdx requesting resource cpu=100m on Node pe9deep4seen-1
Aug 24 12:42:54.806: INFO: Pod cilium-operator-75f7897945-8qqz2 requesting resource cpu=0m on Node pe9deep4seen-2
Aug 24 12:42:54.807: INFO: Pod cilium-rcknz requesting resource cpu=0m on Node pe9deep4seen-2
Aug 24 12:42:54.807: INFO: Pod cilium-wpzgb requesting resource cpu=0m on Node pe9deep4seen-1
Aug 24 12:42:54.807: INFO: Pod cilium-xgc44 requesting resource cpu=0m on Node pe9deep4seen-3
Aug 24 12:42:54.807: INFO: Pod coredns-787d4945fb-8jnm5 requesting resource cpu=100m on Node pe9deep4seen-1
Aug 24 12:42:54.807: INFO: Pod coredns-787d4945fb-d76z6 requesting resource cpu=100m on Node pe9deep4seen-1
Aug 24 12:42:54.807: INFO: Pod kube-addon-manager-pe9deep4seen-1 requesting resource cpu=5m on Node pe9deep4seen-1
Aug 24 12:42:54.807: INFO: Pod kube-addon-manager-pe9deep4seen-2 requesting resource cpu=5m on Node pe9deep4seen-2
Aug 24 12:42:54.807: INFO: Pod kube-apiserver-pe9deep4seen-1 requesting resource cpu=250m on Node pe9deep4seen-1
Aug 24 12:42:54.807: INFO: Pod kube-apiserver-pe9deep4seen-2 requesting resource cpu=250m on Node pe9deep4seen-2
Aug 24 12:42:54.807: INFO: Pod kube-controller-manager-pe9deep4seen-1 requesting resource cpu=200m on Node pe9deep4seen-1
Aug 24 12:42:54.807: INFO: Pod kube-controller-manager-pe9deep4seen-2 requesting resource cpu=200m on Node pe9deep4seen-2
Aug 24 12:42:54.807: INFO: Pod kube-proxy-8vv8d requesting resource cpu=0m on Node pe9deep4seen-3
Aug 24 12:42:54.808: INFO: Pod kube-proxy-lm2dm requesting resource cpu=0m on Node pe9deep4seen-2
Aug 24 12:42:54.808: INFO: Pod kube-proxy-nr5bs requesting resource cpu=0m on Node pe9deep4seen-1
Aug 24 12:42:54.812: INFO: Pod kube-scheduler-pe9deep4seen-1 requesting resource cpu=100m on Node pe9deep4seen-1
Aug 24 12:42:54.812: INFO: Pod kube-scheduler-pe9deep4seen-2 requesting resource cpu=100m on Node pe9deep4seen-2
Aug 24 12:42:54.812: INFO: Pod sonobuoy requesting resource cpu=0m on Node pe9deep4seen-3
Aug 24 12:42:54.812: INFO: Pod sonobuoy-e2e-job-b3f52dde3e8a4a4e requesting resource cpu=0m on Node pe9deep4seen-3
Aug 24 12:42:54.812: INFO: Pod sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw requesting resource cpu=0m on Node pe9deep4seen-1
Aug 24 12:42:54.812: INFO: Pod sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl requesting resource cpu=0m on Node pe9deep4seen-2
Aug 24 12:42:54.812: INFO: Pod sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 requesting resource cpu=0m on Node pe9deep4seen-3
STEP: Starting Pods to consume most of the cluster CPU. 08/24/23 12:42:54.812
Aug 24 12:42:54.813: INFO: Creating a pod which consumes cpu=521m on Node pe9deep4seen-1
Aug 24 12:42:54.845: INFO: Creating a pod which consumes cpu=661m on Node pe9deep4seen-2
Aug 24 12:42:54.865: INFO: Creating a pod which consumes cpu=1050m on Node pe9deep4seen-3
Aug 24 12:42:54.887: INFO: Waiting up to 5m0s for pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb" in namespace "sched-pred-4161" to be "running"
Aug 24 12:42:54.901: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.170357ms
Aug 24 12:42:56.910: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023291331s
Aug 24 12:42:58.909: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb": Phase="Running", Reason="", readiness=true. Elapsed: 4.022066517s
Aug 24 12:42:58.909: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb" satisfied condition "running"
Aug 24 12:42:58.909: INFO: Waiting up to 5m0s for pod "filler-pod-601963de-77da-4b3a-b77e-f271db96f627" in namespace "sched-pred-4161" to be "running"
Aug 24 12:42:58.916: INFO: Pod "filler-pod-601963de-77da-4b3a-b77e-f271db96f627": Phase="Running", Reason="", readiness=true. Elapsed: 6.579841ms
Aug 24 12:42:58.916: INFO: Pod "filler-pod-601963de-77da-4b3a-b77e-f271db96f627" satisfied condition "running"
Aug 24 12:42:58.916: INFO: Waiting up to 5m0s for pod "filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f" in namespace "sched-pred-4161" to be "running"
Aug 24 12:42:58.922: INFO: Pod "filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f": Phase="Running", Reason="", readiness=true. Elapsed: 6.178732ms
Aug 24 12:42:58.922: INFO: Pod "filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/24/23 12:42:58.922
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e52470b13ca2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4161/filler-pod-601963de-77da-4b3a-b77e-f271db96f627 to pe9deep4seen-2] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e524749338ffe], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e52475757cb26], Reason = [Created], Message = [Created container filler-pod-601963de-77da-4b3a-b77e-f271db96f627] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e52475b5d46b4], Reason = [Started], Message = [Started container filler-pod-601963de-77da-4b3a-b77e-f271db96f627] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e52470e1f8444], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4161/filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f to pe9deep4seen-3] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e524754ae33c0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e524765cf3cc4], Reason = [Created], Message = [Created container filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e524767170727], Reason = [Started], Message = [Started container filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e524707e3568a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4161/filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb to pe9deep4seen-1] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247555f45ac], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-xlckz" : failed to sync configmap cache: timed out waiting for the condition] 08/24/23 12:42:58.933
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247a23ebd50], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:42:58.934
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247adc024de], Reason = [Created], Message = [Created container filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb] 08/24/23 12:42:58.934
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247b038d325], Reason = [Started], Message = [Started container filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb] 08/24/23 12:42:58.934
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177e5247f858ea32], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 08/24/23 12:42:58.955
STEP: removing the label node off the node pe9deep4seen-1 08/24/23 12:42:59.958
STEP: verifying the node doesn't have the label node 08/24/23 12:42:59.988
STEP: removing the label node off the node pe9deep4seen-2 08/24/23 12:42:59.997
STEP: verifying the node doesn't have the label node 08/24/23 12:43:00.036
STEP: removing the label node off the node pe9deep4seen-3 08/24/23 12:43:00.045
STEP: verifying the node doesn't have the label node 08/24/23 12:43:00.085
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:00.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4161" for this suite. 08/24/23 12:43:00.196
------------------------------
â€¢ [SLOW TEST] [5.777 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:42:54.437
    Aug 24 12:42:54.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-pred 08/24/23 12:42:54.444
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:42:54.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:42:54.488
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 12:42:54.496: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 12:42:54.524: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 12:42:54.532: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
    Aug 24 12:42:54.565: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.565: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:42:54.565: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.565: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:42:54.565: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.565: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 12:42:54.565: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.565: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 12:42:54.565: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.565: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.566: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.566: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.566: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.566: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 12:42:54.566: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:42:54.566: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
    Aug 24 12:42:54.586: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.587: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:42:54.587: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.587: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 24 12:42:54.587: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.587: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:42:54.587: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.587: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 12:42:54.587: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.587: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 12:42:54.588: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.588: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 12:42:54.588: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.588: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:42:54.588: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.588: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 12:42:54.588: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 12:42:54.588: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:42:54.588: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:42:54.588: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
    Aug 24 12:42:54.608: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.609: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:42:54.610: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.610: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:42:54.610: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.610: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:42:54.610: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
    Aug 24 12:42:54.611: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 12:42:54.611: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 12:42:54.611: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 12:42:54.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:42:54.611: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 12:42:54.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:42:54.611: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node pe9deep4seen-1 08/24/23 12:42:54.68
    STEP: verifying the node has the label node pe9deep4seen-2 08/24/23 12:42:54.723
    STEP: verifying the node has the label node pe9deep4seen-3 08/24/23 12:42:54.748
    Aug 24 12:42:54.806: INFO: Pod cilium-node-init-95cbk requesting resource cpu=100m on Node pe9deep4seen-2
    Aug 24 12:42:54.806: INFO: Pod cilium-node-init-pdcw9 requesting resource cpu=100m on Node pe9deep4seen-3
    Aug 24 12:42:54.806: INFO: Pod cilium-node-init-wqpdx requesting resource cpu=100m on Node pe9deep4seen-1
    Aug 24 12:42:54.806: INFO: Pod cilium-operator-75f7897945-8qqz2 requesting resource cpu=0m on Node pe9deep4seen-2
    Aug 24 12:42:54.807: INFO: Pod cilium-rcknz requesting resource cpu=0m on Node pe9deep4seen-2
    Aug 24 12:42:54.807: INFO: Pod cilium-wpzgb requesting resource cpu=0m on Node pe9deep4seen-1
    Aug 24 12:42:54.807: INFO: Pod cilium-xgc44 requesting resource cpu=0m on Node pe9deep4seen-3
    Aug 24 12:42:54.807: INFO: Pod coredns-787d4945fb-8jnm5 requesting resource cpu=100m on Node pe9deep4seen-1
    Aug 24 12:42:54.807: INFO: Pod coredns-787d4945fb-d76z6 requesting resource cpu=100m on Node pe9deep4seen-1
    Aug 24 12:42:54.807: INFO: Pod kube-addon-manager-pe9deep4seen-1 requesting resource cpu=5m on Node pe9deep4seen-1
    Aug 24 12:42:54.807: INFO: Pod kube-addon-manager-pe9deep4seen-2 requesting resource cpu=5m on Node pe9deep4seen-2
    Aug 24 12:42:54.807: INFO: Pod kube-apiserver-pe9deep4seen-1 requesting resource cpu=250m on Node pe9deep4seen-1
    Aug 24 12:42:54.807: INFO: Pod kube-apiserver-pe9deep4seen-2 requesting resource cpu=250m on Node pe9deep4seen-2
    Aug 24 12:42:54.807: INFO: Pod kube-controller-manager-pe9deep4seen-1 requesting resource cpu=200m on Node pe9deep4seen-1
    Aug 24 12:42:54.807: INFO: Pod kube-controller-manager-pe9deep4seen-2 requesting resource cpu=200m on Node pe9deep4seen-2
    Aug 24 12:42:54.807: INFO: Pod kube-proxy-8vv8d requesting resource cpu=0m on Node pe9deep4seen-3
    Aug 24 12:42:54.808: INFO: Pod kube-proxy-lm2dm requesting resource cpu=0m on Node pe9deep4seen-2
    Aug 24 12:42:54.808: INFO: Pod kube-proxy-nr5bs requesting resource cpu=0m on Node pe9deep4seen-1
    Aug 24 12:42:54.812: INFO: Pod kube-scheduler-pe9deep4seen-1 requesting resource cpu=100m on Node pe9deep4seen-1
    Aug 24 12:42:54.812: INFO: Pod kube-scheduler-pe9deep4seen-2 requesting resource cpu=100m on Node pe9deep4seen-2
    Aug 24 12:42:54.812: INFO: Pod sonobuoy requesting resource cpu=0m on Node pe9deep4seen-3
    Aug 24 12:42:54.812: INFO: Pod sonobuoy-e2e-job-b3f52dde3e8a4a4e requesting resource cpu=0m on Node pe9deep4seen-3
    Aug 24 12:42:54.812: INFO: Pod sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw requesting resource cpu=0m on Node pe9deep4seen-1
    Aug 24 12:42:54.812: INFO: Pod sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl requesting resource cpu=0m on Node pe9deep4seen-2
    Aug 24 12:42:54.812: INFO: Pod sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 requesting resource cpu=0m on Node pe9deep4seen-3
    STEP: Starting Pods to consume most of the cluster CPU. 08/24/23 12:42:54.812
    Aug 24 12:42:54.813: INFO: Creating a pod which consumes cpu=521m on Node pe9deep4seen-1
    Aug 24 12:42:54.845: INFO: Creating a pod which consumes cpu=661m on Node pe9deep4seen-2
    Aug 24 12:42:54.865: INFO: Creating a pod which consumes cpu=1050m on Node pe9deep4seen-3
    Aug 24 12:42:54.887: INFO: Waiting up to 5m0s for pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb" in namespace "sched-pred-4161" to be "running"
    Aug 24 12:42:54.901: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.170357ms
    Aug 24 12:42:56.910: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023291331s
    Aug 24 12:42:58.909: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb": Phase="Running", Reason="", readiness=true. Elapsed: 4.022066517s
    Aug 24 12:42:58.909: INFO: Pod "filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb" satisfied condition "running"
    Aug 24 12:42:58.909: INFO: Waiting up to 5m0s for pod "filler-pod-601963de-77da-4b3a-b77e-f271db96f627" in namespace "sched-pred-4161" to be "running"
    Aug 24 12:42:58.916: INFO: Pod "filler-pod-601963de-77da-4b3a-b77e-f271db96f627": Phase="Running", Reason="", readiness=true. Elapsed: 6.579841ms
    Aug 24 12:42:58.916: INFO: Pod "filler-pod-601963de-77da-4b3a-b77e-f271db96f627" satisfied condition "running"
    Aug 24 12:42:58.916: INFO: Waiting up to 5m0s for pod "filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f" in namespace "sched-pred-4161" to be "running"
    Aug 24 12:42:58.922: INFO: Pod "filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f": Phase="Running", Reason="", readiness=true. Elapsed: 6.178732ms
    Aug 24 12:42:58.922: INFO: Pod "filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/24/23 12:42:58.922
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e52470b13ca2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4161/filler-pod-601963de-77da-4b3a-b77e-f271db96f627 to pe9deep4seen-2] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e524749338ffe], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e52475757cb26], Reason = [Created], Message = [Created container filler-pod-601963de-77da-4b3a-b77e-f271db96f627] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-601963de-77da-4b3a-b77e-f271db96f627.177e52475b5d46b4], Reason = [Started], Message = [Started container filler-pod-601963de-77da-4b3a-b77e-f271db96f627] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e52470e1f8444], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4161/filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f to pe9deep4seen-3] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e524754ae33c0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e524765cf3cc4], Reason = [Created], Message = [Created container filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f.177e524767170727], Reason = [Started], Message = [Started container filler-pod-afff7594-3235-4b2c-8dc2-7226abf72f4f] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e524707e3568a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4161/filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb to pe9deep4seen-1] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247555f45ac], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-xlckz" : failed to sync configmap cache: timed out waiting for the condition] 08/24/23 12:42:58.933
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247a23ebd50], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:42:58.934
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247adc024de], Reason = [Created], Message = [Created container filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb] 08/24/23 12:42:58.934
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb.177e5247b038d325], Reason = [Started], Message = [Started container filler-pod-f0b264fb-b5e6-4a9e-86b2-1704662572fb] 08/24/23 12:42:58.934
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177e5247f858ea32], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 08/24/23 12:42:58.955
    STEP: removing the label node off the node pe9deep4seen-1 08/24/23 12:42:59.958
    STEP: verifying the node doesn't have the label node 08/24/23 12:42:59.988
    STEP: removing the label node off the node pe9deep4seen-2 08/24/23 12:42:59.997
    STEP: verifying the node doesn't have the label node 08/24/23 12:43:00.036
    STEP: removing the label node off the node pe9deep4seen-3 08/24/23 12:43:00.045
    STEP: verifying the node doesn't have the label node 08/24/23 12:43:00.085
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:00.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4161" for this suite. 08/24/23 12:43:00.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:00.232
Aug 24 12:43:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:43:00.236
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:00.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:00.292
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/24/23 12:43:00.324
Aug 24 12:43:00.325: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524" in namespace "kubelet-test-5915" to be "completed"
Aug 24 12:43:00.342: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524": Phase="Pending", Reason="", readiness=false. Elapsed: 17.285924ms
Aug 24 12:43:02.354: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028509458s
Aug 24 12:43:04.352: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026690629s
Aug 24 12:43:04.352: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:04.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5915" for this suite. 08/24/23 12:43:04.383
------------------------------
â€¢ [4.168 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:00.232
    Aug 24 12:43:00.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:43:00.236
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:00.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:00.292
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/24/23 12:43:00.324
    Aug 24 12:43:00.325: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524" in namespace "kubelet-test-5915" to be "completed"
    Aug 24 12:43:00.342: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524": Phase="Pending", Reason="", readiness=false. Elapsed: 17.285924ms
    Aug 24 12:43:02.354: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028509458s
    Aug 24 12:43:04.352: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026690629s
    Aug 24 12:43:04.352: INFO: Pod "agnhost-host-aliasesefcf80d1-050b-4190-a964-80706e658524" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:04.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5915" for this suite. 08/24/23 12:43:04.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:04.416
Aug 24 12:43:04.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:43:04.42
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:04.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:04.456
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/24/23 12:43:04.462
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/24/23 12:43:04.473
STEP: patching the secret 08/24/23 12:43:04.48
STEP: deleting the secret using a LabelSelector 08/24/23 12:43:04.501
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/24/23 12:43:04.532
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:04.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5976" for this suite. 08/24/23 12:43:04.546
------------------------------
â€¢ [0.153 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:04.416
    Aug 24 12:43:04.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:43:04.42
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:04.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:04.456
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/24/23 12:43:04.462
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/24/23 12:43:04.473
    STEP: patching the secret 08/24/23 12:43:04.48
    STEP: deleting the secret using a LabelSelector 08/24/23 12:43:04.501
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/24/23 12:43:04.532
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:04.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5976" for this suite. 08/24/23 12:43:04.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:04.567
Aug 24 12:43:04.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename containers 08/24/23 12:43:04.57
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:04.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:04.609
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/24/23 12:43:04.614
Aug 24 12:43:04.633: INFO: Waiting up to 5m0s for pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77" in namespace "containers-8272" to be "Succeeded or Failed"
Aug 24 12:43:04.645: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77": Phase="Pending", Reason="", readiness=false. Elapsed: 11.764151ms
Aug 24 12:43:06.656: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022686723s
Aug 24 12:43:08.651: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017847416s
STEP: Saw pod success 08/24/23 12:43:08.651
Aug 24 12:43:08.652: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77" satisfied condition "Succeeded or Failed"
Aug 24 12:43:08.658: INFO: Trying to get logs from node pe9deep4seen-3 pod client-containers-4f406389-df77-4fb5-86de-01dcee496d77 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:43:08.669
Aug 24 12:43:08.687: INFO: Waiting for pod client-containers-4f406389-df77-4fb5-86de-01dcee496d77 to disappear
Aug 24 12:43:08.691: INFO: Pod client-containers-4f406389-df77-4fb5-86de-01dcee496d77 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8272" for this suite. 08/24/23 12:43:08.7
------------------------------
â€¢ [4.146 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:04.567
    Aug 24 12:43:04.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename containers 08/24/23 12:43:04.57
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:04.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:04.609
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/24/23 12:43:04.614
    Aug 24 12:43:04.633: INFO: Waiting up to 5m0s for pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77" in namespace "containers-8272" to be "Succeeded or Failed"
    Aug 24 12:43:04.645: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77": Phase="Pending", Reason="", readiness=false. Elapsed: 11.764151ms
    Aug 24 12:43:06.656: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022686723s
    Aug 24 12:43:08.651: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017847416s
    STEP: Saw pod success 08/24/23 12:43:08.651
    Aug 24 12:43:08.652: INFO: Pod "client-containers-4f406389-df77-4fb5-86de-01dcee496d77" satisfied condition "Succeeded or Failed"
    Aug 24 12:43:08.658: INFO: Trying to get logs from node pe9deep4seen-3 pod client-containers-4f406389-df77-4fb5-86de-01dcee496d77 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:43:08.669
    Aug 24 12:43:08.687: INFO: Waiting for pod client-containers-4f406389-df77-4fb5-86de-01dcee496d77 to disappear
    Aug 24 12:43:08.691: INFO: Pod client-containers-4f406389-df77-4fb5-86de-01dcee496d77 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8272" for this suite. 08/24/23 12:43:08.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:08.719
Aug 24 12:43:08.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename proxy 08/24/23 12:43:08.722
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:08.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:08.757
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 24 12:43:08.762: INFO: Creating pod...
Aug 24 12:43:08.781: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6133" to be "running"
Aug 24 12:43:08.794: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.631026ms
Aug 24 12:43:10.801: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.019921321s
Aug 24 12:43:10.801: INFO: Pod "agnhost" satisfied condition "running"
Aug 24 12:43:10.801: INFO: Creating service...
Aug 24 12:43:10.818: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/DELETE
Aug 24 12:43:10.835: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 12:43:10.836: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/GET
Aug 24 12:43:10.845: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 24 12:43:10.846: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/HEAD
Aug 24 12:43:10.852: INFO: http.Client request:HEAD | StatusCode:200
Aug 24 12:43:10.853: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 24 12:43:10.859: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 12:43:10.859: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/PATCH
Aug 24 12:43:10.865: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 12:43:10.865: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/POST
Aug 24 12:43:10.878: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 12:43:10.879: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/PUT
Aug 24 12:43:10.887: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 24 12:43:10.887: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/DELETE
Aug 24 12:43:10.897: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 12:43:10.897: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/GET
Aug 24 12:43:10.911: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 24 12:43:10.911: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/HEAD
Aug 24 12:43:10.922: INFO: http.Client request:HEAD | StatusCode:200
Aug 24 12:43:10.922: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/OPTIONS
Aug 24 12:43:10.933: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 12:43:10.933: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/PATCH
Aug 24 12:43:10.943: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 12:43:10.943: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/POST
Aug 24 12:43:10.961: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 12:43:10.961: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/PUT
Aug 24 12:43:10.976: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6133" for this suite. 08/24/23 12:43:10.985
------------------------------
â€¢ [2.280 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:08.719
    Aug 24 12:43:08.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename proxy 08/24/23 12:43:08.722
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:08.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:08.757
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 24 12:43:08.762: INFO: Creating pod...
    Aug 24 12:43:08.781: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6133" to be "running"
    Aug 24 12:43:08.794: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.631026ms
    Aug 24 12:43:10.801: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.019921321s
    Aug 24 12:43:10.801: INFO: Pod "agnhost" satisfied condition "running"
    Aug 24 12:43:10.801: INFO: Creating service...
    Aug 24 12:43:10.818: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/DELETE
    Aug 24 12:43:10.835: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 12:43:10.836: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/GET
    Aug 24 12:43:10.845: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 24 12:43:10.846: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/HEAD
    Aug 24 12:43:10.852: INFO: http.Client request:HEAD | StatusCode:200
    Aug 24 12:43:10.853: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 24 12:43:10.859: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 12:43:10.859: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/PATCH
    Aug 24 12:43:10.865: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 12:43:10.865: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/POST
    Aug 24 12:43:10.878: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 12:43:10.879: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/pods/agnhost/proxy/some/path/with/PUT
    Aug 24 12:43:10.887: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 24 12:43:10.887: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/DELETE
    Aug 24 12:43:10.897: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 12:43:10.897: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/GET
    Aug 24 12:43:10.911: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 24 12:43:10.911: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/HEAD
    Aug 24 12:43:10.922: INFO: http.Client request:HEAD | StatusCode:200
    Aug 24 12:43:10.922: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/OPTIONS
    Aug 24 12:43:10.933: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 12:43:10.933: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/PATCH
    Aug 24 12:43:10.943: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 12:43:10.943: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/POST
    Aug 24 12:43:10.961: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 12:43:10.961: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6133/services/test-service/proxy/some/path/with/PUT
    Aug 24 12:43:10.976: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6133" for this suite. 08/24/23 12:43:10.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:11.019
Aug 24 12:43:11.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:43:11.022
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:11.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:11.094
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-72194b3f-abc6-4e6a-8a7b-894e80b216fc 08/24/23 12:43:11.101
STEP: Creating a pod to test consume secrets 08/24/23 12:43:11.114
Aug 24 12:43:11.127: INFO: Waiting up to 5m0s for pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a" in namespace "secrets-1835" to be "Succeeded or Failed"
Aug 24 12:43:11.131: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240055ms
Aug 24 12:43:13.140: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013185953s
Aug 24 12:43:15.140: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012592884s
STEP: Saw pod success 08/24/23 12:43:15.14
Aug 24 12:43:15.140: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a" satisfied condition "Succeeded or Failed"
Aug 24 12:43:15.146: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:43:15.157
Aug 24 12:43:15.178: INFO: Waiting for pod pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a to disappear
Aug 24 12:43:15.184: INFO: Pod pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:15.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1835" for this suite. 08/24/23 12:43:15.193
------------------------------
â€¢ [4.186 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:11.019
    Aug 24 12:43:11.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:43:11.022
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:11.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:11.094
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-72194b3f-abc6-4e6a-8a7b-894e80b216fc 08/24/23 12:43:11.101
    STEP: Creating a pod to test consume secrets 08/24/23 12:43:11.114
    Aug 24 12:43:11.127: INFO: Waiting up to 5m0s for pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a" in namespace "secrets-1835" to be "Succeeded or Failed"
    Aug 24 12:43:11.131: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240055ms
    Aug 24 12:43:13.140: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013185953s
    Aug 24 12:43:15.140: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012592884s
    STEP: Saw pod success 08/24/23 12:43:15.14
    Aug 24 12:43:15.140: INFO: Pod "pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a" satisfied condition "Succeeded or Failed"
    Aug 24 12:43:15.146: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:43:15.157
    Aug 24 12:43:15.178: INFO: Waiting for pod pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a to disappear
    Aug 24 12:43:15.184: INFO: Pod pod-secrets-2404cde3-8baa-499d-b64e-a90ec651846a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:15.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1835" for this suite. 08/24/23 12:43:15.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:15.213
Aug 24 12:43:15.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:43:15.216
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:15.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:15.247
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a 08/24/23 12:43:15.252
Aug 24 12:43:15.270: INFO: Pod name my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a: Found 0 pods out of 1
Aug 24 12:43:20.279: INFO: Pod name my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a: Found 1 pods out of 1
Aug 24 12:43:20.279: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a" are running
Aug 24 12:43:20.279: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9" in namespace "replication-controller-1066" to be "running"
Aug 24 12:43:20.286: INFO: Pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9": Phase="Running", Reason="", readiness=true. Elapsed: 7.261335ms
Aug 24 12:43:20.287: INFO: Pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9" satisfied condition "running"
Aug 24 12:43:20.287: INFO: Pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:15 +0000 UTC Reason: Message:}])
Aug 24 12:43:20.287: INFO: Trying to dial the pod
Aug 24 12:43:25.315: INFO: Controller my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a: Got expected result from replica 1 [my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9]: "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:25.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1066" for this suite. 08/24/23 12:43:25.324
------------------------------
â€¢ [SLOW TEST] [10.123 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:15.213
    Aug 24 12:43:15.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:43:15.216
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:15.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:15.247
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a 08/24/23 12:43:15.252
    Aug 24 12:43:15.270: INFO: Pod name my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a: Found 0 pods out of 1
    Aug 24 12:43:20.279: INFO: Pod name my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a: Found 1 pods out of 1
    Aug 24 12:43:20.279: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a" are running
    Aug 24 12:43:20.279: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9" in namespace "replication-controller-1066" to be "running"
    Aug 24 12:43:20.286: INFO: Pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9": Phase="Running", Reason="", readiness=true. Elapsed: 7.261335ms
    Aug 24 12:43:20.287: INFO: Pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9" satisfied condition "running"
    Aug 24 12:43:20.287: INFO: Pod "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:15 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:43:15 +0000 UTC Reason: Message:}])
    Aug 24 12:43:20.287: INFO: Trying to dial the pod
    Aug 24 12:43:25.315: INFO: Controller my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a: Got expected result from replica 1 [my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9]: "my-hostname-basic-9a693b0f-4845-47fa-93ea-e6b3e472034a-7gqw9", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:25.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1066" for this suite. 08/24/23 12:43:25.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:25.339
Aug 24 12:43:25.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:43:25.343
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:25.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:25.377
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 24 12:43:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 12:43:28.879
Aug 24 12:43:28.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 create -f -'
Aug 24 12:43:30.411: INFO: stderr: ""
Aug 24 12:43:30.411: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 24 12:43:30.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 delete e2e-test-crd-publish-openapi-1541-crds test-cr'
Aug 24 12:43:30.596: INFO: stderr: ""
Aug 24 12:43:30.596: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 24 12:43:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 apply -f -'
Aug 24 12:43:31.797: INFO: stderr: ""
Aug 24 12:43:31.797: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 24 12:43:31.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 delete e2e-test-crd-publish-openapi-1541-crds test-cr'
Aug 24 12:43:31.979: INFO: stderr: ""
Aug 24 12:43:31.979: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/24/23 12:43:31.979
Aug 24 12:43:31.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 explain e2e-test-crd-publish-openapi-1541-crds'
Aug 24 12:43:32.431: INFO: stderr: ""
Aug 24 12:43:32.431: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1541-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:43:34.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2201" for this suite. 08/24/23 12:43:34.772
------------------------------
â€¢ [SLOW TEST] [9.447 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:25.339
    Aug 24 12:43:25.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:43:25.343
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:25.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:25.377
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 24 12:43:25.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 12:43:28.879
    Aug 24 12:43:28.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 create -f -'
    Aug 24 12:43:30.411: INFO: stderr: ""
    Aug 24 12:43:30.411: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 24 12:43:30.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 delete e2e-test-crd-publish-openapi-1541-crds test-cr'
    Aug 24 12:43:30.596: INFO: stderr: ""
    Aug 24 12:43:30.596: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 24 12:43:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 apply -f -'
    Aug 24 12:43:31.797: INFO: stderr: ""
    Aug 24 12:43:31.797: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 24 12:43:31.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 --namespace=crd-publish-openapi-2201 delete e2e-test-crd-publish-openapi-1541-crds test-cr'
    Aug 24 12:43:31.979: INFO: stderr: ""
    Aug 24 12:43:31.979: INFO: stdout: "e2e-test-crd-publish-openapi-1541-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/24/23 12:43:31.979
    Aug 24 12:43:31.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-2201 explain e2e-test-crd-publish-openapi-1541-crds'
    Aug 24 12:43:32.431: INFO: stderr: ""
    Aug 24 12:43:32.431: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1541-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:43:34.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2201" for this suite. 08/24/23 12:43:34.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:43:34.792
Aug 24 12:43:34.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:43:34.795
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:34.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:34.828
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-8c4e7b12-6e90-4d8f-9673-5f3c2cdef86c 08/24/23 12:43:34.841
STEP: Creating secret with name s-test-opt-upd-ecd98a61-7581-4928-be29-0259ac80d28d 08/24/23 12:43:34.849
STEP: Creating the pod 08/24/23 12:43:34.857
Aug 24 12:43:34.874: INFO: Waiting up to 5m0s for pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c" in namespace "secrets-9535" to be "running and ready"
Aug 24 12:43:34.882: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.888432ms
Aug 24 12:43:34.882: INFO: The phase of Pod pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:43:36.891: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016960497s
Aug 24 12:43:36.891: INFO: The phase of Pod pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:43:38.892: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c": Phase="Running", Reason="", readiness=true. Elapsed: 4.017402697s
Aug 24 12:43:38.892: INFO: The phase of Pod pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c is Running (Ready = true)
Aug 24 12:43:38.892: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8c4e7b12-6e90-4d8f-9673-5f3c2cdef86c 08/24/23 12:43:38.966
STEP: Updating secret s-test-opt-upd-ecd98a61-7581-4928-be29-0259ac80d28d 08/24/23 12:43:38.979
STEP: Creating secret with name s-test-opt-create-40cd2069-7fd7-4eaf-b767-d96ec27efd51 08/24/23 12:43:39.001
STEP: waiting to observe update in volume 08/24/23 12:43:39.019
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:45:03.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9535" for this suite. 08/24/23 12:45:03.908
------------------------------
â€¢ [SLOW TEST] [89.134 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:43:34.792
    Aug 24 12:43:34.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:43:34.795
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:43:34.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:43:34.828
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-8c4e7b12-6e90-4d8f-9673-5f3c2cdef86c 08/24/23 12:43:34.841
    STEP: Creating secret with name s-test-opt-upd-ecd98a61-7581-4928-be29-0259ac80d28d 08/24/23 12:43:34.849
    STEP: Creating the pod 08/24/23 12:43:34.857
    Aug 24 12:43:34.874: INFO: Waiting up to 5m0s for pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c" in namespace "secrets-9535" to be "running and ready"
    Aug 24 12:43:34.882: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.888432ms
    Aug 24 12:43:34.882: INFO: The phase of Pod pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:43:36.891: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016960497s
    Aug 24 12:43:36.891: INFO: The phase of Pod pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:43:38.892: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c": Phase="Running", Reason="", readiness=true. Elapsed: 4.017402697s
    Aug 24 12:43:38.892: INFO: The phase of Pod pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c is Running (Ready = true)
    Aug 24 12:43:38.892: INFO: Pod "pod-secrets-cd6d4c3e-afcf-49dd-8f71-2c031f1e973c" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8c4e7b12-6e90-4d8f-9673-5f3c2cdef86c 08/24/23 12:43:38.966
    STEP: Updating secret s-test-opt-upd-ecd98a61-7581-4928-be29-0259ac80d28d 08/24/23 12:43:38.979
    STEP: Creating secret with name s-test-opt-create-40cd2069-7fd7-4eaf-b767-d96ec27efd51 08/24/23 12:43:39.001
    STEP: waiting to observe update in volume 08/24/23 12:43:39.019
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:45:03.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9535" for this suite. 08/24/23 12:45:03.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:45:03.931
Aug 24 12:45:03.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:45:03.935
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:03.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:03.973
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-e2e8f205-6181-41dc-bb08-9c561cf11163 08/24/23 12:45:03.981
STEP: Creating a pod to test consume secrets 08/24/23 12:45:03.991
Aug 24 12:45:04.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5" in namespace "projected-9976" to be "Succeeded or Failed"
Aug 24 12:45:04.018: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.695446ms
Aug 24 12:45:06.026: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015083904s
Aug 24 12:45:08.026: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01495404s
STEP: Saw pod success 08/24/23 12:45:08.026
Aug 24 12:45:08.026: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5" satisfied condition "Succeeded or Failed"
Aug 24 12:45:08.032: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:45:08.044
Aug 24 12:45:08.074: INFO: Waiting for pod pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5 to disappear
Aug 24 12:45:08.085: INFO: Pod pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:45:08.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9976" for this suite. 08/24/23 12:45:08.098
------------------------------
â€¢ [4.181 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:45:03.931
    Aug 24 12:45:03.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:45:03.935
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:03.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:03.973
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-e2e8f205-6181-41dc-bb08-9c561cf11163 08/24/23 12:45:03.981
    STEP: Creating a pod to test consume secrets 08/24/23 12:45:03.991
    Aug 24 12:45:04.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5" in namespace "projected-9976" to be "Succeeded or Failed"
    Aug 24 12:45:04.018: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.695446ms
    Aug 24 12:45:06.026: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015083904s
    Aug 24 12:45:08.026: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01495404s
    STEP: Saw pod success 08/24/23 12:45:08.026
    Aug 24 12:45:08.026: INFO: Pod "pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5" satisfied condition "Succeeded or Failed"
    Aug 24 12:45:08.032: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:45:08.044
    Aug 24 12:45:08.074: INFO: Waiting for pod pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5 to disappear
    Aug 24 12:45:08.085: INFO: Pod pod-projected-secrets-18d11c13-90a8-47a7-8828-bf96ab7781e5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:45:08.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9976" for this suite. 08/24/23 12:45:08.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:45:08.115
Aug 24 12:45:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 12:45:08.117
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:08.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:08.151
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-219 08/24/23 12:45:08.157
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-219 08/24/23 12:45:08.186
Aug 24 12:45:08.216: INFO: Found 0 stateful pods, waiting for 1
Aug 24 12:45:18.226: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/24/23 12:45:18.237
STEP: Getting /status 08/24/23 12:45:18.254
Aug 24 12:45:18.265: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/24/23 12:45:18.265
Aug 24 12:45:18.285: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/24/23 12:45:18.286
Aug 24 12:45:18.291: INFO: Observed &StatefulSet event: ADDED
Aug 24 12:45:18.292: INFO: Found Statefulset ss in namespace statefulset-219 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 12:45:18.292: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/24/23 12:45:18.292
Aug 24 12:45:18.292: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 24 12:45:18.306: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/24/23 12:45:18.306
Aug 24 12:45:18.310: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:45:18.311: INFO: Deleting all statefulset in ns statefulset-219
Aug 24 12:45:18.317: INFO: Scaling statefulset ss to 0
Aug 24 12:45:28.373: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:45:28.384: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:45:28.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-219" for this suite. 08/24/23 12:45:28.455
------------------------------
â€¢ [SLOW TEST] [20.356 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:45:08.115
    Aug 24 12:45:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:45:08.117
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:08.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:08.151
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-219 08/24/23 12:45:08.157
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-219 08/24/23 12:45:08.186
    Aug 24 12:45:08.216: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 12:45:18.226: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/24/23 12:45:18.237
    STEP: Getting /status 08/24/23 12:45:18.254
    Aug 24 12:45:18.265: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/24/23 12:45:18.265
    Aug 24 12:45:18.285: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/24/23 12:45:18.286
    Aug 24 12:45:18.291: INFO: Observed &StatefulSet event: ADDED
    Aug 24 12:45:18.292: INFO: Found Statefulset ss in namespace statefulset-219 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 12:45:18.292: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/24/23 12:45:18.292
    Aug 24 12:45:18.292: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 24 12:45:18.306: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/24/23 12:45:18.306
    Aug 24 12:45:18.310: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:45:18.311: INFO: Deleting all statefulset in ns statefulset-219
    Aug 24 12:45:18.317: INFO: Scaling statefulset ss to 0
    Aug 24 12:45:28.373: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:45:28.384: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:45:28.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-219" for this suite. 08/24/23 12:45:28.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:45:28.481
Aug 24 12:45:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename ingress 08/24/23 12:45:28.486
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:28.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:28.53
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/24/23 12:45:28.535
STEP: getting /apis/networking.k8s.io 08/24/23 12:45:28.539
STEP: getting /apis/networking.k8s.iov1 08/24/23 12:45:28.541
STEP: creating 08/24/23 12:45:28.552
STEP: getting 08/24/23 12:45:28.582
STEP: listing 08/24/23 12:45:28.588
STEP: watching 08/24/23 12:45:28.594
Aug 24 12:45:28.594: INFO: starting watch
STEP: cluster-wide listing 08/24/23 12:45:28.596
STEP: cluster-wide watching 08/24/23 12:45:28.601
Aug 24 12:45:28.602: INFO: starting watch
STEP: patching 08/24/23 12:45:28.603
STEP: updating 08/24/23 12:45:28.613
Aug 24 12:45:28.628: INFO: waiting for watch events with expected annotations
Aug 24 12:45:28.629: INFO: saw patched and updated annotations
STEP: patching /status 08/24/23 12:45:28.629
STEP: updating /status 08/24/23 12:45:28.64
STEP: get /status 08/24/23 12:45:28.656
STEP: deleting 08/24/23 12:45:28.662
STEP: deleting a collection 08/24/23 12:45:28.686
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:45:28.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-1897" for this suite. 08/24/23 12:45:28.726
------------------------------
â€¢ [0.261 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:45:28.481
    Aug 24 12:45:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename ingress 08/24/23 12:45:28.486
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:28.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:28.53
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/24/23 12:45:28.535
    STEP: getting /apis/networking.k8s.io 08/24/23 12:45:28.539
    STEP: getting /apis/networking.k8s.iov1 08/24/23 12:45:28.541
    STEP: creating 08/24/23 12:45:28.552
    STEP: getting 08/24/23 12:45:28.582
    STEP: listing 08/24/23 12:45:28.588
    STEP: watching 08/24/23 12:45:28.594
    Aug 24 12:45:28.594: INFO: starting watch
    STEP: cluster-wide listing 08/24/23 12:45:28.596
    STEP: cluster-wide watching 08/24/23 12:45:28.601
    Aug 24 12:45:28.602: INFO: starting watch
    STEP: patching 08/24/23 12:45:28.603
    STEP: updating 08/24/23 12:45:28.613
    Aug 24 12:45:28.628: INFO: waiting for watch events with expected annotations
    Aug 24 12:45:28.629: INFO: saw patched and updated annotations
    STEP: patching /status 08/24/23 12:45:28.629
    STEP: updating /status 08/24/23 12:45:28.64
    STEP: get /status 08/24/23 12:45:28.656
    STEP: deleting 08/24/23 12:45:28.662
    STEP: deleting a collection 08/24/23 12:45:28.686
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:45:28.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-1897" for this suite. 08/24/23 12:45:28.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:45:28.745
Aug 24 12:45:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 12:45:28.747
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:28.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:28.783
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/24/23 12:45:28.787
STEP: delete the rc 08/24/23 12:45:33.808
STEP: wait for all pods to be garbage collected 08/24/23 12:45:33.822
STEP: Gathering metrics 08/24/23 12:45:38.841
Aug 24 12:45:38.893: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
Aug 24 12:45:38.900: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.412189ms
Aug 24 12:45:38.900: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
Aug 24 12:45:38.900: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
Aug 24 12:45:39.025: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 12:45:39.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4167" for this suite. 08/24/23 12:45:39.037
------------------------------
â€¢ [SLOW TEST] [10.305 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:45:28.745
    Aug 24 12:45:28.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 12:45:28.747
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:28.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:28.783
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/24/23 12:45:28.787
    STEP: delete the rc 08/24/23 12:45:33.808
    STEP: wait for all pods to be garbage collected 08/24/23 12:45:33.822
    STEP: Gathering metrics 08/24/23 12:45:38.841
    Aug 24 12:45:38.893: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
    Aug 24 12:45:38.900: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.412189ms
    Aug 24 12:45:38.900: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
    Aug 24 12:45:38.900: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
    Aug 24 12:45:39.025: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:45:39.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4167" for this suite. 08/24/23 12:45:39.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:45:39.054
Aug 24 12:45:39.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:45:39.056
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:39.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:39.109
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/24/23 12:45:39.113
STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:45:39.12
STEP: Creating a ResourceQuota with not best effort scope 08/24/23 12:45:41.129
STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:45:41.137
STEP: Creating a best-effort pod 08/24/23 12:45:43.147
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/24/23 12:45:43.171
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/24/23 12:45:45.178
STEP: Deleting the pod 08/24/23 12:45:47.188
STEP: Ensuring resource quota status released the pod usage 08/24/23 12:45:47.215
STEP: Creating a not best-effort pod 08/24/23 12:45:49.225
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/24/23 12:45:49.25
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/24/23 12:45:51.259
STEP: Deleting the pod 08/24/23 12:45:53.268
STEP: Ensuring resource quota status released the pod usage 08/24/23 12:45:53.29
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:45:55.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1027" for this suite. 08/24/23 12:45:55.314
------------------------------
â€¢ [SLOW TEST] [16.277 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:45:39.054
    Aug 24 12:45:39.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:45:39.056
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:39.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:39.109
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/24/23 12:45:39.113
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:45:39.12
    STEP: Creating a ResourceQuota with not best effort scope 08/24/23 12:45:41.129
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 12:45:41.137
    STEP: Creating a best-effort pod 08/24/23 12:45:43.147
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/24/23 12:45:43.171
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/24/23 12:45:45.178
    STEP: Deleting the pod 08/24/23 12:45:47.188
    STEP: Ensuring resource quota status released the pod usage 08/24/23 12:45:47.215
    STEP: Creating a not best-effort pod 08/24/23 12:45:49.225
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/24/23 12:45:49.25
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/24/23 12:45:51.259
    STEP: Deleting the pod 08/24/23 12:45:53.268
    STEP: Ensuring resource quota status released the pod usage 08/24/23 12:45:53.29
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:45:55.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1027" for this suite. 08/24/23 12:45:55.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:45:55.343
Aug 24 12:45:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:45:55.346
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:55.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:55.387
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 12:45:55.474: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 12:46:55.534: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/24/23 12:46:55.541
Aug 24 12:46:55.601: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 24 12:46:55.618: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 24 12:46:55.697: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 24 12:46:55.727: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 24 12:46:55.791: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 24 12:46:55.806: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/24/23 12:46:55.806
Aug 24 12:46:55.806: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9787" to be "running"
Aug 24 12:46:55.827: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.951315ms
Aug 24 12:46:57.836: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029626509s
Aug 24 12:46:59.834: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.028145233s
Aug 24 12:46:59.834: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 24 12:46:59.834: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Aug 24 12:46:59.839: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.717776ms
Aug 24 12:46:59.839: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:46:59.839: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Aug 24 12:46:59.845: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.49978ms
Aug 24 12:46:59.845: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:46:59.845: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Aug 24 12:46:59.851: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.592492ms
Aug 24 12:46:59.851: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:46:59.851: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Aug 24 12:46:59.856: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.543606ms
Aug 24 12:46:59.856: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:46:59.856: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Aug 24 12:46:59.862: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.82329ms
Aug 24 12:46:59.862: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/24/23 12:46:59.862
Aug 24 12:46:59.881: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 24 12:46:59.889: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.362922ms
Aug 24 12:47:01.897: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016014162s
Aug 24 12:47:03.900: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019215351s
Aug 24 12:47:05.896: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014640006s
Aug 24 12:47:05.896: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:05.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9787" for this suite. 08/24/23 12:47:06.091
------------------------------
â€¢ [SLOW TEST] [70.763 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:45:55.343
    Aug 24 12:45:55.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:45:55.346
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:45:55.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:45:55.387
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 12:45:55.474: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 12:46:55.534: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/24/23 12:46:55.541
    Aug 24 12:46:55.601: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 24 12:46:55.618: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 24 12:46:55.697: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 24 12:46:55.727: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 24 12:46:55.791: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 24 12:46:55.806: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/24/23 12:46:55.806
    Aug 24 12:46:55.806: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9787" to be "running"
    Aug 24 12:46:55.827: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.951315ms
    Aug 24 12:46:57.836: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029626509s
    Aug 24 12:46:59.834: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.028145233s
    Aug 24 12:46:59.834: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 24 12:46:59.834: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Aug 24 12:46:59.839: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.717776ms
    Aug 24 12:46:59.839: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:46:59.839: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Aug 24 12:46:59.845: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.49978ms
    Aug 24 12:46:59.845: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:46:59.845: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Aug 24 12:46:59.851: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.592492ms
    Aug 24 12:46:59.851: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:46:59.851: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Aug 24 12:46:59.856: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.543606ms
    Aug 24 12:46:59.856: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:46:59.856: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Aug 24 12:46:59.862: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.82329ms
    Aug 24 12:46:59.862: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/24/23 12:46:59.862
    Aug 24 12:46:59.881: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 24 12:46:59.889: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.362922ms
    Aug 24 12:47:01.897: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016014162s
    Aug 24 12:47:03.900: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019215351s
    Aug 24 12:47:05.896: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014640006s
    Aug 24 12:47:05.896: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:05.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9787" for this suite. 08/24/23 12:47:06.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:06.113
Aug 24 12:47:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:47:06.117
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:06.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:06.207
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 12:47:06.212
Aug 24 12:47:06.231: INFO: Waiting up to 5m0s for pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7" in namespace "emptydir-3878" to be "Succeeded or Failed"
Aug 24 12:47:06.240: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.406719ms
Aug 24 12:47:08.250: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019130513s
Aug 24 12:47:10.248: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017485106s
STEP: Saw pod success 08/24/23 12:47:10.248
Aug 24 12:47:10.248: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7" satisfied condition "Succeeded or Failed"
Aug 24 12:47:10.255: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-8cd50240-9e2b-45da-a967-7b68dff3fea7 container test-container: <nil>
STEP: delete the pod 08/24/23 12:47:10.285
Aug 24 12:47:10.318: INFO: Waiting for pod pod-8cd50240-9e2b-45da-a967-7b68dff3fea7 to disappear
Aug 24 12:47:10.328: INFO: Pod pod-8cd50240-9e2b-45da-a967-7b68dff3fea7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:10.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3878" for this suite. 08/24/23 12:47:10.346
------------------------------
â€¢ [4.250 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:06.113
    Aug 24 12:47:06.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:47:06.117
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:06.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:06.207
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 12:47:06.212
    Aug 24 12:47:06.231: INFO: Waiting up to 5m0s for pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7" in namespace "emptydir-3878" to be "Succeeded or Failed"
    Aug 24 12:47:06.240: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.406719ms
    Aug 24 12:47:08.250: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019130513s
    Aug 24 12:47:10.248: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017485106s
    STEP: Saw pod success 08/24/23 12:47:10.248
    Aug 24 12:47:10.248: INFO: Pod "pod-8cd50240-9e2b-45da-a967-7b68dff3fea7" satisfied condition "Succeeded or Failed"
    Aug 24 12:47:10.255: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-8cd50240-9e2b-45da-a967-7b68dff3fea7 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:47:10.285
    Aug 24 12:47:10.318: INFO: Waiting for pod pod-8cd50240-9e2b-45da-a967-7b68dff3fea7 to disappear
    Aug 24 12:47:10.328: INFO: Pod pod-8cd50240-9e2b-45da-a967-7b68dff3fea7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:10.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3878" for this suite. 08/24/23 12:47:10.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:10.367
Aug 24 12:47:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:47:10.372
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:10.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:10.416
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/24/23 12:47:10.432
STEP: watching for the Service to be added 08/24/23 12:47:10.462
Aug 24 12:47:10.465: INFO: Found Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 24 12:47:10.466: INFO: Service test-service-s2vz7 created
STEP: Getting /status 08/24/23 12:47:10.467
Aug 24 12:47:10.478: INFO: Service test-service-s2vz7 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/24/23 12:47:10.479
STEP: watching for the Service to be patched 08/24/23 12:47:10.498
Aug 24 12:47:10.502: INFO: observed Service test-service-s2vz7 in namespace services-8062 with annotations: map[] & LoadBalancer: {[]}
Aug 24 12:47:10.502: INFO: Found Service test-service-s2vz7 in namespace services-8062 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 24 12:47:10.502: INFO: Service test-service-s2vz7 has service status patched
STEP: updating the ServiceStatus 08/24/23 12:47:10.502
Aug 24 12:47:10.528: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/24/23 12:47:10.528
Aug 24 12:47:10.532: INFO: Observed Service test-service-s2vz7 in namespace services-8062 with annotations: map[] & Conditions: {[]}
Aug 24 12:47:10.533: INFO: Observed event: &Service{ObjectMeta:{test-service-s2vz7  services-8062  549e2de0-6d72-45c1-8f74-27d36156db0e 25498 0 2023-08-24 12:47:10 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-24 12:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-24 12:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.38.185,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.38.185],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 24 12:47:10.534: INFO: Found Service test-service-s2vz7 in namespace services-8062 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 12:47:10.534: INFO: Service test-service-s2vz7 has service status updated
STEP: patching the service 08/24/23 12:47:10.534
STEP: watching for the Service to be patched 08/24/23 12:47:10.558
Aug 24 12:47:10.562: INFO: observed Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true]
Aug 24 12:47:10.563: INFO: observed Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true]
Aug 24 12:47:10.563: INFO: observed Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true]
Aug 24 12:47:10.564: INFO: Found Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service:patched test-service-static:true]
Aug 24 12:47:10.564: INFO: Service test-service-s2vz7 patched
STEP: deleting the service 08/24/23 12:47:10.565
STEP: watching for the Service to be deleted 08/24/23 12:47:10.594
Aug 24 12:47:10.599: INFO: Observed event: ADDED
Aug 24 12:47:10.599: INFO: Observed event: MODIFIED
Aug 24 12:47:10.599: INFO: Observed event: MODIFIED
Aug 24 12:47:10.599: INFO: Observed event: MODIFIED
Aug 24 12:47:10.599: INFO: Found Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 24 12:47:10.600: INFO: Service test-service-s2vz7 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:10.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8062" for this suite. 08/24/23 12:47:10.614
------------------------------
â€¢ [0.265 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:10.367
    Aug 24 12:47:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:47:10.372
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:10.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:10.416
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/24/23 12:47:10.432
    STEP: watching for the Service to be added 08/24/23 12:47:10.462
    Aug 24 12:47:10.465: INFO: Found Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 24 12:47:10.466: INFO: Service test-service-s2vz7 created
    STEP: Getting /status 08/24/23 12:47:10.467
    Aug 24 12:47:10.478: INFO: Service test-service-s2vz7 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/24/23 12:47:10.479
    STEP: watching for the Service to be patched 08/24/23 12:47:10.498
    Aug 24 12:47:10.502: INFO: observed Service test-service-s2vz7 in namespace services-8062 with annotations: map[] & LoadBalancer: {[]}
    Aug 24 12:47:10.502: INFO: Found Service test-service-s2vz7 in namespace services-8062 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 24 12:47:10.502: INFO: Service test-service-s2vz7 has service status patched
    STEP: updating the ServiceStatus 08/24/23 12:47:10.502
    Aug 24 12:47:10.528: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/24/23 12:47:10.528
    Aug 24 12:47:10.532: INFO: Observed Service test-service-s2vz7 in namespace services-8062 with annotations: map[] & Conditions: {[]}
    Aug 24 12:47:10.533: INFO: Observed event: &Service{ObjectMeta:{test-service-s2vz7  services-8062  549e2de0-6d72-45c1-8f74-27d36156db0e 25498 0 2023-08-24 12:47:10 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-24 12:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-24 12:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.38.185,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.38.185],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 24 12:47:10.534: INFO: Found Service test-service-s2vz7 in namespace services-8062 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 12:47:10.534: INFO: Service test-service-s2vz7 has service status updated
    STEP: patching the service 08/24/23 12:47:10.534
    STEP: watching for the Service to be patched 08/24/23 12:47:10.558
    Aug 24 12:47:10.562: INFO: observed Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true]
    Aug 24 12:47:10.563: INFO: observed Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true]
    Aug 24 12:47:10.563: INFO: observed Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service-static:true]
    Aug 24 12:47:10.564: INFO: Found Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service:patched test-service-static:true]
    Aug 24 12:47:10.564: INFO: Service test-service-s2vz7 patched
    STEP: deleting the service 08/24/23 12:47:10.565
    STEP: watching for the Service to be deleted 08/24/23 12:47:10.594
    Aug 24 12:47:10.599: INFO: Observed event: ADDED
    Aug 24 12:47:10.599: INFO: Observed event: MODIFIED
    Aug 24 12:47:10.599: INFO: Observed event: MODIFIED
    Aug 24 12:47:10.599: INFO: Observed event: MODIFIED
    Aug 24 12:47:10.599: INFO: Found Service test-service-s2vz7 in namespace services-8062 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 24 12:47:10.600: INFO: Service test-service-s2vz7 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:10.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8062" for this suite. 08/24/23 12:47:10.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:10.636
Aug 24 12:47:10.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename events 08/24/23 12:47:10.639
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:10.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:10.682
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/24/23 12:47:10.692
STEP: listing all events in all namespaces 08/24/23 12:47:10.704
STEP: patching the test event 08/24/23 12:47:10.717
STEP: fetching the test event 08/24/23 12:47:10.731
STEP: updating the test event 08/24/23 12:47:10.737
STEP: getting the test event 08/24/23 12:47:10.757
STEP: deleting the test event 08/24/23 12:47:10.764
STEP: listing all events in all namespaces 08/24/23 12:47:10.783
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:10.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8265" for this suite. 08/24/23 12:47:10.801
------------------------------
â€¢ [0.190 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:10.636
    Aug 24 12:47:10.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename events 08/24/23 12:47:10.639
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:10.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:10.682
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/24/23 12:47:10.692
    STEP: listing all events in all namespaces 08/24/23 12:47:10.704
    STEP: patching the test event 08/24/23 12:47:10.717
    STEP: fetching the test event 08/24/23 12:47:10.731
    STEP: updating the test event 08/24/23 12:47:10.737
    STEP: getting the test event 08/24/23 12:47:10.757
    STEP: deleting the test event 08/24/23 12:47:10.764
    STEP: listing all events in all namespaces 08/24/23 12:47:10.783
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:10.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8265" for this suite. 08/24/23 12:47:10.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:10.834
Aug 24 12:47:10.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:47:10.841
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:10.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:10.887
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:47:10.961
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:47:10.971
Aug 24 12:47:10.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:47:10.990: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:47:12.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:47:12.099: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:47:13.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 12:47:13.028: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:47:14.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:47:14.013: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 08/24/23 12:47:14.022
Aug 24 12:47:14.032: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/24/23 12:47:14.032
Aug 24 12:47:14.052: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/24/23 12:47:14.052
Aug 24 12:47:14.057: INFO: Observed &DaemonSet event: ADDED
Aug 24 12:47:14.058: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.059: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.059: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.060: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.061: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.061: INFO: Found daemon set daemon-set in namespace daemonsets-4892 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 12:47:14.061: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/24/23 12:47:14.062
STEP: watching for the daemon set status to be patched 08/24/23 12:47:14.077
Aug 24 12:47:14.081: INFO: Observed &DaemonSet event: ADDED
Aug 24 12:47:14.081: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.082: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.082: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.083: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.083: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.084: INFO: Observed daemon set daemon-set in namespace daemonsets-4892 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 12:47:14.084: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:47:14.084: INFO: Found daemon set daemon-set in namespace daemonsets-4892 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 24 12:47:14.084: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:47:14.095
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4892, will wait for the garbage collector to delete the pods 08/24/23 12:47:14.095
Aug 24 12:47:14.167: INFO: Deleting DaemonSet.extensions daemon-set took: 16.497703ms
Aug 24 12:47:14.269: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.232696ms
Aug 24 12:47:16.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:47:16.375: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:47:16.380: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25654"},"items":null}

Aug 24 12:47:16.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25654"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:16.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4892" for this suite. 08/24/23 12:47:16.46
------------------------------
â€¢ [SLOW TEST] [5.639 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:10.834
    Aug 24 12:47:10.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:47:10.841
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:10.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:10.887
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:47:10.961
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:47:10.971
    Aug 24 12:47:10.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:47:10.990: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:47:12.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:47:12.099: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:47:13.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 12:47:13.028: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:47:14.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:47:14.013: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 08/24/23 12:47:14.022
    Aug 24 12:47:14.032: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/24/23 12:47:14.032
    Aug 24 12:47:14.052: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/24/23 12:47:14.052
    Aug 24 12:47:14.057: INFO: Observed &DaemonSet event: ADDED
    Aug 24 12:47:14.058: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.059: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.059: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.060: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.061: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.061: INFO: Found daemon set daemon-set in namespace daemonsets-4892 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 12:47:14.061: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/24/23 12:47:14.062
    STEP: watching for the daemon set status to be patched 08/24/23 12:47:14.077
    Aug 24 12:47:14.081: INFO: Observed &DaemonSet event: ADDED
    Aug 24 12:47:14.081: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.082: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.082: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.083: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.083: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.084: INFO: Observed daemon set daemon-set in namespace daemonsets-4892 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 12:47:14.084: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:47:14.084: INFO: Found daemon set daemon-set in namespace daemonsets-4892 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 24 12:47:14.084: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:47:14.095
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4892, will wait for the garbage collector to delete the pods 08/24/23 12:47:14.095
    Aug 24 12:47:14.167: INFO: Deleting DaemonSet.extensions daemon-set took: 16.497703ms
    Aug 24 12:47:14.269: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.232696ms
    Aug 24 12:47:16.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:47:16.375: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:47:16.380: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25654"},"items":null}

    Aug 24 12:47:16.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25654"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:16.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4892" for this suite. 08/24/23 12:47:16.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:16.477
Aug 24 12:47:16.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:47:16.482
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:16.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:16.535
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7751 08/24/23 12:47:16.545
STEP: changing the ExternalName service to type=NodePort 08/24/23 12:47:16.568
STEP: creating replication controller externalname-service in namespace services-7751 08/24/23 12:47:16.647
I0824 12:47:16.680548      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7751, replica count: 2
I0824 12:47:19.732596      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:47:19.732: INFO: Creating new exec pod
Aug 24 12:47:19.744: INFO: Waiting up to 5m0s for pod "execpodc77gx" in namespace "services-7751" to be "running"
Aug 24 12:47:19.750: INFO: Pod "execpodc77gx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.170672ms
Aug 24 12:47:21.760: INFO: Pod "execpodc77gx": Phase="Running", Reason="", readiness=true. Elapsed: 2.01645538s
Aug 24 12:47:21.760: INFO: Pod "execpodc77gx" satisfied condition "running"
Aug 24 12:47:22.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 24 12:47:23.093: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 24 12:47:23.093: INFO: stdout: ""
Aug 24 12:47:23.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 10.233.25.114 80'
Aug 24 12:47:23.331: INFO: stderr: "+ nc -v -z -w 2 10.233.25.114 80\nConnection to 10.233.25.114 80 port [tcp/http] succeeded!\n"
Aug 24 12:47:23.332: INFO: stdout: ""
Aug 24 12:47:23.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 192.168.121.130 31564'
Aug 24 12:47:23.631: INFO: stderr: "+ nc -v -z -w 2 192.168.121.130 31564\nConnection to 192.168.121.130 31564 port [tcp/*] succeeded!\n"
Aug 24 12:47:23.631: INFO: stdout: ""
Aug 24 12:47:23.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 31564'
Aug 24 12:47:23.923: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 31564\nConnection to 192.168.121.111 31564 port [tcp/*] succeeded!\n"
Aug 24 12:47:23.923: INFO: stdout: ""
Aug 24 12:47:23.923: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:23.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7751" for this suite. 08/24/23 12:47:24.002
------------------------------
â€¢ [SLOW TEST] [7.547 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:16.477
    Aug 24 12:47:16.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:47:16.482
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:16.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:16.535
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7751 08/24/23 12:47:16.545
    STEP: changing the ExternalName service to type=NodePort 08/24/23 12:47:16.568
    STEP: creating replication controller externalname-service in namespace services-7751 08/24/23 12:47:16.647
    I0824 12:47:16.680548      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7751, replica count: 2
    I0824 12:47:19.732596      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:47:19.732: INFO: Creating new exec pod
    Aug 24 12:47:19.744: INFO: Waiting up to 5m0s for pod "execpodc77gx" in namespace "services-7751" to be "running"
    Aug 24 12:47:19.750: INFO: Pod "execpodc77gx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.170672ms
    Aug 24 12:47:21.760: INFO: Pod "execpodc77gx": Phase="Running", Reason="", readiness=true. Elapsed: 2.01645538s
    Aug 24 12:47:21.760: INFO: Pod "execpodc77gx" satisfied condition "running"
    Aug 24 12:47:22.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 24 12:47:23.093: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 24 12:47:23.093: INFO: stdout: ""
    Aug 24 12:47:23.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 10.233.25.114 80'
    Aug 24 12:47:23.331: INFO: stderr: "+ nc -v -z -w 2 10.233.25.114 80\nConnection to 10.233.25.114 80 port [tcp/http] succeeded!\n"
    Aug 24 12:47:23.332: INFO: stdout: ""
    Aug 24 12:47:23.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 192.168.121.130 31564'
    Aug 24 12:47:23.631: INFO: stderr: "+ nc -v -z -w 2 192.168.121.130 31564\nConnection to 192.168.121.130 31564 port [tcp/*] succeeded!\n"
    Aug 24 12:47:23.631: INFO: stdout: ""
    Aug 24 12:47:23.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-7751 exec execpodc77gx -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 31564'
    Aug 24 12:47:23.923: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 31564\nConnection to 192.168.121.111 31564 port [tcp/*] succeeded!\n"
    Aug 24 12:47:23.923: INFO: stdout: ""
    Aug 24 12:47:23.923: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:23.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7751" for this suite. 08/24/23 12:47:24.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:24.033
Aug 24 12:47:24.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:47:24.036
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:24.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:24.08
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 24 12:47:24.096: INFO: Got root ca configmap in namespace "svcaccounts-1361"
Aug 24 12:47:24.108: INFO: Deleted root ca configmap in namespace "svcaccounts-1361"
STEP: waiting for a new root ca configmap created 08/24/23 12:47:24.609
Aug 24 12:47:24.617: INFO: Recreated root ca configmap in namespace "svcaccounts-1361"
Aug 24 12:47:24.625: INFO: Updated root ca configmap in namespace "svcaccounts-1361"
STEP: waiting for the root ca configmap reconciled 08/24/23 12:47:25.126
Aug 24 12:47:25.134: INFO: Reconciled root ca configmap in namespace "svcaccounts-1361"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:25.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1361" for this suite. 08/24/23 12:47:25.148
------------------------------
â€¢ [1.127 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:24.033
    Aug 24 12:47:24.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:47:24.036
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:24.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:24.08
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 24 12:47:24.096: INFO: Got root ca configmap in namespace "svcaccounts-1361"
    Aug 24 12:47:24.108: INFO: Deleted root ca configmap in namespace "svcaccounts-1361"
    STEP: waiting for a new root ca configmap created 08/24/23 12:47:24.609
    Aug 24 12:47:24.617: INFO: Recreated root ca configmap in namespace "svcaccounts-1361"
    Aug 24 12:47:24.625: INFO: Updated root ca configmap in namespace "svcaccounts-1361"
    STEP: waiting for the root ca configmap reconciled 08/24/23 12:47:25.126
    Aug 24 12:47:25.134: INFO: Reconciled root ca configmap in namespace "svcaccounts-1361"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:25.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1361" for this suite. 08/24/23 12:47:25.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:25.166
Aug 24 12:47:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:47:25.168
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:25.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:25.202
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:47:25.23
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:47:25.662
STEP: Deploying the webhook pod 08/24/23 12:47:25.68
STEP: Wait for the deployment to be ready 08/24/23 12:47:25.706
Aug 24 12:47:25.720: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 12:47:27.74
STEP: Verifying the service has paired with the endpoint 08/24/23 12:47:27.763
Aug 24 12:47:28.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 12:47:28.771
STEP: create a pod that should be denied by the webhook 08/24/23 12:47:28.812
STEP: create a pod that causes the webhook to hang 08/24/23 12:47:28.84
STEP: create a configmap that should be denied by the webhook 08/24/23 12:47:38.856
STEP: create a configmap that should be admitted by the webhook 08/24/23 12:47:38.884
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 12:47:38.906
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 12:47:38.922
STEP: create a namespace that bypass the webhook 08/24/23 12:47:38.933
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/24/23 12:47:38.948
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1648" for this suite. 08/24/23 12:47:39.169
STEP: Destroying namespace "webhook-1648-markers" for this suite. 08/24/23 12:47:39.184
------------------------------
â€¢ [SLOW TEST] [14.083 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:25.166
    Aug 24 12:47:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:47:25.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:25.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:25.202
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:47:25.23
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:47:25.662
    STEP: Deploying the webhook pod 08/24/23 12:47:25.68
    STEP: Wait for the deployment to be ready 08/24/23 12:47:25.706
    Aug 24 12:47:25.720: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 12:47:27.74
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:47:27.763
    Aug 24 12:47:28.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 12:47:28.771
    STEP: create a pod that should be denied by the webhook 08/24/23 12:47:28.812
    STEP: create a pod that causes the webhook to hang 08/24/23 12:47:28.84
    STEP: create a configmap that should be denied by the webhook 08/24/23 12:47:38.856
    STEP: create a configmap that should be admitted by the webhook 08/24/23 12:47:38.884
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 12:47:38.906
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 12:47:38.922
    STEP: create a namespace that bypass the webhook 08/24/23 12:47:38.933
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/24/23 12:47:38.948
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:39.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1648" for this suite. 08/24/23 12:47:39.169
    STEP: Destroying namespace "webhook-1648-markers" for this suite. 08/24/23 12:47:39.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:39.259
Aug 24 12:47:39.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:47:39.264
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:39.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:39.309
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 24 12:47:39.317: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/24/23 12:47:40.345
STEP: Checking rc "condition-test" has the desired failure condition set 08/24/23 12:47:40.36
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/24/23 12:47:41.389
Aug 24 12:47:41.424: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/24/23 12:47:41.424
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:42.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-423" for this suite. 08/24/23 12:47:42.453
------------------------------
â€¢ [3.207 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:39.259
    Aug 24 12:47:39.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:47:39.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:39.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:39.309
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 24 12:47:39.317: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/24/23 12:47:40.345
    STEP: Checking rc "condition-test" has the desired failure condition set 08/24/23 12:47:40.36
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/24/23 12:47:41.389
    Aug 24 12:47:41.424: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/24/23 12:47:41.424
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:42.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-423" for this suite. 08/24/23 12:47:42.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:42.469
Aug 24 12:47:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 12:47:42.471
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:42.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:42.506
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/24/23 12:47:42.511
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/24/23 12:47:42.512
STEP: creating a pod to probe DNS 08/24/23 12:47:42.512
STEP: submitting the pod to kubernetes 08/24/23 12:47:42.513
Aug 24 12:47:42.533: INFO: Waiting up to 15m0s for pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5" in namespace "dns-8588" to be "running"
Aug 24 12:47:42.546: INFO: Pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.260961ms
Aug 24 12:47:44.560: INFO: Pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.027131175s
Aug 24 12:47:44.560: INFO: Pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:47:44.56
STEP: looking for the results for each expected name from probers 08/24/23 12:47:44.573
Aug 24 12:47:44.630: INFO: DNS probes using dns-8588/dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5 succeeded

STEP: deleting the pod 08/24/23 12:47:44.63
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:44.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8588" for this suite. 08/24/23 12:47:44.681
------------------------------
â€¢ [2.262 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:42.469
    Aug 24 12:47:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 12:47:42.471
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:42.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:42.506
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/24/23 12:47:42.511
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/24/23 12:47:42.512
    STEP: creating a pod to probe DNS 08/24/23 12:47:42.512
    STEP: submitting the pod to kubernetes 08/24/23 12:47:42.513
    Aug 24 12:47:42.533: INFO: Waiting up to 15m0s for pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5" in namespace "dns-8588" to be "running"
    Aug 24 12:47:42.546: INFO: Pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.260961ms
    Aug 24 12:47:44.560: INFO: Pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.027131175s
    Aug 24 12:47:44.560: INFO: Pod "dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:47:44.56
    STEP: looking for the results for each expected name from probers 08/24/23 12:47:44.573
    Aug 24 12:47:44.630: INFO: DNS probes using dns-8588/dns-test-a21d899d-5012-4c09-8d1c-4b19f9c1d5c5 succeeded

    STEP: deleting the pod 08/24/23 12:47:44.63
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:44.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8588" for this suite. 08/24/23 12:47:44.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:44.742
Aug 24 12:47:44.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:47:44.747
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:44.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:44.798
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:47:44.809
Aug 24 12:47:44.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3" in namespace "projected-5205" to be "Succeeded or Failed"
Aug 24 12:47:44.865: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.912237ms
Aug 24 12:47:46.872: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042632301s
Aug 24 12:47:48.873: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043669152s
STEP: Saw pod success 08/24/23 12:47:48.874
Aug 24 12:47:48.874: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3" satisfied condition "Succeeded or Failed"
Aug 24 12:47:48.881: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3 container client-container: <nil>
STEP: delete the pod 08/24/23 12:47:48.893
Aug 24 12:47:48.919: INFO: Waiting for pod downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3 to disappear
Aug 24 12:47:48.929: INFO: Pod downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:48.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5205" for this suite. 08/24/23 12:47:48.941
------------------------------
â€¢ [4.213 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:44.742
    Aug 24 12:47:44.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:47:44.747
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:44.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:44.798
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:47:44.809
    Aug 24 12:47:44.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3" in namespace "projected-5205" to be "Succeeded or Failed"
    Aug 24 12:47:44.865: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.912237ms
    Aug 24 12:47:46.872: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042632301s
    Aug 24 12:47:48.873: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043669152s
    STEP: Saw pod success 08/24/23 12:47:48.874
    Aug 24 12:47:48.874: INFO: Pod "downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3" satisfied condition "Succeeded or Failed"
    Aug 24 12:47:48.881: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:47:48.893
    Aug 24 12:47:48.919: INFO: Waiting for pod downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3 to disappear
    Aug 24 12:47:48.929: INFO: Pod downwardapi-volume-d3186109-7cd4-4c0c-8d58-94207edd72d3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:48.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5205" for this suite. 08/24/23 12:47:48.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:48.963
Aug 24 12:47:48.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:47:48.965
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:48.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:49.004
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-90ab91b3-b5dd-4bf8-9342-b4255699fbd5 08/24/23 12:47:49.011
STEP: Creating a pod to test consume secrets 08/24/23 12:47:49.021
Aug 24 12:47:49.038: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5" in namespace "projected-6838" to be "Succeeded or Failed"
Aug 24 12:47:49.044: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.425482ms
Aug 24 12:47:51.054: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015873628s
Aug 24 12:47:53.053: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014370119s
STEP: Saw pod success 08/24/23 12:47:53.053
Aug 24 12:47:53.054: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5" satisfied condition "Succeeded or Failed"
Aug 24 12:47:53.061: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:47:53.076
Aug 24 12:47:53.094: INFO: Waiting for pod pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5 to disappear
Aug 24 12:47:53.100: INFO: Pod pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:53.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6838" for this suite. 08/24/23 12:47:53.11
------------------------------
â€¢ [4.160 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:48.963
    Aug 24 12:47:48.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:47:48.965
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:48.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:49.004
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-90ab91b3-b5dd-4bf8-9342-b4255699fbd5 08/24/23 12:47:49.011
    STEP: Creating a pod to test consume secrets 08/24/23 12:47:49.021
    Aug 24 12:47:49.038: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5" in namespace "projected-6838" to be "Succeeded or Failed"
    Aug 24 12:47:49.044: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.425482ms
    Aug 24 12:47:51.054: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015873628s
    Aug 24 12:47:53.053: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014370119s
    STEP: Saw pod success 08/24/23 12:47:53.053
    Aug 24 12:47:53.054: INFO: Pod "pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5" satisfied condition "Succeeded or Failed"
    Aug 24 12:47:53.061: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:47:53.076
    Aug 24 12:47:53.094: INFO: Waiting for pod pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5 to disappear
    Aug 24 12:47:53.100: INFO: Pod pod-projected-secrets-84fd1a82-1ac9-4a86-9420-a8c8cfe3dcc5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:53.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6838" for this suite. 08/24/23 12:47:53.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:53.126
Aug 24 12:47:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:47:53.128
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:53.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:53.156
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:47:53.161
Aug 24 12:47:53.182: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e" in namespace "downward-api-7376" to be "Succeeded or Failed"
Aug 24 12:47:53.200: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.607ms
Aug 24 12:47:55.210: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027371728s
Aug 24 12:47:57.210: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026975203s
STEP: Saw pod success 08/24/23 12:47:57.21
Aug 24 12:47:57.210: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e" satisfied condition "Succeeded or Failed"
Aug 24 12:47:57.215: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e container client-container: <nil>
STEP: delete the pod 08/24/23 12:47:57.227
Aug 24 12:47:57.251: INFO: Waiting for pod downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e to disappear
Aug 24 12:47:57.258: INFO: Pod downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:47:57.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7376" for this suite. 08/24/23 12:47:57.268
------------------------------
â€¢ [4.156 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:53.126
    Aug 24 12:47:53.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:47:53.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:53.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:53.156
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:47:53.161
    Aug 24 12:47:53.182: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e" in namespace "downward-api-7376" to be "Succeeded or Failed"
    Aug 24 12:47:53.200: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.607ms
    Aug 24 12:47:55.210: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027371728s
    Aug 24 12:47:57.210: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026975203s
    STEP: Saw pod success 08/24/23 12:47:57.21
    Aug 24 12:47:57.210: INFO: Pod "downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e" satisfied condition "Succeeded or Failed"
    Aug 24 12:47:57.215: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e container client-container: <nil>
    STEP: delete the pod 08/24/23 12:47:57.227
    Aug 24 12:47:57.251: INFO: Waiting for pod downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e to disappear
    Aug 24 12:47:57.258: INFO: Pod downwardapi-volume-21da27fa-143c-4927-af44-ede01958e98e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:47:57.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7376" for this suite. 08/24/23 12:47:57.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:47:57.289
Aug 24 12:47:57.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:47:57.291
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:57.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:57.323
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 24 12:47:57.329: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 24 12:47:57.348: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 12:48:02.358: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:48:02.359
Aug 24 12:48:02.359: INFO: Creating deployment "test-rolling-update-deployment"
Aug 24 12:48:02.374: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 24 12:48:02.390: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 24 12:48:04.404: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 24 12:48:04.411: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:48:04.432: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5338  50aaa261-1616-4a69-8e90-551739a91f7a 26209 1 2023-08-24 12:48:02 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-24 12:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c2808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:48:02 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-24 12:48:03 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 12:48:04.439: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-5338  4ffbda43-b586-4c57-b462-89db37ba49d7 26198 1 2023-08-24 12:48:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 50aaa261-1616-4a69-8e90-551739a91f7a 0xc0052c2ce7 0xc0052c2ce8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50aaa261-1616-4a69-8e90-551739a91f7a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c2d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:48:04.439: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 24 12:48:04.439: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5338  b678bb20-8ec6-4e6c-b5b0-51e8bcb847a6 26208 2 2023-08-24 12:47:57 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 50aaa261-1616-4a69-8e90-551739a91f7a 0xc0052c2bb7 0xc0052c2bb8}] [] [{e2e.test Update apps/v1 2023-08-24 12:47:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50aaa261-1616-4a69-8e90-551739a91f7a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0052c2c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:48:04.452: INFO: Pod "test-rolling-update-deployment-7549d9f46d-n5msk" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-n5msk test-rolling-update-deployment-7549d9f46d- deployment-5338  c801e47d-ac07-4a69-b827-099b42966883 26197 0 2023-08-24 12:48:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 4ffbda43-b586-4c57-b462-89db37ba49d7 0xc00539ccc7 0xc00539ccc8}] [] [{kube-controller-manager Update v1 2023-08-24 12:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ffbda43-b586-4c57-b462-89db37ba49d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zllj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zllj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.36,StartTime:2023-08-24 12:48:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:48:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://01a1d07d83574a93cceeef0ea3a53416ff810474030028cde566625d60d2667c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:04.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5338" for this suite. 08/24/23 12:48:04.463
------------------------------
â€¢ [SLOW TEST] [7.193 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:47:57.289
    Aug 24 12:47:57.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:47:57.291
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:47:57.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:47:57.323
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 24 12:47:57.329: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 24 12:47:57.348: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 12:48:02.358: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:48:02.359
    Aug 24 12:48:02.359: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 24 12:48:02.374: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 24 12:48:02.390: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 24 12:48:04.404: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 24 12:48:04.411: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:48:04.432: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5338  50aaa261-1616-4a69-8e90-551739a91f7a 26209 1 2023-08-24 12:48:02 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-24 12:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c2808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:48:02 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-24 12:48:03 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 12:48:04.439: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-5338  4ffbda43-b586-4c57-b462-89db37ba49d7 26198 1 2023-08-24 12:48:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 50aaa261-1616-4a69-8e90-551739a91f7a 0xc0052c2ce7 0xc0052c2ce8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50aaa261-1616-4a69-8e90-551739a91f7a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c2d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:48:04.439: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 24 12:48:04.439: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5338  b678bb20-8ec6-4e6c-b5b0-51e8bcb847a6 26208 2 2023-08-24 12:47:57 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 50aaa261-1616-4a69-8e90-551739a91f7a 0xc0052c2bb7 0xc0052c2bb8}] [] [{e2e.test Update apps/v1 2023-08-24 12:47:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50aaa261-1616-4a69-8e90-551739a91f7a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0052c2c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:48:04.452: INFO: Pod "test-rolling-update-deployment-7549d9f46d-n5msk" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-n5msk test-rolling-update-deployment-7549d9f46d- deployment-5338  c801e47d-ac07-4a69-b827-099b42966883 26197 0 2023-08-24 12:48:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 4ffbda43-b586-4c57-b462-89db37ba49d7 0xc00539ccc7 0xc00539ccc8}] [] [{kube-controller-manager Update v1 2023-08-24 12:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ffbda43-b586-4c57-b462-89db37ba49d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zllj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zllj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:48:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.36,StartTime:2023-08-24 12:48:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:48:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://01a1d07d83574a93cceeef0ea3a53416ff810474030028cde566625d60d2667c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:04.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5338" for this suite. 08/24/23 12:48:04.463
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:04.484
Aug 24 12:48:04.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:48:04.487
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:04.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:04.524
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/24/23 12:48:04.528
Aug 24 12:48:04.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4654 create -f -'
Aug 24 12:48:05.870: INFO: stderr: ""
Aug 24 12:48:05.870: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/24/23 12:48:05.87
Aug 24 12:48:06.880: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:48:06.880: INFO: Found 0 / 1
Aug 24 12:48:07.878: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:48:07.878: INFO: Found 1 / 1
Aug 24 12:48:07.879: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/24/23 12:48:07.879
Aug 24 12:48:07.885: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:48:07.885: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 24 12:48:07.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4654 patch pod agnhost-primary-chc95 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 24 12:48:08.102: INFO: stderr: ""
Aug 24 12:48:08.102: INFO: stdout: "pod/agnhost-primary-chc95 patched\n"
STEP: checking annotations 08/24/23 12:48:08.102
Aug 24 12:48:08.109: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:48:08.109: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:08.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4654" for this suite. 08/24/23 12:48:08.117
------------------------------
â€¢ [3.647 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:04.484
    Aug 24 12:48:04.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:48:04.487
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:04.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:04.524
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/24/23 12:48:04.528
    Aug 24 12:48:04.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4654 create -f -'
    Aug 24 12:48:05.870: INFO: stderr: ""
    Aug 24 12:48:05.870: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/24/23 12:48:05.87
    Aug 24 12:48:06.880: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:48:06.880: INFO: Found 0 / 1
    Aug 24 12:48:07.878: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:48:07.878: INFO: Found 1 / 1
    Aug 24 12:48:07.879: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/24/23 12:48:07.879
    Aug 24 12:48:07.885: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:48:07.885: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 24 12:48:07.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4654 patch pod agnhost-primary-chc95 -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 24 12:48:08.102: INFO: stderr: ""
    Aug 24 12:48:08.102: INFO: stdout: "pod/agnhost-primary-chc95 patched\n"
    STEP: checking annotations 08/24/23 12:48:08.102
    Aug 24 12:48:08.109: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:48:08.109: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:08.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4654" for this suite. 08/24/23 12:48:08.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:08.136
Aug 24 12:48:08.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename tables 08/24/23 12:48:08.142
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:08.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:08.187
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:08.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4716" for this suite. 08/24/23 12:48:08.209
------------------------------
â€¢ [0.091 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:08.136
    Aug 24 12:48:08.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename tables 08/24/23 12:48:08.142
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:08.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:08.187
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:08.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4716" for this suite. 08/24/23 12:48:08.209
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:08.228
Aug 24 12:48:08.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename namespaces 08/24/23 12:48:08.23
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:08.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:08.269
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-rpx6b" 08/24/23 12:48:08.274
Aug 24 12:48:08.306: INFO: Namespace "e2e-ns-rpx6b-5069" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-rpx6b-5069" 08/24/23 12:48:08.306
Aug 24 12:48:08.324: INFO: Namespace "e2e-ns-rpx6b-5069" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-rpx6b-5069" 08/24/23 12:48:08.325
Aug 24 12:48:08.343: INFO: Namespace "e2e-ns-rpx6b-5069" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:08.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4251" for this suite. 08/24/23 12:48:08.357
STEP: Destroying namespace "e2e-ns-rpx6b-5069" for this suite. 08/24/23 12:48:08.369
------------------------------
â€¢ [0.154 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:08.228
    Aug 24 12:48:08.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:48:08.23
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:08.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:08.269
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-rpx6b" 08/24/23 12:48:08.274
    Aug 24 12:48:08.306: INFO: Namespace "e2e-ns-rpx6b-5069" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-rpx6b-5069" 08/24/23 12:48:08.306
    Aug 24 12:48:08.324: INFO: Namespace "e2e-ns-rpx6b-5069" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-rpx6b-5069" 08/24/23 12:48:08.325
    Aug 24 12:48:08.343: INFO: Namespace "e2e-ns-rpx6b-5069" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:08.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4251" for this suite. 08/24/23 12:48:08.357
    STEP: Destroying namespace "e2e-ns-rpx6b-5069" for this suite. 08/24/23 12:48:08.369
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:08.383
Aug 24 12:48:08.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:48:08.385
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:08.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:08.415
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:48:08.43
Aug 24 12:48:08.447: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5460" to be "running and ready"
Aug 24 12:48:08.456: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.64686ms
Aug 24 12:48:08.456: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:48:10.463: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015349344s
Aug 24 12:48:10.463: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 12:48:10.463: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/24/23 12:48:10.469
Aug 24 12:48:10.480: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5460" to be "running and ready"
Aug 24 12:48:10.486: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075808ms
Aug 24 12:48:10.486: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:48:12.500: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01999678s
Aug 24 12:48:12.500: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 24 12:48:12.500: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/24/23 12:48:12.505
Aug 24 12:48:12.515: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 24 12:48:12.522: INFO: Pod pod-with-prestop-http-hook still exists
Aug 24 12:48:14.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 24 12:48:14.532: INFO: Pod pod-with-prestop-http-hook still exists
Aug 24 12:48:16.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 24 12:48:16.532: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/24/23 12:48:16.532
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:16.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5460" for this suite. 08/24/23 12:48:16.573
------------------------------
â€¢ [SLOW TEST] [8.200 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:08.383
    Aug 24 12:48:08.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:48:08.385
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:08.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:08.415
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:48:08.43
    Aug 24 12:48:08.447: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5460" to be "running and ready"
    Aug 24 12:48:08.456: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.64686ms
    Aug 24 12:48:08.456: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:48:10.463: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015349344s
    Aug 24 12:48:10.463: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 12:48:10.463: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/24/23 12:48:10.469
    Aug 24 12:48:10.480: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5460" to be "running and ready"
    Aug 24 12:48:10.486: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075808ms
    Aug 24 12:48:10.486: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:48:12.500: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01999678s
    Aug 24 12:48:12.500: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 24 12:48:12.500: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/24/23 12:48:12.505
    Aug 24 12:48:12.515: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 24 12:48:12.522: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 24 12:48:14.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 24 12:48:14.532: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 24 12:48:16.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 24 12:48:16.532: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/24/23 12:48:16.532
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:16.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5460" for this suite. 08/24/23 12:48:16.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:16.587
Aug 24 12:48:16.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename cronjob 08/24/23 12:48:16.591
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:16.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:16.624
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/24/23 12:48:16.629
STEP: creating 08/24/23 12:48:16.629
STEP: getting 08/24/23 12:48:16.641
STEP: listing 08/24/23 12:48:16.646
STEP: watching 08/24/23 12:48:16.651
Aug 24 12:48:16.652: INFO: starting watch
STEP: cluster-wide listing 08/24/23 12:48:16.653
STEP: cluster-wide watching 08/24/23 12:48:16.658
Aug 24 12:48:16.659: INFO: starting watch
STEP: patching 08/24/23 12:48:16.661
STEP: updating 08/24/23 12:48:16.671
Aug 24 12:48:16.683: INFO: waiting for watch events with expected annotations
Aug 24 12:48:16.684: INFO: saw patched and updated annotations
STEP: patching /status 08/24/23 12:48:16.684
STEP: updating /status 08/24/23 12:48:16.696
STEP: get /status 08/24/23 12:48:16.711
STEP: deleting 08/24/23 12:48:16.719
STEP: deleting a collection 08/24/23 12:48:16.748
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:16.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2986" for this suite. 08/24/23 12:48:16.779
------------------------------
â€¢ [0.225 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:16.587
    Aug 24 12:48:16.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename cronjob 08/24/23 12:48:16.591
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:16.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:16.624
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/24/23 12:48:16.629
    STEP: creating 08/24/23 12:48:16.629
    STEP: getting 08/24/23 12:48:16.641
    STEP: listing 08/24/23 12:48:16.646
    STEP: watching 08/24/23 12:48:16.651
    Aug 24 12:48:16.652: INFO: starting watch
    STEP: cluster-wide listing 08/24/23 12:48:16.653
    STEP: cluster-wide watching 08/24/23 12:48:16.658
    Aug 24 12:48:16.659: INFO: starting watch
    STEP: patching 08/24/23 12:48:16.661
    STEP: updating 08/24/23 12:48:16.671
    Aug 24 12:48:16.683: INFO: waiting for watch events with expected annotations
    Aug 24 12:48:16.684: INFO: saw patched and updated annotations
    STEP: patching /status 08/24/23 12:48:16.684
    STEP: updating /status 08/24/23 12:48:16.696
    STEP: get /status 08/24/23 12:48:16.711
    STEP: deleting 08/24/23 12:48:16.719
    STEP: deleting a collection 08/24/23 12:48:16.748
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:16.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2986" for this suite. 08/24/23 12:48:16.779
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:16.812
Aug 24 12:48:16.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:48:16.814
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:16.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:16.844
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-01fa1198-388a-430e-8e02-af5f9dc6b8e3 08/24/23 12:48:16.848
STEP: Creating a pod to test consume configMaps 08/24/23 12:48:16.858
Aug 24 12:48:16.878: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a" in namespace "projected-3319" to be "Succeeded or Failed"
Aug 24 12:48:16.893: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.073579ms
Aug 24 12:48:18.902: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02358387s
Aug 24 12:48:20.901: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022798717s
STEP: Saw pod success 08/24/23 12:48:20.901
Aug 24 12:48:20.902: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a" satisfied condition "Succeeded or Failed"
Aug 24 12:48:20.907: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:48:20.92
Aug 24 12:48:20.942: INFO: Waiting for pod pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a to disappear
Aug 24 12:48:20.948: INFO: Pod pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:20.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3319" for this suite. 08/24/23 12:48:20.961
------------------------------
â€¢ [4.163 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:16.812
    Aug 24 12:48:16.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:48:16.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:16.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:16.844
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-01fa1198-388a-430e-8e02-af5f9dc6b8e3 08/24/23 12:48:16.848
    STEP: Creating a pod to test consume configMaps 08/24/23 12:48:16.858
    Aug 24 12:48:16.878: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a" in namespace "projected-3319" to be "Succeeded or Failed"
    Aug 24 12:48:16.893: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.073579ms
    Aug 24 12:48:18.902: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02358387s
    Aug 24 12:48:20.901: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022798717s
    STEP: Saw pod success 08/24/23 12:48:20.901
    Aug 24 12:48:20.902: INFO: Pod "pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a" satisfied condition "Succeeded or Failed"
    Aug 24 12:48:20.907: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:48:20.92
    Aug 24 12:48:20.942: INFO: Waiting for pod pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a to disappear
    Aug 24 12:48:20.948: INFO: Pod pod-projected-configmaps-e98578c8-6ae3-47be-86dd-adac842ac64a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:20.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3319" for this suite. 08/24/23 12:48:20.961
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:20.975
Aug 24 12:48:20.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 12:48:20.978
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:21.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:21.014
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/24/23 12:48:21.02
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
 08/24/23 12:48:21.031
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
 08/24/23 12:48:21.032
STEP: creating a pod to probe DNS 08/24/23 12:48:21.032
STEP: submitting the pod to kubernetes 08/24/23 12:48:21.032
Aug 24 12:48:21.061: INFO: Waiting up to 15m0s for pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c" in namespace "dns-6893" to be "running"
Aug 24 12:48:21.074: INFO: Pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.687062ms
Aug 24 12:48:23.083: INFO: Pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.022355005s
Aug 24 12:48:23.083: INFO: Pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:48:23.083
STEP: looking for the results for each expected name from probers 08/24/23 12:48:23.089
Aug 24 12:48:23.106: INFO: DNS probes using dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c succeeded

STEP: deleting the pod 08/24/23 12:48:23.106
STEP: changing the externalName to bar.example.com 08/24/23 12:48:23.128
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
 08/24/23 12:48:23.15
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
 08/24/23 12:48:23.15
STEP: creating a second pod to probe DNS 08/24/23 12:48:23.15
STEP: submitting the pod to kubernetes 08/24/23 12:48:23.151
Aug 24 12:48:23.166: INFO: Waiting up to 15m0s for pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3" in namespace "dns-6893" to be "running"
Aug 24 12:48:23.180: INFO: Pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.396638ms
Aug 24 12:48:25.193: INFO: Pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.027037822s
Aug 24 12:48:25.193: INFO: Pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:48:25.193
STEP: looking for the results for each expected name from probers 08/24/23 12:48:25.202
Aug 24 12:48:25.220: INFO: File jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local from pod  dns-6893/dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 24 12:48:25.220: INFO: Lookups using dns-6893/dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3 failed for: [jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local]

Aug 24 12:48:30.237: INFO: DNS probes using dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3 succeeded

STEP: deleting the pod 08/24/23 12:48:30.237
STEP: changing the service to type=ClusterIP 08/24/23 12:48:30.257
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
 08/24/23 12:48:30.331
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
 08/24/23 12:48:30.331
STEP: creating a third pod to probe DNS 08/24/23 12:48:30.332
STEP: submitting the pod to kubernetes 08/24/23 12:48:30.339
Aug 24 12:48:30.352: INFO: Waiting up to 15m0s for pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7" in namespace "dns-6893" to be "running"
Aug 24 12:48:30.363: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.74265ms
Aug 24 12:48:32.372: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019965081s
Aug 24 12:48:34.374: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7": Phase="Running", Reason="", readiness=true. Elapsed: 4.02185716s
Aug 24 12:48:34.374: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:48:34.374
STEP: looking for the results for each expected name from probers 08/24/23 12:48:34.384
Aug 24 12:48:34.405: INFO: DNS probes using dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7 succeeded

STEP: deleting the pod 08/24/23 12:48:34.405
STEP: deleting the test externalName service 08/24/23 12:48:34.431
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:34.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6893" for this suite. 08/24/23 12:48:34.477
------------------------------
â€¢ [SLOW TEST] [13.518 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:20.975
    Aug 24 12:48:20.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 12:48:20.978
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:21.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:21.014
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/24/23 12:48:21.02
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
     08/24/23 12:48:21.031
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
     08/24/23 12:48:21.032
    STEP: creating a pod to probe DNS 08/24/23 12:48:21.032
    STEP: submitting the pod to kubernetes 08/24/23 12:48:21.032
    Aug 24 12:48:21.061: INFO: Waiting up to 15m0s for pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c" in namespace "dns-6893" to be "running"
    Aug 24 12:48:21.074: INFO: Pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.687062ms
    Aug 24 12:48:23.083: INFO: Pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.022355005s
    Aug 24 12:48:23.083: INFO: Pod "dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:48:23.083
    STEP: looking for the results for each expected name from probers 08/24/23 12:48:23.089
    Aug 24 12:48:23.106: INFO: DNS probes using dns-test-d71ac2b1-5f60-4abf-9e5e-96cdc1fb2a2c succeeded

    STEP: deleting the pod 08/24/23 12:48:23.106
    STEP: changing the externalName to bar.example.com 08/24/23 12:48:23.128
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
     08/24/23 12:48:23.15
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
     08/24/23 12:48:23.15
    STEP: creating a second pod to probe DNS 08/24/23 12:48:23.15
    STEP: submitting the pod to kubernetes 08/24/23 12:48:23.151
    Aug 24 12:48:23.166: INFO: Waiting up to 15m0s for pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3" in namespace "dns-6893" to be "running"
    Aug 24 12:48:23.180: INFO: Pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.396638ms
    Aug 24 12:48:25.193: INFO: Pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.027037822s
    Aug 24 12:48:25.193: INFO: Pod "dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:48:25.193
    STEP: looking for the results for each expected name from probers 08/24/23 12:48:25.202
    Aug 24 12:48:25.220: INFO: File jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local from pod  dns-6893/dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 24 12:48:25.220: INFO: Lookups using dns-6893/dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3 failed for: [jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local]

    Aug 24 12:48:30.237: INFO: DNS probes using dns-test-7a9e124a-37f8-44de-bcaf-6ebe25e0daa3 succeeded

    STEP: deleting the pod 08/24/23 12:48:30.237
    STEP: changing the service to type=ClusterIP 08/24/23 12:48:30.257
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
     08/24/23 12:48:30.331
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6893.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6893.svc.cluster.local; sleep 1; done
     08/24/23 12:48:30.331
    STEP: creating a third pod to probe DNS 08/24/23 12:48:30.332
    STEP: submitting the pod to kubernetes 08/24/23 12:48:30.339
    Aug 24 12:48:30.352: INFO: Waiting up to 15m0s for pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7" in namespace "dns-6893" to be "running"
    Aug 24 12:48:30.363: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.74265ms
    Aug 24 12:48:32.372: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019965081s
    Aug 24 12:48:34.374: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7": Phase="Running", Reason="", readiness=true. Elapsed: 4.02185716s
    Aug 24 12:48:34.374: INFO: Pod "dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:48:34.374
    STEP: looking for the results for each expected name from probers 08/24/23 12:48:34.384
    Aug 24 12:48:34.405: INFO: DNS probes using dns-test-9419b4df-14e7-4830-8482-e1c10ce779c7 succeeded

    STEP: deleting the pod 08/24/23 12:48:34.405
    STEP: deleting the test externalName service 08/24/23 12:48:34.431
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:34.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6893" for this suite. 08/24/23 12:48:34.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:34.503
Aug 24 12:48:34.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:48:34.506
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:34.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:34.544
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 12:48:34.549
Aug 24 12:48:34.562: INFO: Waiting up to 5m0s for pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e" in namespace "emptydir-9232" to be "Succeeded or Failed"
Aug 24 12:48:34.579: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.431982ms
Aug 24 12:48:36.585: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023084401s
Aug 24 12:48:38.587: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025082521s
STEP: Saw pod success 08/24/23 12:48:38.588
Aug 24 12:48:38.588: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e" satisfied condition "Succeeded or Failed"
Aug 24 12:48:38.595: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-ac7be750-d190-4700-bcac-ce2317ae0c6e container test-container: <nil>
STEP: delete the pod 08/24/23 12:48:38.607
Aug 24 12:48:38.626: INFO: Waiting for pod pod-ac7be750-d190-4700-bcac-ce2317ae0c6e to disappear
Aug 24 12:48:38.633: INFO: Pod pod-ac7be750-d190-4700-bcac-ce2317ae0c6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9232" for this suite. 08/24/23 12:48:38.643
------------------------------
â€¢ [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:34.503
    Aug 24 12:48:34.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:48:34.506
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:34.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:34.544
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 12:48:34.549
    Aug 24 12:48:34.562: INFO: Waiting up to 5m0s for pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e" in namespace "emptydir-9232" to be "Succeeded or Failed"
    Aug 24 12:48:34.579: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.431982ms
    Aug 24 12:48:36.585: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023084401s
    Aug 24 12:48:38.587: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025082521s
    STEP: Saw pod success 08/24/23 12:48:38.588
    Aug 24 12:48:38.588: INFO: Pod "pod-ac7be750-d190-4700-bcac-ce2317ae0c6e" satisfied condition "Succeeded or Failed"
    Aug 24 12:48:38.595: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-ac7be750-d190-4700-bcac-ce2317ae0c6e container test-container: <nil>
    STEP: delete the pod 08/24/23 12:48:38.607
    Aug 24 12:48:38.626: INFO: Waiting for pod pod-ac7be750-d190-4700-bcac-ce2317ae0c6e to disappear
    Aug 24 12:48:38.633: INFO: Pod pod-ac7be750-d190-4700-bcac-ce2317ae0c6e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9232" for this suite. 08/24/23 12:48:38.643
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:38.656
Aug 24 12:48:38.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:48:38.658
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:38.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:38.705
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9721" for this suite. 08/24/23 12:48:38.801
------------------------------
â€¢ [0.156 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:38.656
    Aug 24 12:48:38.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:48:38.658
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:38.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:38.705
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:38.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9721" for this suite. 08/24/23 12:48:38.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:38.814
Aug 24 12:48:38.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:48:38.816
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:38.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:38.855
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/24/23 12:48:38.861
STEP: getting /apis/node.k8s.io 08/24/23 12:48:38.865
STEP: getting /apis/node.k8s.io/v1 08/24/23 12:48:38.867
STEP: creating 08/24/23 12:48:38.869
STEP: watching 08/24/23 12:48:38.9
Aug 24 12:48:38.901: INFO: starting watch
STEP: getting 08/24/23 12:48:38.911
STEP: listing 08/24/23 12:48:38.916
STEP: patching 08/24/23 12:48:38.923
STEP: updating 08/24/23 12:48:38.932
Aug 24 12:48:38.942: INFO: waiting for watch events with expected annotations
STEP: deleting 08/24/23 12:48:38.942
STEP: deleting a collection 08/24/23 12:48:38.974
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 12:48:39.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6688" for this suite. 08/24/23 12:48:39.011
------------------------------
â€¢ [0.215 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:38.814
    Aug 24 12:48:38.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:48:38.816
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:38.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:38.855
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/24/23 12:48:38.861
    STEP: getting /apis/node.k8s.io 08/24/23 12:48:38.865
    STEP: getting /apis/node.k8s.io/v1 08/24/23 12:48:38.867
    STEP: creating 08/24/23 12:48:38.869
    STEP: watching 08/24/23 12:48:38.9
    Aug 24 12:48:38.901: INFO: starting watch
    STEP: getting 08/24/23 12:48:38.911
    STEP: listing 08/24/23 12:48:38.916
    STEP: patching 08/24/23 12:48:38.923
    STEP: updating 08/24/23 12:48:38.932
    Aug 24 12:48:38.942: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/24/23 12:48:38.942
    STEP: deleting a collection 08/24/23 12:48:38.974
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:48:39.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6688" for this suite. 08/24/23 12:48:39.011
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:48:39.029
Aug 24 12:48:39.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-runtime 08/24/23 12:48:39.034
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:39.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:39.084
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/24/23 12:48:39.107
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/24/23 12:48:57.324
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/24/23 12:48:57.33
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/24/23 12:48:57.341
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/24/23 12:48:57.341
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/24/23 12:48:57.404
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/24/23 12:49:00.458
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/24/23 12:49:02.477
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/24/23 12:49:02.487
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/24/23 12:49:02.488
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/24/23 12:49:02.517
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/24/23 12:49:03.543
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/24/23 12:49:06.581
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/24/23 12:49:06.592
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/24/23 12:49:06.592
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8223" for this suite. 08/24/23 12:49:06.642
------------------------------
â€¢ [SLOW TEST] [27.624 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:48:39.029
    Aug 24 12:48:39.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-runtime 08/24/23 12:48:39.034
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:48:39.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:48:39.084
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/24/23 12:48:39.107
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/24/23 12:48:57.324
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/24/23 12:48:57.33
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/24/23 12:48:57.341
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/24/23 12:48:57.341
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/24/23 12:48:57.404
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/24/23 12:49:00.458
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/24/23 12:49:02.477
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/24/23 12:49:02.487
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/24/23 12:49:02.488
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/24/23 12:49:02.517
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/24/23 12:49:03.543
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/24/23 12:49:06.581
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/24/23 12:49:06.592
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/24/23 12:49:06.592
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8223" for this suite. 08/24/23 12:49:06.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:06.666
Aug 24 12:49:06.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:49:06.669
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:06.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:06.712
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/24/23 12:49:06.722
Aug 24 12:49:06.724: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4046 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/24/23 12:49:06.825
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:06.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4046" for this suite. 08/24/23 12:49:06.857
------------------------------
â€¢ [0.202 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:06.666
    Aug 24 12:49:06.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:49:06.669
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:06.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:06.712
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/24/23 12:49:06.722
    Aug 24 12:49:06.724: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4046 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/24/23 12:49:06.825
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:06.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4046" for this suite. 08/24/23 12:49:06.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:06.87
Aug 24 12:49:06.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 12:49:06.872
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:06.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:06.908
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-4ebc090b-bbf0-4080-b312-ae83bc9120d3 08/24/23 12:49:06.913
STEP: Creating a pod to test consume configMaps 08/24/23 12:49:06.922
Aug 24 12:49:06.935: INFO: Waiting up to 5m0s for pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f" in namespace "configmap-9502" to be "Succeeded or Failed"
Aug 24 12:49:06.946: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.214007ms
Aug 24 12:49:08.954: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018469689s
Aug 24 12:49:10.952: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017349187s
STEP: Saw pod success 08/24/23 12:49:10.953
Aug 24 12:49:10.953: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f" satisfied condition "Succeeded or Failed"
Aug 24 12:49:10.960: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:49:10.974
Aug 24 12:49:11.001: INFO: Waiting for pod pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f to disappear
Aug 24 12:49:11.006: INFO: Pod pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9502" for this suite. 08/24/23 12:49:11.018
------------------------------
â€¢ [4.190 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:06.87
    Aug 24 12:49:06.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 12:49:06.872
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:06.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:06.908
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-4ebc090b-bbf0-4080-b312-ae83bc9120d3 08/24/23 12:49:06.913
    STEP: Creating a pod to test consume configMaps 08/24/23 12:49:06.922
    Aug 24 12:49:06.935: INFO: Waiting up to 5m0s for pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f" in namespace "configmap-9502" to be "Succeeded or Failed"
    Aug 24 12:49:06.946: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.214007ms
    Aug 24 12:49:08.954: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018469689s
    Aug 24 12:49:10.952: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017349187s
    STEP: Saw pod success 08/24/23 12:49:10.953
    Aug 24 12:49:10.953: INFO: Pod "pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f" satisfied condition "Succeeded or Failed"
    Aug 24 12:49:10.960: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:49:10.974
    Aug 24 12:49:11.001: INFO: Waiting for pod pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f to disappear
    Aug 24 12:49:11.006: INFO: Pod pod-configmaps-d88424e8-d46f-4c36-9da2-c3f4424e485f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9502" for this suite. 08/24/23 12:49:11.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:11.061
Aug 24 12:49:11.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename watch 08/24/23 12:49:11.063
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:11.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:11.095
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/24/23 12:49:11.101
STEP: creating a new configmap 08/24/23 12:49:11.104
STEP: modifying the configmap once 08/24/23 12:49:11.113
STEP: changing the label value of the configmap 08/24/23 12:49:11.128
STEP: Expecting to observe a delete notification for the watched object 08/24/23 12:49:11.142
Aug 24 12:49:11.143: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26793 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:49:11.144: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26794 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:49:11.145: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26795 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/24/23 12:49:11.145
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/24/23 12:49:11.165
STEP: changing the label value of the configmap back 08/24/23 12:49:21.166
STEP: modifying the configmap a third time 08/24/23 12:49:21.184
STEP: deleting the configmap 08/24/23 12:49:21.201
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/24/23 12:49:21.213
Aug 24 12:49:21.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26850 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:49:21.214: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26851 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:49:21.215: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26852 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1304" for this suite. 08/24/23 12:49:21.232
------------------------------
â€¢ [SLOW TEST] [10.188 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:11.061
    Aug 24 12:49:11.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename watch 08/24/23 12:49:11.063
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:11.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:11.095
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/24/23 12:49:11.101
    STEP: creating a new configmap 08/24/23 12:49:11.104
    STEP: modifying the configmap once 08/24/23 12:49:11.113
    STEP: changing the label value of the configmap 08/24/23 12:49:11.128
    STEP: Expecting to observe a delete notification for the watched object 08/24/23 12:49:11.142
    Aug 24 12:49:11.143: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26793 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:49:11.144: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26794 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:49:11.145: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26795 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/24/23 12:49:11.145
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/24/23 12:49:11.165
    STEP: changing the label value of the configmap back 08/24/23 12:49:21.166
    STEP: modifying the configmap a third time 08/24/23 12:49:21.184
    STEP: deleting the configmap 08/24/23 12:49:21.201
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/24/23 12:49:21.213
    Aug 24 12:49:21.213: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26850 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:49:21.214: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26851 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:49:21.215: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  13447dbf-1e5f-4024-b34e-def135439317 26852 0 2023-08-24 12:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:49:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1304" for this suite. 08/24/23 12:49:21.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:21.269
Aug 24 12:49:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:49:21.271
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:21.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:21.312
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:49:21.321
Aug 24 12:49:21.340: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1" in namespace "downward-api-5862" to be "Succeeded or Failed"
Aug 24 12:49:21.350: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.592018ms
Aug 24 12:49:23.359: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018358522s
Aug 24 12:49:25.361: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020640331s
STEP: Saw pod success 08/24/23 12:49:25.362
Aug 24 12:49:25.362: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1" satisfied condition "Succeeded or Failed"
Aug 24 12:49:25.371: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1 container client-container: <nil>
STEP: delete the pod 08/24/23 12:49:25.385
Aug 24 12:49:25.409: INFO: Waiting for pod downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1 to disappear
Aug 24 12:49:25.417: INFO: Pod downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:25.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5862" for this suite. 08/24/23 12:49:25.428
------------------------------
â€¢ [4.170 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:21.269
    Aug 24 12:49:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:49:21.271
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:21.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:21.312
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:49:21.321
    Aug 24 12:49:21.340: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1" in namespace "downward-api-5862" to be "Succeeded or Failed"
    Aug 24 12:49:21.350: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.592018ms
    Aug 24 12:49:23.359: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018358522s
    Aug 24 12:49:25.361: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020640331s
    STEP: Saw pod success 08/24/23 12:49:25.362
    Aug 24 12:49:25.362: INFO: Pod "downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1" satisfied condition "Succeeded or Failed"
    Aug 24 12:49:25.371: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:49:25.385
    Aug 24 12:49:25.409: INFO: Waiting for pod downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1 to disappear
    Aug 24 12:49:25.417: INFO: Pod downwardapi-volume-1b073102-14d0-488f-97da-2d0fdd8620e1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:25.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5862" for this suite. 08/24/23 12:49:25.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:25.44
Aug 24 12:49:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename certificates 08/24/23 12:49:25.444
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:25.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:25.473
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/24/23 12:49:27.724
STEP: getting /apis/certificates.k8s.io 08/24/23 12:49:27.729
STEP: getting /apis/certificates.k8s.io/v1 08/24/23 12:49:27.731
STEP: creating 08/24/23 12:49:27.733
STEP: getting 08/24/23 12:49:27.762
STEP: listing 08/24/23 12:49:27.768
STEP: watching 08/24/23 12:49:27.774
Aug 24 12:49:27.775: INFO: starting watch
STEP: patching 08/24/23 12:49:27.776
STEP: updating 08/24/23 12:49:27.789
Aug 24 12:49:27.798: INFO: waiting for watch events with expected annotations
Aug 24 12:49:27.798: INFO: saw patched and updated annotations
STEP: getting /approval 08/24/23 12:49:27.798
STEP: patching /approval 08/24/23 12:49:27.806
STEP: updating /approval 08/24/23 12:49:27.826
STEP: getting /status 08/24/23 12:49:27.841
STEP: patching /status 08/24/23 12:49:27.846
STEP: updating /status 08/24/23 12:49:27.861
STEP: deleting 08/24/23 12:49:27.875
STEP: deleting a collection 08/24/23 12:49:27.898
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-2209" for this suite. 08/24/23 12:49:27.953
------------------------------
â€¢ [2.532 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:25.44
    Aug 24 12:49:25.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename certificates 08/24/23 12:49:25.444
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:25.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:25.473
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/24/23 12:49:27.724
    STEP: getting /apis/certificates.k8s.io 08/24/23 12:49:27.729
    STEP: getting /apis/certificates.k8s.io/v1 08/24/23 12:49:27.731
    STEP: creating 08/24/23 12:49:27.733
    STEP: getting 08/24/23 12:49:27.762
    STEP: listing 08/24/23 12:49:27.768
    STEP: watching 08/24/23 12:49:27.774
    Aug 24 12:49:27.775: INFO: starting watch
    STEP: patching 08/24/23 12:49:27.776
    STEP: updating 08/24/23 12:49:27.789
    Aug 24 12:49:27.798: INFO: waiting for watch events with expected annotations
    Aug 24 12:49:27.798: INFO: saw patched and updated annotations
    STEP: getting /approval 08/24/23 12:49:27.798
    STEP: patching /approval 08/24/23 12:49:27.806
    STEP: updating /approval 08/24/23 12:49:27.826
    STEP: getting /status 08/24/23 12:49:27.841
    STEP: patching /status 08/24/23 12:49:27.846
    STEP: updating /status 08/24/23 12:49:27.861
    STEP: deleting 08/24/23 12:49:27.875
    STEP: deleting a collection 08/24/23 12:49:27.898
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-2209" for this suite. 08/24/23 12:49:27.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:27.976
Aug 24 12:49:27.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:49:27.98
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:28.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:28.008
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:49:28.015
Aug 24 12:49:28.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5" in namespace "projected-7286" to be "Succeeded or Failed"
Aug 24 12:49:28.088: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5": Phase="Pending", Reason="", readiness=false. Elapsed: 53.011687ms
Aug 24 12:49:30.101: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066656554s
Aug 24 12:49:32.096: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061652944s
STEP: Saw pod success 08/24/23 12:49:32.096
Aug 24 12:49:32.097: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5" satisfied condition "Succeeded or Failed"
Aug 24 12:49:32.103: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5 container client-container: <nil>
STEP: delete the pod 08/24/23 12:49:32.115
Aug 24 12:49:32.142: INFO: Waiting for pod downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5 to disappear
Aug 24 12:49:32.148: INFO: Pod downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:32.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7286" for this suite. 08/24/23 12:49:32.159
------------------------------
â€¢ [4.198 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:27.976
    Aug 24 12:49:27.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:49:27.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:28.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:28.008
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:49:28.015
    Aug 24 12:49:28.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5" in namespace "projected-7286" to be "Succeeded or Failed"
    Aug 24 12:49:28.088: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5": Phase="Pending", Reason="", readiness=false. Elapsed: 53.011687ms
    Aug 24 12:49:30.101: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066656554s
    Aug 24 12:49:32.096: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061652944s
    STEP: Saw pod success 08/24/23 12:49:32.096
    Aug 24 12:49:32.097: INFO: Pod "downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5" satisfied condition "Succeeded or Failed"
    Aug 24 12:49:32.103: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:49:32.115
    Aug 24 12:49:32.142: INFO: Waiting for pod downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5 to disappear
    Aug 24 12:49:32.148: INFO: Pod downwardapi-volume-264d7747-8f9a-4bf7-ab23-0d10542656c5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:32.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7286" for this suite. 08/24/23 12:49:32.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:32.178
Aug 24 12:49:32.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:49:32.18
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:32.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:32.215
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:49:32.22
Aug 24 12:49:32.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191" in namespace "projected-5010" to be "Succeeded or Failed"
Aug 24 12:49:32.246: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191": Phase="Pending", Reason="", readiness=false. Elapsed: 8.803028ms
Aug 24 12:49:34.261: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023307253s
Aug 24 12:49:36.253: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015999699s
STEP: Saw pod success 08/24/23 12:49:36.253
Aug 24 12:49:36.254: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191" satisfied condition "Succeeded or Failed"
Aug 24 12:49:36.260: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191 container client-container: <nil>
STEP: delete the pod 08/24/23 12:49:36.269
Aug 24 12:49:36.296: INFO: Waiting for pod downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191 to disappear
Aug 24 12:49:36.305: INFO: Pod downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:36.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5010" for this suite. 08/24/23 12:49:36.315
------------------------------
â€¢ [4.153 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:32.178
    Aug 24 12:49:32.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:49:32.18
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:32.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:32.215
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:49:32.22
    Aug 24 12:49:32.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191" in namespace "projected-5010" to be "Succeeded or Failed"
    Aug 24 12:49:32.246: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191": Phase="Pending", Reason="", readiness=false. Elapsed: 8.803028ms
    Aug 24 12:49:34.261: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023307253s
    Aug 24 12:49:36.253: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015999699s
    STEP: Saw pod success 08/24/23 12:49:36.253
    Aug 24 12:49:36.254: INFO: Pod "downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191" satisfied condition "Succeeded or Failed"
    Aug 24 12:49:36.260: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:49:36.269
    Aug 24 12:49:36.296: INFO: Waiting for pod downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191 to disappear
    Aug 24 12:49:36.305: INFO: Pod downwardapi-volume-c7d36457-02f2-48c9-af0a-51dbdfd01191 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:36.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5010" for this suite. 08/24/23 12:49:36.315
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:36.334
Aug 24 12:49:36.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svc-latency 08/24/23 12:49:36.337
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:36.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:36.386
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 24 12:49:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2999 08/24/23 12:49:36.393
I0824 12:49:36.405795      14 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2999, replica count: 1
I0824 12:49:37.457760      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 12:49:38.458412      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:49:38.582: INFO: Created: latency-svc-9cb5k
Aug 24 12:49:38.604: INFO: Got endpoints: latency-svc-9cb5k [43.942554ms]
Aug 24 12:49:38.640: INFO: Created: latency-svc-qqqhr
Aug 24 12:49:38.660: INFO: Got endpoints: latency-svc-qqqhr [56.063584ms]
Aug 24 12:49:38.675: INFO: Created: latency-svc-2llmf
Aug 24 12:49:38.690: INFO: Got endpoints: latency-svc-2llmf [84.157523ms]
Aug 24 12:49:38.698: INFO: Created: latency-svc-6lcfp
Aug 24 12:49:38.714: INFO: Created: latency-svc-zgpb7
Aug 24 12:49:38.721: INFO: Got endpoints: latency-svc-6lcfp [114.472563ms]
Aug 24 12:49:38.733: INFO: Got endpoints: latency-svc-zgpb7 [125.146115ms]
Aug 24 12:49:38.736: INFO: Created: latency-svc-t42np
Aug 24 12:49:38.751: INFO: Created: latency-svc-6blnt
Aug 24 12:49:38.758: INFO: Got endpoints: latency-svc-t42np [151.579914ms]
Aug 24 12:49:38.765: INFO: Got endpoints: latency-svc-6blnt [156.814153ms]
Aug 24 12:49:38.777: INFO: Created: latency-svc-n2k2d
Aug 24 12:49:38.785: INFO: Got endpoints: latency-svc-n2k2d [170.228686ms]
Aug 24 12:49:38.808: INFO: Created: latency-svc-n5m44
Aug 24 12:49:38.809: INFO: Created: latency-svc-bs9f9
Aug 24 12:49:38.824: INFO: Got endpoints: latency-svc-bs9f9 [212.587727ms]
Aug 24 12:49:38.825: INFO: Created: latency-svc-w6gvq
Aug 24 12:49:38.830: INFO: Got endpoints: latency-svc-n5m44 [219.626729ms]
Aug 24 12:49:38.848: INFO: Got endpoints: latency-svc-w6gvq [233.126352ms]
Aug 24 12:49:38.857: INFO: Created: latency-svc-2vssk
Aug 24 12:49:38.859: INFO: Got endpoints: latency-svc-2vssk [243.49353ms]
Aug 24 12:49:38.863: INFO: Created: latency-svc-6v2lz
Aug 24 12:49:38.883: INFO: Got endpoints: latency-svc-6v2lz [267.892004ms]
Aug 24 12:49:38.892: INFO: Created: latency-svc-ks8mh
Aug 24 12:49:38.906: INFO: Got endpoints: latency-svc-ks8mh [290.706541ms]
Aug 24 12:49:39.115: INFO: Created: latency-svc-cgwfk
Aug 24 12:49:39.115: INFO: Created: latency-svc-ttl6v
Aug 24 12:49:39.116: INFO: Created: latency-svc-2pq94
Aug 24 12:49:39.131: INFO: Created: latency-svc-qnbdz
Aug 24 12:49:39.131: INFO: Created: latency-svc-4j5jk
Aug 24 12:49:39.131: INFO: Created: latency-svc-4fmm7
Aug 24 12:49:39.132: INFO: Created: latency-svc-9sgmk
Aug 24 12:49:39.132: INFO: Created: latency-svc-p76hw
Aug 24 12:49:39.135: INFO: Created: latency-svc-bpmlm
Aug 24 12:49:39.137: INFO: Created: latency-svc-xwxfq
Aug 24 12:49:39.137: INFO: Created: latency-svc-n7bpk
Aug 24 12:49:39.138: INFO: Created: latency-svc-zl9tb
Aug 24 12:49:39.140: INFO: Created: latency-svc-jq2hc
Aug 24 12:49:39.140: INFO: Created: latency-svc-tc2df
Aug 24 12:49:39.142: INFO: Created: latency-svc-8ntmw
Aug 24 12:49:39.172: INFO: Got endpoints: latency-svc-2pq94 [555.98563ms]
Aug 24 12:49:39.173: INFO: Got endpoints: latency-svc-4j5jk [267.366666ms]
Aug 24 12:49:39.178: INFO: Got endpoints: latency-svc-ttl6v [456.158586ms]
Aug 24 12:49:39.186: INFO: Got endpoints: latency-svc-cgwfk [420.848329ms]
Aug 24 12:49:39.215: INFO: Got endpoints: latency-svc-p76hw [524.373733ms]
Aug 24 12:49:39.226: INFO: Got endpoints: latency-svc-4fmm7 [395.646699ms]
Aug 24 12:49:39.227: INFO: Got endpoints: latency-svc-9sgmk [403.464416ms]
Aug 24 12:49:39.228: INFO: Got endpoints: latency-svc-qnbdz [369.231289ms]
Aug 24 12:49:39.232: INFO: Got endpoints: latency-svc-xwxfq [473.879001ms]
Aug 24 12:49:39.276: INFO: Created: latency-svc-b7x6g
Aug 24 12:49:39.293: INFO: Got endpoints: latency-svc-n7bpk [444.71012ms]
Aug 24 12:49:39.295: INFO: Got endpoints: latency-svc-zl9tb [634.312931ms]
Aug 24 12:49:39.296: INFO: Got endpoints: latency-svc-8ntmw [562.625256ms]
Aug 24 12:49:39.299: INFO: Got endpoints: latency-svc-bpmlm [513.680755ms]
Aug 24 12:49:39.299: INFO: Got endpoints: latency-svc-jq2hc [683.271252ms]
Aug 24 12:49:39.322: INFO: Got endpoints: latency-svc-tc2df [439.362204ms]
Aug 24 12:49:39.330: INFO: Created: latency-svc-jj5s8
Aug 24 12:49:39.341: INFO: Got endpoints: latency-svc-b7x6g [168.904394ms]
Aug 24 12:49:39.342: INFO: Got endpoints: latency-svc-jj5s8 [126.951742ms]
Aug 24 12:49:39.363: INFO: Created: latency-svc-lgnq4
Aug 24 12:49:39.369: INFO: Got endpoints: latency-svc-lgnq4 [143.417106ms]
Aug 24 12:49:39.380: INFO: Created: latency-svc-2nvb4
Aug 24 12:49:39.404: INFO: Got endpoints: latency-svc-2nvb4 [226.139178ms]
Aug 24 12:49:39.424: INFO: Created: latency-svc-cqdzp
Aug 24 12:49:39.440: INFO: Got endpoints: latency-svc-cqdzp [254.217879ms]
Aug 24 12:49:39.496: INFO: Created: latency-svc-4n8sm
Aug 24 12:49:39.498: INFO: Got endpoints: latency-svc-4n8sm [324.455077ms]
Aug 24 12:49:39.517: INFO: Created: latency-svc-lr4bp
Aug 24 12:49:39.525: INFO: Got endpoints: latency-svc-lr4bp [297.939326ms]
Aug 24 12:49:39.547: INFO: Created: latency-svc-q7cw8
Aug 24 12:49:39.560: INFO: Got endpoints: latency-svc-q7cw8 [327.216494ms]
Aug 24 12:49:39.574: INFO: Created: latency-svc-jdjkr
Aug 24 12:49:39.587: INFO: Got endpoints: latency-svc-jdjkr [358.332465ms]
Aug 24 12:49:39.615: INFO: Created: latency-svc-6k7sw
Aug 24 12:49:39.630: INFO: Got endpoints: latency-svc-6k7sw [334.957207ms]
Aug 24 12:49:39.638: INFO: Created: latency-svc-6ghm9
Aug 24 12:49:39.664: INFO: Got endpoints: latency-svc-6ghm9 [364.161732ms]
Aug 24 12:49:39.682: INFO: Created: latency-svc-4pf74
Aug 24 12:49:39.709: INFO: Created: latency-svc-s5zj4
Aug 24 12:49:39.713: INFO: Got endpoints: latency-svc-4pf74 [419.678946ms]
Aug 24 12:49:39.722: INFO: Created: latency-svc-qw78k
Aug 24 12:49:39.734: INFO: Got endpoints: latency-svc-s5zj4 [438.756726ms]
Aug 24 12:49:39.755: INFO: Created: latency-svc-nfh8g
Aug 24 12:49:39.759: INFO: Got endpoints: latency-svc-qw78k [459.622892ms]
Aug 24 12:49:39.766: INFO: Got endpoints: latency-svc-nfh8g [443.892767ms]
Aug 24 12:49:39.786: INFO: Created: latency-svc-6qghf
Aug 24 12:49:39.797: INFO: Created: latency-svc-w7knl
Aug 24 12:49:39.803: INFO: Got endpoints: latency-svc-6qghf [461.754553ms]
Aug 24 12:49:39.818: INFO: Got endpoints: latency-svc-w7knl [476.051439ms]
Aug 24 12:49:39.840: INFO: Created: latency-svc-f9skq
Aug 24 12:49:39.846: INFO: Got endpoints: latency-svc-f9skq [476.162731ms]
Aug 24 12:49:39.994: INFO: Created: latency-svc-cm8pj
Aug 24 12:49:39.995: INFO: Created: latency-svc-kdtct
Aug 24 12:49:40.001: INFO: Created: latency-svc-8zw9x
Aug 24 12:49:40.002: INFO: Created: latency-svc-2bq8t
Aug 24 12:49:40.032: INFO: Created: latency-svc-jh87n
Aug 24 12:49:40.033: INFO: Created: latency-svc-cxjkm
Aug 24 12:49:40.034: INFO: Created: latency-svc-52g2z
Aug 24 12:49:40.034: INFO: Created: latency-svc-zg5fc
Aug 24 12:49:40.034: INFO: Created: latency-svc-n2r5t
Aug 24 12:49:40.035: INFO: Created: latency-svc-b46mh
Aug 24 12:49:40.034: INFO: Created: latency-svc-k7hjk
Aug 24 12:49:40.035: INFO: Created: latency-svc-bdlhc
Aug 24 12:49:40.036: INFO: Created: latency-svc-sbk4j
Aug 24 12:49:40.036: INFO: Created: latency-svc-nbrhv
Aug 24 12:49:40.037: INFO: Created: latency-svc-5s86b
Aug 24 12:49:40.086: INFO: Got endpoints: latency-svc-cm8pj [422.2223ms]
Aug 24 12:49:40.112: INFO: Got endpoints: latency-svc-k7hjk [266.254622ms]
Aug 24 12:49:40.113: INFO: Got endpoints: latency-svc-2bq8t [482.3205ms]
Aug 24 12:49:40.114: INFO: Got endpoints: latency-svc-kdtct [525.576932ms]
Aug 24 12:49:40.114: INFO: Got endpoints: latency-svc-8zw9x [673.462216ms]
Aug 24 12:49:40.135: INFO: Got endpoints: latency-svc-cxjkm [730.239621ms]
Aug 24 12:49:40.163: INFO: Got endpoints: latency-svc-jh87n [449.721282ms]
Aug 24 12:49:40.171: INFO: Created: latency-svc-vkkhs
Aug 24 12:49:40.203: INFO: Got endpoints: latency-svc-sbk4j [399.618346ms]
Aug 24 12:49:40.205: INFO: Got endpoints: latency-svc-5s86b [470.635399ms]
Aug 24 12:49:40.207: INFO: Got endpoints: latency-svc-n2r5t [709.105444ms]
Aug 24 12:49:40.209: INFO: Got endpoints: latency-svc-52g2z [442.562917ms]
Aug 24 12:49:40.215: INFO: Created: latency-svc-g56hc
Aug 24 12:49:40.221: INFO: Got endpoints: latency-svc-zg5fc [695.078929ms]
Aug 24 12:49:40.221: INFO: Got endpoints: latency-svc-b46mh [462.625648ms]
Aug 24 12:49:40.237: INFO: Created: latency-svc-qzqhl
Aug 24 12:49:40.240: INFO: Got endpoints: latency-svc-nbrhv [421.873402ms]
Aug 24 12:49:40.240: INFO: Got endpoints: latency-svc-bdlhc [680.53894ms]
Aug 24 12:49:40.249: INFO: Created: latency-svc-xdsdp
Aug 24 12:49:40.251: INFO: Got endpoints: latency-svc-vkkhs [165.530931ms]
Aug 24 12:49:40.261: INFO: Created: latency-svc-6p87w
Aug 24 12:49:40.271: INFO: Created: latency-svc-7d86s
Aug 24 12:49:40.296: INFO: Got endpoints: latency-svc-g56hc [183.093192ms]
Aug 24 12:49:40.346: INFO: Got endpoints: latency-svc-qzqhl [232.327456ms]
Aug 24 12:49:40.396: INFO: Got endpoints: latency-svc-xdsdp [283.06359ms]
Aug 24 12:49:40.430: INFO: Created: latency-svc-b9zg7
Aug 24 12:49:40.430: INFO: Created: latency-svc-mn5zq
Aug 24 12:49:40.434: INFO: Created: latency-svc-cqd2t
Aug 24 12:49:40.455: INFO: Got endpoints: latency-svc-6p87w [341.10324ms]
Aug 24 12:49:40.466: INFO: Created: latency-svc-znb8r
Aug 24 12:49:40.479: INFO: Created: latency-svc-ksvf5
Aug 24 12:49:40.479: INFO: Created: latency-svc-xkwcp
Aug 24 12:49:40.480: INFO: Created: latency-svc-zlnjs
Aug 24 12:49:40.480: INFO: Created: latency-svc-b8skg
Aug 24 12:49:40.481: INFO: Created: latency-svc-bfl6x
Aug 24 12:49:40.481: INFO: Created: latency-svc-9kc8c
Aug 24 12:49:40.482: INFO: Created: latency-svc-rzzr5
Aug 24 12:49:40.491: INFO: Created: latency-svc-kvpxf
Aug 24 12:49:40.492: INFO: Created: latency-svc-xvbp6
Aug 24 12:49:40.504: INFO: Got endpoints: latency-svc-7d86s [368.862262ms]
Aug 24 12:49:40.516: INFO: Created: latency-svc-56g2n
Aug 24 12:49:40.532: INFO: Created: latency-svc-jmp46
Aug 24 12:49:40.548: INFO: Got endpoints: latency-svc-mn5zq [296.066397ms]
Aug 24 12:49:40.568: INFO: Created: latency-svc-rbb2n
Aug 24 12:49:40.595: INFO: Got endpoints: latency-svc-b9zg7 [431.110147ms]
Aug 24 12:49:40.613: INFO: Created: latency-svc-d2dsr
Aug 24 12:49:40.653: INFO: Got endpoints: latency-svc-cqd2t [450.566273ms]
Aug 24 12:49:40.677: INFO: Created: latency-svc-kp989
Aug 24 12:49:40.703: INFO: Got endpoints: latency-svc-znb8r [406.392154ms]
Aug 24 12:49:40.733: INFO: Created: latency-svc-czrlm
Aug 24 12:49:40.752: INFO: Got endpoints: latency-svc-xvbp6 [530.214258ms]
Aug 24 12:49:40.779: INFO: Created: latency-svc-nttk4
Aug 24 12:49:40.799: INFO: Got endpoints: latency-svc-kvpxf [589.786908ms]
Aug 24 12:49:40.821: INFO: Created: latency-svc-8rfmr
Aug 24 12:49:40.847: INFO: Got endpoints: latency-svc-zlnjs [451.511288ms]
Aug 24 12:49:40.875: INFO: Created: latency-svc-xsbls
Aug 24 12:49:40.896: INFO: Got endpoints: latency-svc-xkwcp [675.331847ms]
Aug 24 12:49:40.914: INFO: Created: latency-svc-54gxm
Aug 24 12:49:40.950: INFO: Got endpoints: latency-svc-rzzr5 [709.640934ms]
Aug 24 12:49:40.972: INFO: Created: latency-svc-4hdfk
Aug 24 12:49:40.994: INFO: Got endpoints: latency-svc-9kc8c [754.236635ms]
Aug 24 12:49:41.013: INFO: Created: latency-svc-sbt88
Aug 24 12:49:41.052: INFO: Got endpoints: latency-svc-ksvf5 [844.367675ms]
Aug 24 12:49:41.082: INFO: Created: latency-svc-f42s8
Aug 24 12:49:41.096: INFO: Got endpoints: latency-svc-b8skg [749.689492ms]
Aug 24 12:49:41.130: INFO: Created: latency-svc-fdpsf
Aug 24 12:49:41.145: INFO: Got endpoints: latency-svc-bfl6x [939.574799ms]
Aug 24 12:49:41.180: INFO: Created: latency-svc-khbt8
Aug 24 12:49:41.195: INFO: Got endpoints: latency-svc-56g2n [739.944485ms]
Aug 24 12:49:41.225: INFO: Created: latency-svc-msjpv
Aug 24 12:49:41.247: INFO: Got endpoints: latency-svc-jmp46 [742.85621ms]
Aug 24 12:49:41.263: INFO: Created: latency-svc-rvlcq
Aug 24 12:49:41.294: INFO: Got endpoints: latency-svc-rbb2n [745.832041ms]
Aug 24 12:49:41.320: INFO: Created: latency-svc-zc8vx
Aug 24 12:49:41.353: INFO: Got endpoints: latency-svc-d2dsr [758.19094ms]
Aug 24 12:49:41.385: INFO: Created: latency-svc-n4s86
Aug 24 12:49:41.392: INFO: Got endpoints: latency-svc-kp989 [738.286619ms]
Aug 24 12:49:41.429: INFO: Created: latency-svc-dktvt
Aug 24 12:49:41.446: INFO: Got endpoints: latency-svc-czrlm [742.966587ms]
Aug 24 12:49:41.473: INFO: Created: latency-svc-9dlw2
Aug 24 12:49:41.497: INFO: Got endpoints: latency-svc-nttk4 [744.152701ms]
Aug 24 12:49:41.522: INFO: Created: latency-svc-gp4pk
Aug 24 12:49:41.542: INFO: Got endpoints: latency-svc-8rfmr [743.040996ms]
Aug 24 12:49:41.560: INFO: Created: latency-svc-wwljq
Aug 24 12:49:41.595: INFO: Got endpoints: latency-svc-xsbls [747.613239ms]
Aug 24 12:49:41.621: INFO: Created: latency-svc-56wvd
Aug 24 12:49:41.653: INFO: Got endpoints: latency-svc-54gxm [756.857646ms]
Aug 24 12:49:41.674: INFO: Created: latency-svc-6hqlh
Aug 24 12:49:41.702: INFO: Got endpoints: latency-svc-4hdfk [751.972848ms]
Aug 24 12:49:41.724: INFO: Created: latency-svc-4pmxr
Aug 24 12:49:41.746: INFO: Got endpoints: latency-svc-sbt88 [752.031469ms]
Aug 24 12:49:41.783: INFO: Created: latency-svc-9cgvj
Aug 24 12:49:41.797: INFO: Got endpoints: latency-svc-f42s8 [745.202476ms]
Aug 24 12:49:41.814: INFO: Created: latency-svc-hprcf
Aug 24 12:49:41.848: INFO: Got endpoints: latency-svc-fdpsf [751.342216ms]
Aug 24 12:49:41.874: INFO: Created: latency-svc-h59vd
Aug 24 12:49:41.900: INFO: Got endpoints: latency-svc-khbt8 [754.894792ms]
Aug 24 12:49:41.928: INFO: Created: latency-svc-fxzp4
Aug 24 12:49:41.949: INFO: Got endpoints: latency-svc-msjpv [753.692722ms]
Aug 24 12:49:41.972: INFO: Created: latency-svc-l78qs
Aug 24 12:49:41.992: INFO: Got endpoints: latency-svc-rvlcq [745.388433ms]
Aug 24 12:49:42.015: INFO: Created: latency-svc-jdf4t
Aug 24 12:49:42.057: INFO: Got endpoints: latency-svc-zc8vx [762.980733ms]
Aug 24 12:49:42.109: INFO: Got endpoints: latency-svc-n4s86 [755.333375ms]
Aug 24 12:49:42.116: INFO: Created: latency-svc-m99q8
Aug 24 12:49:42.132: INFO: Created: latency-svc-v2gk8
Aug 24 12:49:42.148: INFO: Got endpoints: latency-svc-dktvt [756.328611ms]
Aug 24 12:49:42.209: INFO: Got endpoints: latency-svc-9dlw2 [762.193337ms]
Aug 24 12:49:42.218: INFO: Created: latency-svc-txcpg
Aug 24 12:49:42.245: INFO: Created: latency-svc-xk89s
Aug 24 12:49:42.257: INFO: Got endpoints: latency-svc-gp4pk [759.561923ms]
Aug 24 12:49:42.276: INFO: Created: latency-svc-pxlkb
Aug 24 12:49:42.299: INFO: Got endpoints: latency-svc-wwljq [756.892822ms]
Aug 24 12:49:42.318: INFO: Created: latency-svc-xlql7
Aug 24 12:49:42.350: INFO: Got endpoints: latency-svc-56wvd [755.004249ms]
Aug 24 12:49:42.376: INFO: Created: latency-svc-7br9g
Aug 24 12:49:42.400: INFO: Got endpoints: latency-svc-6hqlh [746.248804ms]
Aug 24 12:49:42.417: INFO: Created: latency-svc-2h4m5
Aug 24 12:49:42.442: INFO: Got endpoints: latency-svc-4pmxr [740.170969ms]
Aug 24 12:49:42.461: INFO: Created: latency-svc-2frd7
Aug 24 12:49:42.499: INFO: Got endpoints: latency-svc-9cgvj [752.447734ms]
Aug 24 12:49:42.530: INFO: Created: latency-svc-xz6sh
Aug 24 12:49:42.550: INFO: Got endpoints: latency-svc-hprcf [753.024943ms]
Aug 24 12:49:42.569: INFO: Created: latency-svc-5q4vg
Aug 24 12:49:42.599: INFO: Got endpoints: latency-svc-h59vd [750.879335ms]
Aug 24 12:49:42.622: INFO: Created: latency-svc-hr77d
Aug 24 12:49:42.646: INFO: Got endpoints: latency-svc-fxzp4 [745.462525ms]
Aug 24 12:49:42.707: INFO: Got endpoints: latency-svc-l78qs [756.057255ms]
Aug 24 12:49:42.718: INFO: Created: latency-svc-n7zd5
Aug 24 12:49:42.746: INFO: Created: latency-svc-z2gfh
Aug 24 12:49:42.751: INFO: Got endpoints: latency-svc-jdf4t [758.676096ms]
Aug 24 12:49:42.774: INFO: Created: latency-svc-9xmn9
Aug 24 12:49:42.807: INFO: Got endpoints: latency-svc-m99q8 [749.83088ms]
Aug 24 12:49:42.852: INFO: Created: latency-svc-sgtkq
Aug 24 12:49:42.865: INFO: Got endpoints: latency-svc-v2gk8 [756.461061ms]
Aug 24 12:49:42.889: INFO: Created: latency-svc-6khsb
Aug 24 12:49:42.898: INFO: Got endpoints: latency-svc-txcpg [749.576614ms]
Aug 24 12:49:42.923: INFO: Created: latency-svc-vcwz5
Aug 24 12:49:42.950: INFO: Got endpoints: latency-svc-xk89s [741.281238ms]
Aug 24 12:49:42.973: INFO: Created: latency-svc-gbrqx
Aug 24 12:49:42.994: INFO: Got endpoints: latency-svc-pxlkb [736.268587ms]
Aug 24 12:49:43.020: INFO: Created: latency-svc-jc6k7
Aug 24 12:49:43.055: INFO: Got endpoints: latency-svc-xlql7 [754.968091ms]
Aug 24 12:49:43.078: INFO: Created: latency-svc-nql7r
Aug 24 12:49:43.103: INFO: Got endpoints: latency-svc-7br9g [752.897191ms]
Aug 24 12:49:43.126: INFO: Created: latency-svc-kcmpw
Aug 24 12:49:43.152: INFO: Got endpoints: latency-svc-2h4m5 [752.467021ms]
Aug 24 12:49:43.170: INFO: Created: latency-svc-xzbw4
Aug 24 12:49:43.197: INFO: Got endpoints: latency-svc-2frd7 [754.149943ms]
Aug 24 12:49:43.220: INFO: Created: latency-svc-4r46w
Aug 24 12:49:43.247: INFO: Got endpoints: latency-svc-xz6sh [747.252052ms]
Aug 24 12:49:43.270: INFO: Created: latency-svc-hv7xk
Aug 24 12:49:43.297: INFO: Got endpoints: latency-svc-5q4vg [746.171399ms]
Aug 24 12:49:43.316: INFO: Created: latency-svc-q7q5c
Aug 24 12:49:43.351: INFO: Got endpoints: latency-svc-hr77d [750.983062ms]
Aug 24 12:49:43.388: INFO: Created: latency-svc-tqshm
Aug 24 12:49:43.433: INFO: Got endpoints: latency-svc-n7zd5 [786.630555ms]
Aug 24 12:49:43.447: INFO: Got endpoints: latency-svc-z2gfh [739.644572ms]
Aug 24 12:49:43.474: INFO: Created: latency-svc-kg7nq
Aug 24 12:49:43.486: INFO: Created: latency-svc-7qtxw
Aug 24 12:49:43.498: INFO: Got endpoints: latency-svc-9xmn9 [746.96027ms]
Aug 24 12:49:43.526: INFO: Created: latency-svc-w7czn
Aug 24 12:49:43.543: INFO: Got endpoints: latency-svc-sgtkq [735.955632ms]
Aug 24 12:49:43.565: INFO: Created: latency-svc-h68hb
Aug 24 12:49:43.605: INFO: Got endpoints: latency-svc-6khsb [739.828753ms]
Aug 24 12:49:43.628: INFO: Created: latency-svc-ggtpz
Aug 24 12:49:43.650: INFO: Got endpoints: latency-svc-vcwz5 [751.903141ms]
Aug 24 12:49:43.678: INFO: Created: latency-svc-rjg8m
Aug 24 12:49:43.704: INFO: Got endpoints: latency-svc-gbrqx [753.434976ms]
Aug 24 12:49:43.729: INFO: Created: latency-svc-5x2gz
Aug 24 12:49:43.753: INFO: Got endpoints: latency-svc-jc6k7 [759.513432ms]
Aug 24 12:49:43.789: INFO: Created: latency-svc-dsfcp
Aug 24 12:49:43.805: INFO: Got endpoints: latency-svc-nql7r [749.79641ms]
Aug 24 12:49:43.834: INFO: Created: latency-svc-929rk
Aug 24 12:49:43.845: INFO: Got endpoints: latency-svc-kcmpw [741.353014ms]
Aug 24 12:49:43.876: INFO: Created: latency-svc-2q628
Aug 24 12:49:43.897: INFO: Got endpoints: latency-svc-xzbw4 [744.76965ms]
Aug 24 12:49:43.925: INFO: Created: latency-svc-mh9cq
Aug 24 12:49:43.947: INFO: Got endpoints: latency-svc-4r46w [750.304479ms]
Aug 24 12:49:43.972: INFO: Created: latency-svc-g7grs
Aug 24 12:49:44.003: INFO: Got endpoints: latency-svc-hv7xk [755.463143ms]
Aug 24 12:49:44.034: INFO: Created: latency-svc-lflqk
Aug 24 12:49:44.058: INFO: Got endpoints: latency-svc-q7q5c [761.426379ms]
Aug 24 12:49:44.093: INFO: Created: latency-svc-jdwxq
Aug 24 12:49:44.097: INFO: Got endpoints: latency-svc-tqshm [745.363757ms]
Aug 24 12:49:44.124: INFO: Created: latency-svc-hszqz
Aug 24 12:49:44.158: INFO: Got endpoints: latency-svc-kg7nq [725.583608ms]
Aug 24 12:49:44.180: INFO: Created: latency-svc-drq8k
Aug 24 12:49:44.194: INFO: Got endpoints: latency-svc-7qtxw [747.369181ms]
Aug 24 12:49:44.240: INFO: Created: latency-svc-2892g
Aug 24 12:49:44.254: INFO: Got endpoints: latency-svc-w7czn [755.818828ms]
Aug 24 12:49:44.272: INFO: Created: latency-svc-qxjhd
Aug 24 12:49:44.296: INFO: Got endpoints: latency-svc-h68hb [752.325984ms]
Aug 24 12:49:44.315: INFO: Created: latency-svc-nd78f
Aug 24 12:49:44.359: INFO: Got endpoints: latency-svc-ggtpz [753.546815ms]
Aug 24 12:49:44.379: INFO: Created: latency-svc-66rlh
Aug 24 12:49:44.405: INFO: Got endpoints: latency-svc-rjg8m [754.524584ms]
Aug 24 12:49:44.438: INFO: Created: latency-svc-97xcb
Aug 24 12:49:44.449: INFO: Got endpoints: latency-svc-5x2gz [745.244082ms]
Aug 24 12:49:44.477: INFO: Created: latency-svc-6pwhl
Aug 24 12:49:44.502: INFO: Got endpoints: latency-svc-dsfcp [747.803982ms]
Aug 24 12:49:44.529: INFO: Created: latency-svc-m2lrw
Aug 24 12:49:44.550: INFO: Got endpoints: latency-svc-929rk [744.596411ms]
Aug 24 12:49:44.571: INFO: Created: latency-svc-c925k
Aug 24 12:49:44.600: INFO: Got endpoints: latency-svc-2q628 [754.73698ms]
Aug 24 12:49:44.623: INFO: Created: latency-svc-jd24p
Aug 24 12:49:44.648: INFO: Got endpoints: latency-svc-mh9cq [751.037493ms]
Aug 24 12:49:44.670: INFO: Created: latency-svc-wkm9h
Aug 24 12:49:44.695: INFO: Got endpoints: latency-svc-g7grs [748.161432ms]
Aug 24 12:49:44.728: INFO: Created: latency-svc-f48jq
Aug 24 12:49:44.749: INFO: Got endpoints: latency-svc-lflqk [746.388308ms]
Aug 24 12:49:44.770: INFO: Created: latency-svc-hs8z4
Aug 24 12:49:44.796: INFO: Got endpoints: latency-svc-jdwxq [737.443036ms]
Aug 24 12:49:44.818: INFO: Created: latency-svc-bt6x2
Aug 24 12:49:44.850: INFO: Got endpoints: latency-svc-hszqz [752.852564ms]
Aug 24 12:49:44.887: INFO: Created: latency-svc-dq2jh
Aug 24 12:49:44.896: INFO: Got endpoints: latency-svc-drq8k [736.948578ms]
Aug 24 12:49:44.913: INFO: Created: latency-svc-xtsbz
Aug 24 12:49:44.950: INFO: Got endpoints: latency-svc-2892g [755.445596ms]
Aug 24 12:49:44.978: INFO: Created: latency-svc-qk2n7
Aug 24 12:49:44.995: INFO: Got endpoints: latency-svc-qxjhd [740.452097ms]
Aug 24 12:49:45.019: INFO: Created: latency-svc-j87hp
Aug 24 12:49:45.048: INFO: Got endpoints: latency-svc-nd78f [752.085646ms]
Aug 24 12:49:45.067: INFO: Created: latency-svc-2b8rl
Aug 24 12:49:45.099: INFO: Got endpoints: latency-svc-66rlh [739.046774ms]
Aug 24 12:49:45.158: INFO: Got endpoints: latency-svc-97xcb [752.557387ms]
Aug 24 12:49:45.171: INFO: Created: latency-svc-zct75
Aug 24 12:49:45.191: INFO: Created: latency-svc-wwblm
Aug 24 12:49:45.199: INFO: Got endpoints: latency-svc-6pwhl [749.246766ms]
Aug 24 12:49:45.226: INFO: Created: latency-svc-fct7b
Aug 24 12:49:45.245: INFO: Got endpoints: latency-svc-m2lrw [742.048931ms]
Aug 24 12:49:45.267: INFO: Created: latency-svc-498gp
Aug 24 12:49:45.311: INFO: Got endpoints: latency-svc-c925k [761.795133ms]
Aug 24 12:49:45.332: INFO: Created: latency-svc-sgh82
Aug 24 12:49:45.350: INFO: Got endpoints: latency-svc-jd24p [750.068854ms]
Aug 24 12:49:45.373: INFO: Created: latency-svc-xxd99
Aug 24 12:49:45.398: INFO: Got endpoints: latency-svc-wkm9h [749.147848ms]
Aug 24 12:49:45.430: INFO: Created: latency-svc-ldrxd
Aug 24 12:49:45.447: INFO: Got endpoints: latency-svc-f48jq [751.105578ms]
Aug 24 12:49:45.473: INFO: Created: latency-svc-xgjx9
Aug 24 12:49:45.501: INFO: Got endpoints: latency-svc-hs8z4 [751.601103ms]
Aug 24 12:49:45.524: INFO: Created: latency-svc-hp4d6
Aug 24 12:49:45.549: INFO: Got endpoints: latency-svc-bt6x2 [753.324886ms]
Aug 24 12:49:45.570: INFO: Created: latency-svc-jftmq
Aug 24 12:49:45.594: INFO: Got endpoints: latency-svc-dq2jh [743.432288ms]
Aug 24 12:49:45.619: INFO: Created: latency-svc-smvpq
Aug 24 12:49:45.647: INFO: Got endpoints: latency-svc-xtsbz [750.772838ms]
Aug 24 12:49:45.671: INFO: Created: latency-svc-2wcwp
Aug 24 12:49:45.718: INFO: Got endpoints: latency-svc-qk2n7 [767.378734ms]
Aug 24 12:49:45.739: INFO: Created: latency-svc-2ch44
Aug 24 12:49:45.754: INFO: Got endpoints: latency-svc-j87hp [758.924222ms]
Aug 24 12:49:45.787: INFO: Created: latency-svc-c7z45
Aug 24 12:49:45.813: INFO: Got endpoints: latency-svc-2b8rl [763.926802ms]
Aug 24 12:49:45.848: INFO: Created: latency-svc-x2x6f
Aug 24 12:49:45.855: INFO: Got endpoints: latency-svc-zct75 [756.182524ms]
Aug 24 12:49:45.913: INFO: Created: latency-svc-h28gh
Aug 24 12:49:45.914: INFO: Got endpoints: latency-svc-wwblm [756.183161ms]
Aug 24 12:49:45.941: INFO: Created: latency-svc-vq79v
Aug 24 12:49:45.985: INFO: Got endpoints: latency-svc-fct7b [785.796909ms]
Aug 24 12:49:46.004: INFO: Got endpoints: latency-svc-498gp [758.129698ms]
Aug 24 12:49:46.015: INFO: Created: latency-svc-t252r
Aug 24 12:49:46.034: INFO: Created: latency-svc-j4xgp
Aug 24 12:49:46.050: INFO: Got endpoints: latency-svc-sgh82 [738.182833ms]
Aug 24 12:49:46.083: INFO: Created: latency-svc-g7mhd
Aug 24 12:49:46.105: INFO: Got endpoints: latency-svc-xxd99 [754.273066ms]
Aug 24 12:49:46.129: INFO: Created: latency-svc-vrvw8
Aug 24 12:49:46.161: INFO: Got endpoints: latency-svc-ldrxd [762.750453ms]
Aug 24 12:49:46.186: INFO: Created: latency-svc-26qlb
Aug 24 12:49:46.195: INFO: Got endpoints: latency-svc-xgjx9 [748.424692ms]
Aug 24 12:49:46.225: INFO: Created: latency-svc-lrd4s
Aug 24 12:49:46.248: INFO: Got endpoints: latency-svc-hp4d6 [746.357612ms]
Aug 24 12:49:46.283: INFO: Created: latency-svc-wvrqs
Aug 24 12:49:46.296: INFO: Got endpoints: latency-svc-jftmq [746.142966ms]
Aug 24 12:49:46.315: INFO: Created: latency-svc-c6rhv
Aug 24 12:49:46.350: INFO: Got endpoints: latency-svc-smvpq [756.73964ms]
Aug 24 12:49:46.373: INFO: Created: latency-svc-mgpjc
Aug 24 12:49:46.397: INFO: Got endpoints: latency-svc-2wcwp [749.739289ms]
Aug 24 12:49:46.413: INFO: Created: latency-svc-8mbbv
Aug 24 12:49:46.447: INFO: Got endpoints: latency-svc-2ch44 [729.113656ms]
Aug 24 12:49:46.493: INFO: Got endpoints: latency-svc-c7z45 [739.214166ms]
Aug 24 12:49:46.549: INFO: Got endpoints: latency-svc-x2x6f [736.601131ms]
Aug 24 12:49:46.596: INFO: Got endpoints: latency-svc-h28gh [736.094386ms]
Aug 24 12:49:46.648: INFO: Got endpoints: latency-svc-vq79v [733.823164ms]
Aug 24 12:49:46.699: INFO: Got endpoints: latency-svc-t252r [713.915127ms]
Aug 24 12:49:46.745: INFO: Got endpoints: latency-svc-j4xgp [740.854057ms]
Aug 24 12:49:46.820: INFO: Got endpoints: latency-svc-g7mhd [769.818609ms]
Aug 24 12:49:46.851: INFO: Got endpoints: latency-svc-vrvw8 [746.065416ms]
Aug 24 12:49:46.903: INFO: Got endpoints: latency-svc-26qlb [742.370763ms]
Aug 24 12:49:46.956: INFO: Got endpoints: latency-svc-lrd4s [760.059827ms]
Aug 24 12:49:47.003: INFO: Got endpoints: latency-svc-wvrqs [754.527854ms]
Aug 24 12:49:47.058: INFO: Got endpoints: latency-svc-c6rhv [762.441084ms]
Aug 24 12:49:47.099: INFO: Got endpoints: latency-svc-mgpjc [748.122958ms]
Aug 24 12:49:47.156: INFO: Got endpoints: latency-svc-8mbbv [758.732316ms]
Aug 24 12:49:47.156: INFO: Latencies: [56.063584ms 84.157523ms 114.472563ms 125.146115ms 126.951742ms 143.417106ms 151.579914ms 156.814153ms 165.530931ms 168.904394ms 170.228686ms 183.093192ms 212.587727ms 219.626729ms 226.139178ms 232.327456ms 233.126352ms 243.49353ms 254.217879ms 266.254622ms 267.366666ms 267.892004ms 283.06359ms 290.706541ms 296.066397ms 297.939326ms 324.455077ms 327.216494ms 334.957207ms 341.10324ms 358.332465ms 364.161732ms 368.862262ms 369.231289ms 395.646699ms 399.618346ms 403.464416ms 406.392154ms 419.678946ms 420.848329ms 421.873402ms 422.2223ms 431.110147ms 438.756726ms 439.362204ms 442.562917ms 443.892767ms 444.71012ms 449.721282ms 450.566273ms 451.511288ms 456.158586ms 459.622892ms 461.754553ms 462.625648ms 470.635399ms 473.879001ms 476.051439ms 476.162731ms 482.3205ms 513.680755ms 524.373733ms 525.576932ms 530.214258ms 555.98563ms 562.625256ms 589.786908ms 634.312931ms 673.462216ms 675.331847ms 680.53894ms 683.271252ms 695.078929ms 709.105444ms 709.640934ms 713.915127ms 725.583608ms 729.113656ms 730.239621ms 733.823164ms 735.955632ms 736.094386ms 736.268587ms 736.601131ms 736.948578ms 737.443036ms 738.182833ms 738.286619ms 739.046774ms 739.214166ms 739.644572ms 739.828753ms 739.944485ms 740.170969ms 740.452097ms 740.854057ms 741.281238ms 741.353014ms 742.048931ms 742.370763ms 742.85621ms 742.966587ms 743.040996ms 743.432288ms 744.152701ms 744.596411ms 744.76965ms 745.202476ms 745.244082ms 745.363757ms 745.388433ms 745.462525ms 745.832041ms 746.065416ms 746.142966ms 746.171399ms 746.248804ms 746.357612ms 746.388308ms 746.96027ms 747.252052ms 747.369181ms 747.613239ms 747.803982ms 748.122958ms 748.161432ms 748.424692ms 749.147848ms 749.246766ms 749.576614ms 749.689492ms 749.739289ms 749.79641ms 749.83088ms 750.068854ms 750.304479ms 750.772838ms 750.879335ms 750.983062ms 751.037493ms 751.105578ms 751.342216ms 751.601103ms 751.903141ms 751.972848ms 752.031469ms 752.085646ms 752.325984ms 752.447734ms 752.467021ms 752.557387ms 752.852564ms 752.897191ms 753.024943ms 753.324886ms 753.434976ms 753.546815ms 753.692722ms 754.149943ms 754.236635ms 754.273066ms 754.524584ms 754.527854ms 754.73698ms 754.894792ms 754.968091ms 755.004249ms 755.333375ms 755.445596ms 755.463143ms 755.818828ms 756.057255ms 756.182524ms 756.183161ms 756.328611ms 756.461061ms 756.73964ms 756.857646ms 756.892822ms 758.129698ms 758.19094ms 758.676096ms 758.732316ms 758.924222ms 759.513432ms 759.561923ms 760.059827ms 761.426379ms 761.795133ms 762.193337ms 762.441084ms 762.750453ms 762.980733ms 763.926802ms 767.378734ms 769.818609ms 785.796909ms 786.630555ms 844.367675ms 939.574799ms]
Aug 24 12:49:47.157: INFO: 50 %ile: 742.85621ms
Aug 24 12:49:47.157: INFO: 90 %ile: 758.19094ms
Aug 24 12:49:47.157: INFO: 99 %ile: 844.367675ms
Aug 24 12:49:47.157: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:47.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-2999" for this suite. 08/24/23 12:49:47.172
------------------------------
â€¢ [SLOW TEST] [10.853 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:36.334
    Aug 24 12:49:36.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svc-latency 08/24/23 12:49:36.337
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:36.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:36.386
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 24 12:49:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-2999 08/24/23 12:49:36.393
    I0824 12:49:36.405795      14 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2999, replica count: 1
    I0824 12:49:37.457760      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 12:49:38.458412      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:49:38.582: INFO: Created: latency-svc-9cb5k
    Aug 24 12:49:38.604: INFO: Got endpoints: latency-svc-9cb5k [43.942554ms]
    Aug 24 12:49:38.640: INFO: Created: latency-svc-qqqhr
    Aug 24 12:49:38.660: INFO: Got endpoints: latency-svc-qqqhr [56.063584ms]
    Aug 24 12:49:38.675: INFO: Created: latency-svc-2llmf
    Aug 24 12:49:38.690: INFO: Got endpoints: latency-svc-2llmf [84.157523ms]
    Aug 24 12:49:38.698: INFO: Created: latency-svc-6lcfp
    Aug 24 12:49:38.714: INFO: Created: latency-svc-zgpb7
    Aug 24 12:49:38.721: INFO: Got endpoints: latency-svc-6lcfp [114.472563ms]
    Aug 24 12:49:38.733: INFO: Got endpoints: latency-svc-zgpb7 [125.146115ms]
    Aug 24 12:49:38.736: INFO: Created: latency-svc-t42np
    Aug 24 12:49:38.751: INFO: Created: latency-svc-6blnt
    Aug 24 12:49:38.758: INFO: Got endpoints: latency-svc-t42np [151.579914ms]
    Aug 24 12:49:38.765: INFO: Got endpoints: latency-svc-6blnt [156.814153ms]
    Aug 24 12:49:38.777: INFO: Created: latency-svc-n2k2d
    Aug 24 12:49:38.785: INFO: Got endpoints: latency-svc-n2k2d [170.228686ms]
    Aug 24 12:49:38.808: INFO: Created: latency-svc-n5m44
    Aug 24 12:49:38.809: INFO: Created: latency-svc-bs9f9
    Aug 24 12:49:38.824: INFO: Got endpoints: latency-svc-bs9f9 [212.587727ms]
    Aug 24 12:49:38.825: INFO: Created: latency-svc-w6gvq
    Aug 24 12:49:38.830: INFO: Got endpoints: latency-svc-n5m44 [219.626729ms]
    Aug 24 12:49:38.848: INFO: Got endpoints: latency-svc-w6gvq [233.126352ms]
    Aug 24 12:49:38.857: INFO: Created: latency-svc-2vssk
    Aug 24 12:49:38.859: INFO: Got endpoints: latency-svc-2vssk [243.49353ms]
    Aug 24 12:49:38.863: INFO: Created: latency-svc-6v2lz
    Aug 24 12:49:38.883: INFO: Got endpoints: latency-svc-6v2lz [267.892004ms]
    Aug 24 12:49:38.892: INFO: Created: latency-svc-ks8mh
    Aug 24 12:49:38.906: INFO: Got endpoints: latency-svc-ks8mh [290.706541ms]
    Aug 24 12:49:39.115: INFO: Created: latency-svc-cgwfk
    Aug 24 12:49:39.115: INFO: Created: latency-svc-ttl6v
    Aug 24 12:49:39.116: INFO: Created: latency-svc-2pq94
    Aug 24 12:49:39.131: INFO: Created: latency-svc-qnbdz
    Aug 24 12:49:39.131: INFO: Created: latency-svc-4j5jk
    Aug 24 12:49:39.131: INFO: Created: latency-svc-4fmm7
    Aug 24 12:49:39.132: INFO: Created: latency-svc-9sgmk
    Aug 24 12:49:39.132: INFO: Created: latency-svc-p76hw
    Aug 24 12:49:39.135: INFO: Created: latency-svc-bpmlm
    Aug 24 12:49:39.137: INFO: Created: latency-svc-xwxfq
    Aug 24 12:49:39.137: INFO: Created: latency-svc-n7bpk
    Aug 24 12:49:39.138: INFO: Created: latency-svc-zl9tb
    Aug 24 12:49:39.140: INFO: Created: latency-svc-jq2hc
    Aug 24 12:49:39.140: INFO: Created: latency-svc-tc2df
    Aug 24 12:49:39.142: INFO: Created: latency-svc-8ntmw
    Aug 24 12:49:39.172: INFO: Got endpoints: latency-svc-2pq94 [555.98563ms]
    Aug 24 12:49:39.173: INFO: Got endpoints: latency-svc-4j5jk [267.366666ms]
    Aug 24 12:49:39.178: INFO: Got endpoints: latency-svc-ttl6v [456.158586ms]
    Aug 24 12:49:39.186: INFO: Got endpoints: latency-svc-cgwfk [420.848329ms]
    Aug 24 12:49:39.215: INFO: Got endpoints: latency-svc-p76hw [524.373733ms]
    Aug 24 12:49:39.226: INFO: Got endpoints: latency-svc-4fmm7 [395.646699ms]
    Aug 24 12:49:39.227: INFO: Got endpoints: latency-svc-9sgmk [403.464416ms]
    Aug 24 12:49:39.228: INFO: Got endpoints: latency-svc-qnbdz [369.231289ms]
    Aug 24 12:49:39.232: INFO: Got endpoints: latency-svc-xwxfq [473.879001ms]
    Aug 24 12:49:39.276: INFO: Created: latency-svc-b7x6g
    Aug 24 12:49:39.293: INFO: Got endpoints: latency-svc-n7bpk [444.71012ms]
    Aug 24 12:49:39.295: INFO: Got endpoints: latency-svc-zl9tb [634.312931ms]
    Aug 24 12:49:39.296: INFO: Got endpoints: latency-svc-8ntmw [562.625256ms]
    Aug 24 12:49:39.299: INFO: Got endpoints: latency-svc-bpmlm [513.680755ms]
    Aug 24 12:49:39.299: INFO: Got endpoints: latency-svc-jq2hc [683.271252ms]
    Aug 24 12:49:39.322: INFO: Got endpoints: latency-svc-tc2df [439.362204ms]
    Aug 24 12:49:39.330: INFO: Created: latency-svc-jj5s8
    Aug 24 12:49:39.341: INFO: Got endpoints: latency-svc-b7x6g [168.904394ms]
    Aug 24 12:49:39.342: INFO: Got endpoints: latency-svc-jj5s8 [126.951742ms]
    Aug 24 12:49:39.363: INFO: Created: latency-svc-lgnq4
    Aug 24 12:49:39.369: INFO: Got endpoints: latency-svc-lgnq4 [143.417106ms]
    Aug 24 12:49:39.380: INFO: Created: latency-svc-2nvb4
    Aug 24 12:49:39.404: INFO: Got endpoints: latency-svc-2nvb4 [226.139178ms]
    Aug 24 12:49:39.424: INFO: Created: latency-svc-cqdzp
    Aug 24 12:49:39.440: INFO: Got endpoints: latency-svc-cqdzp [254.217879ms]
    Aug 24 12:49:39.496: INFO: Created: latency-svc-4n8sm
    Aug 24 12:49:39.498: INFO: Got endpoints: latency-svc-4n8sm [324.455077ms]
    Aug 24 12:49:39.517: INFO: Created: latency-svc-lr4bp
    Aug 24 12:49:39.525: INFO: Got endpoints: latency-svc-lr4bp [297.939326ms]
    Aug 24 12:49:39.547: INFO: Created: latency-svc-q7cw8
    Aug 24 12:49:39.560: INFO: Got endpoints: latency-svc-q7cw8 [327.216494ms]
    Aug 24 12:49:39.574: INFO: Created: latency-svc-jdjkr
    Aug 24 12:49:39.587: INFO: Got endpoints: latency-svc-jdjkr [358.332465ms]
    Aug 24 12:49:39.615: INFO: Created: latency-svc-6k7sw
    Aug 24 12:49:39.630: INFO: Got endpoints: latency-svc-6k7sw [334.957207ms]
    Aug 24 12:49:39.638: INFO: Created: latency-svc-6ghm9
    Aug 24 12:49:39.664: INFO: Got endpoints: latency-svc-6ghm9 [364.161732ms]
    Aug 24 12:49:39.682: INFO: Created: latency-svc-4pf74
    Aug 24 12:49:39.709: INFO: Created: latency-svc-s5zj4
    Aug 24 12:49:39.713: INFO: Got endpoints: latency-svc-4pf74 [419.678946ms]
    Aug 24 12:49:39.722: INFO: Created: latency-svc-qw78k
    Aug 24 12:49:39.734: INFO: Got endpoints: latency-svc-s5zj4 [438.756726ms]
    Aug 24 12:49:39.755: INFO: Created: latency-svc-nfh8g
    Aug 24 12:49:39.759: INFO: Got endpoints: latency-svc-qw78k [459.622892ms]
    Aug 24 12:49:39.766: INFO: Got endpoints: latency-svc-nfh8g [443.892767ms]
    Aug 24 12:49:39.786: INFO: Created: latency-svc-6qghf
    Aug 24 12:49:39.797: INFO: Created: latency-svc-w7knl
    Aug 24 12:49:39.803: INFO: Got endpoints: latency-svc-6qghf [461.754553ms]
    Aug 24 12:49:39.818: INFO: Got endpoints: latency-svc-w7knl [476.051439ms]
    Aug 24 12:49:39.840: INFO: Created: latency-svc-f9skq
    Aug 24 12:49:39.846: INFO: Got endpoints: latency-svc-f9skq [476.162731ms]
    Aug 24 12:49:39.994: INFO: Created: latency-svc-cm8pj
    Aug 24 12:49:39.995: INFO: Created: latency-svc-kdtct
    Aug 24 12:49:40.001: INFO: Created: latency-svc-8zw9x
    Aug 24 12:49:40.002: INFO: Created: latency-svc-2bq8t
    Aug 24 12:49:40.032: INFO: Created: latency-svc-jh87n
    Aug 24 12:49:40.033: INFO: Created: latency-svc-cxjkm
    Aug 24 12:49:40.034: INFO: Created: latency-svc-52g2z
    Aug 24 12:49:40.034: INFO: Created: latency-svc-zg5fc
    Aug 24 12:49:40.034: INFO: Created: latency-svc-n2r5t
    Aug 24 12:49:40.035: INFO: Created: latency-svc-b46mh
    Aug 24 12:49:40.034: INFO: Created: latency-svc-k7hjk
    Aug 24 12:49:40.035: INFO: Created: latency-svc-bdlhc
    Aug 24 12:49:40.036: INFO: Created: latency-svc-sbk4j
    Aug 24 12:49:40.036: INFO: Created: latency-svc-nbrhv
    Aug 24 12:49:40.037: INFO: Created: latency-svc-5s86b
    Aug 24 12:49:40.086: INFO: Got endpoints: latency-svc-cm8pj [422.2223ms]
    Aug 24 12:49:40.112: INFO: Got endpoints: latency-svc-k7hjk [266.254622ms]
    Aug 24 12:49:40.113: INFO: Got endpoints: latency-svc-2bq8t [482.3205ms]
    Aug 24 12:49:40.114: INFO: Got endpoints: latency-svc-kdtct [525.576932ms]
    Aug 24 12:49:40.114: INFO: Got endpoints: latency-svc-8zw9x [673.462216ms]
    Aug 24 12:49:40.135: INFO: Got endpoints: latency-svc-cxjkm [730.239621ms]
    Aug 24 12:49:40.163: INFO: Got endpoints: latency-svc-jh87n [449.721282ms]
    Aug 24 12:49:40.171: INFO: Created: latency-svc-vkkhs
    Aug 24 12:49:40.203: INFO: Got endpoints: latency-svc-sbk4j [399.618346ms]
    Aug 24 12:49:40.205: INFO: Got endpoints: latency-svc-5s86b [470.635399ms]
    Aug 24 12:49:40.207: INFO: Got endpoints: latency-svc-n2r5t [709.105444ms]
    Aug 24 12:49:40.209: INFO: Got endpoints: latency-svc-52g2z [442.562917ms]
    Aug 24 12:49:40.215: INFO: Created: latency-svc-g56hc
    Aug 24 12:49:40.221: INFO: Got endpoints: latency-svc-zg5fc [695.078929ms]
    Aug 24 12:49:40.221: INFO: Got endpoints: latency-svc-b46mh [462.625648ms]
    Aug 24 12:49:40.237: INFO: Created: latency-svc-qzqhl
    Aug 24 12:49:40.240: INFO: Got endpoints: latency-svc-nbrhv [421.873402ms]
    Aug 24 12:49:40.240: INFO: Got endpoints: latency-svc-bdlhc [680.53894ms]
    Aug 24 12:49:40.249: INFO: Created: latency-svc-xdsdp
    Aug 24 12:49:40.251: INFO: Got endpoints: latency-svc-vkkhs [165.530931ms]
    Aug 24 12:49:40.261: INFO: Created: latency-svc-6p87w
    Aug 24 12:49:40.271: INFO: Created: latency-svc-7d86s
    Aug 24 12:49:40.296: INFO: Got endpoints: latency-svc-g56hc [183.093192ms]
    Aug 24 12:49:40.346: INFO: Got endpoints: latency-svc-qzqhl [232.327456ms]
    Aug 24 12:49:40.396: INFO: Got endpoints: latency-svc-xdsdp [283.06359ms]
    Aug 24 12:49:40.430: INFO: Created: latency-svc-b9zg7
    Aug 24 12:49:40.430: INFO: Created: latency-svc-mn5zq
    Aug 24 12:49:40.434: INFO: Created: latency-svc-cqd2t
    Aug 24 12:49:40.455: INFO: Got endpoints: latency-svc-6p87w [341.10324ms]
    Aug 24 12:49:40.466: INFO: Created: latency-svc-znb8r
    Aug 24 12:49:40.479: INFO: Created: latency-svc-ksvf5
    Aug 24 12:49:40.479: INFO: Created: latency-svc-xkwcp
    Aug 24 12:49:40.480: INFO: Created: latency-svc-zlnjs
    Aug 24 12:49:40.480: INFO: Created: latency-svc-b8skg
    Aug 24 12:49:40.481: INFO: Created: latency-svc-bfl6x
    Aug 24 12:49:40.481: INFO: Created: latency-svc-9kc8c
    Aug 24 12:49:40.482: INFO: Created: latency-svc-rzzr5
    Aug 24 12:49:40.491: INFO: Created: latency-svc-kvpxf
    Aug 24 12:49:40.492: INFO: Created: latency-svc-xvbp6
    Aug 24 12:49:40.504: INFO: Got endpoints: latency-svc-7d86s [368.862262ms]
    Aug 24 12:49:40.516: INFO: Created: latency-svc-56g2n
    Aug 24 12:49:40.532: INFO: Created: latency-svc-jmp46
    Aug 24 12:49:40.548: INFO: Got endpoints: latency-svc-mn5zq [296.066397ms]
    Aug 24 12:49:40.568: INFO: Created: latency-svc-rbb2n
    Aug 24 12:49:40.595: INFO: Got endpoints: latency-svc-b9zg7 [431.110147ms]
    Aug 24 12:49:40.613: INFO: Created: latency-svc-d2dsr
    Aug 24 12:49:40.653: INFO: Got endpoints: latency-svc-cqd2t [450.566273ms]
    Aug 24 12:49:40.677: INFO: Created: latency-svc-kp989
    Aug 24 12:49:40.703: INFO: Got endpoints: latency-svc-znb8r [406.392154ms]
    Aug 24 12:49:40.733: INFO: Created: latency-svc-czrlm
    Aug 24 12:49:40.752: INFO: Got endpoints: latency-svc-xvbp6 [530.214258ms]
    Aug 24 12:49:40.779: INFO: Created: latency-svc-nttk4
    Aug 24 12:49:40.799: INFO: Got endpoints: latency-svc-kvpxf [589.786908ms]
    Aug 24 12:49:40.821: INFO: Created: latency-svc-8rfmr
    Aug 24 12:49:40.847: INFO: Got endpoints: latency-svc-zlnjs [451.511288ms]
    Aug 24 12:49:40.875: INFO: Created: latency-svc-xsbls
    Aug 24 12:49:40.896: INFO: Got endpoints: latency-svc-xkwcp [675.331847ms]
    Aug 24 12:49:40.914: INFO: Created: latency-svc-54gxm
    Aug 24 12:49:40.950: INFO: Got endpoints: latency-svc-rzzr5 [709.640934ms]
    Aug 24 12:49:40.972: INFO: Created: latency-svc-4hdfk
    Aug 24 12:49:40.994: INFO: Got endpoints: latency-svc-9kc8c [754.236635ms]
    Aug 24 12:49:41.013: INFO: Created: latency-svc-sbt88
    Aug 24 12:49:41.052: INFO: Got endpoints: latency-svc-ksvf5 [844.367675ms]
    Aug 24 12:49:41.082: INFO: Created: latency-svc-f42s8
    Aug 24 12:49:41.096: INFO: Got endpoints: latency-svc-b8skg [749.689492ms]
    Aug 24 12:49:41.130: INFO: Created: latency-svc-fdpsf
    Aug 24 12:49:41.145: INFO: Got endpoints: latency-svc-bfl6x [939.574799ms]
    Aug 24 12:49:41.180: INFO: Created: latency-svc-khbt8
    Aug 24 12:49:41.195: INFO: Got endpoints: latency-svc-56g2n [739.944485ms]
    Aug 24 12:49:41.225: INFO: Created: latency-svc-msjpv
    Aug 24 12:49:41.247: INFO: Got endpoints: latency-svc-jmp46 [742.85621ms]
    Aug 24 12:49:41.263: INFO: Created: latency-svc-rvlcq
    Aug 24 12:49:41.294: INFO: Got endpoints: latency-svc-rbb2n [745.832041ms]
    Aug 24 12:49:41.320: INFO: Created: latency-svc-zc8vx
    Aug 24 12:49:41.353: INFO: Got endpoints: latency-svc-d2dsr [758.19094ms]
    Aug 24 12:49:41.385: INFO: Created: latency-svc-n4s86
    Aug 24 12:49:41.392: INFO: Got endpoints: latency-svc-kp989 [738.286619ms]
    Aug 24 12:49:41.429: INFO: Created: latency-svc-dktvt
    Aug 24 12:49:41.446: INFO: Got endpoints: latency-svc-czrlm [742.966587ms]
    Aug 24 12:49:41.473: INFO: Created: latency-svc-9dlw2
    Aug 24 12:49:41.497: INFO: Got endpoints: latency-svc-nttk4 [744.152701ms]
    Aug 24 12:49:41.522: INFO: Created: latency-svc-gp4pk
    Aug 24 12:49:41.542: INFO: Got endpoints: latency-svc-8rfmr [743.040996ms]
    Aug 24 12:49:41.560: INFO: Created: latency-svc-wwljq
    Aug 24 12:49:41.595: INFO: Got endpoints: latency-svc-xsbls [747.613239ms]
    Aug 24 12:49:41.621: INFO: Created: latency-svc-56wvd
    Aug 24 12:49:41.653: INFO: Got endpoints: latency-svc-54gxm [756.857646ms]
    Aug 24 12:49:41.674: INFO: Created: latency-svc-6hqlh
    Aug 24 12:49:41.702: INFO: Got endpoints: latency-svc-4hdfk [751.972848ms]
    Aug 24 12:49:41.724: INFO: Created: latency-svc-4pmxr
    Aug 24 12:49:41.746: INFO: Got endpoints: latency-svc-sbt88 [752.031469ms]
    Aug 24 12:49:41.783: INFO: Created: latency-svc-9cgvj
    Aug 24 12:49:41.797: INFO: Got endpoints: latency-svc-f42s8 [745.202476ms]
    Aug 24 12:49:41.814: INFO: Created: latency-svc-hprcf
    Aug 24 12:49:41.848: INFO: Got endpoints: latency-svc-fdpsf [751.342216ms]
    Aug 24 12:49:41.874: INFO: Created: latency-svc-h59vd
    Aug 24 12:49:41.900: INFO: Got endpoints: latency-svc-khbt8 [754.894792ms]
    Aug 24 12:49:41.928: INFO: Created: latency-svc-fxzp4
    Aug 24 12:49:41.949: INFO: Got endpoints: latency-svc-msjpv [753.692722ms]
    Aug 24 12:49:41.972: INFO: Created: latency-svc-l78qs
    Aug 24 12:49:41.992: INFO: Got endpoints: latency-svc-rvlcq [745.388433ms]
    Aug 24 12:49:42.015: INFO: Created: latency-svc-jdf4t
    Aug 24 12:49:42.057: INFO: Got endpoints: latency-svc-zc8vx [762.980733ms]
    Aug 24 12:49:42.109: INFO: Got endpoints: latency-svc-n4s86 [755.333375ms]
    Aug 24 12:49:42.116: INFO: Created: latency-svc-m99q8
    Aug 24 12:49:42.132: INFO: Created: latency-svc-v2gk8
    Aug 24 12:49:42.148: INFO: Got endpoints: latency-svc-dktvt [756.328611ms]
    Aug 24 12:49:42.209: INFO: Got endpoints: latency-svc-9dlw2 [762.193337ms]
    Aug 24 12:49:42.218: INFO: Created: latency-svc-txcpg
    Aug 24 12:49:42.245: INFO: Created: latency-svc-xk89s
    Aug 24 12:49:42.257: INFO: Got endpoints: latency-svc-gp4pk [759.561923ms]
    Aug 24 12:49:42.276: INFO: Created: latency-svc-pxlkb
    Aug 24 12:49:42.299: INFO: Got endpoints: latency-svc-wwljq [756.892822ms]
    Aug 24 12:49:42.318: INFO: Created: latency-svc-xlql7
    Aug 24 12:49:42.350: INFO: Got endpoints: latency-svc-56wvd [755.004249ms]
    Aug 24 12:49:42.376: INFO: Created: latency-svc-7br9g
    Aug 24 12:49:42.400: INFO: Got endpoints: latency-svc-6hqlh [746.248804ms]
    Aug 24 12:49:42.417: INFO: Created: latency-svc-2h4m5
    Aug 24 12:49:42.442: INFO: Got endpoints: latency-svc-4pmxr [740.170969ms]
    Aug 24 12:49:42.461: INFO: Created: latency-svc-2frd7
    Aug 24 12:49:42.499: INFO: Got endpoints: latency-svc-9cgvj [752.447734ms]
    Aug 24 12:49:42.530: INFO: Created: latency-svc-xz6sh
    Aug 24 12:49:42.550: INFO: Got endpoints: latency-svc-hprcf [753.024943ms]
    Aug 24 12:49:42.569: INFO: Created: latency-svc-5q4vg
    Aug 24 12:49:42.599: INFO: Got endpoints: latency-svc-h59vd [750.879335ms]
    Aug 24 12:49:42.622: INFO: Created: latency-svc-hr77d
    Aug 24 12:49:42.646: INFO: Got endpoints: latency-svc-fxzp4 [745.462525ms]
    Aug 24 12:49:42.707: INFO: Got endpoints: latency-svc-l78qs [756.057255ms]
    Aug 24 12:49:42.718: INFO: Created: latency-svc-n7zd5
    Aug 24 12:49:42.746: INFO: Created: latency-svc-z2gfh
    Aug 24 12:49:42.751: INFO: Got endpoints: latency-svc-jdf4t [758.676096ms]
    Aug 24 12:49:42.774: INFO: Created: latency-svc-9xmn9
    Aug 24 12:49:42.807: INFO: Got endpoints: latency-svc-m99q8 [749.83088ms]
    Aug 24 12:49:42.852: INFO: Created: latency-svc-sgtkq
    Aug 24 12:49:42.865: INFO: Got endpoints: latency-svc-v2gk8 [756.461061ms]
    Aug 24 12:49:42.889: INFO: Created: latency-svc-6khsb
    Aug 24 12:49:42.898: INFO: Got endpoints: latency-svc-txcpg [749.576614ms]
    Aug 24 12:49:42.923: INFO: Created: latency-svc-vcwz5
    Aug 24 12:49:42.950: INFO: Got endpoints: latency-svc-xk89s [741.281238ms]
    Aug 24 12:49:42.973: INFO: Created: latency-svc-gbrqx
    Aug 24 12:49:42.994: INFO: Got endpoints: latency-svc-pxlkb [736.268587ms]
    Aug 24 12:49:43.020: INFO: Created: latency-svc-jc6k7
    Aug 24 12:49:43.055: INFO: Got endpoints: latency-svc-xlql7 [754.968091ms]
    Aug 24 12:49:43.078: INFO: Created: latency-svc-nql7r
    Aug 24 12:49:43.103: INFO: Got endpoints: latency-svc-7br9g [752.897191ms]
    Aug 24 12:49:43.126: INFO: Created: latency-svc-kcmpw
    Aug 24 12:49:43.152: INFO: Got endpoints: latency-svc-2h4m5 [752.467021ms]
    Aug 24 12:49:43.170: INFO: Created: latency-svc-xzbw4
    Aug 24 12:49:43.197: INFO: Got endpoints: latency-svc-2frd7 [754.149943ms]
    Aug 24 12:49:43.220: INFO: Created: latency-svc-4r46w
    Aug 24 12:49:43.247: INFO: Got endpoints: latency-svc-xz6sh [747.252052ms]
    Aug 24 12:49:43.270: INFO: Created: latency-svc-hv7xk
    Aug 24 12:49:43.297: INFO: Got endpoints: latency-svc-5q4vg [746.171399ms]
    Aug 24 12:49:43.316: INFO: Created: latency-svc-q7q5c
    Aug 24 12:49:43.351: INFO: Got endpoints: latency-svc-hr77d [750.983062ms]
    Aug 24 12:49:43.388: INFO: Created: latency-svc-tqshm
    Aug 24 12:49:43.433: INFO: Got endpoints: latency-svc-n7zd5 [786.630555ms]
    Aug 24 12:49:43.447: INFO: Got endpoints: latency-svc-z2gfh [739.644572ms]
    Aug 24 12:49:43.474: INFO: Created: latency-svc-kg7nq
    Aug 24 12:49:43.486: INFO: Created: latency-svc-7qtxw
    Aug 24 12:49:43.498: INFO: Got endpoints: latency-svc-9xmn9 [746.96027ms]
    Aug 24 12:49:43.526: INFO: Created: latency-svc-w7czn
    Aug 24 12:49:43.543: INFO: Got endpoints: latency-svc-sgtkq [735.955632ms]
    Aug 24 12:49:43.565: INFO: Created: latency-svc-h68hb
    Aug 24 12:49:43.605: INFO: Got endpoints: latency-svc-6khsb [739.828753ms]
    Aug 24 12:49:43.628: INFO: Created: latency-svc-ggtpz
    Aug 24 12:49:43.650: INFO: Got endpoints: latency-svc-vcwz5 [751.903141ms]
    Aug 24 12:49:43.678: INFO: Created: latency-svc-rjg8m
    Aug 24 12:49:43.704: INFO: Got endpoints: latency-svc-gbrqx [753.434976ms]
    Aug 24 12:49:43.729: INFO: Created: latency-svc-5x2gz
    Aug 24 12:49:43.753: INFO: Got endpoints: latency-svc-jc6k7 [759.513432ms]
    Aug 24 12:49:43.789: INFO: Created: latency-svc-dsfcp
    Aug 24 12:49:43.805: INFO: Got endpoints: latency-svc-nql7r [749.79641ms]
    Aug 24 12:49:43.834: INFO: Created: latency-svc-929rk
    Aug 24 12:49:43.845: INFO: Got endpoints: latency-svc-kcmpw [741.353014ms]
    Aug 24 12:49:43.876: INFO: Created: latency-svc-2q628
    Aug 24 12:49:43.897: INFO: Got endpoints: latency-svc-xzbw4 [744.76965ms]
    Aug 24 12:49:43.925: INFO: Created: latency-svc-mh9cq
    Aug 24 12:49:43.947: INFO: Got endpoints: latency-svc-4r46w [750.304479ms]
    Aug 24 12:49:43.972: INFO: Created: latency-svc-g7grs
    Aug 24 12:49:44.003: INFO: Got endpoints: latency-svc-hv7xk [755.463143ms]
    Aug 24 12:49:44.034: INFO: Created: latency-svc-lflqk
    Aug 24 12:49:44.058: INFO: Got endpoints: latency-svc-q7q5c [761.426379ms]
    Aug 24 12:49:44.093: INFO: Created: latency-svc-jdwxq
    Aug 24 12:49:44.097: INFO: Got endpoints: latency-svc-tqshm [745.363757ms]
    Aug 24 12:49:44.124: INFO: Created: latency-svc-hszqz
    Aug 24 12:49:44.158: INFO: Got endpoints: latency-svc-kg7nq [725.583608ms]
    Aug 24 12:49:44.180: INFO: Created: latency-svc-drq8k
    Aug 24 12:49:44.194: INFO: Got endpoints: latency-svc-7qtxw [747.369181ms]
    Aug 24 12:49:44.240: INFO: Created: latency-svc-2892g
    Aug 24 12:49:44.254: INFO: Got endpoints: latency-svc-w7czn [755.818828ms]
    Aug 24 12:49:44.272: INFO: Created: latency-svc-qxjhd
    Aug 24 12:49:44.296: INFO: Got endpoints: latency-svc-h68hb [752.325984ms]
    Aug 24 12:49:44.315: INFO: Created: latency-svc-nd78f
    Aug 24 12:49:44.359: INFO: Got endpoints: latency-svc-ggtpz [753.546815ms]
    Aug 24 12:49:44.379: INFO: Created: latency-svc-66rlh
    Aug 24 12:49:44.405: INFO: Got endpoints: latency-svc-rjg8m [754.524584ms]
    Aug 24 12:49:44.438: INFO: Created: latency-svc-97xcb
    Aug 24 12:49:44.449: INFO: Got endpoints: latency-svc-5x2gz [745.244082ms]
    Aug 24 12:49:44.477: INFO: Created: latency-svc-6pwhl
    Aug 24 12:49:44.502: INFO: Got endpoints: latency-svc-dsfcp [747.803982ms]
    Aug 24 12:49:44.529: INFO: Created: latency-svc-m2lrw
    Aug 24 12:49:44.550: INFO: Got endpoints: latency-svc-929rk [744.596411ms]
    Aug 24 12:49:44.571: INFO: Created: latency-svc-c925k
    Aug 24 12:49:44.600: INFO: Got endpoints: latency-svc-2q628 [754.73698ms]
    Aug 24 12:49:44.623: INFO: Created: latency-svc-jd24p
    Aug 24 12:49:44.648: INFO: Got endpoints: latency-svc-mh9cq [751.037493ms]
    Aug 24 12:49:44.670: INFO: Created: latency-svc-wkm9h
    Aug 24 12:49:44.695: INFO: Got endpoints: latency-svc-g7grs [748.161432ms]
    Aug 24 12:49:44.728: INFO: Created: latency-svc-f48jq
    Aug 24 12:49:44.749: INFO: Got endpoints: latency-svc-lflqk [746.388308ms]
    Aug 24 12:49:44.770: INFO: Created: latency-svc-hs8z4
    Aug 24 12:49:44.796: INFO: Got endpoints: latency-svc-jdwxq [737.443036ms]
    Aug 24 12:49:44.818: INFO: Created: latency-svc-bt6x2
    Aug 24 12:49:44.850: INFO: Got endpoints: latency-svc-hszqz [752.852564ms]
    Aug 24 12:49:44.887: INFO: Created: latency-svc-dq2jh
    Aug 24 12:49:44.896: INFO: Got endpoints: latency-svc-drq8k [736.948578ms]
    Aug 24 12:49:44.913: INFO: Created: latency-svc-xtsbz
    Aug 24 12:49:44.950: INFO: Got endpoints: latency-svc-2892g [755.445596ms]
    Aug 24 12:49:44.978: INFO: Created: latency-svc-qk2n7
    Aug 24 12:49:44.995: INFO: Got endpoints: latency-svc-qxjhd [740.452097ms]
    Aug 24 12:49:45.019: INFO: Created: latency-svc-j87hp
    Aug 24 12:49:45.048: INFO: Got endpoints: latency-svc-nd78f [752.085646ms]
    Aug 24 12:49:45.067: INFO: Created: latency-svc-2b8rl
    Aug 24 12:49:45.099: INFO: Got endpoints: latency-svc-66rlh [739.046774ms]
    Aug 24 12:49:45.158: INFO: Got endpoints: latency-svc-97xcb [752.557387ms]
    Aug 24 12:49:45.171: INFO: Created: latency-svc-zct75
    Aug 24 12:49:45.191: INFO: Created: latency-svc-wwblm
    Aug 24 12:49:45.199: INFO: Got endpoints: latency-svc-6pwhl [749.246766ms]
    Aug 24 12:49:45.226: INFO: Created: latency-svc-fct7b
    Aug 24 12:49:45.245: INFO: Got endpoints: latency-svc-m2lrw [742.048931ms]
    Aug 24 12:49:45.267: INFO: Created: latency-svc-498gp
    Aug 24 12:49:45.311: INFO: Got endpoints: latency-svc-c925k [761.795133ms]
    Aug 24 12:49:45.332: INFO: Created: latency-svc-sgh82
    Aug 24 12:49:45.350: INFO: Got endpoints: latency-svc-jd24p [750.068854ms]
    Aug 24 12:49:45.373: INFO: Created: latency-svc-xxd99
    Aug 24 12:49:45.398: INFO: Got endpoints: latency-svc-wkm9h [749.147848ms]
    Aug 24 12:49:45.430: INFO: Created: latency-svc-ldrxd
    Aug 24 12:49:45.447: INFO: Got endpoints: latency-svc-f48jq [751.105578ms]
    Aug 24 12:49:45.473: INFO: Created: latency-svc-xgjx9
    Aug 24 12:49:45.501: INFO: Got endpoints: latency-svc-hs8z4 [751.601103ms]
    Aug 24 12:49:45.524: INFO: Created: latency-svc-hp4d6
    Aug 24 12:49:45.549: INFO: Got endpoints: latency-svc-bt6x2 [753.324886ms]
    Aug 24 12:49:45.570: INFO: Created: latency-svc-jftmq
    Aug 24 12:49:45.594: INFO: Got endpoints: latency-svc-dq2jh [743.432288ms]
    Aug 24 12:49:45.619: INFO: Created: latency-svc-smvpq
    Aug 24 12:49:45.647: INFO: Got endpoints: latency-svc-xtsbz [750.772838ms]
    Aug 24 12:49:45.671: INFO: Created: latency-svc-2wcwp
    Aug 24 12:49:45.718: INFO: Got endpoints: latency-svc-qk2n7 [767.378734ms]
    Aug 24 12:49:45.739: INFO: Created: latency-svc-2ch44
    Aug 24 12:49:45.754: INFO: Got endpoints: latency-svc-j87hp [758.924222ms]
    Aug 24 12:49:45.787: INFO: Created: latency-svc-c7z45
    Aug 24 12:49:45.813: INFO: Got endpoints: latency-svc-2b8rl [763.926802ms]
    Aug 24 12:49:45.848: INFO: Created: latency-svc-x2x6f
    Aug 24 12:49:45.855: INFO: Got endpoints: latency-svc-zct75 [756.182524ms]
    Aug 24 12:49:45.913: INFO: Created: latency-svc-h28gh
    Aug 24 12:49:45.914: INFO: Got endpoints: latency-svc-wwblm [756.183161ms]
    Aug 24 12:49:45.941: INFO: Created: latency-svc-vq79v
    Aug 24 12:49:45.985: INFO: Got endpoints: latency-svc-fct7b [785.796909ms]
    Aug 24 12:49:46.004: INFO: Got endpoints: latency-svc-498gp [758.129698ms]
    Aug 24 12:49:46.015: INFO: Created: latency-svc-t252r
    Aug 24 12:49:46.034: INFO: Created: latency-svc-j4xgp
    Aug 24 12:49:46.050: INFO: Got endpoints: latency-svc-sgh82 [738.182833ms]
    Aug 24 12:49:46.083: INFO: Created: latency-svc-g7mhd
    Aug 24 12:49:46.105: INFO: Got endpoints: latency-svc-xxd99 [754.273066ms]
    Aug 24 12:49:46.129: INFO: Created: latency-svc-vrvw8
    Aug 24 12:49:46.161: INFO: Got endpoints: latency-svc-ldrxd [762.750453ms]
    Aug 24 12:49:46.186: INFO: Created: latency-svc-26qlb
    Aug 24 12:49:46.195: INFO: Got endpoints: latency-svc-xgjx9 [748.424692ms]
    Aug 24 12:49:46.225: INFO: Created: latency-svc-lrd4s
    Aug 24 12:49:46.248: INFO: Got endpoints: latency-svc-hp4d6 [746.357612ms]
    Aug 24 12:49:46.283: INFO: Created: latency-svc-wvrqs
    Aug 24 12:49:46.296: INFO: Got endpoints: latency-svc-jftmq [746.142966ms]
    Aug 24 12:49:46.315: INFO: Created: latency-svc-c6rhv
    Aug 24 12:49:46.350: INFO: Got endpoints: latency-svc-smvpq [756.73964ms]
    Aug 24 12:49:46.373: INFO: Created: latency-svc-mgpjc
    Aug 24 12:49:46.397: INFO: Got endpoints: latency-svc-2wcwp [749.739289ms]
    Aug 24 12:49:46.413: INFO: Created: latency-svc-8mbbv
    Aug 24 12:49:46.447: INFO: Got endpoints: latency-svc-2ch44 [729.113656ms]
    Aug 24 12:49:46.493: INFO: Got endpoints: latency-svc-c7z45 [739.214166ms]
    Aug 24 12:49:46.549: INFO: Got endpoints: latency-svc-x2x6f [736.601131ms]
    Aug 24 12:49:46.596: INFO: Got endpoints: latency-svc-h28gh [736.094386ms]
    Aug 24 12:49:46.648: INFO: Got endpoints: latency-svc-vq79v [733.823164ms]
    Aug 24 12:49:46.699: INFO: Got endpoints: latency-svc-t252r [713.915127ms]
    Aug 24 12:49:46.745: INFO: Got endpoints: latency-svc-j4xgp [740.854057ms]
    Aug 24 12:49:46.820: INFO: Got endpoints: latency-svc-g7mhd [769.818609ms]
    Aug 24 12:49:46.851: INFO: Got endpoints: latency-svc-vrvw8 [746.065416ms]
    Aug 24 12:49:46.903: INFO: Got endpoints: latency-svc-26qlb [742.370763ms]
    Aug 24 12:49:46.956: INFO: Got endpoints: latency-svc-lrd4s [760.059827ms]
    Aug 24 12:49:47.003: INFO: Got endpoints: latency-svc-wvrqs [754.527854ms]
    Aug 24 12:49:47.058: INFO: Got endpoints: latency-svc-c6rhv [762.441084ms]
    Aug 24 12:49:47.099: INFO: Got endpoints: latency-svc-mgpjc [748.122958ms]
    Aug 24 12:49:47.156: INFO: Got endpoints: latency-svc-8mbbv [758.732316ms]
    Aug 24 12:49:47.156: INFO: Latencies: [56.063584ms 84.157523ms 114.472563ms 125.146115ms 126.951742ms 143.417106ms 151.579914ms 156.814153ms 165.530931ms 168.904394ms 170.228686ms 183.093192ms 212.587727ms 219.626729ms 226.139178ms 232.327456ms 233.126352ms 243.49353ms 254.217879ms 266.254622ms 267.366666ms 267.892004ms 283.06359ms 290.706541ms 296.066397ms 297.939326ms 324.455077ms 327.216494ms 334.957207ms 341.10324ms 358.332465ms 364.161732ms 368.862262ms 369.231289ms 395.646699ms 399.618346ms 403.464416ms 406.392154ms 419.678946ms 420.848329ms 421.873402ms 422.2223ms 431.110147ms 438.756726ms 439.362204ms 442.562917ms 443.892767ms 444.71012ms 449.721282ms 450.566273ms 451.511288ms 456.158586ms 459.622892ms 461.754553ms 462.625648ms 470.635399ms 473.879001ms 476.051439ms 476.162731ms 482.3205ms 513.680755ms 524.373733ms 525.576932ms 530.214258ms 555.98563ms 562.625256ms 589.786908ms 634.312931ms 673.462216ms 675.331847ms 680.53894ms 683.271252ms 695.078929ms 709.105444ms 709.640934ms 713.915127ms 725.583608ms 729.113656ms 730.239621ms 733.823164ms 735.955632ms 736.094386ms 736.268587ms 736.601131ms 736.948578ms 737.443036ms 738.182833ms 738.286619ms 739.046774ms 739.214166ms 739.644572ms 739.828753ms 739.944485ms 740.170969ms 740.452097ms 740.854057ms 741.281238ms 741.353014ms 742.048931ms 742.370763ms 742.85621ms 742.966587ms 743.040996ms 743.432288ms 744.152701ms 744.596411ms 744.76965ms 745.202476ms 745.244082ms 745.363757ms 745.388433ms 745.462525ms 745.832041ms 746.065416ms 746.142966ms 746.171399ms 746.248804ms 746.357612ms 746.388308ms 746.96027ms 747.252052ms 747.369181ms 747.613239ms 747.803982ms 748.122958ms 748.161432ms 748.424692ms 749.147848ms 749.246766ms 749.576614ms 749.689492ms 749.739289ms 749.79641ms 749.83088ms 750.068854ms 750.304479ms 750.772838ms 750.879335ms 750.983062ms 751.037493ms 751.105578ms 751.342216ms 751.601103ms 751.903141ms 751.972848ms 752.031469ms 752.085646ms 752.325984ms 752.447734ms 752.467021ms 752.557387ms 752.852564ms 752.897191ms 753.024943ms 753.324886ms 753.434976ms 753.546815ms 753.692722ms 754.149943ms 754.236635ms 754.273066ms 754.524584ms 754.527854ms 754.73698ms 754.894792ms 754.968091ms 755.004249ms 755.333375ms 755.445596ms 755.463143ms 755.818828ms 756.057255ms 756.182524ms 756.183161ms 756.328611ms 756.461061ms 756.73964ms 756.857646ms 756.892822ms 758.129698ms 758.19094ms 758.676096ms 758.732316ms 758.924222ms 759.513432ms 759.561923ms 760.059827ms 761.426379ms 761.795133ms 762.193337ms 762.441084ms 762.750453ms 762.980733ms 763.926802ms 767.378734ms 769.818609ms 785.796909ms 786.630555ms 844.367675ms 939.574799ms]
    Aug 24 12:49:47.157: INFO: 50 %ile: 742.85621ms
    Aug 24 12:49:47.157: INFO: 90 %ile: 758.19094ms
    Aug 24 12:49:47.157: INFO: 99 %ile: 844.367675ms
    Aug 24 12:49:47.157: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:47.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-2999" for this suite. 08/24/23 12:49:47.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:47.188
Aug 24 12:49:47.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:49:47.19
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:47.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:47.251
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 24 12:49:47.265: INFO: Creating deployment "test-recreate-deployment"
Aug 24 12:49:47.287: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 24 12:49:47.305: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 24 12:49:49.323: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 24 12:49:49.329: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 24 12:49:49.345: INFO: Updating deployment test-recreate-deployment
Aug 24 12:49:49.345: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:49:49.530: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1237  ddec4587-9af9-43ba-b44c-19d7ffe96f28 27887 2 2023-08-24 12:49:47 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053fc538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 12:49:49 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-24 12:49:49 +0000 UTC,LastTransitionTime:2023-08-24 12:49:47 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 24 12:49:49.536: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1237  1531e081-47a4-4588-b765-01d4f8d9a459 27883 1 2023-08-24 12:49:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ddec4587-9af9-43ba-b44c-19d7ffe96f28 0xc0052c3d00 0xc0052c3d01}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddec4587-9af9-43ba-b44c-19d7ffe96f28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c3d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:49:49.536: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 24 12:49:49.537: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1237  c1fa604a-627c-47d9-b206-b995bb271828 27874 2 2023-08-24 12:49:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ddec4587-9af9-43ba-b44c-19d7ffe96f28 0xc0052c3be7 0xc0052c3be8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddec4587-9af9-43ba-b44c-19d7ffe96f28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c3c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:49:49.548: INFO: Pod "test-recreate-deployment-cff6dc657-z7glk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-z7glk test-recreate-deployment-cff6dc657- deployment-1237  31479ae4-904d-4faa-aa06-6c9d7c4a1229 27885 0 2023-08-24 12:49:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1531e081-47a4-4588-b765-01d4f8d9a459 0xc00545e210 0xc00545e211}] [] [{kube-controller-manager Update v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1531e081-47a4-4588-b765-01d4f8d9a459\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-874kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-874kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:,StartTime:2023-08-24 12:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:49:49.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1237" for this suite. 08/24/23 12:49:49.573
------------------------------
â€¢ [2.403 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:47.188
    Aug 24 12:49:47.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:49:47.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:47.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:47.251
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 24 12:49:47.265: INFO: Creating deployment "test-recreate-deployment"
    Aug 24 12:49:47.287: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 24 12:49:47.305: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 24 12:49:49.323: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 24 12:49:49.329: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 24 12:49:49.345: INFO: Updating deployment test-recreate-deployment
    Aug 24 12:49:49.345: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:49:49.530: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1237  ddec4587-9af9-43ba-b44c-19d7ffe96f28 27887 2 2023-08-24 12:49:47 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053fc538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 12:49:49 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-24 12:49:49 +0000 UTC,LastTransitionTime:2023-08-24 12:49:47 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 24 12:49:49.536: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1237  1531e081-47a4-4588-b765-01d4f8d9a459 27883 1 2023-08-24 12:49:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ddec4587-9af9-43ba-b44c-19d7ffe96f28 0xc0052c3d00 0xc0052c3d01}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddec4587-9af9-43ba-b44c-19d7ffe96f28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c3d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:49:49.536: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 24 12:49:49.537: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1237  c1fa604a-627c-47d9-b206-b995bb271828 27874 2 2023-08-24 12:49:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ddec4587-9af9-43ba-b44c-19d7ffe96f28 0xc0052c3be7 0xc0052c3be8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddec4587-9af9-43ba-b44c-19d7ffe96f28\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052c3c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:49:49.548: INFO: Pod "test-recreate-deployment-cff6dc657-z7glk" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-z7glk test-recreate-deployment-cff6dc657- deployment-1237  31479ae4-904d-4faa-aa06-6c9d7c4a1229 27885 0 2023-08-24 12:49:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1531e081-47a4-4588-b765-01d4f8d9a459 0xc00545e210 0xc00545e211}] [] [{kube-controller-manager Update v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1531e081-47a4-4588-b765-01d4f8d9a459\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:49:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-874kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-874kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:,StartTime:2023-08-24 12:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:49:49.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1237" for this suite. 08/24/23 12:49:49.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:49:49.603
Aug 24 12:49:49.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:49:49.616
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:49.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:49.646
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-8936 08/24/23 12:49:49.651
STEP: creating service affinity-nodeport-transition in namespace services-8936 08/24/23 12:49:49.651
STEP: creating replication controller affinity-nodeport-transition in namespace services-8936 08/24/23 12:49:49.673
I0824 12:49:49.690893      14 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8936, replica count: 3
I0824 12:49:52.742661      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:49:52.764: INFO: Creating new exec pod
Aug 24 12:49:52.778: INFO: Waiting up to 5m0s for pod "execpod-affinitytd6g2" in namespace "services-8936" to be "running"
Aug 24 12:49:52.790: INFO: Pod "execpod-affinitytd6g2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.503485ms
Aug 24 12:49:54.798: INFO: Pod "execpod-affinitytd6g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019656115s
Aug 24 12:49:54.798: INFO: Pod "execpod-affinitytd6g2" satisfied condition "running"
Aug 24 12:49:55.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 24 12:49:56.504: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 24 12:49:56.504: INFO: stdout: ""
Aug 24 12:49:56.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 10.233.11.116 80'
Aug 24 12:49:57.076: INFO: stderr: "+ nc -v -z -w 2 10.233.11.116 80\nConnection to 10.233.11.116 80 port [tcp/http] succeeded!\n"
Aug 24 12:49:57.076: INFO: stdout: ""
Aug 24 12:49:57.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 32222'
Aug 24 12:49:57.450: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 32222\nConnection to 192.168.121.111 32222 port [tcp/*] succeeded!\n"
Aug 24 12:49:57.450: INFO: stdout: ""
Aug 24 12:49:57.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.127 32222'
Aug 24 12:49:57.939: INFO: stderr: "+ nc -v -z -w 2 192.168.121.127 32222\nConnection to 192.168.121.127 32222 port [tcp/*] succeeded!\n"
Aug 24 12:49:57.939: INFO: stdout: ""
Aug 24 12:49:57.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.127:32222/ ; done'
Aug 24 12:49:58.793: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n"
Aug 24 12:49:58.793: INFO: stdout: "\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-w552j\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-w552j\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-w552j\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-rtspk"
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-w552j
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-w552j
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-w552j
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:58.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.127:32222/ ; done'
Aug 24 12:49:59.741: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n"
Aug 24 12:49:59.745: INFO: stdout: "\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk"
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
Aug 24 12:49:59.745: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8936, will wait for the garbage collector to delete the pods 08/24/23 12:49:59.803
Aug 24 12:49:59.884: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.490455ms
Aug 24 12:50:00.886: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.001647703s
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:50:03.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8936" for this suite. 08/24/23 12:50:03.36
------------------------------
â€¢ [SLOW TEST] [13.784 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:49:49.603
    Aug 24 12:49:49.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:49:49.616
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:49:49.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:49:49.646
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-8936 08/24/23 12:49:49.651
    STEP: creating service affinity-nodeport-transition in namespace services-8936 08/24/23 12:49:49.651
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8936 08/24/23 12:49:49.673
    I0824 12:49:49.690893      14 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8936, replica count: 3
    I0824 12:49:52.742661      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:49:52.764: INFO: Creating new exec pod
    Aug 24 12:49:52.778: INFO: Waiting up to 5m0s for pod "execpod-affinitytd6g2" in namespace "services-8936" to be "running"
    Aug 24 12:49:52.790: INFO: Pod "execpod-affinitytd6g2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.503485ms
    Aug 24 12:49:54.798: INFO: Pod "execpod-affinitytd6g2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019656115s
    Aug 24 12:49:54.798: INFO: Pod "execpod-affinitytd6g2" satisfied condition "running"
    Aug 24 12:49:55.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 24 12:49:56.504: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 24 12:49:56.504: INFO: stdout: ""
    Aug 24 12:49:56.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 10.233.11.116 80'
    Aug 24 12:49:57.076: INFO: stderr: "+ nc -v -z -w 2 10.233.11.116 80\nConnection to 10.233.11.116 80 port [tcp/http] succeeded!\n"
    Aug 24 12:49:57.076: INFO: stdout: ""
    Aug 24 12:49:57.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 32222'
    Aug 24 12:49:57.450: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 32222\nConnection to 192.168.121.111 32222 port [tcp/*] succeeded!\n"
    Aug 24 12:49:57.450: INFO: stdout: ""
    Aug 24 12:49:57.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c nc -v -z -w 2 192.168.121.127 32222'
    Aug 24 12:49:57.939: INFO: stderr: "+ nc -v -z -w 2 192.168.121.127 32222\nConnection to 192.168.121.127 32222 port [tcp/*] succeeded!\n"
    Aug 24 12:49:57.939: INFO: stdout: ""
    Aug 24 12:49:57.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.127:32222/ ; done'
    Aug 24 12:49:58.793: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n"
    Aug 24 12:49:58.793: INFO: stdout: "\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-w552j\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-w552j\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-w552j\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-krljd\naffinity-nodeport-transition-rtspk"
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-w552j
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-w552j
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-w552j
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-krljd
    Aug 24 12:49:58.793: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:58.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-8936 exec execpod-affinitytd6g2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.127:32222/ ; done'
    Aug 24 12:49:59.741: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.127:32222/\n"
    Aug 24 12:49:59.745: INFO: stdout: "\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk\naffinity-nodeport-transition-rtspk"
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Received response from host: affinity-nodeport-transition-rtspk
    Aug 24 12:49:59.745: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8936, will wait for the garbage collector to delete the pods 08/24/23 12:49:59.803
    Aug 24 12:49:59.884: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.490455ms
    Aug 24 12:50:00.886: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.001647703s
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:50:03.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8936" for this suite. 08/24/23 12:50:03.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:50:03.388
Aug 24 12:50:03.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename init-container 08/24/23 12:50:03.401
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:03.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:03.454
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/24/23 12:50:03.461
Aug 24 12:50:03.462: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:50:07.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5226" for this suite. 08/24/23 12:50:07.179
------------------------------
â€¢ [3.806 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:50:03.388
    Aug 24 12:50:03.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename init-container 08/24/23 12:50:03.401
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:03.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:03.454
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/24/23 12:50:03.461
    Aug 24 12:50:03.462: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:50:07.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5226" for this suite. 08/24/23 12:50:07.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:50:07.203
Aug 24 12:50:07.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:50:07.205
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:07.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:07.241
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-f9b98797-562e-4578-baae-dc3b055f3e03 08/24/23 12:50:07.248
STEP: Creating a pod to test consume secrets 08/24/23 12:50:07.256
Aug 24 12:50:07.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a" in namespace "projected-7457" to be "Succeeded or Failed"
Aug 24 12:50:07.285: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.792048ms
Aug 24 12:50:09.293: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020797044s
Aug 24 12:50:11.292: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01951191s
STEP: Saw pod success 08/24/23 12:50:11.292
Aug 24 12:50:11.292: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a" satisfied condition "Succeeded or Failed"
Aug 24 12:50:11.297: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:50:11.313
Aug 24 12:50:11.335: INFO: Waiting for pod pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a to disappear
Aug 24 12:50:11.341: INFO: Pod pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:50:11.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7457" for this suite. 08/24/23 12:50:11.35
------------------------------
â€¢ [4.172 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:50:07.203
    Aug 24 12:50:07.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:50:07.205
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:07.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:07.241
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-f9b98797-562e-4578-baae-dc3b055f3e03 08/24/23 12:50:07.248
    STEP: Creating a pod to test consume secrets 08/24/23 12:50:07.256
    Aug 24 12:50:07.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a" in namespace "projected-7457" to be "Succeeded or Failed"
    Aug 24 12:50:07.285: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.792048ms
    Aug 24 12:50:09.293: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020797044s
    Aug 24 12:50:11.292: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01951191s
    STEP: Saw pod success 08/24/23 12:50:11.292
    Aug 24 12:50:11.292: INFO: Pod "pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a" satisfied condition "Succeeded or Failed"
    Aug 24 12:50:11.297: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:50:11.313
    Aug 24 12:50:11.335: INFO: Waiting for pod pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a to disappear
    Aug 24 12:50:11.341: INFO: Pod pod-projected-secrets-e29cdcfc-f967-4a69-a76f-ff448ee99f8a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:50:11.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7457" for this suite. 08/24/23 12:50:11.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:50:11.379
Aug 24 12:50:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:50:11.382
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:11.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:11.423
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/24/23 12:50:11.426
Aug 24 12:50:11.464: INFO: Waiting up to 5m0s for pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9" in namespace "var-expansion-7129" to be "Succeeded or Failed"
Aug 24 12:50:11.470: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509366ms
Aug 24 12:50:13.478: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013530736s
Aug 24 12:50:15.478: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013468717s
STEP: Saw pod success 08/24/23 12:50:15.478
Aug 24 12:50:15.479: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9" satisfied condition "Succeeded or Failed"
Aug 24 12:50:15.484: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:50:15.494
Aug 24 12:50:15.516: INFO: Waiting for pod var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9 to disappear
Aug 24 12:50:15.522: INFO: Pod var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:50:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7129" for this suite. 08/24/23 12:50:15.531
------------------------------
â€¢ [4.166 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:50:11.379
    Aug 24 12:50:11.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:50:11.382
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:11.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:11.423
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/24/23 12:50:11.426
    Aug 24 12:50:11.464: INFO: Waiting up to 5m0s for pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9" in namespace "var-expansion-7129" to be "Succeeded or Failed"
    Aug 24 12:50:11.470: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509366ms
    Aug 24 12:50:13.478: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013530736s
    Aug 24 12:50:15.478: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013468717s
    STEP: Saw pod success 08/24/23 12:50:15.478
    Aug 24 12:50:15.479: INFO: Pod "var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9" satisfied condition "Succeeded or Failed"
    Aug 24 12:50:15.484: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:50:15.494
    Aug 24 12:50:15.516: INFO: Waiting for pod var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9 to disappear
    Aug 24 12:50:15.522: INFO: Pod var-expansion-c4ab10ce-5b03-498b-97bb-76b23568d9e9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:50:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7129" for this suite. 08/24/23 12:50:15.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:50:15.549
Aug 24 12:50:15.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:50:15.552
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:15.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:15.586
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:50:15.592
Aug 24 12:50:15.610: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6" in namespace "projected-5808" to be "Succeeded or Failed"
Aug 24 12:50:15.621: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.947834ms
Aug 24 12:50:17.638: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027736153s
Aug 24 12:50:19.631: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020705116s
STEP: Saw pod success 08/24/23 12:50:19.631
Aug 24 12:50:19.631: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6" satisfied condition "Succeeded or Failed"
Aug 24 12:50:19.641: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6 container client-container: <nil>
STEP: delete the pod 08/24/23 12:50:19.662
Aug 24 12:50:19.693: INFO: Waiting for pod downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6 to disappear
Aug 24 12:50:19.702: INFO: Pod downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:50:19.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5808" for this suite. 08/24/23 12:50:19.71
------------------------------
â€¢ [4.172 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:50:15.549
    Aug 24 12:50:15.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:50:15.552
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:15.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:15.586
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:50:15.592
    Aug 24 12:50:15.610: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6" in namespace "projected-5808" to be "Succeeded or Failed"
    Aug 24 12:50:15.621: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.947834ms
    Aug 24 12:50:17.638: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027736153s
    Aug 24 12:50:19.631: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020705116s
    STEP: Saw pod success 08/24/23 12:50:19.631
    Aug 24 12:50:19.631: INFO: Pod "downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6" satisfied condition "Succeeded or Failed"
    Aug 24 12:50:19.641: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:50:19.662
    Aug 24 12:50:19.693: INFO: Waiting for pod downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6 to disappear
    Aug 24 12:50:19.702: INFO: Pod downwardapi-volume-79844474-adb7-4047-b549-2664d695e5d6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:50:19.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5808" for this suite. 08/24/23 12:50:19.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:50:19.727
Aug 24 12:50:19.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 12:50:19.729
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:19.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:19.775
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 in namespace container-probe-7226 08/24/23 12:50:19.78
Aug 24 12:50:19.794: INFO: Waiting up to 5m0s for pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0" in namespace "container-probe-7226" to be "not pending"
Aug 24 12:50:19.799: INFO: Pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174102ms
Aug 24 12:50:21.808: INFO: Pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013454541s
Aug 24 12:50:21.808: INFO: Pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0" satisfied condition "not pending"
Aug 24 12:50:21.808: INFO: Started pod busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 in namespace container-probe-7226
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 12:50:21.808
Aug 24 12:50:21.813: INFO: Initial restart count of pod busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 is 0
Aug 24 12:51:12.052: INFO: Restart count of pod container-probe-7226/busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 is now 1 (50.23850926s elapsed)
STEP: deleting the pod 08/24/23 12:51:12.052
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:12.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7226" for this suite. 08/24/23 12:51:12.09
------------------------------
â€¢ [SLOW TEST] [52.387 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:50:19.727
    Aug 24 12:50:19.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 12:50:19.729
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:50:19.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:50:19.775
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 in namespace container-probe-7226 08/24/23 12:50:19.78
    Aug 24 12:50:19.794: INFO: Waiting up to 5m0s for pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0" in namespace "container-probe-7226" to be "not pending"
    Aug 24 12:50:19.799: INFO: Pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174102ms
    Aug 24 12:50:21.808: INFO: Pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013454541s
    Aug 24 12:50:21.808: INFO: Pod "busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0" satisfied condition "not pending"
    Aug 24 12:50:21.808: INFO: Started pod busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 in namespace container-probe-7226
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 12:50:21.808
    Aug 24 12:50:21.813: INFO: Initial restart count of pod busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 is 0
    Aug 24 12:51:12.052: INFO: Restart count of pod container-probe-7226/busybox-32ad0cdb-b3d9-4666-96b5-8ce77e6a14f0 is now 1 (50.23850926s elapsed)
    STEP: deleting the pod 08/24/23 12:51:12.052
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:12.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7226" for this suite. 08/24/23 12:51:12.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:12.119
Aug 24 12:51:12.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename job 08/24/23 12:51:12.122
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:12.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:12.16
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/24/23 12:51:12.164
STEP: Ensuring active pods == parallelism 08/24/23 12:51:12.181
STEP: Orphaning one of the Job's Pods 08/24/23 12:51:14.192
Aug 24 12:51:14.722: INFO: Successfully updated pod "adopt-release-pqfn5"
STEP: Checking that the Job readopts the Pod 08/24/23 12:51:14.722
Aug 24 12:51:14.723: INFO: Waiting up to 15m0s for pod "adopt-release-pqfn5" in namespace "job-1788" to be "adopted"
Aug 24 12:51:14.728: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 5.282528ms
Aug 24 12:51:16.737: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014503649s
Aug 24 12:51:16.737: INFO: Pod "adopt-release-pqfn5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/24/23 12:51:16.737
Aug 24 12:51:17.268: INFO: Successfully updated pod "adopt-release-pqfn5"
STEP: Checking that the Job releases the Pod 08/24/23 12:51:17.268
Aug 24 12:51:17.268: INFO: Waiting up to 15m0s for pod "adopt-release-pqfn5" in namespace "job-1788" to be "released"
Aug 24 12:51:17.292: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 23.614887ms
Aug 24 12:51:19.301: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.032448772s
Aug 24 12:51:19.301: INFO: Pod "adopt-release-pqfn5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:19.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1788" for this suite. 08/24/23 12:51:19.312
------------------------------
â€¢ [SLOW TEST] [7.207 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:12.119
    Aug 24 12:51:12.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename job 08/24/23 12:51:12.122
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:12.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:12.16
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/24/23 12:51:12.164
    STEP: Ensuring active pods == parallelism 08/24/23 12:51:12.181
    STEP: Orphaning one of the Job's Pods 08/24/23 12:51:14.192
    Aug 24 12:51:14.722: INFO: Successfully updated pod "adopt-release-pqfn5"
    STEP: Checking that the Job readopts the Pod 08/24/23 12:51:14.722
    Aug 24 12:51:14.723: INFO: Waiting up to 15m0s for pod "adopt-release-pqfn5" in namespace "job-1788" to be "adopted"
    Aug 24 12:51:14.728: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 5.282528ms
    Aug 24 12:51:16.737: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.014503649s
    Aug 24 12:51:16.737: INFO: Pod "adopt-release-pqfn5" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/24/23 12:51:16.737
    Aug 24 12:51:17.268: INFO: Successfully updated pod "adopt-release-pqfn5"
    STEP: Checking that the Job releases the Pod 08/24/23 12:51:17.268
    Aug 24 12:51:17.268: INFO: Waiting up to 15m0s for pod "adopt-release-pqfn5" in namespace "job-1788" to be "released"
    Aug 24 12:51:17.292: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 23.614887ms
    Aug 24 12:51:19.301: INFO: Pod "adopt-release-pqfn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.032448772s
    Aug 24 12:51:19.301: INFO: Pod "adopt-release-pqfn5" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:19.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1788" for this suite. 08/24/23 12:51:19.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:19.33
Aug 24 12:51:19.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:51:19.333
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:19.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:19.365
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:51:19.371
Aug 24 12:51:19.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa" in namespace "projected-6860" to be "Succeeded or Failed"
Aug 24 12:51:19.396: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.698434ms
Aug 24 12:51:21.406: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.015717371s
Aug 24 12:51:23.432: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Running", Reason="", readiness=false. Elapsed: 4.042095887s
Aug 24 12:51:25.407: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016760615s
STEP: Saw pod success 08/24/23 12:51:25.408
Aug 24 12:51:25.409: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa" satisfied condition "Succeeded or Failed"
Aug 24 12:51:25.417: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa container client-container: <nil>
STEP: delete the pod 08/24/23 12:51:25.433
Aug 24 12:51:25.455: INFO: Waiting for pod downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa to disappear
Aug 24 12:51:25.460: INFO: Pod downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:25.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6860" for this suite. 08/24/23 12:51:25.47
------------------------------
â€¢ [SLOW TEST] [6.153 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:19.33
    Aug 24 12:51:19.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:51:19.333
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:19.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:19.365
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:51:19.371
    Aug 24 12:51:19.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa" in namespace "projected-6860" to be "Succeeded or Failed"
    Aug 24 12:51:19.396: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.698434ms
    Aug 24 12:51:21.406: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.015717371s
    Aug 24 12:51:23.432: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Running", Reason="", readiness=false. Elapsed: 4.042095887s
    Aug 24 12:51:25.407: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016760615s
    STEP: Saw pod success 08/24/23 12:51:25.408
    Aug 24 12:51:25.409: INFO: Pod "downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa" satisfied condition "Succeeded or Failed"
    Aug 24 12:51:25.417: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa container client-container: <nil>
    STEP: delete the pod 08/24/23 12:51:25.433
    Aug 24 12:51:25.455: INFO: Waiting for pod downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa to disappear
    Aug 24 12:51:25.460: INFO: Pod downwardapi-volume-ace5bc77-fdfb-4ed1-b1af-c85cf858f0aa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:25.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6860" for this suite. 08/24/23 12:51:25.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:25.485
Aug 24 12:51:25.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 12:51:25.488
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:25.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:25.52
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/24/23 12:51:25.526
Aug 24 12:51:25.544: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1113  f2fc250a-0446-42df-8fe9-153df8cce098 29323 0 2023-08-24 12:51:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-24 12:51:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5pm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5pm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:51:25.546: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1113" to be "running and ready"
Aug 24 12:51:25.553: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62375ms
Aug 24 12:51:25.553: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:51:27.561: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014879875s
Aug 24 12:51:27.561: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 24 12:51:27.562: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/24/23 12:51:27.562
Aug 24 12:51:27.562: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1113 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:51:27.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:51:27.564: INFO: ExecWithOptions: Clientset creation
Aug 24 12:51:27.564: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1113/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/24/23 12:51:27.715
Aug 24 12:51:27.716: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1113 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:51:27.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:51:27.717: INFO: ExecWithOptions: Clientset creation
Aug 24 12:51:27.717: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1113/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 12:51:27.875: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:27.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1113" for this suite. 08/24/23 12:51:27.911
------------------------------
â€¢ [2.441 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:25.485
    Aug 24 12:51:25.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 12:51:25.488
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:25.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:25.52
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/24/23 12:51:25.526
    Aug 24 12:51:25.544: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1113  f2fc250a-0446-42df-8fe9-153df8cce098 29323 0 2023-08-24 12:51:25 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-24 12:51:25 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5pm2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5pm2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:51:25.546: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1113" to be "running and ready"
    Aug 24 12:51:25.553: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62375ms
    Aug 24 12:51:25.553: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:51:27.561: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014879875s
    Aug 24 12:51:27.561: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 24 12:51:27.562: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/24/23 12:51:27.562
    Aug 24 12:51:27.562: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1113 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:51:27.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:51:27.564: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:51:27.564: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1113/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/24/23 12:51:27.715
    Aug 24 12:51:27.716: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1113 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:51:27.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:51:27.717: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:51:27.717: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1113/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 12:51:27.875: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:27.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1113" for this suite. 08/24/23 12:51:27.911
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:27.928
Aug 24 12:51:27.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:51:27.932
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:27.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:27.969
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/24/23 12:51:27.974
Aug 24 12:51:27.994: INFO: Waiting up to 5m0s for pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3" in namespace "downward-api-4095" to be "Succeeded or Failed"
Aug 24 12:51:28.007: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.223254ms
Aug 24 12:51:30.016: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022030081s
Aug 24 12:51:32.018: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024166289s
STEP: Saw pod success 08/24/23 12:51:32.018
Aug 24 12:51:32.019: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3" satisfied condition "Succeeded or Failed"
Aug 24 12:51:32.026: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:51:32.041
Aug 24 12:51:32.069: INFO: Waiting for pod downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3 to disappear
Aug 24 12:51:32.076: INFO: Pod downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:32.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4095" for this suite. 08/24/23 12:51:32.09
------------------------------
â€¢ [4.175 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:27.928
    Aug 24 12:51:27.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:51:27.932
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:27.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:27.969
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/24/23 12:51:27.974
    Aug 24 12:51:27.994: INFO: Waiting up to 5m0s for pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3" in namespace "downward-api-4095" to be "Succeeded or Failed"
    Aug 24 12:51:28.007: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.223254ms
    Aug 24 12:51:30.016: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022030081s
    Aug 24 12:51:32.018: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024166289s
    STEP: Saw pod success 08/24/23 12:51:32.018
    Aug 24 12:51:32.019: INFO: Pod "downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3" satisfied condition "Succeeded or Failed"
    Aug 24 12:51:32.026: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:51:32.041
    Aug 24 12:51:32.069: INFO: Waiting for pod downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3 to disappear
    Aug 24 12:51:32.076: INFO: Pod downward-api-e0b37d56-43d9-4f1e-84d9-81e7e5be9bf3 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:32.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4095" for this suite. 08/24/23 12:51:32.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:32.109
Aug 24 12:51:32.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:51:32.112
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:32.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:32.144
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 24 12:51:32.165: INFO: Waiting up to 2m0s for pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" in namespace "var-expansion-6197" to be "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:51:32.169: INFO: Pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224346ms
Aug 24 12:51:34.175: INFO: Pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010414746s
Aug 24 12:51:34.175: INFO: Pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:51:34.175: INFO: Deleting pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" in namespace "var-expansion-6197"
Aug 24 12:51:34.190: INFO: Wait up to 5m0s for pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6197" for this suite. 08/24/23 12:51:36.217
------------------------------
â€¢ [4.122 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:32.109
    Aug 24 12:51:32.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:51:32.112
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:32.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:32.144
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 24 12:51:32.165: INFO: Waiting up to 2m0s for pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" in namespace "var-expansion-6197" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:51:32.169: INFO: Pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.224346ms
    Aug 24 12:51:34.175: INFO: Pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010414746s
    Aug 24 12:51:34.175: INFO: Pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:51:34.175: INFO: Deleting pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" in namespace "var-expansion-6197"
    Aug 24 12:51:34.190: INFO: Wait up to 5m0s for pod "var-expansion-3246b1d7-cd7f-4c76-95da-16c8dd223c91" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6197" for this suite. 08/24/23 12:51:36.217
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:36.233
Aug 24 12:51:36.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:51:36.236
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:36.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:36.267
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/24/23 12:51:36.273
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:51:36.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5794" for this suite. 08/24/23 12:51:36.289
------------------------------
â€¢ [0.069 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:36.233
    Aug 24 12:51:36.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:51:36.236
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:36.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:36.267
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/24/23 12:51:36.273
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:51:36.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5794" for this suite. 08/24/23 12:51:36.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:51:36.311
Aug 24 12:51:36.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename endpointslice 08/24/23 12:51:36.313
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:36.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:36.343
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/24/23 12:51:41.524
STEP: referencing matching pods with named port 08/24/23 12:51:46.544
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/24/23 12:51:51.566
STEP: recreating EndpointSlices after they've been deleted 08/24/23 12:51:56.583
Aug 24 12:51:56.622: INFO: EndpointSlice for Service endpointslice-647/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:06.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-647" for this suite. 08/24/23 12:52:06.657
------------------------------
â€¢ [SLOW TEST] [30.357 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:51:36.311
    Aug 24 12:51:36.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename endpointslice 08/24/23 12:51:36.313
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:51:36.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:51:36.343
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/24/23 12:51:41.524
    STEP: referencing matching pods with named port 08/24/23 12:51:46.544
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/24/23 12:51:51.566
    STEP: recreating EndpointSlices after they've been deleted 08/24/23 12:51:56.583
    Aug 24 12:51:56.622: INFO: EndpointSlice for Service endpointslice-647/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:06.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-647" for this suite. 08/24/23 12:52:06.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:06.679
Aug 24 12:52:06.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:52:06.681
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:06.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:06.712
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 12:52:06.717
Aug 24 12:52:06.734: INFO: Waiting up to 5m0s for pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176" in namespace "emptydir-274" to be "Succeeded or Failed"
Aug 24 12:52:06.745: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Pending", Reason="", readiness=false. Elapsed: 10.797918ms
Aug 24 12:52:08.753: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Running", Reason="", readiness=false. Elapsed: 2.019449407s
Aug 24 12:52:10.754: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Running", Reason="", readiness=false. Elapsed: 4.020020792s
Aug 24 12:52:12.758: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024407564s
STEP: Saw pod success 08/24/23 12:52:12.758
Aug 24 12:52:12.759: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176" satisfied condition "Succeeded or Failed"
Aug 24 12:52:12.770: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-6fb0ea8b-ac36-4936-932d-47bdebabc176 container test-container: <nil>
STEP: delete the pod 08/24/23 12:52:12.79
Aug 24 12:52:12.809: INFO: Waiting for pod pod-6fb0ea8b-ac36-4936-932d-47bdebabc176 to disappear
Aug 24 12:52:12.818: INFO: Pod pod-6fb0ea8b-ac36-4936-932d-47bdebabc176 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:12.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-274" for this suite. 08/24/23 12:52:12.83
------------------------------
â€¢ [SLOW TEST] [6.163 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:06.679
    Aug 24 12:52:06.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:52:06.681
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:06.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:06.712
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 12:52:06.717
    Aug 24 12:52:06.734: INFO: Waiting up to 5m0s for pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176" in namespace "emptydir-274" to be "Succeeded or Failed"
    Aug 24 12:52:06.745: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Pending", Reason="", readiness=false. Elapsed: 10.797918ms
    Aug 24 12:52:08.753: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Running", Reason="", readiness=false. Elapsed: 2.019449407s
    Aug 24 12:52:10.754: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Running", Reason="", readiness=false. Elapsed: 4.020020792s
    Aug 24 12:52:12.758: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024407564s
    STEP: Saw pod success 08/24/23 12:52:12.758
    Aug 24 12:52:12.759: INFO: Pod "pod-6fb0ea8b-ac36-4936-932d-47bdebabc176" satisfied condition "Succeeded or Failed"
    Aug 24 12:52:12.770: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-6fb0ea8b-ac36-4936-932d-47bdebabc176 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:52:12.79
    Aug 24 12:52:12.809: INFO: Waiting for pod pod-6fb0ea8b-ac36-4936-932d-47bdebabc176 to disappear
    Aug 24 12:52:12.818: INFO: Pod pod-6fb0ea8b-ac36-4936-932d-47bdebabc176 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:12.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-274" for this suite. 08/24/23 12:52:12.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:12.848
Aug 24 12:52:12.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:52:12.851
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:12.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:12.882
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/24/23 12:52:12.888
STEP: submitting the pod to kubernetes 08/24/23 12:52:12.888
Aug 24 12:52:12.903: INFO: Waiting up to 5m0s for pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" in namespace "pods-2510" to be "running and ready"
Aug 24 12:52:12.909: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.954585ms
Aug 24 12:52:12.909: INFO: The phase of Pod pod-update-fa459655-e0c0-4766-befe-20c66240b18b is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:52:14.918: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014195722s
Aug 24 12:52:14.918: INFO: The phase of Pod pod-update-fa459655-e0c0-4766-befe-20c66240b18b is Running (Ready = true)
Aug 24 12:52:14.918: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/24/23 12:52:14.923
STEP: updating the pod 08/24/23 12:52:14.927
Aug 24 12:52:15.453: INFO: Successfully updated pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b"
Aug 24 12:52:15.453: INFO: Waiting up to 5m0s for pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" in namespace "pods-2510" to be "running"
Aug 24 12:52:15.460: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b": Phase="Running", Reason="", readiness=true. Elapsed: 6.85731ms
Aug 24 12:52:15.461: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/24/23 12:52:15.461
Aug 24 12:52:15.480: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:15.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2510" for this suite. 08/24/23 12:52:15.489
------------------------------
â€¢ [2.653 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:12.848
    Aug 24 12:52:12.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:52:12.851
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:12.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:12.882
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/24/23 12:52:12.888
    STEP: submitting the pod to kubernetes 08/24/23 12:52:12.888
    Aug 24 12:52:12.903: INFO: Waiting up to 5m0s for pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" in namespace "pods-2510" to be "running and ready"
    Aug 24 12:52:12.909: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.954585ms
    Aug 24 12:52:12.909: INFO: The phase of Pod pod-update-fa459655-e0c0-4766-befe-20c66240b18b is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:52:14.918: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014195722s
    Aug 24 12:52:14.918: INFO: The phase of Pod pod-update-fa459655-e0c0-4766-befe-20c66240b18b is Running (Ready = true)
    Aug 24 12:52:14.918: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/24/23 12:52:14.923
    STEP: updating the pod 08/24/23 12:52:14.927
    Aug 24 12:52:15.453: INFO: Successfully updated pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b"
    Aug 24 12:52:15.453: INFO: Waiting up to 5m0s for pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" in namespace "pods-2510" to be "running"
    Aug 24 12:52:15.460: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b": Phase="Running", Reason="", readiness=true. Elapsed: 6.85731ms
    Aug 24 12:52:15.461: INFO: Pod "pod-update-fa459655-e0c0-4766-befe-20c66240b18b" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/24/23 12:52:15.461
    Aug 24 12:52:15.480: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:15.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2510" for this suite. 08/24/23 12:52:15.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:15.509
Aug 24 12:52:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:52:15.511
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:15.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:15.542
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:52:15.576
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:52:16.713
STEP: Deploying the webhook pod 08/24/23 12:52:16.724
STEP: Wait for the deployment to be ready 08/24/23 12:52:16.745
Aug 24 12:52:16.757: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 12:52:18.775
STEP: Verifying the service has paired with the endpoint 08/24/23 12:52:18.814
Aug 24 12:52:19.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 24 12:52:19.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7273-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 12:52:20.351
STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 12:52:20.405
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:23.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4489" for this suite. 08/24/23 12:52:23.268
STEP: Destroying namespace "webhook-4489-markers" for this suite. 08/24/23 12:52:23.28
------------------------------
â€¢ [SLOW TEST] [7.799 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:15.509
    Aug 24 12:52:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:52:15.511
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:15.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:15.542
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:52:15.576
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:52:16.713
    STEP: Deploying the webhook pod 08/24/23 12:52:16.724
    STEP: Wait for the deployment to be ready 08/24/23 12:52:16.745
    Aug 24 12:52:16.757: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 12:52:18.775
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:52:18.814
    Aug 24 12:52:19.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 24 12:52:19.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7273-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 12:52:20.351
    STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 12:52:20.405
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:23.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4489" for this suite. 08/24/23 12:52:23.268
    STEP: Destroying namespace "webhook-4489-markers" for this suite. 08/24/23 12:52:23.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:23.308
Aug 24 12:52:23.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:52:23.326
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:23.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:23.387
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-e9a6a897-7fb4-40b2-aa46-1d0155180810 08/24/23 12:52:23.392
STEP: Creating a pod to test consume configMaps 08/24/23 12:52:23.402
Aug 24 12:52:23.415: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6" in namespace "projected-4971" to be "Succeeded or Failed"
Aug 24 12:52:23.423: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.202365ms
Aug 24 12:52:25.431: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016161998s
Aug 24 12:52:27.437: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02250166s
STEP: Saw pod success 08/24/23 12:52:27.438
Aug 24 12:52:27.438: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6" satisfied condition "Succeeded or Failed"
Aug 24 12:52:27.447: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:52:27.457
Aug 24 12:52:27.485: INFO: Waiting for pod pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6 to disappear
Aug 24 12:52:27.492: INFO: Pod pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4971" for this suite. 08/24/23 12:52:27.501
------------------------------
â€¢ [4.203 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:23.308
    Aug 24 12:52:23.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:52:23.326
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:23.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:23.387
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-e9a6a897-7fb4-40b2-aa46-1d0155180810 08/24/23 12:52:23.392
    STEP: Creating a pod to test consume configMaps 08/24/23 12:52:23.402
    Aug 24 12:52:23.415: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6" in namespace "projected-4971" to be "Succeeded or Failed"
    Aug 24 12:52:23.423: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.202365ms
    Aug 24 12:52:25.431: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016161998s
    Aug 24 12:52:27.437: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02250166s
    STEP: Saw pod success 08/24/23 12:52:27.438
    Aug 24 12:52:27.438: INFO: Pod "pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6" satisfied condition "Succeeded or Failed"
    Aug 24 12:52:27.447: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:52:27.457
    Aug 24 12:52:27.485: INFO: Waiting for pod pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6 to disappear
    Aug 24 12:52:27.492: INFO: Pod pod-projected-configmaps-6a2479f2-f422-4763-85ba-d15f866bdce6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4971" for this suite. 08/24/23 12:52:27.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:27.512
Aug 24 12:52:27.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename namespaces 08/24/23 12:52:27.515
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:27.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:27.57
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/24/23 12:52:27.574
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:27.598
STEP: Creating a pod in the namespace 08/24/23 12:52:27.603
STEP: Waiting for the pod to have running status 08/24/23 12:52:27.615
Aug 24 12:52:27.615: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2593" to be "running"
Aug 24 12:52:27.624: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717765ms
Aug 24 12:52:29.633: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017907745s
Aug 24 12:52:29.634: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/24/23 12:52:29.634
STEP: Waiting for the namespace to be removed. 08/24/23 12:52:29.648
STEP: Recreating the namespace 08/24/23 12:52:40.655
STEP: Verifying there are no pods in the namespace 08/24/23 12:52:40.688
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:40.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2896" for this suite. 08/24/23 12:52:40.711
STEP: Destroying namespace "nsdeletetest-2593" for this suite. 08/24/23 12:52:40.724
Aug 24 12:52:40.736: INFO: Namespace nsdeletetest-2593 was already deleted
STEP: Destroying namespace "nsdeletetest-9708" for this suite. 08/24/23 12:52:40.736
------------------------------
â€¢ [SLOW TEST] [13.240 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:27.512
    Aug 24 12:52:27.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:52:27.515
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:27.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:27.57
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/24/23 12:52:27.574
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:27.598
    STEP: Creating a pod in the namespace 08/24/23 12:52:27.603
    STEP: Waiting for the pod to have running status 08/24/23 12:52:27.615
    Aug 24 12:52:27.615: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-2593" to be "running"
    Aug 24 12:52:27.624: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717765ms
    Aug 24 12:52:29.633: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017907745s
    Aug 24 12:52:29.634: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/24/23 12:52:29.634
    STEP: Waiting for the namespace to be removed. 08/24/23 12:52:29.648
    STEP: Recreating the namespace 08/24/23 12:52:40.655
    STEP: Verifying there are no pods in the namespace 08/24/23 12:52:40.688
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:40.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2896" for this suite. 08/24/23 12:52:40.711
    STEP: Destroying namespace "nsdeletetest-2593" for this suite. 08/24/23 12:52:40.724
    Aug 24 12:52:40.736: INFO: Namespace nsdeletetest-2593 was already deleted
    STEP: Destroying namespace "nsdeletetest-9708" for this suite. 08/24/23 12:52:40.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:40.768
Aug 24 12:52:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:52:40.772
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:40.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:40.813
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:52:40.846
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:52:41.297
STEP: Deploying the webhook pod 08/24/23 12:52:41.312
STEP: Wait for the deployment to be ready 08/24/23 12:52:41.334
Aug 24 12:52:41.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:52:43.38
STEP: Verifying the service has paired with the endpoint 08/24/23 12:52:43.402
Aug 24 12:52:44.404: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/24/23 12:52:44.418
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/24/23 12:52:44.468
STEP: Creating a configMap that should not be mutated 08/24/23 12:52:44.48
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/24/23 12:52:44.496
STEP: Creating a configMap that should be mutated 08/24/23 12:52:44.511
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:52:44.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1332" for this suite. 08/24/23 12:52:44.663
STEP: Destroying namespace "webhook-1332-markers" for this suite. 08/24/23 12:52:44.682
------------------------------
â€¢ [3.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:40.768
    Aug 24 12:52:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:52:40.772
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:40.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:40.813
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:52:40.846
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:52:41.297
    STEP: Deploying the webhook pod 08/24/23 12:52:41.312
    STEP: Wait for the deployment to be ready 08/24/23 12:52:41.334
    Aug 24 12:52:41.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:52:43.38
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:52:43.402
    Aug 24 12:52:44.404: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/24/23 12:52:44.418
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/24/23 12:52:44.468
    STEP: Creating a configMap that should not be mutated 08/24/23 12:52:44.48
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/24/23 12:52:44.496
    STEP: Creating a configMap that should be mutated 08/24/23 12:52:44.511
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:52:44.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1332" for this suite. 08/24/23 12:52:44.663
    STEP: Destroying namespace "webhook-1332-markers" for this suite. 08/24/23 12:52:44.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:52:44.703
Aug 24 12:52:44.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:52:44.712
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:44.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:44.746
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/24/23 12:52:44.752
Aug 24 12:52:44.773: INFO: Waiting up to 2m0s for pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" in namespace "var-expansion-3288" to be "running"
Aug 24 12:52:44.786: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.467141ms
Aug 24 12:52:46.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019810537s
Aug 24 12:52:48.799: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026225847s
Aug 24 12:52:50.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020565347s
Aug 24 12:52:52.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022989172s
Aug 24 12:52:54.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023358783s
Aug 24 12:52:56.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023619186s
Aug 24 12:52:58.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019307125s
Aug 24 12:53:00.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.0232011s
Aug 24 12:53:02.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020031934s
Aug 24 12:53:04.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019842395s
Aug 24 12:53:06.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01894307s
Aug 24 12:53:08.800: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.027159788s
Aug 24 12:53:10.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.022278012s
Aug 24 12:53:12.799: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025488579s
Aug 24 12:53:14.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022333889s
Aug 24 12:53:16.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.020223641s
Aug 24 12:53:18.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.02415456s
Aug 24 12:53:20.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019614186s
Aug 24 12:53:22.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019275248s
Aug 24 12:53:24.801: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 40.028021342s
Aug 24 12:53:26.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02104119s
Aug 24 12:53:28.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023071981s
Aug 24 12:53:30.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.02047384s
Aug 24 12:53:32.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.022019974s
Aug 24 12:53:34.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019897165s
Aug 24 12:53:36.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022183457s
Aug 24 12:53:38.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 54.021512327s
Aug 24 12:53:40.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 56.022238419s
Aug 24 12:53:42.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 58.022937322s
Aug 24 12:53:44.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020587458s
Aug 24 12:53:46.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.022707793s
Aug 24 12:53:48.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022384391s
Aug 24 12:53:50.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023881691s
Aug 24 12:53:52.799: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025673235s
Aug 24 12:53:54.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018430102s
Aug 24 12:53:56.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020145911s
Aug 24 12:53:58.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020787287s
Aug 24 12:54:00.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022844591s
Aug 24 12:54:02.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.020904018s
Aug 24 12:54:04.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.021419708s
Aug 24 12:54:06.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.019310772s
Aug 24 12:54:08.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.022434757s
Aug 24 12:54:10.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022073342s
Aug 24 12:54:12.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021331326s
Aug 24 12:54:14.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021009243s
Aug 24 12:54:16.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020303635s
Aug 24 12:54:18.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024205867s
Aug 24 12:54:20.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.020891064s
Aug 24 12:54:22.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.019483089s
Aug 24 12:54:24.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02119001s
Aug 24 12:54:26.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019918064s
Aug 24 12:54:28.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.020838882s
Aug 24 12:54:30.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.018921894s
Aug 24 12:54:32.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.020751858s
Aug 24 12:54:34.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02179022s
Aug 24 12:54:36.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020157686s
Aug 24 12:54:38.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022010292s
Aug 24 12:54:40.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022617565s
Aug 24 12:54:42.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019907627s
Aug 24 12:54:44.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022017979s
Aug 24 12:54:44.801: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028212831s
STEP: updating the pod 08/24/23 12:54:44.802
Aug 24 12:54:45.325: INFO: Successfully updated pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7"
STEP: waiting for pod running 08/24/23 12:54:45.325
Aug 24 12:54:45.325: INFO: Waiting up to 2m0s for pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" in namespace "var-expansion-3288" to be "running"
Aug 24 12:54:45.337: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.857727ms
Aug 24 12:54:47.348: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02248608s
Aug 24 12:54:47.348: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" satisfied condition "running"
STEP: deleting the pod gracefully 08/24/23 12:54:47.348
Aug 24 12:54:47.349: INFO: Deleting pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" in namespace "var-expansion-3288"
Aug 24 12:54:47.369: INFO: Wait up to 5m0s for pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:55:19.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3288" for this suite. 08/24/23 12:55:19.458
------------------------------
â€¢ [SLOW TEST] [154.777 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:52:44.703
    Aug 24 12:52:44.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:52:44.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:52:44.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:52:44.746
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/24/23 12:52:44.752
    Aug 24 12:52:44.773: INFO: Waiting up to 2m0s for pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" in namespace "var-expansion-3288" to be "running"
    Aug 24 12:52:44.786: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.467141ms
    Aug 24 12:52:46.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019810537s
    Aug 24 12:52:48.799: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026225847s
    Aug 24 12:52:50.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020565347s
    Aug 24 12:52:52.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022989172s
    Aug 24 12:52:54.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023358783s
    Aug 24 12:52:56.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023619186s
    Aug 24 12:52:58.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019307125s
    Aug 24 12:53:00.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.0232011s
    Aug 24 12:53:02.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.020031934s
    Aug 24 12:53:04.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019842395s
    Aug 24 12:53:06.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01894307s
    Aug 24 12:53:08.800: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.027159788s
    Aug 24 12:53:10.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.022278012s
    Aug 24 12:53:12.799: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025488579s
    Aug 24 12:53:14.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022333889s
    Aug 24 12:53:16.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.020223641s
    Aug 24 12:53:18.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.02415456s
    Aug 24 12:53:20.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019614186s
    Aug 24 12:53:22.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019275248s
    Aug 24 12:53:24.801: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 40.028021342s
    Aug 24 12:53:26.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02104119s
    Aug 24 12:53:28.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023071981s
    Aug 24 12:53:30.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.02047384s
    Aug 24 12:53:32.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.022019974s
    Aug 24 12:53:34.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019897165s
    Aug 24 12:53:36.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022183457s
    Aug 24 12:53:38.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 54.021512327s
    Aug 24 12:53:40.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 56.022238419s
    Aug 24 12:53:42.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 58.022937322s
    Aug 24 12:53:44.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020587458s
    Aug 24 12:53:46.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.022707793s
    Aug 24 12:53:48.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022384391s
    Aug 24 12:53:50.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.023881691s
    Aug 24 12:53:52.799: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025673235s
    Aug 24 12:53:54.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018430102s
    Aug 24 12:53:56.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020145911s
    Aug 24 12:53:58.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020787287s
    Aug 24 12:54:00.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022844591s
    Aug 24 12:54:02.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.020904018s
    Aug 24 12:54:04.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.021419708s
    Aug 24 12:54:06.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.019310772s
    Aug 24 12:54:08.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.022434757s
    Aug 24 12:54:10.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022073342s
    Aug 24 12:54:12.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.021331326s
    Aug 24 12:54:14.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021009243s
    Aug 24 12:54:16.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.020303635s
    Aug 24 12:54:18.797: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024205867s
    Aug 24 12:54:20.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.020891064s
    Aug 24 12:54:22.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.019483089s
    Aug 24 12:54:24.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02119001s
    Aug 24 12:54:26.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019918064s
    Aug 24 12:54:28.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.020838882s
    Aug 24 12:54:30.792: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.018921894s
    Aug 24 12:54:32.794: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.020751858s
    Aug 24 12:54:34.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02179022s
    Aug 24 12:54:36.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020157686s
    Aug 24 12:54:38.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022010292s
    Aug 24 12:54:40.796: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022617565s
    Aug 24 12:54:42.793: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019907627s
    Aug 24 12:54:44.795: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022017979s
    Aug 24 12:54:44.801: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028212831s
    STEP: updating the pod 08/24/23 12:54:44.802
    Aug 24 12:54:45.325: INFO: Successfully updated pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7"
    STEP: waiting for pod running 08/24/23 12:54:45.325
    Aug 24 12:54:45.325: INFO: Waiting up to 2m0s for pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" in namespace "var-expansion-3288" to be "running"
    Aug 24 12:54:45.337: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.857727ms
    Aug 24 12:54:47.348: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02248608s
    Aug 24 12:54:47.348: INFO: Pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" satisfied condition "running"
    STEP: deleting the pod gracefully 08/24/23 12:54:47.348
    Aug 24 12:54:47.349: INFO: Deleting pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" in namespace "var-expansion-3288"
    Aug 24 12:54:47.369: INFO: Wait up to 5m0s for pod "var-expansion-5b8771d2-fa4d-4e94-bf6e-0678636055c7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:55:19.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3288" for this suite. 08/24/23 12:55:19.458
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:55:19.485
Aug 24 12:55:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename security-context-test 08/24/23 12:55:19.489
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:19.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:19.54
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 24 12:55:19.567: INFO: Waiting up to 5m0s for pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6" in namespace "security-context-test-3979" to be "Succeeded or Failed"
Aug 24 12:55:19.577: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.50449ms
Aug 24 12:55:21.585: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017863602s
Aug 24 12:55:23.585: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017454298s
Aug 24 12:55:23.585: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 12:55:23.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3979" for this suite. 08/24/23 12:55:23.594
------------------------------
â€¢ [4.123 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:55:19.485
    Aug 24 12:55:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename security-context-test 08/24/23 12:55:19.489
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:19.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:19.54
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 24 12:55:19.567: INFO: Waiting up to 5m0s for pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6" in namespace "security-context-test-3979" to be "Succeeded or Failed"
    Aug 24 12:55:19.577: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.50449ms
    Aug 24 12:55:21.585: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017863602s
    Aug 24 12:55:23.585: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017454298s
    Aug 24 12:55:23.585: INFO: Pod "busybox-user-65534-39941976-142b-4687-a7a8-4810afcf20b6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:55:23.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3979" for this suite. 08/24/23 12:55:23.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:55:23.609
Aug 24 12:55:23.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename namespaces 08/24/23 12:55:23.611
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:23.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:23.649
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/24/23 12:55:23.655
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:23.687
STEP: Creating a service in the namespace 08/24/23 12:55:23.691
STEP: Deleting the namespace 08/24/23 12:55:23.712
STEP: Waiting for the namespace to be removed. 08/24/23 12:55:23.736
STEP: Recreating the namespace 08/24/23 12:55:29.744
STEP: Verifying there is no service in the namespace 08/24/23 12:55:29.772
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:55:29.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7975" for this suite. 08/24/23 12:55:29.794
STEP: Destroying namespace "nsdeletetest-6532" for this suite. 08/24/23 12:55:29.805
Aug 24 12:55:29.810: INFO: Namespace nsdeletetest-6532 was already deleted
STEP: Destroying namespace "nsdeletetest-1683" for this suite. 08/24/23 12:55:29.81
------------------------------
â€¢ [SLOW TEST] [6.212 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:55:23.609
    Aug 24 12:55:23.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:55:23.611
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:23.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:23.649
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/24/23 12:55:23.655
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:23.687
    STEP: Creating a service in the namespace 08/24/23 12:55:23.691
    STEP: Deleting the namespace 08/24/23 12:55:23.712
    STEP: Waiting for the namespace to be removed. 08/24/23 12:55:23.736
    STEP: Recreating the namespace 08/24/23 12:55:29.744
    STEP: Verifying there is no service in the namespace 08/24/23 12:55:29.772
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:55:29.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7975" for this suite. 08/24/23 12:55:29.794
    STEP: Destroying namespace "nsdeletetest-6532" for this suite. 08/24/23 12:55:29.805
    Aug 24 12:55:29.810: INFO: Namespace nsdeletetest-6532 was already deleted
    STEP: Destroying namespace "nsdeletetest-1683" for this suite. 08/24/23 12:55:29.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:55:29.822
Aug 24 12:55:29.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename ephemeral-containers-test 08/24/23 12:55:29.824
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:29.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:29.852
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/24/23 12:55:29.857
Aug 24 12:55:29.872: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8428" to be "running and ready"
Aug 24 12:55:29.883: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.051852ms
Aug 24 12:55:29.883: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:55:31.894: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022296668s
Aug 24 12:55:31.894: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 24 12:55:31.895: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/24/23 12:55:31.902
Aug 24 12:55:31.937: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8428" to be "container debugger running"
Aug 24 12:55:31.943: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.289046ms
Aug 24 12:55:33.951: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013584441s
Aug 24 12:55:35.949: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011915044s
Aug 24 12:55:35.949: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/24/23 12:55:35.95
Aug 24 12:55:35.950: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8428 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:55:35.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 12:55:35.952: INFO: ExecWithOptions: Clientset creation
Aug 24 12:55:35.952: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-8428/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 24 12:55:36.095: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:55:36.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-8428" for this suite. 08/24/23 12:55:36.134
------------------------------
â€¢ [SLOW TEST] [6.324 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:55:29.822
    Aug 24 12:55:29.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/24/23 12:55:29.824
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:29.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:29.852
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/24/23 12:55:29.857
    Aug 24 12:55:29.872: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8428" to be "running and ready"
    Aug 24 12:55:29.883: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.051852ms
    Aug 24 12:55:29.883: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:55:31.894: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022296668s
    Aug 24 12:55:31.894: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 24 12:55:31.895: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/24/23 12:55:31.902
    Aug 24 12:55:31.937: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8428" to be "container debugger running"
    Aug 24 12:55:31.943: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.289046ms
    Aug 24 12:55:33.951: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013584441s
    Aug 24 12:55:35.949: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011915044s
    Aug 24 12:55:35.949: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/24/23 12:55:35.95
    Aug 24 12:55:35.950: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8428 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:55:35.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 12:55:35.952: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:55:35.952: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-8428/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 24 12:55:36.095: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:55:36.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-8428" for this suite. 08/24/23 12:55:36.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:55:36.15
Aug 24 12:55:36.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename lease-test 08/24/23 12:55:36.152
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:36.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:36.185
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 24 12:55:36.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-5599" for this suite. 08/24/23 12:55:36.305
------------------------------
â€¢ [0.167 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:55:36.15
    Aug 24 12:55:36.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename lease-test 08/24/23 12:55:36.152
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:36.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:36.185
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:55:36.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-5599" for this suite. 08/24/23 12:55:36.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:55:36.323
Aug 24 12:55:36.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 12:55:36.325
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:36.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:36.355
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/24/23 12:55:36.374
STEP: delete the rc 08/24/23 12:55:41.855
STEP: wait for the rc to be deleted 08/24/23 12:55:41.948
Aug 24 12:55:43.534: INFO: 87 pods remaining
Aug 24 12:55:43.534: INFO: 79 pods has nil DeletionTimestamp
Aug 24 12:55:43.534: INFO: 
Aug 24 12:55:44.997: INFO: 84 pods remaining
Aug 24 12:55:44.997: INFO: 67 pods has nil DeletionTimestamp
Aug 24 12:55:44.998: INFO: 
Aug 24 12:55:46.595: INFO: 69 pods remaining
Aug 24 12:55:46.595: INFO: 41 pods has nil DeletionTimestamp
Aug 24 12:55:46.595: INFO: 
Aug 24 12:55:47.080: INFO: 60 pods remaining
Aug 24 12:55:47.080: INFO: 31 pods has nil DeletionTimestamp
Aug 24 12:55:47.080: INFO: 
Aug 24 12:55:48.591: INFO: 56 pods remaining
Aug 24 12:55:48.591: INFO: 17 pods has nil DeletionTimestamp
Aug 24 12:55:48.591: INFO: 
Aug 24 12:55:49.695: INFO: 49 pods remaining
Aug 24 12:55:49.695: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:49.695: INFO: 
Aug 24 12:55:50.134: INFO: 43 pods remaining
Aug 24 12:55:50.151: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:50.161: INFO: 
Aug 24 12:55:51.020: INFO: 34 pods remaining
Aug 24 12:55:51.214: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:51.214: INFO: 
Aug 24 12:55:52.107: INFO: 29 pods remaining
Aug 24 12:55:52.107: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:52.107: INFO: 
Aug 24 12:55:53.172: INFO: 21 pods remaining
Aug 24 12:55:53.172: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:53.172: INFO: 
Aug 24 12:55:53.979: INFO: 14 pods remaining
Aug 24 12:55:53.979: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:53.979: INFO: 
Aug 24 12:55:55.066: INFO: 8 pods remaining
Aug 24 12:55:55.066: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:55.066: INFO: 
Aug 24 12:55:56.096: INFO: 1 pods remaining
Aug 24 12:55:56.096: INFO: 0 pods has nil DeletionTimestamp
Aug 24 12:55:56.096: INFO: 
STEP: Gathering metrics 08/24/23 12:55:57.044
Aug 24 12:55:57.226: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
Aug 24 12:55:57.447: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 220.264385ms
Aug 24 12:55:57.447: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
Aug 24 12:55:57.447: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
Aug 24 12:55:57.872: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 12:55:57.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-50" for this suite. 08/24/23 12:55:57.907
------------------------------
â€¢ [SLOW TEST] [21.643 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:55:36.323
    Aug 24 12:55:36.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 12:55:36.325
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:36.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:36.355
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/24/23 12:55:36.374
    STEP: delete the rc 08/24/23 12:55:41.855
    STEP: wait for the rc to be deleted 08/24/23 12:55:41.948
    Aug 24 12:55:43.534: INFO: 87 pods remaining
    Aug 24 12:55:43.534: INFO: 79 pods has nil DeletionTimestamp
    Aug 24 12:55:43.534: INFO: 
    Aug 24 12:55:44.997: INFO: 84 pods remaining
    Aug 24 12:55:44.997: INFO: 67 pods has nil DeletionTimestamp
    Aug 24 12:55:44.998: INFO: 
    Aug 24 12:55:46.595: INFO: 69 pods remaining
    Aug 24 12:55:46.595: INFO: 41 pods has nil DeletionTimestamp
    Aug 24 12:55:46.595: INFO: 
    Aug 24 12:55:47.080: INFO: 60 pods remaining
    Aug 24 12:55:47.080: INFO: 31 pods has nil DeletionTimestamp
    Aug 24 12:55:47.080: INFO: 
    Aug 24 12:55:48.591: INFO: 56 pods remaining
    Aug 24 12:55:48.591: INFO: 17 pods has nil DeletionTimestamp
    Aug 24 12:55:48.591: INFO: 
    Aug 24 12:55:49.695: INFO: 49 pods remaining
    Aug 24 12:55:49.695: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:49.695: INFO: 
    Aug 24 12:55:50.134: INFO: 43 pods remaining
    Aug 24 12:55:50.151: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:50.161: INFO: 
    Aug 24 12:55:51.020: INFO: 34 pods remaining
    Aug 24 12:55:51.214: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:51.214: INFO: 
    Aug 24 12:55:52.107: INFO: 29 pods remaining
    Aug 24 12:55:52.107: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:52.107: INFO: 
    Aug 24 12:55:53.172: INFO: 21 pods remaining
    Aug 24 12:55:53.172: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:53.172: INFO: 
    Aug 24 12:55:53.979: INFO: 14 pods remaining
    Aug 24 12:55:53.979: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:53.979: INFO: 
    Aug 24 12:55:55.066: INFO: 8 pods remaining
    Aug 24 12:55:55.066: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:55.066: INFO: 
    Aug 24 12:55:56.096: INFO: 1 pods remaining
    Aug 24 12:55:56.096: INFO: 0 pods has nil DeletionTimestamp
    Aug 24 12:55:56.096: INFO: 
    STEP: Gathering metrics 08/24/23 12:55:57.044
    Aug 24 12:55:57.226: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
    Aug 24 12:55:57.447: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 220.264385ms
    Aug 24 12:55:57.447: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
    Aug 24 12:55:57.447: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
    Aug 24 12:55:57.872: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:55:57.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-50" for this suite. 08/24/23 12:55:57.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:55:57.971
Aug 24 12:55:57.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename job 08/24/23 12:55:57.988
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:58.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:58.165
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/24/23 12:55:58.221
STEP: Ensuring job reaches completions 08/24/23 12:55:58.478
STEP: Ensuring pods with index for job exist 08/24/23 12:56:14.548
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:14.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4113" for this suite. 08/24/23 12:56:14.578
------------------------------
â€¢ [SLOW TEST] [16.632 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:55:57.971
    Aug 24 12:55:57.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename job 08/24/23 12:55:57.988
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:55:58.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:55:58.165
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/24/23 12:55:58.221
    STEP: Ensuring job reaches completions 08/24/23 12:55:58.478
    STEP: Ensuring pods with index for job exist 08/24/23 12:56:14.548
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:14.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4113" for this suite. 08/24/23 12:56:14.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:14.618
Aug 24 12:56:14.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 12:56:14.62
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:14.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:14.694
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/24/23 12:56:14.703
Aug 24 12:56:14.718: INFO: Waiting up to 5m0s for pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e" in namespace "downward-api-4585" to be "Succeeded or Failed"
Aug 24 12:56:14.749: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.904668ms
Aug 24 12:56:16.758: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03966067s
Aug 24 12:56:18.758: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039302187s
STEP: Saw pod success 08/24/23 12:56:18.758
Aug 24 12:56:18.759: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e" satisfied condition "Succeeded or Failed"
Aug 24 12:56:18.769: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:56:18.792
Aug 24 12:56:18.818: INFO: Waiting for pod downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e to disappear
Aug 24 12:56:18.823: INFO: Pod downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:18.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4585" for this suite. 08/24/23 12:56:18.833
------------------------------
â€¢ [4.233 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:14.618
    Aug 24 12:56:14.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:56:14.62
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:14.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:14.694
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/24/23 12:56:14.703
    Aug 24 12:56:14.718: INFO: Waiting up to 5m0s for pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e" in namespace "downward-api-4585" to be "Succeeded or Failed"
    Aug 24 12:56:14.749: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.904668ms
    Aug 24 12:56:16.758: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03966067s
    Aug 24 12:56:18.758: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039302187s
    STEP: Saw pod success 08/24/23 12:56:18.758
    Aug 24 12:56:18.759: INFO: Pod "downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e" satisfied condition "Succeeded or Failed"
    Aug 24 12:56:18.769: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:56:18.792
    Aug 24 12:56:18.818: INFO: Waiting for pod downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e to disappear
    Aug 24 12:56:18.823: INFO: Pod downward-api-b92be33a-a1cf-4e21-b930-29ad84b7917e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:18.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4585" for this suite. 08/24/23 12:56:18.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:18.855
Aug 24 12:56:18.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:56:18.856
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:18.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:18.895
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 24 12:56:18.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/24/23 12:56:22.833
Aug 24 12:56:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
Aug 24 12:56:25.103: INFO: stderr: ""
Aug 24 12:56:25.103: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 24 12:56:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 delete e2e-test-crd-publish-openapi-4766-crds test-foo'
Aug 24 12:56:25.316: INFO: stderr: ""
Aug 24 12:56:25.317: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 24 12:56:25.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 apply -f -'
Aug 24 12:56:26.688: INFO: stderr: ""
Aug 24 12:56:26.688: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 24 12:56:26.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 delete e2e-test-crd-publish-openapi-4766-crds test-foo'
Aug 24 12:56:26.887: INFO: stderr: ""
Aug 24 12:56:26.887: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/24/23 12:56:26.887
Aug 24 12:56:26.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
Aug 24 12:56:27.374: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/24/23 12:56:27.374
Aug 24 12:56:27.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
Aug 24 12:56:27.924: INFO: rc: 1
Aug 24 12:56:27.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 apply -f -'
Aug 24 12:56:28.390: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/24/23 12:56:28.39
Aug 24 12:56:28.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
Aug 24 12:56:28.801: INFO: rc: 1
Aug 24 12:56:28.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 apply -f -'
Aug 24 12:56:29.250: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/24/23 12:56:29.25
Aug 24 12:56:29.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds'
Aug 24 12:56:29.726: INFO: stderr: ""
Aug 24 12:56:29.726: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/24/23 12:56:29.727
Aug 24 12:56:29.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.metadata'
Aug 24 12:56:30.156: INFO: stderr: ""
Aug 24 12:56:30.156: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 24 12:56:30.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.spec'
Aug 24 12:56:30.685: INFO: stderr: ""
Aug 24 12:56:30.685: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 24 12:56:30.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.spec.bars'
Aug 24 12:56:31.179: INFO: stderr: ""
Aug 24 12:56:31.179: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/24/23 12:56:31.18
Aug 24 12:56:31.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.spec.bars2'
Aug 24 12:56:31.758: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:34.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8975" for this suite. 08/24/23 12:56:34.177
------------------------------
â€¢ [SLOW TEST] [15.344 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:18.855
    Aug 24 12:56:18.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:56:18.856
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:18.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:18.895
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 24 12:56:18.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/24/23 12:56:22.833
    Aug 24 12:56:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
    Aug 24 12:56:25.103: INFO: stderr: ""
    Aug 24 12:56:25.103: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 24 12:56:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 delete e2e-test-crd-publish-openapi-4766-crds test-foo'
    Aug 24 12:56:25.316: INFO: stderr: ""
    Aug 24 12:56:25.317: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 24 12:56:25.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 apply -f -'
    Aug 24 12:56:26.688: INFO: stderr: ""
    Aug 24 12:56:26.688: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 24 12:56:26.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 delete e2e-test-crd-publish-openapi-4766-crds test-foo'
    Aug 24 12:56:26.887: INFO: stderr: ""
    Aug 24 12:56:26.887: INFO: stdout: "e2e-test-crd-publish-openapi-4766-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/24/23 12:56:26.887
    Aug 24 12:56:26.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
    Aug 24 12:56:27.374: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/24/23 12:56:27.374
    Aug 24 12:56:27.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
    Aug 24 12:56:27.924: INFO: rc: 1
    Aug 24 12:56:27.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 apply -f -'
    Aug 24 12:56:28.390: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/24/23 12:56:28.39
    Aug 24 12:56:28.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 create -f -'
    Aug 24 12:56:28.801: INFO: rc: 1
    Aug 24 12:56:28.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 --namespace=crd-publish-openapi-8975 apply -f -'
    Aug 24 12:56:29.250: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/24/23 12:56:29.25
    Aug 24 12:56:29.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds'
    Aug 24 12:56:29.726: INFO: stderr: ""
    Aug 24 12:56:29.726: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/24/23 12:56:29.727
    Aug 24 12:56:29.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.metadata'
    Aug 24 12:56:30.156: INFO: stderr: ""
    Aug 24 12:56:30.156: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 24 12:56:30.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.spec'
    Aug 24 12:56:30.685: INFO: stderr: ""
    Aug 24 12:56:30.685: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 24 12:56:30.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.spec.bars'
    Aug 24 12:56:31.179: INFO: stderr: ""
    Aug 24 12:56:31.179: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4766-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/24/23 12:56:31.18
    Aug 24 12:56:31.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=crd-publish-openapi-8975 explain e2e-test-crd-publish-openapi-4766-crds.spec.bars2'
    Aug 24 12:56:31.758: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:34.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8975" for this suite. 08/24/23 12:56:34.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:34.212
Aug 24 12:56:34.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 12:56:34.219
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:34.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:34.264
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-1910 08/24/23 12:56:34.271
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[] 08/24/23 12:56:34.293
Aug 24 12:56:34.311: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 24 12:56:35.335: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1910 08/24/23 12:56:35.336
Aug 24 12:56:35.355: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1910" to be "running and ready"
Aug 24 12:56:35.371: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.61824ms
Aug 24 12:56:35.372: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:56:37.381: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026582729s
Aug 24 12:56:37.382: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:56:39.385: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.030186977s
Aug 24 12:56:39.385: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 24 12:56:39.385: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[pod1:[100]] 08/24/23 12:56:39.396
Aug 24 12:56:39.439: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1910 08/24/23 12:56:39.439
Aug 24 12:56:39.457: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1910" to be "running and ready"
Aug 24 12:56:39.464: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779289ms
Aug 24 12:56:39.464: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:56:41.472: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014749175s
Aug 24 12:56:41.472: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 24 12:56:41.472: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[pod1:[100] pod2:[101]] 08/24/23 12:56:41.478
Aug 24 12:56:41.500: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/24/23 12:56:41.5
Aug 24 12:56:41.500: INFO: Creating new exec pod
Aug 24 12:56:41.510: INFO: Waiting up to 5m0s for pod "execpodc572b" in namespace "services-1910" to be "running"
Aug 24 12:56:41.519: INFO: Pod "execpodc572b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.682102ms
Aug 24 12:56:43.528: INFO: Pod "execpodc572b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01764322s
Aug 24 12:56:43.528: INFO: Pod "execpodc572b" satisfied condition "running"
Aug 24 12:56:44.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 24 12:56:44.852: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 24 12:56:44.852: INFO: stdout: ""
Aug 24 12:56:44.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 10.233.15.247 80'
Aug 24 12:56:45.113: INFO: stderr: "+ nc -v -z -w 2 10.233.15.247 80\nConnection to 10.233.15.247 80 port [tcp/http] succeeded!\n"
Aug 24 12:56:45.113: INFO: stdout: ""
Aug 24 12:56:45.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 24 12:56:45.404: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 24 12:56:45.404: INFO: stdout: ""
Aug 24 12:56:45.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 10.233.15.247 81'
Aug 24 12:56:45.654: INFO: stderr: "+ nc -v -z -w 2 10.233.15.247 81\nConnection to 10.233.15.247 81 port [tcp/*] succeeded!\n"
Aug 24 12:56:45.654: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1910 08/24/23 12:56:45.655
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[pod2:[101]] 08/24/23 12:56:45.691
Aug 24 12:56:45.790: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1910 08/24/23 12:56:45.79
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[] 08/24/23 12:56:45.858
Aug 24 12:56:45.902: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:45.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1910" for this suite. 08/24/23 12:56:45.98
------------------------------
â€¢ [SLOW TEST] [11.802 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:34.212
    Aug 24 12:56:34.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 12:56:34.219
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:34.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:34.264
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-1910 08/24/23 12:56:34.271
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[] 08/24/23 12:56:34.293
    Aug 24 12:56:34.311: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 24 12:56:35.335: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1910 08/24/23 12:56:35.336
    Aug 24 12:56:35.355: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1910" to be "running and ready"
    Aug 24 12:56:35.371: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.61824ms
    Aug 24 12:56:35.372: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:56:37.381: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026582729s
    Aug 24 12:56:37.382: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:56:39.385: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.030186977s
    Aug 24 12:56:39.385: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 24 12:56:39.385: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[pod1:[100]] 08/24/23 12:56:39.396
    Aug 24 12:56:39.439: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1910 08/24/23 12:56:39.439
    Aug 24 12:56:39.457: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1910" to be "running and ready"
    Aug 24 12:56:39.464: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.779289ms
    Aug 24 12:56:39.464: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:56:41.472: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014749175s
    Aug 24 12:56:41.472: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 24 12:56:41.472: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[pod1:[100] pod2:[101]] 08/24/23 12:56:41.478
    Aug 24 12:56:41.500: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/24/23 12:56:41.5
    Aug 24 12:56:41.500: INFO: Creating new exec pod
    Aug 24 12:56:41.510: INFO: Waiting up to 5m0s for pod "execpodc572b" in namespace "services-1910" to be "running"
    Aug 24 12:56:41.519: INFO: Pod "execpodc572b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.682102ms
    Aug 24 12:56:43.528: INFO: Pod "execpodc572b": Phase="Running", Reason="", readiness=true. Elapsed: 2.01764322s
    Aug 24 12:56:43.528: INFO: Pod "execpodc572b" satisfied condition "running"
    Aug 24 12:56:44.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 24 12:56:44.852: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 24 12:56:44.852: INFO: stdout: ""
    Aug 24 12:56:44.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 10.233.15.247 80'
    Aug 24 12:56:45.113: INFO: stderr: "+ nc -v -z -w 2 10.233.15.247 80\nConnection to 10.233.15.247 80 port [tcp/http] succeeded!\n"
    Aug 24 12:56:45.113: INFO: stdout: ""
    Aug 24 12:56:45.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 24 12:56:45.404: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 24 12:56:45.404: INFO: stdout: ""
    Aug 24 12:56:45.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1910 exec execpodc572b -- /bin/sh -x -c nc -v -z -w 2 10.233.15.247 81'
    Aug 24 12:56:45.654: INFO: stderr: "+ nc -v -z -w 2 10.233.15.247 81\nConnection to 10.233.15.247 81 port [tcp/*] succeeded!\n"
    Aug 24 12:56:45.654: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1910 08/24/23 12:56:45.655
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[pod2:[101]] 08/24/23 12:56:45.691
    Aug 24 12:56:45.790: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1910 08/24/23 12:56:45.79
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1910 to expose endpoints map[] 08/24/23 12:56:45.858
    Aug 24 12:56:45.902: INFO: successfully validated that service multi-endpoint-test in namespace services-1910 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:45.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1910" for this suite. 08/24/23 12:56:45.98
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:46.015
Aug 24 12:56:46.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:56:46.026
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:46.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:46.055
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/24/23 12:56:46.064
Aug 24 12:56:46.114: INFO: Waiting up to 5m0s for pod "pod-4jnpp" in namespace "pods-7894" to be "running"
Aug 24 12:56:46.122: INFO: Pod "pod-4jnpp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.526876ms
Aug 24 12:56:48.133: INFO: Pod "pod-4jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017959639s
Aug 24 12:56:48.133: INFO: Pod "pod-4jnpp" satisfied condition "running"
STEP: patching /status 08/24/23 12:56:48.133
Aug 24 12:56:48.151: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:48.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7894" for this suite. 08/24/23 12:56:48.164
------------------------------
â€¢ [2.164 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:46.015
    Aug 24 12:56:46.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:56:46.026
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:46.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:46.055
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/24/23 12:56:46.064
    Aug 24 12:56:46.114: INFO: Waiting up to 5m0s for pod "pod-4jnpp" in namespace "pods-7894" to be "running"
    Aug 24 12:56:46.122: INFO: Pod "pod-4jnpp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.526876ms
    Aug 24 12:56:48.133: INFO: Pod "pod-4jnpp": Phase="Running", Reason="", readiness=true. Elapsed: 2.017959639s
    Aug 24 12:56:48.133: INFO: Pod "pod-4jnpp" satisfied condition "running"
    STEP: patching /status 08/24/23 12:56:48.133
    Aug 24 12:56:48.151: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:48.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7894" for this suite. 08/24/23 12:56:48.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:48.183
Aug 24 12:56:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:56:48.187
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:48.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:48.229
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:56:48.235
Aug 24 12:56:48.268: INFO: Waiting up to 5m0s for pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e" in namespace "emptydir-9373" to be "Succeeded or Failed"
Aug 24 12:56:48.279: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.387842ms
Aug 24 12:56:50.292: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02347984s
Aug 24 12:56:52.289: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020246243s
STEP: Saw pod success 08/24/23 12:56:52.289
Aug 24 12:56:52.290: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e" satisfied condition "Succeeded or Failed"
Aug 24 12:56:52.297: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-9a2b6cb0-2736-44ef-8729-c01a432c853e container test-container: <nil>
STEP: delete the pod 08/24/23 12:56:52.307
Aug 24 12:56:52.331: INFO: Waiting for pod pod-9a2b6cb0-2736-44ef-8729-c01a432c853e to disappear
Aug 24 12:56:52.339: INFO: Pod pod-9a2b6cb0-2736-44ef-8729-c01a432c853e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:52.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9373" for this suite. 08/24/23 12:56:52.351
------------------------------
â€¢ [4.183 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:48.183
    Aug 24 12:56:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:56:48.187
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:48.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:48.229
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:56:48.235
    Aug 24 12:56:48.268: INFO: Waiting up to 5m0s for pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e" in namespace "emptydir-9373" to be "Succeeded or Failed"
    Aug 24 12:56:48.279: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.387842ms
    Aug 24 12:56:50.292: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02347984s
    Aug 24 12:56:52.289: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020246243s
    STEP: Saw pod success 08/24/23 12:56:52.289
    Aug 24 12:56:52.290: INFO: Pod "pod-9a2b6cb0-2736-44ef-8729-c01a432c853e" satisfied condition "Succeeded or Failed"
    Aug 24 12:56:52.297: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-9a2b6cb0-2736-44ef-8729-c01a432c853e container test-container: <nil>
    STEP: delete the pod 08/24/23 12:56:52.307
    Aug 24 12:56:52.331: INFO: Waiting for pod pod-9a2b6cb0-2736-44ef-8729-c01a432c853e to disappear
    Aug 24 12:56:52.339: INFO: Pod pod-9a2b6cb0-2736-44ef-8729-c01a432c853e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:52.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9373" for this suite. 08/24/23 12:56:52.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:52.371
Aug 24 12:56:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:56:52.373
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:52.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:52.411
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:56:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2668" for this suite. 08/24/23 12:56:56.517
------------------------------
â€¢ [4.163 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:52.371
    Aug 24 12:56:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:56:52.373
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:52.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:52.411
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:56:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2668" for this suite. 08/24/23 12:56:56.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:56:56.546
Aug 24 12:56:56.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 12:56:56.549
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:56.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:56.602
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:56:56.642
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:56:57.749
STEP: Deploying the webhook pod 08/24/23 12:56:57.761
STEP: Wait for the deployment to be ready 08/24/23 12:56:57.795
Aug 24 12:56:57.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:56:59.832
STEP: Verifying the service has paired with the endpoint 08/24/23 12:56:59.846
Aug 24 12:57:00.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/24/23 12:57:00.856
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:57:00.887
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/24/23 12:57:00.906
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:57:00.928
STEP: Patching a validating webhook configuration's rules to include the create operation 08/24/23 12:57:00.948
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:57:00.96
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:00.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9350" for this suite. 08/24/23 12:57:01.104
STEP: Destroying namespace "webhook-9350-markers" for this suite. 08/24/23 12:57:01.121
------------------------------
â€¢ [4.594 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:56:56.546
    Aug 24 12:56:56.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 12:56:56.549
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:56:56.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:56:56.602
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:56:56.642
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:56:57.749
    STEP: Deploying the webhook pod 08/24/23 12:56:57.761
    STEP: Wait for the deployment to be ready 08/24/23 12:56:57.795
    Aug 24 12:56:57.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:56:59.832
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:56:59.846
    Aug 24 12:57:00.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/24/23 12:57:00.856
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:57:00.887
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/24/23 12:57:00.906
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:57:00.928
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/24/23 12:57:00.948
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:57:00.96
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:00.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9350" for this suite. 08/24/23 12:57:01.104
    STEP: Destroying namespace "webhook-9350-markers" for this suite. 08/24/23 12:57:01.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:01.147
Aug 24 12:57:01.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:57:01.155
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:01.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:01.217
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Aug 24 12:57:01.278: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:57:01.292
Aug 24 12:57:01.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:57:01.307: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:57:02.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:57:02.339: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:57:03.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 12:57:03.324: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 12:57:04.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:57:04.328: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 08/24/23 12:57:04.363
STEP: Check that daemon pods images are updated. 08/24/23 12:57:04.403
Aug 24 12:57:04.411: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:04.412: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:04.412: INFO: Wrong image for pod: daemon-set-86vd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:05.434: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:05.434: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:06.438: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:06.438: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:07.434: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:07.434: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:07.434: INFO: Pod daemon-set-nsmmr is not available
Aug 24 12:57:08.439: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:08.439: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:08.440: INFO: Pod daemon-set-nsmmr is not available
Aug 24 12:57:09.433: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:10.435: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:10.435: INFO: Pod daemon-set-6449t is not available
Aug 24 12:57:11.435: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:57:11.436: INFO: Pod daemon-set-6449t is not available
Aug 24 12:57:13.435: INFO: Pod daemon-set-qb4hw is not available
STEP: Check that daemon pods are still running on every node of the cluster. 08/24/23 12:57:13.45
Aug 24 12:57:13.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:57:13.474: INFO: Node pe9deep4seen-3 is running 0 daemon pod, expected 1
Aug 24 12:57:14.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:57:14.493: INFO: Node pe9deep4seen-3 is running 0 daemon pod, expected 1
Aug 24 12:57:15.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:57:15.490: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:57:15.519
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6657, will wait for the garbage collector to delete the pods 08/24/23 12:57:15.52
Aug 24 12:57:15.590: INFO: Deleting DaemonSet.extensions daemon-set took: 13.635162ms
Aug 24 12:57:15.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.908939ms
Aug 24 12:57:17.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:57:17.698: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:57:17.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32557"},"items":null}

Aug 24 12:57:17.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32557"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:17.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6657" for this suite. 08/24/23 12:57:17.751
------------------------------
â€¢ [SLOW TEST] [16.621 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:01.147
    Aug 24 12:57:01.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:57:01.155
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:01.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:01.217
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Aug 24 12:57:01.278: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:57:01.292
    Aug 24 12:57:01.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:57:01.307: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:57:02.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:57:02.339: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:57:03.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 12:57:03.324: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 12:57:04.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:57:04.328: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 08/24/23 12:57:04.363
    STEP: Check that daemon pods images are updated. 08/24/23 12:57:04.403
    Aug 24 12:57:04.411: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:04.412: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:04.412: INFO: Wrong image for pod: daemon-set-86vd6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:05.434: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:05.434: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:06.438: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:06.438: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:07.434: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:07.434: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:07.434: INFO: Pod daemon-set-nsmmr is not available
    Aug 24 12:57:08.439: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:08.439: INFO: Wrong image for pod: daemon-set-75pkb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:08.440: INFO: Pod daemon-set-nsmmr is not available
    Aug 24 12:57:09.433: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:10.435: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:10.435: INFO: Pod daemon-set-6449t is not available
    Aug 24 12:57:11.435: INFO: Wrong image for pod: daemon-set-5mbns. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:57:11.436: INFO: Pod daemon-set-6449t is not available
    Aug 24 12:57:13.435: INFO: Pod daemon-set-qb4hw is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 08/24/23 12:57:13.45
    Aug 24 12:57:13.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:57:13.474: INFO: Node pe9deep4seen-3 is running 0 daemon pod, expected 1
    Aug 24 12:57:14.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:57:14.493: INFO: Node pe9deep4seen-3 is running 0 daemon pod, expected 1
    Aug 24 12:57:15.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:57:15.490: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:57:15.519
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6657, will wait for the garbage collector to delete the pods 08/24/23 12:57:15.52
    Aug 24 12:57:15.590: INFO: Deleting DaemonSet.extensions daemon-set took: 13.635162ms
    Aug 24 12:57:15.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.908939ms
    Aug 24 12:57:17.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:57:17.698: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:57:17.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32557"},"items":null}

    Aug 24 12:57:17.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32557"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:17.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6657" for this suite. 08/24/23 12:57:17.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:17.781
Aug 24 12:57:17.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-pred 08/24/23 12:57:17.784
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:17.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:17.823
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 12:57:17.835: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 12:57:17.864: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 12:57:17.870: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
Aug 24 12:57:17.890: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.890: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:57:17.890: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.890: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:57:17.890: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.890: INFO: 	Container coredns ready: true, restart count 0
Aug 24 12:57:17.891: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.891: INFO: 	Container coredns ready: true, restart count 0
Aug 24 12:57:17.891: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.891: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 12:57:17.891: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.891: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 12:57:17.891: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.892: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 12:57:17.892: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.892: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:57:17.892: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.892: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 12:57:17.892: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 12:57:17.892: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:57:17.892: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:57:17.893: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
Aug 24 12:57:17.909: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.909: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:57:17.909: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.909: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 24 12:57:17.909: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.909: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:57:17.910: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.910: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 12:57:17.910: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.910: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 12:57:17.910: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.910: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 12:57:17.910: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.911: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:57:17.911: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.911: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 12:57:17.911: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 12:57:17.911: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:57:17.911: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:57:17.911: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
Aug 24 12:57:17.926: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.926: INFO: 	Container node-init ready: true, restart count 0
Aug 24 12:57:17.926: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.926: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 12:57:17.926: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.926: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 12:57:17.927: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
Aug 24 12:57:17.927: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 12:57:17.927: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 12:57:17.927: INFO: 	Container e2e ready: true, restart count 0
Aug 24 12:57:17.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:57:17.927: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 12:57:17.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:57:17.928: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/24/23 12:57:17.928
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177e530ff96d5dcc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 08/24/23 12:57:18
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:19.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-631" for this suite. 08/24/23 12:57:19.016
------------------------------
â€¢ [1.259 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:17.781
    Aug 24 12:57:17.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-pred 08/24/23 12:57:17.784
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:17.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:17.823
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 12:57:17.835: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 12:57:17.864: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 12:57:17.870: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
    Aug 24 12:57:17.890: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.890: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:57:17.890: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.890: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:57:17.890: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.890: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 12:57:17.891: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.891: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 12:57:17.891: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.891: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 12:57:17.891: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.891: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 12:57:17.891: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.892: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 12:57:17.892: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.892: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:57:17.892: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.892: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 12:57:17.892: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 12:57:17.892: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:57:17.892: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:57:17.893: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
    Aug 24 12:57:17.909: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.909: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:57:17.909: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.909: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 24 12:57:17.909: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.909: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:57:17.910: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.910: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 12:57:17.910: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.910: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 12:57:17.910: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.910: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 12:57:17.910: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.911: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:57:17.911: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.911: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 12:57:17.911: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 12:57:17.911: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:57:17.911: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:57:17.911: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
    Aug 24 12:57:17.926: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.926: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 12:57:17.926: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.926: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 12:57:17.926: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.926: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 12:57:17.927: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
    Aug 24 12:57:17.927: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 12:57:17.927: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 12:57:17.927: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 12:57:17.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:57:17.927: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 12:57:17.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:57:17.928: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/24/23 12:57:17.928
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177e530ff96d5dcc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 08/24/23 12:57:18
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:19.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-631" for this suite. 08/24/23 12:57:19.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:19.043
Aug 24 12:57:19.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename deployment 08/24/23 12:57:19.046
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:19.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:19.11
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 24 12:57:19.134: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 24 12:57:24.141: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:57:24.141
Aug 24 12:57:24.141: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/24/23 12:57:24.165
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:57:24.195: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-876  08bc5a4f-be9a-4f1e-8d95-37926dc55853 32645 1 2023-08-24 12:57:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f32588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 24 12:57:24.205: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-876  7502dbbe-1d0c-4d3d-8a7d-00489806acc5 32648 1 2023-08-24 12:57:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 08bc5a4f-be9a-4f1e-8d95-37926dc55853 0xc006f329e7 0xc006f329e8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08bc5a4f-be9a-4f1e-8d95-37926dc55853\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f32a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:57:24.205: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 24 12:57:24.205: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-876  0fc0b9cd-2e58-47aa-adda-8fd44c447314 32647 1 2023-08-24 12:57:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 08bc5a4f-be9a-4f1e-8d95-37926dc55853 0xc006f328b7 0xc006f328b8}] [] [{e2e.test Update apps/v1 2023-08-24 12:57:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:57:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"08bc5a4f-be9a-4f1e-8d95-37926dc55853\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006f32978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:57:24.247: INFO: Pod "test-cleanup-controller-zvkcb" is available:
&Pod{ObjectMeta:{test-cleanup-controller-zvkcb test-cleanup-controller- deployment-876  38099be2-a563-4e61-8f02-4142b171c10d 32589 0 2023-08-24 12:57:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 0fc0b9cd-2e58-47aa-adda-8fd44c447314 0xc006f33027 0xc006f33028}] [] [{kube-controller-manager Update v1 2023-08-24 12:57:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0fc0b9cd-2e58-47aa-adda-8fd44c447314\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:57:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hs9xt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hs9xt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.167,StartTime:2023-08-24 12:57:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:57:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9924c9d01f0ff26fe5df3bab4f7b20ce58556748d133f422df1fbaa08d57577a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 12:57:24.248: INFO: Pod "test-cleanup-deployment-7698ff6f6b-d9l44" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-d9l44 test-cleanup-deployment-7698ff6f6b- deployment-876  a6b80ce7-6633-4139-8155-3551287e2e45 32651 0 2023-08-24 12:57:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 7502dbbe-1d0c-4d3d-8a7d-00489806acc5 0xc006f33227 0xc006f33228}] [] [{kube-controller-manager Update v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7502dbbe-1d0c-4d3d-8a7d-00489806acc5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xdmlr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xdmlr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-876" for this suite. 08/24/23 12:57:24.267
------------------------------
â€¢ [SLOW TEST] [5.239 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:19.043
    Aug 24 12:57:19.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename deployment 08/24/23 12:57:19.046
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:19.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:19.11
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 24 12:57:19.134: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 24 12:57:24.141: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:57:24.141
    Aug 24 12:57:24.141: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/24/23 12:57:24.165
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:57:24.195: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-876  08bc5a4f-be9a-4f1e-8d95-37926dc55853 32645 1 2023-08-24 12:57:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f32588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 24 12:57:24.205: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-876  7502dbbe-1d0c-4d3d-8a7d-00489806acc5 32648 1 2023-08-24 12:57:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 08bc5a4f-be9a-4f1e-8d95-37926dc55853 0xc006f329e7 0xc006f329e8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08bc5a4f-be9a-4f1e-8d95-37926dc55853\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f32a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:57:24.205: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Aug 24 12:57:24.205: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-876  0fc0b9cd-2e58-47aa-adda-8fd44c447314 32647 1 2023-08-24 12:57:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 08bc5a4f-be9a-4f1e-8d95-37926dc55853 0xc006f328b7 0xc006f328b8}] [] [{e2e.test Update apps/v1 2023-08-24 12:57:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:57:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"08bc5a4f-be9a-4f1e-8d95-37926dc55853\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006f32978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:57:24.247: INFO: Pod "test-cleanup-controller-zvkcb" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-zvkcb test-cleanup-controller- deployment-876  38099be2-a563-4e61-8f02-4142b171c10d 32589 0 2023-08-24 12:57:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 0fc0b9cd-2e58-47aa-adda-8fd44c447314 0xc006f33027 0xc006f33028}] [] [{kube-controller-manager Update v1 2023-08-24 12:57:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0fc0b9cd-2e58-47aa-adda-8fd44c447314\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:57:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hs9xt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hs9xt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.130,PodIP:10.233.66.167,StartTime:2023-08-24 12:57:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:57:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9924c9d01f0ff26fe5df3bab4f7b20ce58556748d133f422df1fbaa08d57577a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 12:57:24.248: INFO: Pod "test-cleanup-deployment-7698ff6f6b-d9l44" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-d9l44 test-cleanup-deployment-7698ff6f6b- deployment-876  a6b80ce7-6633-4139-8155-3551287e2e45 32651 0 2023-08-24 12:57:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 7502dbbe-1d0c-4d3d-8a7d-00489806acc5 0xc006f33227 0xc006f33228}] [] [{kube-controller-manager Update v1 2023-08-24 12:57:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7502dbbe-1d0c-4d3d-8a7d-00489806acc5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xdmlr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xdmlr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:pe9deep4seen-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:57:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-876" for this suite. 08/24/23 12:57:24.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:24.286
Aug 24 12:57:24.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 12:57:24.288
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:24.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:24.316
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:57:24.321
Aug 24 12:57:24.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514" in namespace "projected-5559" to be "Succeeded or Failed"
Aug 24 12:57:24.347: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514": Phase="Pending", Reason="", readiness=false. Elapsed: 10.869105ms
Aug 24 12:57:26.354: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017987536s
Aug 24 12:57:28.355: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01932651s
STEP: Saw pod success 08/24/23 12:57:28.356
Aug 24 12:57:28.356: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514" satisfied condition "Succeeded or Failed"
Aug 24 12:57:28.362: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514 container client-container: <nil>
STEP: delete the pod 08/24/23 12:57:28.374
Aug 24 12:57:28.404: INFO: Waiting for pod downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514 to disappear
Aug 24 12:57:28.410: INFO: Pod downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5559" for this suite. 08/24/23 12:57:28.421
------------------------------
â€¢ [4.148 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:24.286
    Aug 24 12:57:24.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 12:57:24.288
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:24.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:24.316
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:57:24.321
    Aug 24 12:57:24.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514" in namespace "projected-5559" to be "Succeeded or Failed"
    Aug 24 12:57:24.347: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514": Phase="Pending", Reason="", readiness=false. Elapsed: 10.869105ms
    Aug 24 12:57:26.354: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017987536s
    Aug 24 12:57:28.355: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01932651s
    STEP: Saw pod success 08/24/23 12:57:28.356
    Aug 24 12:57:28.356: INFO: Pod "downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514" satisfied condition "Succeeded or Failed"
    Aug 24 12:57:28.362: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:57:28.374
    Aug 24 12:57:28.404: INFO: Waiting for pod downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514 to disappear
    Aug 24 12:57:28.410: INFO: Pod downwardapi-volume-59e09f6d-8fd7-44a8-a9bb-c988d4aa7514 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5559" for this suite. 08/24/23 12:57:28.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:28.439
Aug 24 12:57:28.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replicaset 08/24/23 12:57:28.441
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:28.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:28.485
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 24 12:57:28.514: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 12:57:33.526: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:57:33.526
STEP: Scaling up "test-rs" replicaset  08/24/23 12:57:33.527
Aug 24 12:57:33.548: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/24/23 12:57:33.548
W0824 12:57:33.563843      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 24 12:57:33.579: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 12:57:33.631: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 12:57:33.656: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 12:57:33.724: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 12:57:35.022: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 2, AvailableReplicas 2
Aug 24 12:57:35.843: INFO: observed Replicaset test-rs in namespace replicaset-4058 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:35.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4058" for this suite. 08/24/23 12:57:35.857
------------------------------
â€¢ [SLOW TEST] [7.449 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:28.439
    Aug 24 12:57:28.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replicaset 08/24/23 12:57:28.441
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:28.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:28.485
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 24 12:57:28.514: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 12:57:33.526: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:57:33.526
    STEP: Scaling up "test-rs" replicaset  08/24/23 12:57:33.527
    Aug 24 12:57:33.548: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/24/23 12:57:33.548
    W0824 12:57:33.563843      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 24 12:57:33.579: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 12:57:33.631: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 12:57:33.656: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 12:57:33.724: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 12:57:35.022: INFO: observed ReplicaSet test-rs in namespace replicaset-4058 with ReadyReplicas 2, AvailableReplicas 2
    Aug 24 12:57:35.843: INFO: observed Replicaset test-rs in namespace replicaset-4058 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:35.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4058" for this suite. 08/24/23 12:57:35.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:35.892
Aug 24 12:57:35.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 12:57:35.894
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:35.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:35.931
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/24/23 12:57:35.936
Aug 24 12:57:35.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4945 cluster-info'
Aug 24 12:57:36.098: INFO: stderr: ""
Aug 24 12:57:36.098: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:36.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4945" for this suite. 08/24/23 12:57:36.107
------------------------------
â€¢ [0.229 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:35.892
    Aug 24 12:57:35.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:57:35.894
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:35.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:35.931
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/24/23 12:57:35.936
    Aug 24 12:57:35.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-4945 cluster-info'
    Aug 24 12:57:36.098: INFO: stderr: ""
    Aug 24 12:57:36.098: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:36.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4945" for this suite. 08/24/23 12:57:36.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:36.125
Aug 24 12:57:36.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 12:57:36.128
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:36.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:36.16
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7918.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7918.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/24/23 12:57:36.165
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7918.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7918.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/24/23 12:57:36.166
STEP: creating a pod to probe /etc/hosts 08/24/23 12:57:36.166
STEP: submitting the pod to kubernetes 08/24/23 12:57:36.166
Aug 24 12:57:36.182: INFO: Waiting up to 15m0s for pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5" in namespace "dns-7918" to be "running"
Aug 24 12:57:36.196: INFO: Pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026712ms
Aug 24 12:57:38.204: INFO: Pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022162251s
Aug 24 12:57:38.204: INFO: Pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:57:38.204
STEP: looking for the results for each expected name from probers 08/24/23 12:57:38.21
Aug 24 12:57:38.242: INFO: DNS probes using dns-7918/dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5 succeeded

STEP: deleting the pod 08/24/23 12:57:38.243
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:38.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7918" for this suite. 08/24/23 12:57:38.282
------------------------------
â€¢ [2.176 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:36.125
    Aug 24 12:57:36.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 12:57:36.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:36.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:36.16
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7918.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7918.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/24/23 12:57:36.165
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7918.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7918.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/24/23 12:57:36.166
    STEP: creating a pod to probe /etc/hosts 08/24/23 12:57:36.166
    STEP: submitting the pod to kubernetes 08/24/23 12:57:36.166
    Aug 24 12:57:36.182: INFO: Waiting up to 15m0s for pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5" in namespace "dns-7918" to be "running"
    Aug 24 12:57:36.196: INFO: Pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026712ms
    Aug 24 12:57:38.204: INFO: Pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022162251s
    Aug 24 12:57:38.204: INFO: Pod "dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:57:38.204
    STEP: looking for the results for each expected name from probers 08/24/23 12:57:38.21
    Aug 24 12:57:38.242: INFO: DNS probes using dns-7918/dns-test-ec226304-1c01-4c09-b5b0-0c2cfa2d52a5 succeeded

    STEP: deleting the pod 08/24/23 12:57:38.243
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:38.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7918" for this suite. 08/24/23 12:57:38.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:38.303
Aug 24 12:57:38.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replicaset 08/24/23 12:57:38.304
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:38.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:38.336
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/24/23 12:57:38.34
Aug 24 12:57:38.372: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:57:38.372
Aug 24 12:57:38.372: INFO: Waiting up to 5m0s for pod "test-rs-7zh6g" in namespace "replicaset-7669" to be "running"
Aug 24 12:57:38.407: INFO: Pod "test-rs-7zh6g": Phase="Pending", Reason="", readiness=false. Elapsed: 34.614979ms
Aug 24 12:57:40.417: INFO: Pod "test-rs-7zh6g": Phase="Running", Reason="", readiness=true. Elapsed: 2.044908739s
Aug 24 12:57:40.417: INFO: Pod "test-rs-7zh6g" satisfied condition "running"
STEP: getting scale subresource 08/24/23 12:57:40.417
STEP: updating a scale subresource 08/24/23 12:57:40.422
STEP: verifying the replicaset Spec.Replicas was modified 08/24/23 12:57:40.436
STEP: Patch a scale subresource 08/24/23 12:57:40.443
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:40.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7669" for this suite. 08/24/23 12:57:40.5
------------------------------
â€¢ [2.209 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:38.303
    Aug 24 12:57:38.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replicaset 08/24/23 12:57:38.304
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:38.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:38.336
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/24/23 12:57:38.34
    Aug 24 12:57:38.372: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:57:38.372
    Aug 24 12:57:38.372: INFO: Waiting up to 5m0s for pod "test-rs-7zh6g" in namespace "replicaset-7669" to be "running"
    Aug 24 12:57:38.407: INFO: Pod "test-rs-7zh6g": Phase="Pending", Reason="", readiness=false. Elapsed: 34.614979ms
    Aug 24 12:57:40.417: INFO: Pod "test-rs-7zh6g": Phase="Running", Reason="", readiness=true. Elapsed: 2.044908739s
    Aug 24 12:57:40.417: INFO: Pod "test-rs-7zh6g" satisfied condition "running"
    STEP: getting scale subresource 08/24/23 12:57:40.417
    STEP: updating a scale subresource 08/24/23 12:57:40.422
    STEP: verifying the replicaset Spec.Replicas was modified 08/24/23 12:57:40.436
    STEP: Patch a scale subresource 08/24/23 12:57:40.443
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:40.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7669" for this suite. 08/24/23 12:57:40.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:40.516
Aug 24 12:57:40.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 12:57:40.518
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:40.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:40.551
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-3446/secret-test-a14a0f66-f33e-4174-89aa-97ae72da6655 08/24/23 12:57:40.555
STEP: Creating a pod to test consume secrets 08/24/23 12:57:40.561
Aug 24 12:57:40.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c" in namespace "secrets-3446" to be "Succeeded or Failed"
Aug 24 12:57:40.579: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064973ms
Aug 24 12:57:42.589: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016450896s
Aug 24 12:57:44.589: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016178413s
Aug 24 12:57:46.590: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017191857s
STEP: Saw pod success 08/24/23 12:57:46.59
Aug 24 12:57:46.590: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c" satisfied condition "Succeeded or Failed"
Aug 24 12:57:46.596: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c container env-test: <nil>
STEP: delete the pod 08/24/23 12:57:46.614
Aug 24 12:57:46.643: INFO: Waiting for pod pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c to disappear
Aug 24 12:57:46.650: INFO: Pod pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3446" for this suite. 08/24/23 12:57:46.669
------------------------------
â€¢ [SLOW TEST] [6.175 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:40.516
    Aug 24 12:57:40.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 12:57:40.518
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:40.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:40.551
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-3446/secret-test-a14a0f66-f33e-4174-89aa-97ae72da6655 08/24/23 12:57:40.555
    STEP: Creating a pod to test consume secrets 08/24/23 12:57:40.561
    Aug 24 12:57:40.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c" in namespace "secrets-3446" to be "Succeeded or Failed"
    Aug 24 12:57:40.579: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064973ms
    Aug 24 12:57:42.589: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016450896s
    Aug 24 12:57:44.589: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016178413s
    Aug 24 12:57:46.590: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017191857s
    STEP: Saw pod success 08/24/23 12:57:46.59
    Aug 24 12:57:46.590: INFO: Pod "pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c" satisfied condition "Succeeded or Failed"
    Aug 24 12:57:46.596: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c container env-test: <nil>
    STEP: delete the pod 08/24/23 12:57:46.614
    Aug 24 12:57:46.643: INFO: Waiting for pod pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c to disappear
    Aug 24 12:57:46.650: INFO: Pod pod-configmaps-86e53553-0ed3-44dc-b405-094887f0218c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3446" for this suite. 08/24/23 12:57:46.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:46.702
Aug 24 12:57:46.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 12:57:46.707
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:46.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:46.754
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 12:57:46.759
Aug 24 12:57:46.780: INFO: Waiting up to 5m0s for pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5" in namespace "emptydir-7116" to be "Succeeded or Failed"
Aug 24 12:57:46.786: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49543ms
Aug 24 12:57:48.797: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016890481s
Aug 24 12:57:50.794: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01398961s
Aug 24 12:57:52.795: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014241584s
STEP: Saw pod success 08/24/23 12:57:52.795
Aug 24 12:57:52.795: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5" satisfied condition "Succeeded or Failed"
Aug 24 12:57:52.804: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5 container test-container: <nil>
STEP: delete the pod 08/24/23 12:57:52.817
Aug 24 12:57:52.842: INFO: Waiting for pod pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5 to disappear
Aug 24 12:57:52.848: INFO: Pod pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:52.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7116" for this suite. 08/24/23 12:57:52.863
------------------------------
â€¢ [SLOW TEST] [6.177 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:46.702
    Aug 24 12:57:46.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:57:46.707
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:46.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:46.754
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 12:57:46.759
    Aug 24 12:57:46.780: INFO: Waiting up to 5m0s for pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5" in namespace "emptydir-7116" to be "Succeeded or Failed"
    Aug 24 12:57:46.786: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49543ms
    Aug 24 12:57:48.797: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016890481s
    Aug 24 12:57:50.794: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01398961s
    Aug 24 12:57:52.795: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014241584s
    STEP: Saw pod success 08/24/23 12:57:52.795
    Aug 24 12:57:52.795: INFO: Pod "pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5" satisfied condition "Succeeded or Failed"
    Aug 24 12:57:52.804: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:57:52.817
    Aug 24 12:57:52.842: INFO: Waiting for pod pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5 to disappear
    Aug 24 12:57:52.848: INFO: Pod pod-7f4f2ae2-7531-406d-9b7b-f8392fe398b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:52.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7116" for this suite. 08/24/23 12:57:52.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:52.883
Aug 24 12:57:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 12:57:52.885
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:52.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:52.921
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/24/23 12:57:52.926
STEP: setting up watch 08/24/23 12:57:52.927
STEP: submitting the pod to kubernetes 08/24/23 12:57:53.035
STEP: verifying the pod is in kubernetes 08/24/23 12:57:53.065
STEP: verifying pod creation was observed 08/24/23 12:57:53.074
Aug 24 12:57:53.075: INFO: Waiting up to 5m0s for pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a" in namespace "pods-6577" to be "running"
Aug 24 12:57:53.098: INFO: Pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.450218ms
Aug 24 12:57:55.106: INFO: Pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.031184253s
Aug 24 12:57:55.106: INFO: Pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a" satisfied condition "running"
STEP: deleting the pod gracefully 08/24/23 12:57:55.114
STEP: verifying pod deletion was observed 08/24/23 12:57:55.135
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:57:57.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6577" for this suite. 08/24/23 12:57:57.927
------------------------------
â€¢ [SLOW TEST] [5.057 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:52.883
    Aug 24 12:57:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 12:57:52.885
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:52.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:52.921
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/24/23 12:57:52.926
    STEP: setting up watch 08/24/23 12:57:52.927
    STEP: submitting the pod to kubernetes 08/24/23 12:57:53.035
    STEP: verifying the pod is in kubernetes 08/24/23 12:57:53.065
    STEP: verifying pod creation was observed 08/24/23 12:57:53.074
    Aug 24 12:57:53.075: INFO: Waiting up to 5m0s for pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a" in namespace "pods-6577" to be "running"
    Aug 24 12:57:53.098: INFO: Pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.450218ms
    Aug 24 12:57:55.106: INFO: Pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.031184253s
    Aug 24 12:57:55.106: INFO: Pod "pod-submit-remove-77eadba3-a187-4125-9206-949783fadb8a" satisfied condition "running"
    STEP: deleting the pod gracefully 08/24/23 12:57:55.114
    STEP: verifying pod deletion was observed 08/24/23 12:57:55.135
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:57:57.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6577" for this suite. 08/24/23 12:57:57.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:57:57.958
Aug 24 12:57:57.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:57:57.96
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:57.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:58.001
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-954w6" 08/24/23 12:57:58.014
Aug 24 12:57:58.039: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard cpu limit of 500m
Aug 24 12:57:58.040: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-954w6" /status 08/24/23 12:57:58.04
STEP: Confirm /status for "e2e-rq-status-954w6" resourceQuota via watch 08/24/23 12:57:58.056
Aug 24 12:57:58.062: INFO: observed resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList(nil)
Aug 24 12:57:58.062: INFO: Found resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 24 12:57:58.063: INFO: ResourceQuota "e2e-rq-status-954w6" /status was updated
STEP: Patching hard spec values for cpu & memory 08/24/23 12:57:58.087
Aug 24 12:57:58.111: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard cpu limit of 1
Aug 24 12:57:58.111: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-954w6" /status 08/24/23 12:57:58.111
STEP: Confirm /status for "e2e-rq-status-954w6" resourceQuota via watch 08/24/23 12:57:58.121
Aug 24 12:57:58.124: INFO: observed resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 24 12:57:58.124: INFO: Found resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 24 12:57:58.124: INFO: ResourceQuota "e2e-rq-status-954w6" /status was patched
STEP: Get "e2e-rq-status-954w6" /status 08/24/23 12:57:58.124
Aug 24 12:57:58.164: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard cpu of 1
Aug 24 12:57:58.165: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-954w6" /status before checking Spec is unchanged 08/24/23 12:57:58.174
Aug 24 12:57:58.190: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard cpu of 2
Aug 24 12:57:58.190: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard memory of 2Gi
Aug 24 12:57:58.194: INFO: Found resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 24 13:01:28.209: INFO: ResourceQuota "e2e-rq-status-954w6" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:28.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4515" for this suite. 08/24/23 13:01:28.22
------------------------------
â€¢ [SLOW TEST] [210.277 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:57:57.958
    Aug 24 12:57:57.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:57:57.96
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:57:57.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:57:58.001
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-954w6" 08/24/23 12:57:58.014
    Aug 24 12:57:58.039: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard cpu limit of 500m
    Aug 24 12:57:58.040: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-954w6" /status 08/24/23 12:57:58.04
    STEP: Confirm /status for "e2e-rq-status-954w6" resourceQuota via watch 08/24/23 12:57:58.056
    Aug 24 12:57:58.062: INFO: observed resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList(nil)
    Aug 24 12:57:58.062: INFO: Found resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 24 12:57:58.063: INFO: ResourceQuota "e2e-rq-status-954w6" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/24/23 12:57:58.087
    Aug 24 12:57:58.111: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard cpu limit of 1
    Aug 24 12:57:58.111: INFO: Resource quota "e2e-rq-status-954w6" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-954w6" /status 08/24/23 12:57:58.111
    STEP: Confirm /status for "e2e-rq-status-954w6" resourceQuota via watch 08/24/23 12:57:58.121
    Aug 24 12:57:58.124: INFO: observed resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 24 12:57:58.124: INFO: Found resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 24 12:57:58.124: INFO: ResourceQuota "e2e-rq-status-954w6" /status was patched
    STEP: Get "e2e-rq-status-954w6" /status 08/24/23 12:57:58.124
    Aug 24 12:57:58.164: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard cpu of 1
    Aug 24 12:57:58.165: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-954w6" /status before checking Spec is unchanged 08/24/23 12:57:58.174
    Aug 24 12:57:58.190: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard cpu of 2
    Aug 24 12:57:58.190: INFO: Resourcequota "e2e-rq-status-954w6" reports status: hard memory of 2Gi
    Aug 24 12:57:58.194: INFO: Found resourceQuota "e2e-rq-status-954w6" in namespace "resourcequota-4515" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 24 13:01:28.209: INFO: ResourceQuota "e2e-rq-status-954w6" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:28.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4515" for this suite. 08/24/23 13:01:28.22
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:28.235
Aug 24 13:01:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 13:01:28.238
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:28.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:28.286
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/24/23 13:01:28.292
Aug 24 13:01:28.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d" in namespace "downward-api-5279" to be "Succeeded or Failed"
Aug 24 13:01:28.319: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.80886ms
Aug 24 13:01:30.334: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02477605s
Aug 24 13:01:32.328: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01866966s
STEP: Saw pod success 08/24/23 13:01:32.328
Aug 24 13:01:32.328: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d" satisfied condition "Succeeded or Failed"
Aug 24 13:01:32.336: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d container client-container: <nil>
STEP: delete the pod 08/24/23 13:01:32.37
Aug 24 13:01:32.396: INFO: Waiting for pod downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d to disappear
Aug 24 13:01:32.402: INFO: Pod downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:32.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5279" for this suite. 08/24/23 13:01:32.414
------------------------------
â€¢ [4.194 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:28.235
    Aug 24 13:01:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 13:01:28.238
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:28.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:28.286
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/24/23 13:01:28.292
    Aug 24 13:01:28.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d" in namespace "downward-api-5279" to be "Succeeded or Failed"
    Aug 24 13:01:28.319: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.80886ms
    Aug 24 13:01:30.334: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02477605s
    Aug 24 13:01:32.328: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01866966s
    STEP: Saw pod success 08/24/23 13:01:32.328
    Aug 24 13:01:32.328: INFO: Pod "downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d" satisfied condition "Succeeded or Failed"
    Aug 24 13:01:32.336: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d container client-container: <nil>
    STEP: delete the pod 08/24/23 13:01:32.37
    Aug 24 13:01:32.396: INFO: Waiting for pod downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d to disappear
    Aug 24 13:01:32.402: INFO: Pod downwardapi-volume-739aa9b7-9641-469d-9528-908edb339b0d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:32.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5279" for this suite. 08/24/23 13:01:32.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:32.433
Aug 24 13:01:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename containers 08/24/23 13:01:32.436
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:32.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:32.473
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 24 13:01:32.493: INFO: Waiting up to 5m0s for pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7" in namespace "containers-329" to be "running"
Aug 24 13:01:32.502: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007743ms
Aug 24 13:01:34.513: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019759729s
Aug 24 13:01:36.509: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7": Phase="Running", Reason="", readiness=true. Elapsed: 4.015470762s
Aug 24 13:01:36.509: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-329" for this suite. 08/24/23 13:01:36.534
------------------------------
â€¢ [4.117 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:32.433
    Aug 24 13:01:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename containers 08/24/23 13:01:32.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:32.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:32.473
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 24 13:01:32.493: INFO: Waiting up to 5m0s for pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7" in namespace "containers-329" to be "running"
    Aug 24 13:01:32.502: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007743ms
    Aug 24 13:01:34.513: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019759729s
    Aug 24 13:01:36.509: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7": Phase="Running", Reason="", readiness=true. Elapsed: 4.015470762s
    Aug 24 13:01:36.509: INFO: Pod "client-containers-016a2448-e31a-4647-97b4-3276483d50f7" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-329" for this suite. 08/24/23 13:01:36.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:36.568
Aug 24 13:01:36.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 13:01:36.57
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:36.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:36.616
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/24/23 13:01:36.626
Aug 24 13:01:36.626: INFO: Creating e2e-svc-a-sdwwd
Aug 24 13:01:36.653: INFO: Creating e2e-svc-b-pkxnx
Aug 24 13:01:36.680: INFO: Creating e2e-svc-c-wknbr
STEP: deleting service collection 08/24/23 13:01:36.709
Aug 24 13:01:36.789: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:36.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4410" for this suite. 08/24/23 13:01:36.801
------------------------------
â€¢ [0.252 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:36.568
    Aug 24 13:01:36.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 13:01:36.57
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:36.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:36.616
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/24/23 13:01:36.626
    Aug 24 13:01:36.626: INFO: Creating e2e-svc-a-sdwwd
    Aug 24 13:01:36.653: INFO: Creating e2e-svc-b-pkxnx
    Aug 24 13:01:36.680: INFO: Creating e2e-svc-c-wknbr
    STEP: deleting service collection 08/24/23 13:01:36.709
    Aug 24 13:01:36.789: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:36.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4410" for this suite. 08/24/23 13:01:36.801
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:36.822
Aug 24 13:01:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replication-controller 08/24/23 13:01:36.825
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:36.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:36.86
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/24/23 13:01:36.868
STEP: When the matched label of one of its pods change 08/24/23 13:01:36.902
Aug 24 13:01:36.911: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 24 13:01:41.920: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/24/23 13:01:41.942
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:42.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6712" for this suite. 08/24/23 13:01:43.003
------------------------------
â€¢ [SLOW TEST] [6.201 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:36.822
    Aug 24 13:01:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replication-controller 08/24/23 13:01:36.825
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:36.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:36.86
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/24/23 13:01:36.868
    STEP: When the matched label of one of its pods change 08/24/23 13:01:36.902
    Aug 24 13:01:36.911: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 24 13:01:41.920: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/24/23 13:01:41.942
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:42.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6712" for this suite. 08/24/23 13:01:43.003
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:43.024
Aug 24 13:01:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 13:01:43.025
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:43.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:43.07
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/24/23 13:01:43.109
STEP: Creating a ResourceQuota 08/24/23 13:01:48.116
STEP: Ensuring resource quota status is calculated 08/24/23 13:01:48.13
STEP: Creating a Pod that fits quota 08/24/23 13:01:50.137
STEP: Ensuring ResourceQuota status captures the pod usage 08/24/23 13:01:50.164
STEP: Not allowing a pod to be created that exceeds remaining quota 08/24/23 13:01:52.178
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/24/23 13:01:52.187
STEP: Ensuring a pod cannot update its resource requirements 08/24/23 13:01:52.193
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/24/23 13:01:52.202
STEP: Deleting the pod 08/24/23 13:01:54.212
STEP: Ensuring resource quota status released the pod usage 08/24/23 13:01:54.24
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:56.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2201" for this suite. 08/24/23 13:01:56.258
------------------------------
â€¢ [SLOW TEST] [13.248 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:43.024
    Aug 24 13:01:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 13:01:43.025
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:43.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:43.07
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/24/23 13:01:43.109
    STEP: Creating a ResourceQuota 08/24/23 13:01:48.116
    STEP: Ensuring resource quota status is calculated 08/24/23 13:01:48.13
    STEP: Creating a Pod that fits quota 08/24/23 13:01:50.137
    STEP: Ensuring ResourceQuota status captures the pod usage 08/24/23 13:01:50.164
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/24/23 13:01:52.178
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/24/23 13:01:52.187
    STEP: Ensuring a pod cannot update its resource requirements 08/24/23 13:01:52.193
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/24/23 13:01:52.202
    STEP: Deleting the pod 08/24/23 13:01:54.212
    STEP: Ensuring resource quota status released the pod usage 08/24/23 13:01:54.24
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:56.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2201" for this suite. 08/24/23 13:01:56.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:56.302
Aug 24 13:01:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 13:01:56.306
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:56.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:56.341
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 24 13:01:56.397: INFO: Waiting up to 5m0s for pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad" in namespace "emptydir-wrapper-2893" to be "running and ready"
Aug 24 13:01:56.404: INFO: Pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.468149ms
Aug 24 13:01:56.404: INFO: The phase of Pod pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:01:58.412: INFO: Pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.015041016s
Aug 24 13:01:58.412: INFO: The phase of Pod pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad is Running (Ready = true)
Aug 24 13:01:58.412: INFO: Pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/24/23 13:01:58.418
STEP: Cleaning up the configmap 08/24/23 13:01:58.43
STEP: Cleaning up the pod 08/24/23 13:01:58.449
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 13:01:58.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2893" for this suite. 08/24/23 13:01:58.482
------------------------------
â€¢ [2.199 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:56.302
    Aug 24 13:01:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 13:01:56.306
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:56.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:56.341
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 24 13:01:56.397: INFO: Waiting up to 5m0s for pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad" in namespace "emptydir-wrapper-2893" to be "running and ready"
    Aug 24 13:01:56.404: INFO: Pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad": Phase="Pending", Reason="", readiness=false. Elapsed: 7.468149ms
    Aug 24 13:01:56.404: INFO: The phase of Pod pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:01:58.412: INFO: Pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.015041016s
    Aug 24 13:01:58.412: INFO: The phase of Pod pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad is Running (Ready = true)
    Aug 24 13:01:58.412: INFO: Pod "pod-secrets-7dda175a-70d9-4279-aac0-258039a463ad" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/24/23 13:01:58.418
    STEP: Cleaning up the configmap 08/24/23 13:01:58.43
    STEP: Cleaning up the pod 08/24/23 13:01:58.449
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:01:58.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2893" for this suite. 08/24/23 13:01:58.482
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:01:58.502
Aug 24 13:01:58.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-preemption 08/24/23 13:01:58.505
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:58.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:58.536
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 13:01:58.567: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 13:02:58.636: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/24/23 13:02:58.642
Aug 24 13:02:58.678: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 24 13:02:58.695: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 24 13:02:58.762: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 24 13:02:58.824: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 24 13:02:58.913: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 24 13:02:58.938: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/24/23 13:02:58.939
Aug 24 13:02:58.939: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:02:58.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.406598ms
Aug 24 13:03:00.972: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03305483s
Aug 24 13:03:02.970: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.030575975s
Aug 24 13:03:02.970: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 24 13:03:02.970: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:03:02.977: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.026974ms
Aug 24 13:03:02.977: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 13:03:02.978: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:03:02.985: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.417893ms
Aug 24 13:03:02.985: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 13:03:02.985: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:03:02.993: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.88458ms
Aug 24 13:03:02.994: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 13:03:02.994: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:03:03.000: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.183406ms
Aug 24 13:03:03.000: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 13:03:03.001: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:03:03.007: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.580249ms
Aug 24 13:03:03.007: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/24/23 13:03:03.007
Aug 24 13:03:03.026: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1660" to be "running"
Aug 24 13:03:03.034: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.773728ms
Aug 24 13:03:05.046: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019736367s
Aug 24 13:03:07.079: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052975079s
Aug 24 13:03:07.079: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:07.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1660" for this suite. 08/24/23 13:03:07.237
------------------------------
â€¢ [SLOW TEST] [68.750 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:01:58.502
    Aug 24 13:01:58.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 13:01:58.505
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:01:58.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:01:58.536
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 13:01:58.567: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 13:02:58.636: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/24/23 13:02:58.642
    Aug 24 13:02:58.678: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 24 13:02:58.695: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 24 13:02:58.762: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 24 13:02:58.824: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 24 13:02:58.913: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 24 13:02:58.938: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/24/23 13:02:58.939
    Aug 24 13:02:58.939: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:02:58.961: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.406598ms
    Aug 24 13:03:00.972: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03305483s
    Aug 24 13:03:02.970: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.030575975s
    Aug 24 13:03:02.970: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 24 13:03:02.970: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:03:02.977: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.026974ms
    Aug 24 13:03:02.977: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 13:03:02.978: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:03:02.985: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.417893ms
    Aug 24 13:03:02.985: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 13:03:02.985: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:03:02.993: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.88458ms
    Aug 24 13:03:02.994: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 13:03:02.994: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:03:03.000: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.183406ms
    Aug 24 13:03:03.000: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 13:03:03.001: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:03:03.007: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.580249ms
    Aug 24 13:03:03.007: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/24/23 13:03:03.007
    Aug 24 13:03:03.026: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-1660" to be "running"
    Aug 24 13:03:03.034: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.773728ms
    Aug 24 13:03:05.046: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019736367s
    Aug 24 13:03:07.079: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.052975079s
    Aug 24 13:03:07.079: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:07.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1660" for this suite. 08/24/23 13:03:07.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:07.252
Aug 24 13:03:07.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename security-context 08/24/23 13:03:07.255
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:07.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:07.288
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 13:03:07.293
Aug 24 13:03:07.307: INFO: Waiting up to 5m0s for pod "security-context-a974adab-c395-418d-83fe-3ead041d589a" in namespace "security-context-1864" to be "Succeeded or Failed"
Aug 24 13:03:07.315: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.744175ms
Aug 24 13:03:09.325: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017094852s
Aug 24 13:03:11.325: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017152206s
STEP: Saw pod success 08/24/23 13:03:11.325
Aug 24 13:03:11.326: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a" satisfied condition "Succeeded or Failed"
Aug 24 13:03:11.335: INFO: Trying to get logs from node pe9deep4seen-3 pod security-context-a974adab-c395-418d-83fe-3ead041d589a container test-container: <nil>
STEP: delete the pod 08/24/23 13:03:11.366
Aug 24 13:03:11.397: INFO: Waiting for pod security-context-a974adab-c395-418d-83fe-3ead041d589a to disappear
Aug 24 13:03:11.443: INFO: Pod security-context-a974adab-c395-418d-83fe-3ead041d589a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:11.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1864" for this suite. 08/24/23 13:03:11.467
------------------------------
â€¢ [4.237 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:07.252
    Aug 24 13:03:07.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename security-context 08/24/23 13:03:07.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:07.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:07.288
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 13:03:07.293
    Aug 24 13:03:07.307: INFO: Waiting up to 5m0s for pod "security-context-a974adab-c395-418d-83fe-3ead041d589a" in namespace "security-context-1864" to be "Succeeded or Failed"
    Aug 24 13:03:07.315: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.744175ms
    Aug 24 13:03:09.325: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017094852s
    Aug 24 13:03:11.325: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017152206s
    STEP: Saw pod success 08/24/23 13:03:11.325
    Aug 24 13:03:11.326: INFO: Pod "security-context-a974adab-c395-418d-83fe-3ead041d589a" satisfied condition "Succeeded or Failed"
    Aug 24 13:03:11.335: INFO: Trying to get logs from node pe9deep4seen-3 pod security-context-a974adab-c395-418d-83fe-3ead041d589a container test-container: <nil>
    STEP: delete the pod 08/24/23 13:03:11.366
    Aug 24 13:03:11.397: INFO: Waiting for pod security-context-a974adab-c395-418d-83fe-3ead041d589a to disappear
    Aug 24 13:03:11.443: INFO: Pod security-context-a974adab-c395-418d-83fe-3ead041d589a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:11.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1864" for this suite. 08/24/23 13:03:11.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:11.494
Aug 24 13:03:11.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename podtemplate 08/24/23 13:03:11.496
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:11.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:11.542
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:11.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6976" for this suite. 08/24/23 13:03:11.674
------------------------------
â€¢ [0.199 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:11.494
    Aug 24 13:03:11.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename podtemplate 08/24/23 13:03:11.496
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:11.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:11.542
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:11.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6976" for this suite. 08/24/23 13:03:11.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:11.695
Aug 24 13:03:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replicaset 08/24/23 13:03:11.697
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:11.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:11.76
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/24/23 13:03:11.779
STEP: Verify that the required pods have come up. 08/24/23 13:03:11.807
Aug 24 13:03:11.816: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 13:03:16.830: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 13:03:16.83
STEP: Getting /status 08/24/23 13:03:16.83
Aug 24 13:03:16.845: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/24/23 13:03:16.845
Aug 24 13:03:16.878: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/24/23 13:03:16.878
Aug 24 13:03:16.887: INFO: Observed &ReplicaSet event: ADDED
Aug 24 13:03:16.887: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.887: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.889: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.890: INFO: Found replicaset test-rs in namespace replicaset-8107 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 13:03:16.890: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/24/23 13:03:16.89
Aug 24 13:03:16.891: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 24 13:03:16.923: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/24/23 13:03:16.924
Aug 24 13:03:16.940: INFO: Observed &ReplicaSet event: ADDED
Aug 24 13:03:16.940: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.941: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.941: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.941: INFO: Observed replicaset test-rs in namespace replicaset-8107 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 13:03:16.942: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 13:03:16.942: INFO: Found replicaset test-rs in namespace replicaset-8107 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 24 13:03:16.943: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:16.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8107" for this suite. 08/24/23 13:03:16.965
------------------------------
â€¢ [SLOW TEST] [5.303 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:11.695
    Aug 24 13:03:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replicaset 08/24/23 13:03:11.697
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:11.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:11.76
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/24/23 13:03:11.779
    STEP: Verify that the required pods have come up. 08/24/23 13:03:11.807
    Aug 24 13:03:11.816: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 13:03:16.830: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 13:03:16.83
    STEP: Getting /status 08/24/23 13:03:16.83
    Aug 24 13:03:16.845: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/24/23 13:03:16.845
    Aug 24 13:03:16.878: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/24/23 13:03:16.878
    Aug 24 13:03:16.887: INFO: Observed &ReplicaSet event: ADDED
    Aug 24 13:03:16.887: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.887: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.889: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.890: INFO: Found replicaset test-rs in namespace replicaset-8107 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 13:03:16.890: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/24/23 13:03:16.89
    Aug 24 13:03:16.891: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 24 13:03:16.923: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/24/23 13:03:16.924
    Aug 24 13:03:16.940: INFO: Observed &ReplicaSet event: ADDED
    Aug 24 13:03:16.940: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.941: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.941: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.941: INFO: Observed replicaset test-rs in namespace replicaset-8107 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 13:03:16.942: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 13:03:16.942: INFO: Found replicaset test-rs in namespace replicaset-8107 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 24 13:03:16.943: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:16.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8107" for this suite. 08/24/23 13:03:16.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:17.01
Aug 24 13:03:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 13:03:17.013
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:17.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:17.052
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/24/23 13:03:17.057
Aug 24 13:03:17.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 create -f -'
Aug 24 13:03:18.502: INFO: stderr: ""
Aug 24 13:03:18.502: INFO: stdout: "pod/pause created\n"
Aug 24 13:03:18.502: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 24 13:03:18.502: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8228" to be "running and ready"
Aug 24 13:03:18.508: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.237806ms
Aug 24 13:03:18.508: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pe9deep4seen-3' to be 'Running' but was 'Pending'
Aug 24 13:03:20.519: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016792907s
Aug 24 13:03:20.519: INFO: Pod "pause" satisfied condition "running and ready"
Aug 24 13:03:20.520: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/24/23 13:03:20.52
Aug 24 13:03:20.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 label pods pause testing-label=testing-label-value'
Aug 24 13:03:20.698: INFO: stderr: ""
Aug 24 13:03:20.698: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/24/23 13:03:20.698
Aug 24 13:03:20.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get pod pause -L testing-label'
Aug 24 13:03:20.871: INFO: stderr: ""
Aug 24 13:03:20.871: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/24/23 13:03:20.871
Aug 24 13:03:20.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 label pods pause testing-label-'
Aug 24 13:03:21.045: INFO: stderr: ""
Aug 24 13:03:21.045: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/24/23 13:03:21.045
Aug 24 13:03:21.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get pod pause -L testing-label'
Aug 24 13:03:21.193: INFO: stderr: ""
Aug 24 13:03:21.193: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/24/23 13:03:21.194
Aug 24 13:03:21.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 delete --grace-period=0 --force -f -'
Aug 24 13:03:21.336: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 13:03:21.336: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 24 13:03:21.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get rc,svc -l name=pause --no-headers'
Aug 24 13:03:21.515: INFO: stderr: "No resources found in kubectl-8228 namespace.\n"
Aug 24 13:03:21.515: INFO: stdout: ""
Aug 24 13:03:21.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 24 13:03:21.664: INFO: stderr: ""
Aug 24 13:03:21.664: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:21.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8228" for this suite. 08/24/23 13:03:21.672
------------------------------
â€¢ [4.679 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:17.01
    Aug 24 13:03:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 13:03:17.013
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:17.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:17.052
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/24/23 13:03:17.057
    Aug 24 13:03:17.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 create -f -'
    Aug 24 13:03:18.502: INFO: stderr: ""
    Aug 24 13:03:18.502: INFO: stdout: "pod/pause created\n"
    Aug 24 13:03:18.502: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 24 13:03:18.502: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8228" to be "running and ready"
    Aug 24 13:03:18.508: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.237806ms
    Aug 24 13:03:18.508: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'pe9deep4seen-3' to be 'Running' but was 'Pending'
    Aug 24 13:03:20.519: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016792907s
    Aug 24 13:03:20.519: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 24 13:03:20.520: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/24/23 13:03:20.52
    Aug 24 13:03:20.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 label pods pause testing-label=testing-label-value'
    Aug 24 13:03:20.698: INFO: stderr: ""
    Aug 24 13:03:20.698: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/24/23 13:03:20.698
    Aug 24 13:03:20.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get pod pause -L testing-label'
    Aug 24 13:03:20.871: INFO: stderr: ""
    Aug 24 13:03:20.871: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/24/23 13:03:20.871
    Aug 24 13:03:20.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 label pods pause testing-label-'
    Aug 24 13:03:21.045: INFO: stderr: ""
    Aug 24 13:03:21.045: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/24/23 13:03:21.045
    Aug 24 13:03:21.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get pod pause -L testing-label'
    Aug 24 13:03:21.193: INFO: stderr: ""
    Aug 24 13:03:21.193: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/24/23 13:03:21.194
    Aug 24 13:03:21.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 delete --grace-period=0 --force -f -'
    Aug 24 13:03:21.336: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 13:03:21.336: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 24 13:03:21.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get rc,svc -l name=pause --no-headers'
    Aug 24 13:03:21.515: INFO: stderr: "No resources found in kubectl-8228 namespace.\n"
    Aug 24 13:03:21.515: INFO: stdout: ""
    Aug 24 13:03:21.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-8228 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 24 13:03:21.664: INFO: stderr: ""
    Aug 24 13:03:21.664: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:21.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8228" for this suite. 08/24/23 13:03:21.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:21.693
Aug 24 13:03:21.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubectl 08/24/23 13:03:21.695
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:21.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:21.733
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 13:03:21.738
Aug 24 13:03:21.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5537 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 24 13:03:21.911: INFO: stderr: ""
Aug 24 13:03:21.911: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/24/23 13:03:21.911
Aug 24 13:03:21.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5537 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 24 13:03:22.668: INFO: stderr: ""
Aug 24 13:03:22.668: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 13:03:22.668
Aug 24 13:03:22.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5537 delete pods e2e-test-httpd-pod'
Aug 24 13:03:25.021: INFO: stderr: ""
Aug 24 13:03:25.021: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:25.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5537" for this suite. 08/24/23 13:03:25.038
------------------------------
â€¢ [3.361 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:21.693
    Aug 24 13:03:21.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubectl 08/24/23 13:03:21.695
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:21.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:21.733
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 13:03:21.738
    Aug 24 13:03:21.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5537 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 24 13:03:21.911: INFO: stderr: ""
    Aug 24 13:03:21.911: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/24/23 13:03:21.911
    Aug 24 13:03:21.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5537 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 24 13:03:22.668: INFO: stderr: ""
    Aug 24 13:03:22.668: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 13:03:22.668
    Aug 24 13:03:22.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=kubectl-5537 delete pods e2e-test-httpd-pod'
    Aug 24 13:03:25.021: INFO: stderr: ""
    Aug 24 13:03:25.021: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:25.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5537" for this suite. 08/24/23 13:03:25.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:25.056
Aug 24 13:03:25.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 13:03:25.06
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:25.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:25.097
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-68d91b00-e1bd-4433-8a50-1e423d1c1289 08/24/23 13:03:25.174
STEP: Creating a pod to test consume secrets 08/24/23 13:03:25.192
Aug 24 13:03:25.220: INFO: Waiting up to 5m0s for pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1" in namespace "secrets-1166" to be "Succeeded or Failed"
Aug 24 13:03:25.228: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.402208ms
Aug 24 13:03:27.239: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018980947s
Aug 24 13:03:29.238: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018244836s
STEP: Saw pod success 08/24/23 13:03:29.238
Aug 24 13:03:29.239: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1" satisfied condition "Succeeded or Failed"
Aug 24 13:03:29.258: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 13:03:29.275
Aug 24 13:03:29.294: INFO: Waiting for pod pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1 to disappear
Aug 24 13:03:29.301: INFO: Pod pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:29.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1166" for this suite. 08/24/23 13:03:29.311
STEP: Destroying namespace "secret-namespace-3908" for this suite. 08/24/23 13:03:29.324
------------------------------
â€¢ [4.284 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:25.056
    Aug 24 13:03:25.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 13:03:25.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:25.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:25.097
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-68d91b00-e1bd-4433-8a50-1e423d1c1289 08/24/23 13:03:25.174
    STEP: Creating a pod to test consume secrets 08/24/23 13:03:25.192
    Aug 24 13:03:25.220: INFO: Waiting up to 5m0s for pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1" in namespace "secrets-1166" to be "Succeeded or Failed"
    Aug 24 13:03:25.228: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.402208ms
    Aug 24 13:03:27.239: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018980947s
    Aug 24 13:03:29.238: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018244836s
    STEP: Saw pod success 08/24/23 13:03:29.238
    Aug 24 13:03:29.239: INFO: Pod "pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1" satisfied condition "Succeeded or Failed"
    Aug 24 13:03:29.258: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:03:29.275
    Aug 24 13:03:29.294: INFO: Waiting for pod pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1 to disappear
    Aug 24 13:03:29.301: INFO: Pod pod-secrets-099ed169-abef-4876-a70c-a06152e9b1d1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:29.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1166" for this suite. 08/24/23 13:03:29.311
    STEP: Destroying namespace "secret-namespace-3908" for this suite. 08/24/23 13:03:29.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:29.344
Aug 24 13:03:29.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-webhook 08/24/23 13:03:29.346
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:29.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:29.381
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/24/23 13:03:29.39
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 13:03:29.803
STEP: Deploying the custom resource conversion webhook pod 08/24/23 13:03:29.816
STEP: Wait for the deployment to be ready 08/24/23 13:03:29.835
Aug 24 13:03:29.849: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 13:03:31.869
STEP: Verifying the service has paired with the endpoint 08/24/23 13:03:31.889
Aug 24 13:03:32.890: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 24 13:03:32.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Creating a v1 custom resource 08/24/23 13:03:35.983
STEP: Create a v2 custom resource 08/24/23 13:03:36.021
STEP: List CRs in v1 08/24/23 13:03:36.319
STEP: List CRs in v2 08/24/23 13:03:36.335
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:03:36.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9232" for this suite. 08/24/23 13:03:37.012
------------------------------
â€¢ [SLOW TEST] [7.685 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:29.344
    Aug 24 13:03:29.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-webhook 08/24/23 13:03:29.346
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:29.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:29.381
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/24/23 13:03:29.39
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 13:03:29.803
    STEP: Deploying the custom resource conversion webhook pod 08/24/23 13:03:29.816
    STEP: Wait for the deployment to be ready 08/24/23 13:03:29.835
    Aug 24 13:03:29.849: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 13:03:31.869
    STEP: Verifying the service has paired with the endpoint 08/24/23 13:03:31.889
    Aug 24 13:03:32.890: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 24 13:03:32.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Creating a v1 custom resource 08/24/23 13:03:35.983
    STEP: Create a v2 custom resource 08/24/23 13:03:36.021
    STEP: List CRs in v1 08/24/23 13:03:36.319
    STEP: List CRs in v2 08/24/23 13:03:36.335
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:03:36.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9232" for this suite. 08/24/23 13:03:37.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:03:37.032
Aug 24 13:03:37.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pod-network-test 08/24/23 13:03:37.036
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:37.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:37.081
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6847 08/24/23 13:03:37.084
STEP: creating a selector 08/24/23 13:03:37.085
STEP: Creating the service pods in kubernetes 08/24/23 13:03:37.085
Aug 24 13:03:37.085: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 13:03:37.197: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6847" to be "running and ready"
Aug 24 13:03:37.241: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 43.776356ms
Aug 24 13:03:37.241: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:03:39.252: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.055415427s
Aug 24 13:03:39.253: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:41.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.051762711s
Aug 24 13:03:41.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:43.250: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.052898667s
Aug 24 13:03:43.250: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:45.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.052230035s
Aug 24 13:03:45.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:47.247: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.049612734s
Aug 24 13:03:47.247: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:49.257: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.059818298s
Aug 24 13:03:49.257: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:51.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.052019759s
Aug 24 13:03:51.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:53.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.051470696s
Aug 24 13:03:53.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:55.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.050785342s
Aug 24 13:03:55.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:57.254: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.056996786s
Aug 24 13:03:57.254: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 13:03:59.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.050799722s
Aug 24 13:03:59.248: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 13:03:59.248: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 13:03:59.254: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6847" to be "running and ready"
Aug 24 13:03:59.261: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.489882ms
Aug 24 13:03:59.261: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 13:03:59.261: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 13:03:59.266: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6847" to be "running and ready"
Aug 24 13:03:59.272: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.694566ms
Aug 24 13:03:59.272: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 13:03:59.272: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 13:03:59.279
Aug 24 13:03:59.292: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6847" to be "running"
Aug 24 13:03:59.298: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.753689ms
Aug 24 13:04:01.305: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012853958s
Aug 24 13:04:01.306: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 13:04:01.313: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 13:04:01.313: INFO: Breadth first check of 10.233.64.116 on host 192.168.121.127...
Aug 24 13:04:01.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.250:9080/dial?request=hostname&protocol=udp&host=10.233.64.116&port=8081&tries=1'] Namespace:pod-network-test-6847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 13:04:01.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 13:04:01.322: INFO: ExecWithOptions: Clientset creation
Aug 24 13:04:01.322: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.116%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 13:04:01.511: INFO: Waiting for responses: map[]
Aug 24 13:04:01.511: INFO: reached 10.233.64.116 after 0/1 tries
Aug 24 13:04:01.511: INFO: Breadth first check of 10.233.65.245 on host 192.168.121.111...
Aug 24 13:04:01.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.250:9080/dial?request=hostname&protocol=udp&host=10.233.65.245&port=8081&tries=1'] Namespace:pod-network-test-6847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 13:04:01.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 13:04:01.524: INFO: ExecWithOptions: Clientset creation
Aug 24 13:04:01.524: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.245%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 13:04:01.659: INFO: Waiting for responses: map[]
Aug 24 13:04:01.659: INFO: reached 10.233.65.245 after 0/1 tries
Aug 24 13:04:01.660: INFO: Breadth first check of 10.233.66.26 on host 192.168.121.130...
Aug 24 13:04:01.667: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.250:9080/dial?request=hostname&protocol=udp&host=10.233.66.26&port=8081&tries=1'] Namespace:pod-network-test-6847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 13:04:01.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 13:04:01.669: INFO: ExecWithOptions: Clientset creation
Aug 24 13:04:01.669: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.26%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 13:04:01.797: INFO: Waiting for responses: map[]
Aug 24 13:04:01.797: INFO: reached 10.233.66.26 after 0/1 tries
Aug 24 13:04:01.798: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 13:04:01.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6847" for this suite. 08/24/23 13:04:01.807
------------------------------
â€¢ [SLOW TEST] [24.787 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:03:37.032
    Aug 24 13:03:37.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 13:03:37.036
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:03:37.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:03:37.081
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6847 08/24/23 13:03:37.084
    STEP: creating a selector 08/24/23 13:03:37.085
    STEP: Creating the service pods in kubernetes 08/24/23 13:03:37.085
    Aug 24 13:03:37.085: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 13:03:37.197: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6847" to be "running and ready"
    Aug 24 13:03:37.241: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 43.776356ms
    Aug 24 13:03:37.241: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:03:39.252: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.055415427s
    Aug 24 13:03:39.253: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:41.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.051762711s
    Aug 24 13:03:41.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:43.250: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.052898667s
    Aug 24 13:03:43.250: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:45.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.052230035s
    Aug 24 13:03:45.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:47.247: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.049612734s
    Aug 24 13:03:47.247: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:49.257: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.059818298s
    Aug 24 13:03:49.257: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:51.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.052019759s
    Aug 24 13:03:51.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:53.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.051470696s
    Aug 24 13:03:53.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:55.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.050785342s
    Aug 24 13:03:55.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:57.254: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.056996786s
    Aug 24 13:03:57.254: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 13:03:59.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.050799722s
    Aug 24 13:03:59.248: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 13:03:59.248: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 13:03:59.254: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6847" to be "running and ready"
    Aug 24 13:03:59.261: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.489882ms
    Aug 24 13:03:59.261: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 13:03:59.261: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 13:03:59.266: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6847" to be "running and ready"
    Aug 24 13:03:59.272: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.694566ms
    Aug 24 13:03:59.272: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 13:03:59.272: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 13:03:59.279
    Aug 24 13:03:59.292: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6847" to be "running"
    Aug 24 13:03:59.298: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.753689ms
    Aug 24 13:04:01.305: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012853958s
    Aug 24 13:04:01.306: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 13:04:01.313: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 13:04:01.313: INFO: Breadth first check of 10.233.64.116 on host 192.168.121.127...
    Aug 24 13:04:01.319: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.250:9080/dial?request=hostname&protocol=udp&host=10.233.64.116&port=8081&tries=1'] Namespace:pod-network-test-6847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 13:04:01.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 13:04:01.322: INFO: ExecWithOptions: Clientset creation
    Aug 24 13:04:01.322: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.116%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 13:04:01.511: INFO: Waiting for responses: map[]
    Aug 24 13:04:01.511: INFO: reached 10.233.64.116 after 0/1 tries
    Aug 24 13:04:01.511: INFO: Breadth first check of 10.233.65.245 on host 192.168.121.111...
    Aug 24 13:04:01.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.250:9080/dial?request=hostname&protocol=udp&host=10.233.65.245&port=8081&tries=1'] Namespace:pod-network-test-6847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 13:04:01.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 13:04:01.524: INFO: ExecWithOptions: Clientset creation
    Aug 24 13:04:01.524: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.245%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 13:04:01.659: INFO: Waiting for responses: map[]
    Aug 24 13:04:01.659: INFO: reached 10.233.65.245 after 0/1 tries
    Aug 24 13:04:01.660: INFO: Breadth first check of 10.233.66.26 on host 192.168.121.130...
    Aug 24 13:04:01.667: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.250:9080/dial?request=hostname&protocol=udp&host=10.233.66.26&port=8081&tries=1'] Namespace:pod-network-test-6847 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 13:04:01.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 13:04:01.669: INFO: ExecWithOptions: Clientset creation
    Aug 24 13:04:01.669: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6847/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.26%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 13:04:01.797: INFO: Waiting for responses: map[]
    Aug 24 13:04:01.797: INFO: reached 10.233.66.26 after 0/1 tries
    Aug 24 13:04:01.798: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:04:01.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6847" for this suite. 08/24/23 13:04:01.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:04:01.824
Aug 24 13:04:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replication-controller 08/24/23 13:04:01.827
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:01.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:01.868
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-5vx2v" 08/24/23 13:04:01.874
Aug 24 13:04:01.884: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
Aug 24 13:04:02.891: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
Aug 24 13:04:02.901: INFO: Found 1 replicas for "e2e-rc-5vx2v" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-5vx2v" 08/24/23 13:04:02.901
STEP: Updating a scale subresource 08/24/23 13:04:02.907
STEP: Verifying replicas where modified for replication controller "e2e-rc-5vx2v" 08/24/23 13:04:02.931
Aug 24 13:04:02.932: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
Aug 24 13:04:03.950: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
Aug 24 13:04:03.957: INFO: Found 2 replicas for "e2e-rc-5vx2v" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 13:04:03.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9117" for this suite. 08/24/23 13:04:03.969
------------------------------
â€¢ [2.163 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:04:01.824
    Aug 24 13:04:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replication-controller 08/24/23 13:04:01.827
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:01.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:01.868
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-5vx2v" 08/24/23 13:04:01.874
    Aug 24 13:04:01.884: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
    Aug 24 13:04:02.891: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
    Aug 24 13:04:02.901: INFO: Found 1 replicas for "e2e-rc-5vx2v" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-5vx2v" 08/24/23 13:04:02.901
    STEP: Updating a scale subresource 08/24/23 13:04:02.907
    STEP: Verifying replicas where modified for replication controller "e2e-rc-5vx2v" 08/24/23 13:04:02.931
    Aug 24 13:04:02.932: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
    Aug 24 13:04:03.950: INFO: Get Replication Controller "e2e-rc-5vx2v" to confirm replicas
    Aug 24 13:04:03.957: INFO: Found 2 replicas for "e2e-rc-5vx2v" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:04:03.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9117" for this suite. 08/24/23 13:04:03.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:04:03.989
Aug 24 13:04:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 13:04:03.993
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:04.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:04.033
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 13:04:04.064
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:04:05.019
STEP: Deploying the webhook pod 08/24/23 13:04:05.035
STEP: Wait for the deployment to be ready 08/24/23 13:04:05.056
Aug 24 13:04:05.068: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 13:04:07.127
STEP: Verifying the service has paired with the endpoint 08/24/23 13:04:07.165
Aug 24 13:04:08.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/24/23 13:04:08.18
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:08.18
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/24/23 13:04:08.232
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/24/23 13:04:09.254
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:09.255
STEP: Having no error when timeout is longer than webhook latency 08/24/23 13:04:10.318
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:10.319
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/24/23 13:04:15.381
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:15.381
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:04:20.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2664" for this suite. 08/24/23 13:04:20.583
STEP: Destroying namespace "webhook-2664-markers" for this suite. 08/24/23 13:04:20.606
------------------------------
â€¢ [SLOW TEST] [16.655 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:04:03.989
    Aug 24 13:04:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 13:04:03.993
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:04.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:04.033
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 13:04:04.064
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:04:05.019
    STEP: Deploying the webhook pod 08/24/23 13:04:05.035
    STEP: Wait for the deployment to be ready 08/24/23 13:04:05.056
    Aug 24 13:04:05.068: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 13:04:07.127
    STEP: Verifying the service has paired with the endpoint 08/24/23 13:04:07.165
    Aug 24 13:04:08.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/24/23 13:04:08.18
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:08.18
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/24/23 13:04:08.232
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/24/23 13:04:09.254
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:09.255
    STEP: Having no error when timeout is longer than webhook latency 08/24/23 13:04:10.318
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:10.319
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/24/23 13:04:15.381
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 13:04:15.381
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:04:20.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2664" for this suite. 08/24/23 13:04:20.583
    STEP: Destroying namespace "webhook-2664-markers" for this suite. 08/24/23 13:04:20.606
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:04:20.648
Aug 24 13:04:20.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename namespaces 08/24/23 13:04:20.659
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:20.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:20.711
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/24/23 13:04:20.716
STEP: patching the Namespace 08/24/23 13:04:20.759
STEP: get the Namespace and ensuring it has the label 08/24/23 13:04:20.778
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:04:20.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5995" for this suite. 08/24/23 13:04:20.81
STEP: Destroying namespace "nspatchtest-701b82db-e218-4ff0-ae3c-f9d51ccd2b90-6645" for this suite. 08/24/23 13:04:20.832
------------------------------
â€¢ [0.210 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:04:20.648
    Aug 24 13:04:20.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename namespaces 08/24/23 13:04:20.659
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:20.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:20.711
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/24/23 13:04:20.716
    STEP: patching the Namespace 08/24/23 13:04:20.759
    STEP: get the Namespace and ensuring it has the label 08/24/23 13:04:20.778
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:04:20.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5995" for this suite. 08/24/23 13:04:20.81
    STEP: Destroying namespace "nspatchtest-701b82db-e218-4ff0-ae3c-f9d51ccd2b90-6645" for this suite. 08/24/23 13:04:20.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:04:20.861
Aug 24 13:04:20.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename resourcequota 08/24/23 13:04:20.864
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:20.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:20.924
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/24/23 13:04:20.931
STEP: Creating a ResourceQuota 08/24/23 13:04:25.944
STEP: Ensuring resource quota status is calculated 08/24/23 13:04:25.958
STEP: Creating a ReplicationController 08/24/23 13:04:27.968
STEP: Ensuring resource quota status captures replication controller creation 08/24/23 13:04:27.989
STEP: Deleting a ReplicationController 08/24/23 13:04:29.998
STEP: Ensuring resource quota status released usage 08/24/23 13:04:30.011
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 13:04:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3637" for this suite. 08/24/23 13:04:32.03
------------------------------
â€¢ [SLOW TEST] [11.184 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:04:20.861
    Aug 24 13:04:20.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename resourcequota 08/24/23 13:04:20.864
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:20.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:20.924
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/24/23 13:04:20.931
    STEP: Creating a ResourceQuota 08/24/23 13:04:25.944
    STEP: Ensuring resource quota status is calculated 08/24/23 13:04:25.958
    STEP: Creating a ReplicationController 08/24/23 13:04:27.968
    STEP: Ensuring resource quota status captures replication controller creation 08/24/23 13:04:27.989
    STEP: Deleting a ReplicationController 08/24/23 13:04:29.998
    STEP: Ensuring resource quota status released usage 08/24/23 13:04:30.011
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:04:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3637" for this suite. 08/24/23 13:04:32.03
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:04:32.047
Aug 24 13:04:32.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 13:04:32.049
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:32.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:32.081
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 13:04:32.114
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:04:33.621
STEP: Deploying the webhook pod 08/24/23 13:04:33.631
STEP: Wait for the deployment to be ready 08/24/23 13:04:33.656
Aug 24 13:04:33.670: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 13:04:35.687
STEP: Verifying the service has paired with the endpoint 08/24/23 13:04:35.715
Aug 24 13:04:36.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/24/23 13:04:36.728
STEP: create a pod that should be updated by the webhook 08/24/23 13:04:36.773
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:04:36.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6354" for this suite. 08/24/23 13:04:37.007
STEP: Destroying namespace "webhook-6354-markers" for this suite. 08/24/23 13:04:37.048
------------------------------
â€¢ [SLOW TEST] [5.037 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:04:32.047
    Aug 24 13:04:32.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 13:04:32.049
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:32.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:32.081
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 13:04:32.114
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:04:33.621
    STEP: Deploying the webhook pod 08/24/23 13:04:33.631
    STEP: Wait for the deployment to be ready 08/24/23 13:04:33.656
    Aug 24 13:04:33.670: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 13:04:35.687
    STEP: Verifying the service has paired with the endpoint 08/24/23 13:04:35.715
    Aug 24 13:04:36.717: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/24/23 13:04:36.728
    STEP: create a pod that should be updated by the webhook 08/24/23 13:04:36.773
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:04:36.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6354" for this suite. 08/24/23 13:04:37.007
    STEP: Destroying namespace "webhook-6354-markers" for this suite. 08/24/23 13:04:37.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:04:37.087
Aug 24 13:04:37.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 13:04:37.092
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:37.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:37.21
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/24/23 13:04:37.223
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_tcp@PTR;sleep 1; done
 08/24/23 13:04:37.276
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_tcp@PTR;sleep 1; done
 08/24/23 13:04:37.276
STEP: creating a pod to probe DNS 08/24/23 13:04:37.276
STEP: submitting the pod to kubernetes 08/24/23 13:04:37.276
Aug 24 13:04:37.302: INFO: Waiting up to 15m0s for pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca" in namespace "dns-7140" to be "running"
Aug 24 13:04:37.309: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.337895ms
Aug 24 13:04:39.329: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026880179s
Aug 24 13:04:41.319: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.017151349s
Aug 24 13:04:41.319: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca" satisfied condition "running"
STEP: retrieving the pod 08/24/23 13:04:41.319
STEP: looking for the results for each expected name from probers 08/24/23 13:04:41.326
Aug 24 13:04:41.336: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:41.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:41.359: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:41.392: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:41.400: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:41.408: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:41.458: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local]

Aug 24 13:04:46.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:46.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:46.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:46.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:46.566: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

Aug 24 13:04:51.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:51.495: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:51.554: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:51.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:51.633: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

Aug 24 13:04:56.467: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:56.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:56.535: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:56.545: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:04:56.598: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

Aug 24 13:05:01.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:01.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:01.556: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:01.564: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:01.624: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

Aug 24 13:05:06.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:06.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:06.545: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:06.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
Aug 24 13:05:06.613: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

Aug 24 13:05:11.663: INFO: DNS probes using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca succeeded

STEP: deleting the pod 08/24/23 13:05:11.663
STEP: deleting the test service 08/24/23 13:05:11.744
STEP: deleting the test headless service 08/24/23 13:05:11.881
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 13:05:11.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7140" for this suite. 08/24/23 13:05:11.953
------------------------------
â€¢ [SLOW TEST] [34.889 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:04:37.087
    Aug 24 13:04:37.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 13:04:37.092
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:04:37.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:04:37.21
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/24/23 13:04:37.223
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_tcp@PTR;sleep 1; done
     08/24/23 13:04:37.276
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7140.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7140.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7140.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.2.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.2.77_tcp@PTR;sleep 1; done
     08/24/23 13:04:37.276
    STEP: creating a pod to probe DNS 08/24/23 13:04:37.276
    STEP: submitting the pod to kubernetes 08/24/23 13:04:37.276
    Aug 24 13:04:37.302: INFO: Waiting up to 15m0s for pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca" in namespace "dns-7140" to be "running"
    Aug 24 13:04:37.309: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.337895ms
    Aug 24 13:04:39.329: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026880179s
    Aug 24 13:04:41.319: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.017151349s
    Aug 24 13:04:41.319: INFO: Pod "dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 13:04:41.319
    STEP: looking for the results for each expected name from probers 08/24/23 13:04:41.326
    Aug 24 13:04:41.336: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:41.345: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:41.359: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:41.392: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:41.400: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:41.408: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:41.458: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7140.svc.cluster.local]

    Aug 24 13:04:46.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:46.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:46.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:46.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:46.566: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

    Aug 24 13:04:51.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:51.495: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:51.554: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:51.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:51.633: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

    Aug 24 13:04:56.467: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:56.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:56.535: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:56.545: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:04:56.598: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

    Aug 24 13:05:01.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:01.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:01.556: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:01.564: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:01.624: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

    Aug 24 13:05:06.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:06.489: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:06.545: INFO: Unable to read jessie_udp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:06.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-7140.svc.cluster.local from pod dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca: the server could not find the requested resource (get pods dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca)
    Aug 24 13:05:06.613: INFO: Lookups using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca failed for: [wheezy_udp@dns-test-service.dns-7140.svc.cluster.local wheezy_tcp@dns-test-service.dns-7140.svc.cluster.local jessie_udp@dns-test-service.dns-7140.svc.cluster.local jessie_tcp@dns-test-service.dns-7140.svc.cluster.local]

    Aug 24 13:05:11.663: INFO: DNS probes using dns-7140/dns-test-cce890e7-06ce-4fe4-8dfb-3be1d93e73ca succeeded

    STEP: deleting the pod 08/24/23 13:05:11.663
    STEP: deleting the test service 08/24/23 13:05:11.744
    STEP: deleting the test headless service 08/24/23 13:05:11.881
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:05:11.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7140" for this suite. 08/24/23 13:05:11.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:05:11.986
Aug 24 13:05:11.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename disruption 08/24/23 13:05:11.995
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:05:12.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:05:12.055
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/24/23 13:05:12.083
STEP: Updating PodDisruptionBudget status 08/24/23 13:05:14.107
STEP: Waiting for all pods to be running 08/24/23 13:05:14.122
Aug 24 13:05:14.135: INFO: running pods: 0 < 1
STEP: locating a running pod 08/24/23 13:05:16.144
STEP: Waiting for the pdb to be processed 08/24/23 13:05:16.166
STEP: Patching PodDisruptionBudget status 08/24/23 13:05:16.19
STEP: Waiting for the pdb to be processed 08/24/23 13:05:16.211
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 13:05:16.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5159" for this suite. 08/24/23 13:05:16.227
------------------------------
â€¢ [4.253 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:05:11.986
    Aug 24 13:05:11.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename disruption 08/24/23 13:05:11.995
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:05:12.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:05:12.055
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/24/23 13:05:12.083
    STEP: Updating PodDisruptionBudget status 08/24/23 13:05:14.107
    STEP: Waiting for all pods to be running 08/24/23 13:05:14.122
    Aug 24 13:05:14.135: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/24/23 13:05:16.144
    STEP: Waiting for the pdb to be processed 08/24/23 13:05:16.166
    STEP: Patching PodDisruptionBudget status 08/24/23 13:05:16.19
    STEP: Waiting for the pdb to be processed 08/24/23 13:05:16.211
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:05:16.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5159" for this suite. 08/24/23 13:05:16.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:05:16.25
Aug 24 13:05:16.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename watch 08/24/23 13:05:16.252
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:05:16.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:05:16.289
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/24/23 13:05:16.295
STEP: starting a background goroutine to produce watch events 08/24/23 13:05:16.304
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/24/23 13:05:16.304
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 13:05:19.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6129" for this suite. 08/24/23 13:05:19.11
------------------------------
â€¢ [2.916 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:05:16.25
    Aug 24 13:05:16.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename watch 08/24/23 13:05:16.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:05:16.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:05:16.289
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/24/23 13:05:16.295
    STEP: starting a background goroutine to produce watch events 08/24/23 13:05:16.304
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/24/23 13:05:16.304
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:05:19.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6129" for this suite. 08/24/23 13:05:19.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:05:19.182
Aug 24 13:05:19.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 13:05:19.184
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:05:19.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:05:19.215
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-65425213-97d1-4690-99ee-7c74ebbe3964 in namespace container-probe-8132 08/24/23 13:05:19.224
Aug 24 13:05:19.245: INFO: Waiting up to 5m0s for pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964" in namespace "container-probe-8132" to be "not pending"
Aug 24 13:05:19.256: INFO: Pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476536ms
Aug 24 13:05:21.274: INFO: Pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964": Phase="Running", Reason="", readiness=true. Elapsed: 2.028597097s
Aug 24 13:05:21.274: INFO: Pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964" satisfied condition "not pending"
Aug 24 13:05:21.274: INFO: Started pod liveness-65425213-97d1-4690-99ee-7c74ebbe3964 in namespace container-probe-8132
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 13:05:21.274
Aug 24 13:05:21.283: INFO: Initial restart count of pod liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is 0
Aug 24 13:05:41.381: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 1 (20.097929364s elapsed)
Aug 24 13:06:01.500: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 2 (40.217119369s elapsed)
Aug 24 13:06:21.582: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 3 (1m0.299208342s elapsed)
Aug 24 13:06:41.679: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 4 (1m20.396356058s elapsed)
Aug 24 13:07:41.949: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 5 (2m20.665536362s elapsed)
STEP: deleting the pod 08/24/23 13:07:41.949
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 13:07:42.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8132" for this suite. 08/24/23 13:07:42.016
------------------------------
â€¢ [SLOW TEST] [142.853 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:05:19.182
    Aug 24 13:05:19.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 13:05:19.184
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:05:19.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:05:19.215
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-65425213-97d1-4690-99ee-7c74ebbe3964 in namespace container-probe-8132 08/24/23 13:05:19.224
    Aug 24 13:05:19.245: INFO: Waiting up to 5m0s for pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964" in namespace "container-probe-8132" to be "not pending"
    Aug 24 13:05:19.256: INFO: Pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964": Phase="Pending", Reason="", readiness=false. Elapsed: 10.476536ms
    Aug 24 13:05:21.274: INFO: Pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964": Phase="Running", Reason="", readiness=true. Elapsed: 2.028597097s
    Aug 24 13:05:21.274: INFO: Pod "liveness-65425213-97d1-4690-99ee-7c74ebbe3964" satisfied condition "not pending"
    Aug 24 13:05:21.274: INFO: Started pod liveness-65425213-97d1-4690-99ee-7c74ebbe3964 in namespace container-probe-8132
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 13:05:21.274
    Aug 24 13:05:21.283: INFO: Initial restart count of pod liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is 0
    Aug 24 13:05:41.381: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 1 (20.097929364s elapsed)
    Aug 24 13:06:01.500: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 2 (40.217119369s elapsed)
    Aug 24 13:06:21.582: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 3 (1m0.299208342s elapsed)
    Aug 24 13:06:41.679: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 4 (1m20.396356058s elapsed)
    Aug 24 13:07:41.949: INFO: Restart count of pod container-probe-8132/liveness-65425213-97d1-4690-99ee-7c74ebbe3964 is now 5 (2m20.665536362s elapsed)
    STEP: deleting the pod 08/24/23 13:07:41.949
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:07:42.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8132" for this suite. 08/24/23 13:07:42.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:07:42.043
Aug 24 13:07:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename watch 08/24/23 13:07:42.049
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:07:42.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:07:42.088
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/24/23 13:07:42.091
STEP: modifying the configmap once 08/24/23 13:07:42.103
STEP: modifying the configmap a second time 08/24/23 13:07:42.113
STEP: deleting the configmap 08/24/23 13:07:42.123
STEP: creating a watch on configmaps from the resource version returned by the first update 08/24/23 13:07:42.175
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/24/23 13:07:42.178
Aug 24 13:07:42.178: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-957  0827b2d4-13b6-4958-ae01-fb22c2aa4bc6 35506 0 2023-08-24 13:07:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 13:07:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 13:07:42.179: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-957  0827b2d4-13b6-4958-ae01-fb22c2aa4bc6 35507 0 2023-08-24 13:07:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 13:07:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 13:07:42.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-957" for this suite. 08/24/23 13:07:42.212
------------------------------
â€¢ [0.191 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:07:42.043
    Aug 24 13:07:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename watch 08/24/23 13:07:42.049
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:07:42.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:07:42.088
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/24/23 13:07:42.091
    STEP: modifying the configmap once 08/24/23 13:07:42.103
    STEP: modifying the configmap a second time 08/24/23 13:07:42.113
    STEP: deleting the configmap 08/24/23 13:07:42.123
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/24/23 13:07:42.175
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/24/23 13:07:42.178
    Aug 24 13:07:42.178: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-957  0827b2d4-13b6-4958-ae01-fb22c2aa4bc6 35506 0 2023-08-24 13:07:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 13:07:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 13:07:42.179: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-957  0827b2d4-13b6-4958-ae01-fb22c2aa4bc6 35507 0 2023-08-24 13:07:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 13:07:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:07:42.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-957" for this suite. 08/24/23 13:07:42.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:07:42.239
Aug 24 13:07:42.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 13:07:42.241
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:07:42.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:07:42.273
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577 in namespace container-probe-9005 08/24/23 13:07:42.277
Aug 24 13:07:42.295: INFO: Waiting up to 5m0s for pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577" in namespace "container-probe-9005" to be "not pending"
Aug 24 13:07:42.301: INFO: Pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711805ms
Aug 24 13:07:44.312: INFO: Pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577": Phase="Running", Reason="", readiness=true. Elapsed: 2.01722811s
Aug 24 13:07:44.312: INFO: Pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577" satisfied condition "not pending"
Aug 24 13:07:44.313: INFO: Started pod busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577 in namespace container-probe-9005
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 13:07:44.313
Aug 24 13:07:44.320: INFO: Initial restart count of pod busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577 is 0
STEP: deleting the pod 08/24/23 13:11:45.538
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 13:11:45.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9005" for this suite. 08/24/23 13:11:45.613
------------------------------
â€¢ [SLOW TEST] [243.390 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:07:42.239
    Aug 24 13:07:42.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 13:07:42.241
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:07:42.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:07:42.273
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577 in namespace container-probe-9005 08/24/23 13:07:42.277
    Aug 24 13:07:42.295: INFO: Waiting up to 5m0s for pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577" in namespace "container-probe-9005" to be "not pending"
    Aug 24 13:07:42.301: INFO: Pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711805ms
    Aug 24 13:07:44.312: INFO: Pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577": Phase="Running", Reason="", readiness=true. Elapsed: 2.01722811s
    Aug 24 13:07:44.312: INFO: Pod "busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577" satisfied condition "not pending"
    Aug 24 13:07:44.313: INFO: Started pod busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577 in namespace container-probe-9005
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 13:07:44.313
    Aug 24 13:07:44.320: INFO: Initial restart count of pod busybox-c30b9bd1-e4f3-4960-ade4-1e6f5eb2d577 is 0
    STEP: deleting the pod 08/24/23 13:11:45.538
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:11:45.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9005" for this suite. 08/24/23 13:11:45.613
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:11:45.634
Aug 24 13:11:45.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-runtime 08/24/23 13:11:45.638
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:45.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:45.676
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/24/23 13:11:45.683
STEP: wait for the container to reach Succeeded 08/24/23 13:11:45.701
STEP: get the container status 08/24/23 13:11:49.756
STEP: the container should be terminated 08/24/23 13:11:49.765
STEP: the termination message should be set 08/24/23 13:11:49.765
Aug 24 13:11:49.765: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/24/23 13:11:49.766
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 13:11:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1237" for this suite. 08/24/23 13:11:49.81
------------------------------
â€¢ [4.192 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:11:45.634
    Aug 24 13:11:45.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-runtime 08/24/23 13:11:45.638
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:45.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:45.676
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/24/23 13:11:45.683
    STEP: wait for the container to reach Succeeded 08/24/23 13:11:45.701
    STEP: get the container status 08/24/23 13:11:49.756
    STEP: the container should be terminated 08/24/23 13:11:49.765
    STEP: the termination message should be set 08/24/23 13:11:49.765
    Aug 24 13:11:49.765: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/24/23 13:11:49.766
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:11:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1237" for this suite. 08/24/23 13:11:49.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:11:49.84
Aug 24 13:11:49.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename runtimeclass 08/24/23 13:11:49.844
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:49.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:49.882
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4432-delete-me 08/24/23 13:11:49.899
STEP: Waiting for the RuntimeClass to disappear 08/24/23 13:11:49.915
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 13:11:49.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4432" for this suite. 08/24/23 13:11:49.959
------------------------------
â€¢ [0.138 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:11:49.84
    Aug 24 13:11:49.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 13:11:49.844
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:49.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:49.882
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4432-delete-me 08/24/23 13:11:49.899
    STEP: Waiting for the RuntimeClass to disappear 08/24/23 13:11:49.915
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:11:49.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4432" for this suite. 08/24/23 13:11:49.959
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:11:49.981
Aug 24 13:11:49.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 13:11:49.983
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:50.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:50.032
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-ab02b6a7-160b-439f-a5df-6b4983f453ba 08/24/23 13:11:50.037
STEP: Creating a pod to test consume secrets 08/24/23 13:11:50.047
Aug 24 13:11:50.069: INFO: Waiting up to 5m0s for pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8" in namespace "secrets-5049" to be "Succeeded or Failed"
Aug 24 13:11:50.085: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.576541ms
Aug 24 13:11:52.094: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024314532s
Aug 24 13:11:54.095: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025313026s
STEP: Saw pod success 08/24/23 13:11:54.095
Aug 24 13:11:54.095: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8" satisfied condition "Succeeded or Failed"
Aug 24 13:11:54.100: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 13:11:54.138
Aug 24 13:11:54.201: INFO: Waiting for pod pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8 to disappear
Aug 24 13:11:54.207: INFO: Pod pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 13:11:54.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5049" for this suite. 08/24/23 13:11:54.217
------------------------------
â€¢ [4.252 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:11:49.981
    Aug 24 13:11:49.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 13:11:49.983
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:50.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:50.032
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-ab02b6a7-160b-439f-a5df-6b4983f453ba 08/24/23 13:11:50.037
    STEP: Creating a pod to test consume secrets 08/24/23 13:11:50.047
    Aug 24 13:11:50.069: INFO: Waiting up to 5m0s for pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8" in namespace "secrets-5049" to be "Succeeded or Failed"
    Aug 24 13:11:50.085: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.576541ms
    Aug 24 13:11:52.094: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024314532s
    Aug 24 13:11:54.095: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025313026s
    STEP: Saw pod success 08/24/23 13:11:54.095
    Aug 24 13:11:54.095: INFO: Pod "pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8" satisfied condition "Succeeded or Failed"
    Aug 24 13:11:54.100: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:11:54.138
    Aug 24 13:11:54.201: INFO: Waiting for pod pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8 to disappear
    Aug 24 13:11:54.207: INFO: Pod pod-secrets-703c4443-78df-4a66-9b53-9875de2ed2d8 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:11:54.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5049" for this suite. 08/24/23 13:11:54.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:11:54.236
Aug 24 13:11:54.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 13:11:54.238
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:54.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:54.265
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-2db25835-d6ed-40cc-99a7-78e262dfdd69 08/24/23 13:11:54.269
STEP: Creating a pod to test consume configMaps 08/24/23 13:11:54.277
Aug 24 13:11:54.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df" in namespace "configmap-8659" to be "Succeeded or Failed"
Aug 24 13:11:54.305: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295635ms
Aug 24 13:11:56.316: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019236898s
Aug 24 13:11:58.313: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01652131s
STEP: Saw pod success 08/24/23 13:11:58.313
Aug 24 13:11:58.314: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df" satisfied condition "Succeeded or Failed"
Aug 24 13:11:58.324: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df container agnhost-container: <nil>
STEP: delete the pod 08/24/23 13:11:58.34
Aug 24 13:11:58.362: INFO: Waiting for pod pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df to disappear
Aug 24 13:11:58.374: INFO: Pod pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 13:11:58.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8659" for this suite. 08/24/23 13:11:58.405
------------------------------
â€¢ [4.189 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:11:54.236
    Aug 24 13:11:54.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 13:11:54.238
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:54.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:54.265
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-2db25835-d6ed-40cc-99a7-78e262dfdd69 08/24/23 13:11:54.269
    STEP: Creating a pod to test consume configMaps 08/24/23 13:11:54.277
    Aug 24 13:11:54.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df" in namespace "configmap-8659" to be "Succeeded or Failed"
    Aug 24 13:11:54.305: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295635ms
    Aug 24 13:11:56.316: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019236898s
    Aug 24 13:11:58.313: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01652131s
    STEP: Saw pod success 08/24/23 13:11:58.313
    Aug 24 13:11:58.314: INFO: Pod "pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df" satisfied condition "Succeeded or Failed"
    Aug 24 13:11:58.324: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 13:11:58.34
    Aug 24 13:11:58.362: INFO: Waiting for pod pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df to disappear
    Aug 24 13:11:58.374: INFO: Pod pod-configmaps-5f7b80c7-8877-4a54-b377-75917544b9df no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:11:58.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8659" for this suite. 08/24/23 13:11:58.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:11:58.429
Aug 24 13:11:58.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename containers 08/24/23 13:11:58.431
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:58.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:58.478
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/24/23 13:11:58.483
Aug 24 13:11:58.498: INFO: Waiting up to 5m0s for pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590" in namespace "containers-609" to be "Succeeded or Failed"
Aug 24 13:11:58.510: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020836ms
Aug 24 13:12:00.522: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023599534s
Aug 24 13:12:02.534: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035815319s
STEP: Saw pod success 08/24/23 13:12:02.534
Aug 24 13:12:02.534: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590" satisfied condition "Succeeded or Failed"
Aug 24 13:12:02.543: INFO: Trying to get logs from node pe9deep4seen-3 pod client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 13:12:02.564
Aug 24 13:12:02.586: INFO: Waiting for pod client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590 to disappear
Aug 24 13:12:02.593: INFO: Pod client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 13:12:02.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-609" for this suite. 08/24/23 13:12:02.605
------------------------------
â€¢ [4.192 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:11:58.429
    Aug 24 13:11:58.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename containers 08/24/23 13:11:58.431
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:11:58.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:11:58.478
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/24/23 13:11:58.483
    Aug 24 13:11:58.498: INFO: Waiting up to 5m0s for pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590" in namespace "containers-609" to be "Succeeded or Failed"
    Aug 24 13:11:58.510: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020836ms
    Aug 24 13:12:00.522: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023599534s
    Aug 24 13:12:02.534: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035815319s
    STEP: Saw pod success 08/24/23 13:12:02.534
    Aug 24 13:12:02.534: INFO: Pod "client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590" satisfied condition "Succeeded or Failed"
    Aug 24 13:12:02.543: INFO: Trying to get logs from node pe9deep4seen-3 pod client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 13:12:02.564
    Aug 24 13:12:02.586: INFO: Waiting for pod client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590 to disappear
    Aug 24 13:12:02.593: INFO: Pod client-containers-bfdf2d43-dfb8-465a-aa7f-1a86ac911590 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:12:02.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-609" for this suite. 08/24/23 13:12:02.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:12:02.626
Aug 24 13:12:02.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 13:12:02.63
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:12:02.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:12:02.668
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-bc7fdc65-38d3-4b69-b843-b00fb7fb9bf0 08/24/23 13:12:02.675
STEP: Creating a pod to test consume secrets 08/24/23 13:12:02.705
Aug 24 13:12:02.730: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4" in namespace "projected-7250" to be "Succeeded or Failed"
Aug 24 13:12:02.752: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.690669ms
Aug 24 13:12:04.759: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028823397s
Aug 24 13:12:06.762: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031269995s
Aug 24 13:12:08.759: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02886189s
STEP: Saw pod success 08/24/23 13:12:08.759
Aug 24 13:12:08.760: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4" satisfied condition "Succeeded or Failed"
Aug 24 13:12:08.764: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 13:12:08.776
Aug 24 13:12:08.792: INFO: Waiting for pod pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4 to disappear
Aug 24 13:12:08.796: INFO: Pod pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 13:12:08.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7250" for this suite. 08/24/23 13:12:08.804
------------------------------
â€¢ [SLOW TEST] [6.189 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:12:02.626
    Aug 24 13:12:02.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 13:12:02.63
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:12:02.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:12:02.668
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-bc7fdc65-38d3-4b69-b843-b00fb7fb9bf0 08/24/23 13:12:02.675
    STEP: Creating a pod to test consume secrets 08/24/23 13:12:02.705
    Aug 24 13:12:02.730: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4" in namespace "projected-7250" to be "Succeeded or Failed"
    Aug 24 13:12:02.752: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.690669ms
    Aug 24 13:12:04.759: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028823397s
    Aug 24 13:12:06.762: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031269995s
    Aug 24 13:12:08.759: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02886189s
    STEP: Saw pod success 08/24/23 13:12:08.759
    Aug 24 13:12:08.760: INFO: Pod "pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4" satisfied condition "Succeeded or Failed"
    Aug 24 13:12:08.764: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:12:08.776
    Aug 24 13:12:08.792: INFO: Waiting for pod pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4 to disappear
    Aug 24 13:12:08.796: INFO: Pod pod-projected-secrets-3db75477-2594-45bf-9e68-408e6519bbe4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:12:08.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7250" for this suite. 08/24/23 13:12:08.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:12:08.817
Aug 24 13:12:08.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 13:12:08.82
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:12:08.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:12:08.854
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/24/23 13:12:08.859
STEP: submitting the pod to kubernetes 08/24/23 13:12:08.86
Aug 24 13:12:08.877: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" in namespace "pods-5216" to be "running and ready"
Aug 24 13:12:08.893: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Pending", Reason="", readiness=false. Elapsed: 15.433896ms
Aug 24 13:12:08.893: INFO: The phase of Pod pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:12:10.901: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=true. Elapsed: 2.023527006s
Aug 24 13:12:10.901: INFO: The phase of Pod pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640 is Running (Ready = true)
Aug 24 13:12:10.901: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/24/23 13:12:10.907
STEP: updating the pod 08/24/23 13:12:10.913
Aug 24 13:12:11.435: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640"
Aug 24 13:12:11.436: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" in namespace "pods-5216" to be "terminated with reason DeadlineExceeded"
Aug 24 13:12:11.442: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=true. Elapsed: 6.247469ms
Aug 24 13:12:13.458: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=true. Elapsed: 2.021986185s
Aug 24 13:12:15.449: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=false. Elapsed: 4.013159463s
Aug 24 13:12:17.453: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.017125746s
Aug 24 13:12:17.453: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 13:12:17.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5216" for this suite. 08/24/23 13:12:17.462
------------------------------
â€¢ [SLOW TEST] [8.655 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:12:08.817
    Aug 24 13:12:08.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 13:12:08.82
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:12:08.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:12:08.854
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/24/23 13:12:08.859
    STEP: submitting the pod to kubernetes 08/24/23 13:12:08.86
    Aug 24 13:12:08.877: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" in namespace "pods-5216" to be "running and ready"
    Aug 24 13:12:08.893: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Pending", Reason="", readiness=false. Elapsed: 15.433896ms
    Aug 24 13:12:08.893: INFO: The phase of Pod pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:12:10.901: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=true. Elapsed: 2.023527006s
    Aug 24 13:12:10.901: INFO: The phase of Pod pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640 is Running (Ready = true)
    Aug 24 13:12:10.901: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/24/23 13:12:10.907
    STEP: updating the pod 08/24/23 13:12:10.913
    Aug 24 13:12:11.435: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640"
    Aug 24 13:12:11.436: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" in namespace "pods-5216" to be "terminated with reason DeadlineExceeded"
    Aug 24 13:12:11.442: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=true. Elapsed: 6.247469ms
    Aug 24 13:12:13.458: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=true. Elapsed: 2.021986185s
    Aug 24 13:12:15.449: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Running", Reason="", readiness=false. Elapsed: 4.013159463s
    Aug 24 13:12:17.453: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.017125746s
    Aug 24 13:12:17.453: INFO: Pod "pod-update-activedeadlineseconds-f57a9310-f0fc-4494-ab73-34478f1de640" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:12:17.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5216" for this suite. 08/24/23 13:12:17.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:12:17.476
Aug 24 13:12:17.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 13:12:17.478
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:12:17.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:12:17.506
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5572 08/24/23 13:12:17.511
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/24/23 13:12:17.521
Aug 24 13:12:17.543: INFO: Found 0 stateful pods, waiting for 3
Aug 24 13:12:27.556: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 13:12:27.557: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 13:12:27.557: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 13:12:27.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 13:12:27.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 13:12:27.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 13:12:27.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 13:12:37.935
Aug 24 13:12:37.975: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/24/23 13:12:37.975
STEP: Updating Pods in reverse ordinal order 08/24/23 13:12:48.011
Aug 24 13:12:48.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 13:12:48.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 13:12:48.290: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 13:12:48.290: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/24/23 13:12:58.333
Aug 24 13:12:58.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 13:12:58.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 13:12:58.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 13:12:58.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 13:13:08.707: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/24/23 13:13:18.742
Aug 24 13:13:18.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 13:13:19.065: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 13:13:19.065: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 13:13:19.065: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 13:13:29.110: INFO: Deleting all statefulset in ns statefulset-5572
Aug 24 13:13:29.114: INFO: Scaling statefulset ss2 to 0
Aug 24 13:13:39.147: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 13:13:39.158: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 13:13:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5572" for this suite. 08/24/23 13:13:39.229
------------------------------
â€¢ [SLOW TEST] [81.776 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:12:17.476
    Aug 24 13:12:17.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 13:12:17.478
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:12:17.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:12:17.506
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5572 08/24/23 13:12:17.511
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/24/23 13:12:17.521
    Aug 24 13:12:17.543: INFO: Found 0 stateful pods, waiting for 3
    Aug 24 13:12:27.556: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 13:12:27.557: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 13:12:27.557: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 13:12:27.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 13:12:27.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 13:12:27.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 13:12:27.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 13:12:37.935
    Aug 24 13:12:37.975: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/24/23 13:12:37.975
    STEP: Updating Pods in reverse ordinal order 08/24/23 13:12:48.011
    Aug 24 13:12:48.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 13:12:48.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 13:12:48.290: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 13:12:48.290: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/24/23 13:12:58.333
    Aug 24 13:12:58.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 13:12:58.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 13:12:58.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 13:12:58.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 13:13:08.707: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/24/23 13:13:18.742
    Aug 24 13:13:18.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=statefulset-5572 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 13:13:19.065: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 13:13:19.065: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 13:13:19.065: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 13:13:29.110: INFO: Deleting all statefulset in ns statefulset-5572
    Aug 24 13:13:29.114: INFO: Scaling statefulset ss2 to 0
    Aug 24 13:13:39.147: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 13:13:39.158: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:13:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5572" for this suite. 08/24/23 13:13:39.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:13:39.255
Aug 24 13:13:39.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 13:13:39.258
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:39.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:39.289
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/24/23 13:13:39.298
STEP: watching for the ServiceAccount to be added 08/24/23 13:13:39.312
STEP: patching the ServiceAccount 08/24/23 13:13:39.314
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/24/23 13:13:39.323
STEP: deleting the ServiceAccount 08/24/23 13:13:39.33
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 13:13:39.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4877" for this suite. 08/24/23 13:13:39.363
------------------------------
â€¢ [0.118 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:13:39.255
    Aug 24 13:13:39.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 13:13:39.258
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:39.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:39.289
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/24/23 13:13:39.298
    STEP: watching for the ServiceAccount to be added 08/24/23 13:13:39.312
    STEP: patching the ServiceAccount 08/24/23 13:13:39.314
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/24/23 13:13:39.323
    STEP: deleting the ServiceAccount 08/24/23 13:13:39.33
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:13:39.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4877" for this suite. 08/24/23 13:13:39.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:13:39.381
Aug 24 13:13:39.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 13:13:39.383
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:39.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:39.433
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/24/23 13:13:39.441
Aug 24 13:13:39.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
Aug 24 13:13:42.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:13:51.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-282" for this suite. 08/24/23 13:13:51.808
------------------------------
â€¢ [SLOW TEST] [12.454 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:13:39.381
    Aug 24 13:13:39.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 13:13:39.383
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:39.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:39.433
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/24/23 13:13:39.441
    Aug 24 13:13:39.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    Aug 24 13:13:42.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:13:51.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-282" for this suite. 08/24/23 13:13:51.808
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:13:51.838
Aug 24 13:13:51.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-runtime 08/24/23 13:13:51.84
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:51.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:51.869
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/24/23 13:13:51.873
STEP: wait for the container to reach Failed 08/24/23 13:13:51.926
STEP: get the container status 08/24/23 13:13:55.966
STEP: the container should be terminated 08/24/23 13:13:55.972
STEP: the termination message should be set 08/24/23 13:13:55.972
Aug 24 13:13:55.973: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/24/23 13:13:55.973
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 13:13:55.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6740" for this suite. 08/24/23 13:13:55.998
------------------------------
â€¢ [4.170 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:13:51.838
    Aug 24 13:13:51.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-runtime 08/24/23 13:13:51.84
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:51.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:51.869
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/24/23 13:13:51.873
    STEP: wait for the container to reach Failed 08/24/23 13:13:51.926
    STEP: get the container status 08/24/23 13:13:55.966
    STEP: the container should be terminated 08/24/23 13:13:55.972
    STEP: the termination message should be set 08/24/23 13:13:55.972
    Aug 24 13:13:55.973: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/24/23 13:13:55.973
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:13:55.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6740" for this suite. 08/24/23 13:13:55.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:13:56.012
Aug 24 13:13:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename init-container 08/24/23 13:13:56.013
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:56.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:56.048
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/24/23 13:13:56.052
Aug 24 13:13:56.052: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:14:01.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8063" for this suite. 08/24/23 13:14:01.271
------------------------------
â€¢ [SLOW TEST] [5.269 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:13:56.012
    Aug 24 13:13:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename init-container 08/24/23 13:13:56.013
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:13:56.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:13:56.048
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/24/23 13:13:56.052
    Aug 24 13:13:56.052: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:14:01.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8063" for this suite. 08/24/23 13:14:01.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:14:01.282
Aug 24 13:14:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename podtemplate 08/24/23 13:14:01.285
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:01.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:01.315
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/24/23 13:14:01.319
STEP: Replace a pod template 08/24/23 13:14:01.327
Aug 24 13:14:01.351: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 24 13:14:01.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9998" for this suite. 08/24/23 13:14:01.363
------------------------------
â€¢ [0.094 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:14:01.282
    Aug 24 13:14:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename podtemplate 08/24/23 13:14:01.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:01.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:01.315
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/24/23 13:14:01.319
    STEP: Replace a pod template 08/24/23 13:14:01.327
    Aug 24 13:14:01.351: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:14:01.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9998" for this suite. 08/24/23 13:14:01.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:14:01.381
Aug 24 13:14:01.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 13:14:01.383
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:01.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:01.467
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-20aa8bf0-1eb2-43d3-8958-717c63950716 08/24/23 13:14:01.471
STEP: Creating a pod to test consume configMaps 08/24/23 13:14:01.48
Aug 24 13:14:01.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e" in namespace "configmap-2878" to be "Succeeded or Failed"
Aug 24 13:14:01.511: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.818679ms
Aug 24 13:14:03.519: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01444033s
Aug 24 13:14:05.518: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012996637s
STEP: Saw pod success 08/24/23 13:14:05.518
Aug 24 13:14:05.518: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e" satisfied condition "Succeeded or Failed"
Aug 24 13:14:05.528: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e container agnhost-container: <nil>
STEP: delete the pod 08/24/23 13:14:05.559
Aug 24 13:14:05.585: INFO: Waiting for pod pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e to disappear
Aug 24 13:14:05.593: INFO: Pod pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 13:14:05.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2878" for this suite. 08/24/23 13:14:05.6
------------------------------
â€¢ [4.231 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:14:01.381
    Aug 24 13:14:01.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 13:14:01.383
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:01.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:01.467
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-20aa8bf0-1eb2-43d3-8958-717c63950716 08/24/23 13:14:01.471
    STEP: Creating a pod to test consume configMaps 08/24/23 13:14:01.48
    Aug 24 13:14:01.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e" in namespace "configmap-2878" to be "Succeeded or Failed"
    Aug 24 13:14:01.511: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.818679ms
    Aug 24 13:14:03.519: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01444033s
    Aug 24 13:14:05.518: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012996637s
    STEP: Saw pod success 08/24/23 13:14:05.518
    Aug 24 13:14:05.518: INFO: Pod "pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e" satisfied condition "Succeeded or Failed"
    Aug 24 13:14:05.528: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 13:14:05.559
    Aug 24 13:14:05.585: INFO: Waiting for pod pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e to disappear
    Aug 24 13:14:05.593: INFO: Pod pod-configmaps-d2ac5767-95ff-489c-9b6d-d4509021525e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:14:05.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2878" for this suite. 08/24/23 13:14:05.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:14:05.618
Aug 24 13:14:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 13:14:05.621
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:05.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:05.649
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 13:14:05.677
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:14:07.051
STEP: Deploying the webhook pod 08/24/23 13:14:07.062
STEP: Wait for the deployment to be ready 08/24/23 13:14:07.099
Aug 24 13:14:07.118: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 13:14:09.142
STEP: Verifying the service has paired with the endpoint 08/24/23 13:14:09.162
Aug 24 13:14:10.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 24 13:14:10.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4836-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 13:14:10.694
STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 13:14:10.725
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:14:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1871" for this suite. 08/24/23 13:14:13.695
STEP: Destroying namespace "webhook-1871-markers" for this suite. 08/24/23 13:14:13.728
------------------------------
â€¢ [SLOW TEST] [8.130 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:14:05.618
    Aug 24 13:14:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 13:14:05.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:05.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:05.649
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 13:14:05.677
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:14:07.051
    STEP: Deploying the webhook pod 08/24/23 13:14:07.062
    STEP: Wait for the deployment to be ready 08/24/23 13:14:07.099
    Aug 24 13:14:07.118: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 13:14:09.142
    STEP: Verifying the service has paired with the endpoint 08/24/23 13:14:09.162
    Aug 24 13:14:10.163: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 24 13:14:10.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4836-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 13:14:10.694
    STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 13:14:10.725
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:14:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1871" for this suite. 08/24/23 13:14:13.695
    STEP: Destroying namespace "webhook-1871-markers" for this suite. 08/24/23 13:14:13.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:14:13.777
Aug 24 13:14:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename container-probe 08/24/23 13:14:13.78
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:13.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:13.886
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:13.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6021" for this suite. 08/24/23 13:15:13.934
------------------------------
â€¢ [SLOW TEST] [60.168 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:14:13.777
    Aug 24 13:14:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename container-probe 08/24/23 13:14:13.78
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:14:13.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:14:13.886
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:13.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6021" for this suite. 08/24/23 13:15:13.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:13.956
Aug 24 13:15:13.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename discovery 08/24/23 13:15:13.959
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:13.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:14.002
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/24/23 13:15:14.008
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 24 13:15:14.906: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 24 13:15:14.909: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 24 13:15:14.909: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 24 13:15:14.909: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 24 13:15:14.909: INFO: Checking APIGroup: apps
Aug 24 13:15:14.911: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 24 13:15:14.911: INFO: Versions found [{apps/v1 v1}]
Aug 24 13:15:14.911: INFO: apps/v1 matches apps/v1
Aug 24 13:15:14.911: INFO: Checking APIGroup: events.k8s.io
Aug 24 13:15:14.913: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 24 13:15:14.913: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 24 13:15:14.913: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 24 13:15:14.913: INFO: Checking APIGroup: authentication.k8s.io
Aug 24 13:15:14.915: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 24 13:15:14.915: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 24 13:15:14.915: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 24 13:15:14.915: INFO: Checking APIGroup: authorization.k8s.io
Aug 24 13:15:14.917: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 24 13:15:14.917: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 24 13:15:14.917: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 24 13:15:14.917: INFO: Checking APIGroup: autoscaling
Aug 24 13:15:14.919: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 24 13:15:14.919: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 24 13:15:14.919: INFO: autoscaling/v2 matches autoscaling/v2
Aug 24 13:15:14.919: INFO: Checking APIGroup: batch
Aug 24 13:15:14.921: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 24 13:15:14.921: INFO: Versions found [{batch/v1 v1}]
Aug 24 13:15:14.921: INFO: batch/v1 matches batch/v1
Aug 24 13:15:14.921: INFO: Checking APIGroup: certificates.k8s.io
Aug 24 13:15:14.922: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 24 13:15:14.922: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 24 13:15:14.923: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 24 13:15:14.923: INFO: Checking APIGroup: networking.k8s.io
Aug 24 13:15:14.924: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 24 13:15:14.924: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 24 13:15:14.924: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 24 13:15:14.924: INFO: Checking APIGroup: policy
Aug 24 13:15:14.926: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 24 13:15:14.926: INFO: Versions found [{policy/v1 v1}]
Aug 24 13:15:14.926: INFO: policy/v1 matches policy/v1
Aug 24 13:15:14.926: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 24 13:15:14.928: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 24 13:15:14.928: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 24 13:15:14.928: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 24 13:15:14.928: INFO: Checking APIGroup: storage.k8s.io
Aug 24 13:15:14.929: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 24 13:15:14.929: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 24 13:15:14.929: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 24 13:15:14.930: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 24 13:15:14.931: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 24 13:15:14.931: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 24 13:15:14.931: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 24 13:15:14.931: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 24 13:15:14.933: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 24 13:15:14.933: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 24 13:15:14.933: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 24 13:15:14.934: INFO: Checking APIGroup: scheduling.k8s.io
Aug 24 13:15:14.935: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 24 13:15:14.935: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 24 13:15:14.935: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 24 13:15:14.935: INFO: Checking APIGroup: coordination.k8s.io
Aug 24 13:15:14.937: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 24 13:15:14.937: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 24 13:15:14.937: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 24 13:15:14.937: INFO: Checking APIGroup: node.k8s.io
Aug 24 13:15:14.939: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 24 13:15:14.939: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 24 13:15:14.939: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 24 13:15:14.939: INFO: Checking APIGroup: discovery.k8s.io
Aug 24 13:15:14.942: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 24 13:15:14.942: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 24 13:15:14.942: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 24 13:15:14.942: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 24 13:15:14.945: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 24 13:15:14.945: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 24 13:15:14.945: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 24 13:15:14.945: INFO: Checking APIGroup: cilium.io
Aug 24 13:15:14.946: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Aug 24 13:15:14.947: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
Aug 24 13:15:14.947: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3080" for this suite. 08/24/23 13:15:14.956
------------------------------
â€¢ [1.013 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:13.956
    Aug 24 13:15:13.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename discovery 08/24/23 13:15:13.959
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:13.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:14.002
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/24/23 13:15:14.008
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 24 13:15:14.906: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 24 13:15:14.909: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 24 13:15:14.909: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 24 13:15:14.909: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 24 13:15:14.909: INFO: Checking APIGroup: apps
    Aug 24 13:15:14.911: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 24 13:15:14.911: INFO: Versions found [{apps/v1 v1}]
    Aug 24 13:15:14.911: INFO: apps/v1 matches apps/v1
    Aug 24 13:15:14.911: INFO: Checking APIGroup: events.k8s.io
    Aug 24 13:15:14.913: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 24 13:15:14.913: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 24 13:15:14.913: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 24 13:15:14.913: INFO: Checking APIGroup: authentication.k8s.io
    Aug 24 13:15:14.915: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 24 13:15:14.915: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 24 13:15:14.915: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 24 13:15:14.915: INFO: Checking APIGroup: authorization.k8s.io
    Aug 24 13:15:14.917: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 24 13:15:14.917: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 24 13:15:14.917: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 24 13:15:14.917: INFO: Checking APIGroup: autoscaling
    Aug 24 13:15:14.919: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 24 13:15:14.919: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 24 13:15:14.919: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 24 13:15:14.919: INFO: Checking APIGroup: batch
    Aug 24 13:15:14.921: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 24 13:15:14.921: INFO: Versions found [{batch/v1 v1}]
    Aug 24 13:15:14.921: INFO: batch/v1 matches batch/v1
    Aug 24 13:15:14.921: INFO: Checking APIGroup: certificates.k8s.io
    Aug 24 13:15:14.922: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 24 13:15:14.922: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 24 13:15:14.923: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 24 13:15:14.923: INFO: Checking APIGroup: networking.k8s.io
    Aug 24 13:15:14.924: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 24 13:15:14.924: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 24 13:15:14.924: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 24 13:15:14.924: INFO: Checking APIGroup: policy
    Aug 24 13:15:14.926: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 24 13:15:14.926: INFO: Versions found [{policy/v1 v1}]
    Aug 24 13:15:14.926: INFO: policy/v1 matches policy/v1
    Aug 24 13:15:14.926: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 24 13:15:14.928: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 24 13:15:14.928: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 24 13:15:14.928: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 24 13:15:14.928: INFO: Checking APIGroup: storage.k8s.io
    Aug 24 13:15:14.929: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 24 13:15:14.929: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 24 13:15:14.929: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 24 13:15:14.930: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 24 13:15:14.931: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 24 13:15:14.931: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 24 13:15:14.931: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 24 13:15:14.931: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 24 13:15:14.933: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 24 13:15:14.933: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 24 13:15:14.933: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 24 13:15:14.934: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 24 13:15:14.935: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 24 13:15:14.935: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 24 13:15:14.935: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 24 13:15:14.935: INFO: Checking APIGroup: coordination.k8s.io
    Aug 24 13:15:14.937: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 24 13:15:14.937: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 24 13:15:14.937: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 24 13:15:14.937: INFO: Checking APIGroup: node.k8s.io
    Aug 24 13:15:14.939: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 24 13:15:14.939: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 24 13:15:14.939: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 24 13:15:14.939: INFO: Checking APIGroup: discovery.k8s.io
    Aug 24 13:15:14.942: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 24 13:15:14.942: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 24 13:15:14.942: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 24 13:15:14.942: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 24 13:15:14.945: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 24 13:15:14.945: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 24 13:15:14.945: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 24 13:15:14.945: INFO: Checking APIGroup: cilium.io
    Aug 24 13:15:14.946: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Aug 24 13:15:14.947: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
    Aug 24 13:15:14.947: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3080" for this suite. 08/24/23 13:15:14.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:14.973
Aug 24 13:15:14.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename sched-pred 08/24/23 13:15:14.974
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:15.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:15.015
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 13:15:15.021: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 13:15:15.040: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 13:15:15.082: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
Aug 24 13:15:15.104: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.105: INFO: 	Container node-init ready: true, restart count 0
Aug 24 13:15:15.105: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.105: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 13:15:15.105: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.105: INFO: 	Container coredns ready: true, restart count 0
Aug 24 13:15:15.105: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.105: INFO: 	Container coredns ready: true, restart count 0
Aug 24 13:15:15.106: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.106: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 13:15:15.106: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.106: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 13:15:15.106: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.106: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 13:15:15.106: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.106: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 13:15:15.106: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.106: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 13:15:15.107: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 13:15:15.107: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 13:15:15.107: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 13:15:15.107: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
Aug 24 13:15:15.124: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container node-init ready: true, restart count 0
Aug 24 13:15:15.124: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 24 13:15:15.124: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 13:15:15.124: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container kube-addon-manager ready: true, restart count 0
Aug 24 13:15:15.124: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 24 13:15:15.124: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 24 13:15:15.124: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 13:15:15.124: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 24 13:15:15.124: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 13:15:15.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 13:15:15.124: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 13:15:15.124: INFO: 
Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
Aug 24 13:15:15.139: INFO: test-webserver-49e04467-5347-46a7-92e9-074529e73558 from container-probe-6021 started at 2023-08-24 13:14:13 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.139: INFO: 	Container test-webserver ready: false, restart count 0
Aug 24 13:15:15.139: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.140: INFO: 	Container node-init ready: true, restart count 0
Aug 24 13:15:15.140: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.140: INFO: 	Container cilium-agent ready: true, restart count 0
Aug 24 13:15:15.140: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.140: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 24 13:15:15.140: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
Aug 24 13:15:15.140: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 13:15:15.141: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
Aug 24 13:15:15.141: INFO: 	Container e2e ready: true, restart count 0
Aug 24 13:15:15.141: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 13:15:15.141: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
Aug 24 13:15:15.141: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 13:15:15.142: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 13:15:15.142
Aug 24 13:15:15.160: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4812" to be "running"
Aug 24 13:15:15.169: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.858024ms
Aug 24 13:15:17.180: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.020109214s
Aug 24 13:15:17.181: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 13:15:17.19
STEP: Trying to apply a random label on the found node. 08/24/23 13:15:17.218
STEP: verifying the node has the label kubernetes.io/e2e-2a7e3586-9103-4828-8200-3c8df6e49f6c 42 08/24/23 13:15:17.252
STEP: Trying to relaunch the pod, now with labels. 08/24/23 13:15:17.276
Aug 24 13:15:17.296: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4812" to be "not pending"
Aug 24 13:15:17.306: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 10.516011ms
Aug 24 13:15:19.323: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.027023759s
Aug 24 13:15:19.323: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-2a7e3586-9103-4828-8200-3c8df6e49f6c off the node pe9deep4seen-3 08/24/23 13:15:19.338
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2a7e3586-9103-4828-8200-3c8df6e49f6c 08/24/23 13:15:19.37
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:19.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4812" for this suite. 08/24/23 13:15:19.414
------------------------------
â€¢ [4.469 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:14.973
    Aug 24 13:15:14.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename sched-pred 08/24/23 13:15:14.974
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:15.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:15.015
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 13:15:15.021: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 13:15:15.040: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 13:15:15.082: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-1 before test
    Aug 24 13:15:15.104: INFO: cilium-node-init-wqpdx from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.105: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 13:15:15.105: INFO: cilium-wpzgb from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.105: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 13:15:15.105: INFO: coredns-787d4945fb-8jnm5 from kube-system started at 2023-08-24 11:24:04 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.105: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 13:15:15.105: INFO: coredns-787d4945fb-d76z6 from kube-system started at 2023-08-24 11:24:07 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.105: INFO: 	Container coredns ready: true, restart count 0
    Aug 24 13:15:15.106: INFO: kube-addon-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.106: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 13:15:15.106: INFO: kube-apiserver-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.106: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 13:15:15.106: INFO: kube-controller-manager-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.106: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 13:15:15.106: INFO: kube-proxy-nr5bs from kube-system started at 2023-08-24 11:21:24 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.106: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 13:15:15.106: INFO: kube-scheduler-pe9deep4seen-1 from kube-system started at 2023-08-24 11:25:01 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.106: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 13:15:15.107: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-997gw from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 13:15:15.107: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 13:15:15.107: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 13:15:15.107: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-2 before test
    Aug 24 13:15:15.124: INFO: cilium-node-init-95cbk from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: cilium-operator-75f7897945-8qqz2 from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: cilium-rcknz from kube-system started at 2023-08-24 11:22:51 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: kube-addon-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:37 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: kube-apiserver-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: kube-controller-manager-pe9deep4seen-2 from kube-system started at 2023-08-24 11:22:09 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: kube-proxy-lm2dm from kube-system started at 2023-08-24 11:22:03 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: kube-scheduler-pe9deep4seen-2 from kube-system started at 2023-08-24 11:25:19 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-nxmsl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 13:15:15.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 13:15:15.124: INFO: 
    Logging pods the apiserver thinks is on node pe9deep4seen-3 before test
    Aug 24 13:15:15.139: INFO: test-webserver-49e04467-5347-46a7-92e9-074529e73558 from container-probe-6021 started at 2023-08-24 13:14:13 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.139: INFO: 	Container test-webserver ready: false, restart count 0
    Aug 24 13:15:15.139: INFO: cilium-node-init-pdcw9 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.140: INFO: 	Container node-init ready: true, restart count 0
    Aug 24 13:15:15.140: INFO: cilium-xgc44 from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.140: INFO: 	Container cilium-agent ready: true, restart count 0
    Aug 24 13:15:15.140: INFO: kube-proxy-8vv8d from kube-system started at 2023-08-24 11:26:13 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.140: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 24 13:15:15.140: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:19 +0000 UTC (1 container statuses recorded)
    Aug 24 13:15:15.140: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 13:15:15.141: INFO: sonobuoy-e2e-job-b3f52dde3e8a4a4e from sonobuoy started at 2023-08-24 11:38:31 +0000 UTC (2 container statuses recorded)
    Aug 24 13:15:15.141: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 13:15:15.141: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 13:15:15.141: INFO: sonobuoy-systemd-logs-daemon-set-872ad85b7a0e4b9a-p6l72 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
    Aug 24 13:15:15.141: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 13:15:15.142: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 13:15:15.142
    Aug 24 13:15:15.160: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4812" to be "running"
    Aug 24 13:15:15.169: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.858024ms
    Aug 24 13:15:17.180: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.020109214s
    Aug 24 13:15:17.181: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 13:15:17.19
    STEP: Trying to apply a random label on the found node. 08/24/23 13:15:17.218
    STEP: verifying the node has the label kubernetes.io/e2e-2a7e3586-9103-4828-8200-3c8df6e49f6c 42 08/24/23 13:15:17.252
    STEP: Trying to relaunch the pod, now with labels. 08/24/23 13:15:17.276
    Aug 24 13:15:17.296: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4812" to be "not pending"
    Aug 24 13:15:17.306: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 10.516011ms
    Aug 24 13:15:19.323: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.027023759s
    Aug 24 13:15:19.323: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-2a7e3586-9103-4828-8200-3c8df6e49f6c off the node pe9deep4seen-3 08/24/23 13:15:19.338
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-2a7e3586-9103-4828-8200-3c8df6e49f6c 08/24/23 13:15:19.37
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:19.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4812" for this suite. 08/24/23 13:15:19.414
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:19.443
Aug 24 13:15:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename statefulset 08/24/23 13:15:19.447
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:19.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:19.527
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-757 08/24/23 13:15:19.542
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/24/23 13:15:19.564
STEP: Creating pod with conflicting port in namespace statefulset-757 08/24/23 13:15:19.582
STEP: Waiting until pod test-pod will start running in namespace statefulset-757 08/24/23 13:15:19.608
Aug 24 13:15:19.608: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-757" to be "running"
Aug 24 13:15:19.628: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.774541ms
Aug 24 13:15:21.635: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026445687s
Aug 24 13:15:21.635: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-757 08/24/23 13:15:21.635
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-757 08/24/23 13:15:21.643
Aug 24 13:15:21.670: INFO: Observed stateful pod in namespace: statefulset-757, name: ss-0, uid: 85a0bd14-d783-4049-a40c-26a12a5a1d1d, status phase: Pending. Waiting for statefulset controller to delete.
Aug 24 13:15:21.698: INFO: Observed stateful pod in namespace: statefulset-757, name: ss-0, uid: 85a0bd14-d783-4049-a40c-26a12a5a1d1d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 24 13:15:21.718: INFO: Observed stateful pod in namespace: statefulset-757, name: ss-0, uid: 85a0bd14-d783-4049-a40c-26a12a5a1d1d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 24 13:15:21.722: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-757
STEP: Removing pod with conflicting port in namespace statefulset-757 08/24/23 13:15:21.722
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-757 and will be in running state 08/24/23 13:15:21.759
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 13:15:35.833: INFO: Deleting all statefulset in ns statefulset-757
Aug 24 13:15:35.840: INFO: Scaling statefulset ss to 0
Aug 24 13:15:45.882: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 13:15:45.889: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:45.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-757" for this suite. 08/24/23 13:15:45.946
------------------------------
â€¢ [SLOW TEST] [26.518 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:19.443
    Aug 24 13:15:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename statefulset 08/24/23 13:15:19.447
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:19.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:19.527
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-757 08/24/23 13:15:19.542
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/24/23 13:15:19.564
    STEP: Creating pod with conflicting port in namespace statefulset-757 08/24/23 13:15:19.582
    STEP: Waiting until pod test-pod will start running in namespace statefulset-757 08/24/23 13:15:19.608
    Aug 24 13:15:19.608: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-757" to be "running"
    Aug 24 13:15:19.628: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.774541ms
    Aug 24 13:15:21.635: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026445687s
    Aug 24 13:15:21.635: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-757 08/24/23 13:15:21.635
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-757 08/24/23 13:15:21.643
    Aug 24 13:15:21.670: INFO: Observed stateful pod in namespace: statefulset-757, name: ss-0, uid: 85a0bd14-d783-4049-a40c-26a12a5a1d1d, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 24 13:15:21.698: INFO: Observed stateful pod in namespace: statefulset-757, name: ss-0, uid: 85a0bd14-d783-4049-a40c-26a12a5a1d1d, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 24 13:15:21.718: INFO: Observed stateful pod in namespace: statefulset-757, name: ss-0, uid: 85a0bd14-d783-4049-a40c-26a12a5a1d1d, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 24 13:15:21.722: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-757
    STEP: Removing pod with conflicting port in namespace statefulset-757 08/24/23 13:15:21.722
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-757 and will be in running state 08/24/23 13:15:21.759
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 13:15:35.833: INFO: Deleting all statefulset in ns statefulset-757
    Aug 24 13:15:35.840: INFO: Scaling statefulset ss to 0
    Aug 24 13:15:45.882: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 13:15:45.889: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:45.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-757" for this suite. 08/24/23 13:15:45.946
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:45.965
Aug 24 13:15:45.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 13:15:45.967
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:46.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:46.01
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/24/23 13:15:46.013
Aug 24 13:15:46.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70" in namespace "downward-api-4466" to be "Succeeded or Failed"
Aug 24 13:15:46.037: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919654ms
Aug 24 13:15:48.048: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017489176s
Aug 24 13:15:50.047: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016839894s
STEP: Saw pod success 08/24/23 13:15:50.047
Aug 24 13:15:50.048: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70" satisfied condition "Succeeded or Failed"
Aug 24 13:15:50.054: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70 container client-container: <nil>
STEP: delete the pod 08/24/23 13:15:50.086
Aug 24 13:15:50.120: INFO: Waiting for pod downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70 to disappear
Aug 24 13:15:50.127: INFO: Pod downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:50.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4466" for this suite. 08/24/23 13:15:50.15
------------------------------
â€¢ [4.200 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:45.965
    Aug 24 13:15:45.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 13:15:45.967
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:46.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:46.01
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/24/23 13:15:46.013
    Aug 24 13:15:46.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70" in namespace "downward-api-4466" to be "Succeeded or Failed"
    Aug 24 13:15:46.037: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.919654ms
    Aug 24 13:15:48.048: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017489176s
    Aug 24 13:15:50.047: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016839894s
    STEP: Saw pod success 08/24/23 13:15:50.047
    Aug 24 13:15:50.048: INFO: Pod "downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70" satisfied condition "Succeeded or Failed"
    Aug 24 13:15:50.054: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70 container client-container: <nil>
    STEP: delete the pod 08/24/23 13:15:50.086
    Aug 24 13:15:50.120: INFO: Waiting for pod downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70 to disappear
    Aug 24 13:15:50.127: INFO: Pod downwardapi-volume-9cf4db35-3e8a-445b-9a82-f63e23f79a70 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:50.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4466" for this suite. 08/24/23 13:15:50.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:50.167
Aug 24 13:15:50.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename replicaset 08/24/23 13:15:50.169
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:50.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:50.212
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/24/23 13:15:50.218
STEP: Verify that the required pods have come up 08/24/23 13:15:50.233
Aug 24 13:15:50.243: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 24 13:15:55.261: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/24/23 13:15:55.261
Aug 24 13:15:55.269: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/24/23 13:15:55.27
STEP: DeleteCollection of the ReplicaSets 08/24/23 13:15:55.282
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/24/23 13:15:55.305
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:55.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8834" for this suite. 08/24/23 13:15:55.339
------------------------------
â€¢ [SLOW TEST] [5.234 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:50.167
    Aug 24 13:15:50.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename replicaset 08/24/23 13:15:50.169
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:50.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:50.212
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/24/23 13:15:50.218
    STEP: Verify that the required pods have come up 08/24/23 13:15:50.233
    Aug 24 13:15:50.243: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 24 13:15:55.261: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/24/23 13:15:55.261
    Aug 24 13:15:55.269: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/24/23 13:15:55.27
    STEP: DeleteCollection of the ReplicaSets 08/24/23 13:15:55.282
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/24/23 13:15:55.305
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:55.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8834" for this suite. 08/24/23 13:15:55.339
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:55.401
Aug 24 13:15:55.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 13:15:55.406
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:55.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:55.536
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-3c762b17-c12b-4937-82a9-ea61c64370f1 08/24/23 13:15:55.568
STEP: Creating the pod 08/24/23 13:15:55.582
Aug 24 13:15:55.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f" in namespace "configmap-6443" to be "running"
Aug 24 13:15:55.619: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.788532ms
Aug 24 13:15:57.631: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019443279s
Aug 24 13:15:59.628: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f": Phase="Running", Reason="", readiness=false. Elapsed: 4.016167175s
Aug 24 13:15:59.628: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f" satisfied condition "running"
STEP: Waiting for pod with text data 08/24/23 13:15:59.628
STEP: Waiting for pod with binary data 08/24/23 13:15:59.64
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 13:15:59.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6443" for this suite. 08/24/23 13:15:59.665
------------------------------
â€¢ [4.279 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:55.401
    Aug 24 13:15:55.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 13:15:55.406
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:55.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:55.536
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-3c762b17-c12b-4937-82a9-ea61c64370f1 08/24/23 13:15:55.568
    STEP: Creating the pod 08/24/23 13:15:55.582
    Aug 24 13:15:55.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f" in namespace "configmap-6443" to be "running"
    Aug 24 13:15:55.619: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.788532ms
    Aug 24 13:15:57.631: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019443279s
    Aug 24 13:15:59.628: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f": Phase="Running", Reason="", readiness=false. Elapsed: 4.016167175s
    Aug 24 13:15:59.628: INFO: Pod "pod-configmaps-d4c12069-4452-457a-8629-9fa4b378390f" satisfied condition "running"
    STEP: Waiting for pod with text data 08/24/23 13:15:59.628
    STEP: Waiting for pod with binary data 08/24/23 13:15:59.64
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:15:59.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6443" for this suite. 08/24/23 13:15:59.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:15:59.682
Aug 24 13:15:59.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 13:15:59.684
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:59.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:59.712
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-7b1ea301-d718-45b9-ae99-739402a6eb17 08/24/23 13:15:59.716
STEP: Creating a pod to test consume secrets 08/24/23 13:15:59.721
Aug 24 13:15:59.735: INFO: Waiting up to 5m0s for pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e" in namespace "secrets-1659" to be "Succeeded or Failed"
Aug 24 13:15:59.745: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.174606ms
Aug 24 13:16:01.755: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020335982s
Aug 24 13:16:03.757: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021766486s
Aug 24 13:16:05.760: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025450802s
STEP: Saw pod success 08/24/23 13:16:05.76
Aug 24 13:16:05.761: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e" satisfied condition "Succeeded or Failed"
Aug 24 13:16:05.767: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 13:16:05.78
Aug 24 13:16:05.807: INFO: Waiting for pod pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e to disappear
Aug 24 13:16:05.815: INFO: Pod pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 13:16:05.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1659" for this suite. 08/24/23 13:16:05.826
------------------------------
â€¢ [SLOW TEST] [6.165 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:15:59.682
    Aug 24 13:15:59.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 13:15:59.684
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:15:59.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:15:59.712
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-7b1ea301-d718-45b9-ae99-739402a6eb17 08/24/23 13:15:59.716
    STEP: Creating a pod to test consume secrets 08/24/23 13:15:59.721
    Aug 24 13:15:59.735: INFO: Waiting up to 5m0s for pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e" in namespace "secrets-1659" to be "Succeeded or Failed"
    Aug 24 13:15:59.745: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.174606ms
    Aug 24 13:16:01.755: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020335982s
    Aug 24 13:16:03.757: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021766486s
    Aug 24 13:16:05.760: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025450802s
    STEP: Saw pod success 08/24/23 13:16:05.76
    Aug 24 13:16:05.761: INFO: Pod "pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e" satisfied condition "Succeeded or Failed"
    Aug 24 13:16:05.767: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:16:05.78
    Aug 24 13:16:05.807: INFO: Waiting for pod pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e to disappear
    Aug 24 13:16:05.815: INFO: Pod pod-secrets-ad29ccc7-37e7-498d-ad64-3334f179d07e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:16:05.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1659" for this suite. 08/24/23 13:16:05.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:16:05.854
Aug 24 13:16:05.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubelet-test 08/24/23 13:16:05.868
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:05.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:05.922
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 24 13:16:05.947: INFO: Waiting up to 5m0s for pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0" in namespace "kubelet-test-5806" to be "running and ready"
Aug 24 13:16:05.954: INFO: Pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.827637ms
Aug 24 13:16:05.954: INFO: The phase of Pod busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:16:07.969: INFO: Pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.021651617s
Aug 24 13:16:07.969: INFO: The phase of Pod busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0 is Running (Ready = true)
Aug 24 13:16:07.969: INFO: Pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 13:16:07.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5806" for this suite. 08/24/23 13:16:08.02
------------------------------
â€¢ [2.186 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:16:05.854
    Aug 24 13:16:05.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 13:16:05.868
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:05.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:05.922
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 24 13:16:05.947: INFO: Waiting up to 5m0s for pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0" in namespace "kubelet-test-5806" to be "running and ready"
    Aug 24 13:16:05.954: INFO: Pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.827637ms
    Aug 24 13:16:05.954: INFO: The phase of Pod busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:16:07.969: INFO: Pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0": Phase="Running", Reason="", readiness=true. Elapsed: 2.021651617s
    Aug 24 13:16:07.969: INFO: The phase of Pod busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0 is Running (Ready = true)
    Aug 24 13:16:07.969: INFO: Pod "busybox-scheduling-636b1339-bbc9-4466-84e1-5b49287517c0" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:16:07.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5806" for this suite. 08/24/23 13:16:08.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:16:08.049
Aug 24 13:16:08.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename dns 08/24/23 13:16:08.052
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:08.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:08.126
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/24/23 13:16:08.134
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
 08/24/23 13:16:08.146
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
 08/24/23 13:16:08.147
STEP: creating a pod to probe DNS 08/24/23 13:16:08.147
STEP: submitting the pod to kubernetes 08/24/23 13:16:08.147
Aug 24 13:16:08.181: INFO: Waiting up to 15m0s for pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3" in namespace "dns-8260" to be "running"
Aug 24 13:16:08.196: INFO: Pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.773592ms
Aug 24 13:16:10.206: INFO: Pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.025253145s
Aug 24 13:16:10.206: INFO: Pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3" satisfied condition "running"
STEP: retrieving the pod 08/24/23 13:16:10.206
STEP: looking for the results for each expected name from probers 08/24/23 13:16:10.214
Aug 24 13:16:10.227: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.235: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.243: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.249: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.255: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.260: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.268: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.275: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:10.275: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:15.290: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.297: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.306: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.313: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.318: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.324: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.330: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.335: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:15.335: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:20.286: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.292: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.302: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.308: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.313: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.319: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.324: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.329: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:20.329: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:25.283: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.291: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.297: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.303: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.311: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.316: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.324: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.331: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:25.331: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:30.287: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.294: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.300: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.305: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.310: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.316: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.321: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.329: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:30.329: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:35.286: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.295: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.302: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.309: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.317: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.324: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.332: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.340: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:35.340: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:40.284: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:40.292: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:40.300: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:40.305: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
Aug 24 13:16:40.328: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Aug 24 13:16:45.370: INFO: DNS probes using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 succeeded

STEP: deleting the pod 08/24/23 13:16:45.371
STEP: deleting the test headless service 08/24/23 13:16:45.436
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 13:16:45.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8260" for this suite. 08/24/23 13:16:45.498
------------------------------
â€¢ [SLOW TEST] [37.471 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:16:08.049
    Aug 24 13:16:08.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename dns 08/24/23 13:16:08.052
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:08.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:08.126
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/24/23 13:16:08.134
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
     08/24/23 13:16:08.146
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
     08/24/23 13:16:08.147
    STEP: creating a pod to probe DNS 08/24/23 13:16:08.147
    STEP: submitting the pod to kubernetes 08/24/23 13:16:08.147
    Aug 24 13:16:08.181: INFO: Waiting up to 15m0s for pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3" in namespace "dns-8260" to be "running"
    Aug 24 13:16:08.196: INFO: Pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.773592ms
    Aug 24 13:16:10.206: INFO: Pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.025253145s
    Aug 24 13:16:10.206: INFO: Pod "dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 13:16:10.206
    STEP: looking for the results for each expected name from probers 08/24/23 13:16:10.214
    Aug 24 13:16:10.227: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.235: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.243: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.249: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.255: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.260: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.268: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.275: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:10.275: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:15.290: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.297: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.306: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.313: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.318: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.324: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.330: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.335: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:15.335: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:20.286: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.292: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.302: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.308: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.313: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.319: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.324: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.329: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:20.329: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:25.283: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.291: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.297: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.303: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.311: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.316: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.324: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.331: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:25.331: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:30.287: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.294: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.300: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.305: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.310: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.316: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.321: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.329: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:30.329: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:35.286: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.295: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.302: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.309: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.317: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.324: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.332: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.340: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:35.340: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:40.284: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:40.292: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:40.300: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:40.305: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3: the server could not find the requested resource (get pods dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3)
    Aug 24 13:16:40.328: INFO: Lookups using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Aug 24 13:16:45.370: INFO: DNS probes using dns-8260/dns-test-c9bfce13-0dfb-49fc-9d60-0285ab4ca5e3 succeeded

    STEP: deleting the pod 08/24/23 13:16:45.371
    STEP: deleting the test headless service 08/24/23 13:16:45.436
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:16:45.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8260" for this suite. 08/24/23 13:16:45.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:16:45.538
Aug 24 13:16:45.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename daemonsets 08/24/23 13:16:45.54
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:45.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:45.57
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 08/24/23 13:16:45.616
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 13:16:45.628
Aug 24 13:16:45.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 13:16:45.647: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 13:16:46.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 13:16:46.668: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 13:16:47.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 13:16:47.664: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 13:16:48.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 13:16:48.665: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/24/23 13:16:48.67
Aug 24 13:16:48.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 13:16:48.718: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 13:16:49.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 13:16:49.738: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
Aug 24 13:16:50.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 13:16:50.743: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/24/23 13:16:50.743
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 13:16:50.758
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-342, will wait for the garbage collector to delete the pods 08/24/23 13:16:50.758
Aug 24 13:16:50.849: INFO: Deleting DaemonSet.extensions daemon-set took: 31.278103ms
Aug 24 13:16:50.950: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.953585ms
Aug 24 13:16:53.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 13:16:53.157: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 13:16:53.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38182"},"items":null}

Aug 24 13:16:53.170: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38182"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:16:53.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-342" for this suite. 08/24/23 13:16:53.214
------------------------------
â€¢ [SLOW TEST] [7.701 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:16:45.538
    Aug 24 13:16:45.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename daemonsets 08/24/23 13:16:45.54
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:45.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:45.57
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 08/24/23 13:16:45.616
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 13:16:45.628
    Aug 24 13:16:45.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 13:16:45.647: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 13:16:46.668: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 13:16:46.668: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 13:16:47.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 13:16:47.664: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 13:16:48.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 13:16:48.665: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/24/23 13:16:48.67
    Aug 24 13:16:48.718: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 13:16:48.718: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 13:16:49.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 13:16:49.738: INFO: Node pe9deep4seen-1 is running 0 daemon pod, expected 1
    Aug 24 13:16:50.742: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 13:16:50.743: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/24/23 13:16:50.743
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 13:16:50.758
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-342, will wait for the garbage collector to delete the pods 08/24/23 13:16:50.758
    Aug 24 13:16:50.849: INFO: Deleting DaemonSet.extensions daemon-set took: 31.278103ms
    Aug 24 13:16:50.950: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.953585ms
    Aug 24 13:16:53.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 13:16:53.157: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 13:16:53.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38182"},"items":null}

    Aug 24 13:16:53.170: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38182"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:16:53.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-342" for this suite. 08/24/23 13:16:53.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:16:53.251
Aug 24 13:16:53.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename configmap 08/24/23 13:16:53.257
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:53.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:53.295
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/24/23 13:16:53.304
STEP: fetching the ConfigMap 08/24/23 13:16:53.315
STEP: patching the ConfigMap 08/24/23 13:16:53.321
STEP: listing all ConfigMaps in all namespaces with a label selector 08/24/23 13:16:53.332
STEP: deleting the ConfigMap by collection with a label selector 08/24/23 13:16:53.341
STEP: listing all ConfigMaps in test namespace 08/24/23 13:16:53.353
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 13:16:53.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4792" for this suite. 08/24/23 13:16:53.367
------------------------------
â€¢ [0.129 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:16:53.251
    Aug 24 13:16:53.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename configmap 08/24/23 13:16:53.257
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:53.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:53.295
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/24/23 13:16:53.304
    STEP: fetching the ConfigMap 08/24/23 13:16:53.315
    STEP: patching the ConfigMap 08/24/23 13:16:53.321
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/24/23 13:16:53.332
    STEP: deleting the ConfigMap by collection with a label selector 08/24/23 13:16:53.341
    STEP: listing all ConfigMaps in test namespace 08/24/23 13:16:53.353
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:16:53.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4792" for this suite. 08/24/23 13:16:53.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:16:53.385
Aug 24 13:16:53.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename emptydir 08/24/23 13:16:53.388
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:53.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:53.42
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/24/23 13:16:53.424
Aug 24 13:16:53.438: INFO: Waiting up to 5m0s for pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85" in namespace "emptydir-2739" to be "Succeeded or Failed"
Aug 24 13:16:53.453: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85": Phase="Pending", Reason="", readiness=false. Elapsed: 14.933484ms
Aug 24 13:16:55.462: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023401244s
Aug 24 13:16:57.461: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02228143s
STEP: Saw pod success 08/24/23 13:16:57.461
Aug 24 13:16:57.462: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85" satisfied condition "Succeeded or Failed"
Aug 24 13:16:57.468: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-f43dc148-d23a-48f7-809a-19df98f2be85 container test-container: <nil>
STEP: delete the pod 08/24/23 13:16:57.488
Aug 24 13:16:57.509: INFO: Waiting for pod pod-f43dc148-d23a-48f7-809a-19df98f2be85 to disappear
Aug 24 13:16:57.515: INFO: Pod pod-f43dc148-d23a-48f7-809a-19df98f2be85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 13:16:57.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2739" for this suite. 08/24/23 13:16:57.525
------------------------------
â€¢ [4.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:16:53.385
    Aug 24 13:16:53.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename emptydir 08/24/23 13:16:53.388
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:53.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:53.42
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/24/23 13:16:53.424
    Aug 24 13:16:53.438: INFO: Waiting up to 5m0s for pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85" in namespace "emptydir-2739" to be "Succeeded or Failed"
    Aug 24 13:16:53.453: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85": Phase="Pending", Reason="", readiness=false. Elapsed: 14.933484ms
    Aug 24 13:16:55.462: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023401244s
    Aug 24 13:16:57.461: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02228143s
    STEP: Saw pod success 08/24/23 13:16:57.461
    Aug 24 13:16:57.462: INFO: Pod "pod-f43dc148-d23a-48f7-809a-19df98f2be85" satisfied condition "Succeeded or Failed"
    Aug 24 13:16:57.468: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-f43dc148-d23a-48f7-809a-19df98f2be85 container test-container: <nil>
    STEP: delete the pod 08/24/23 13:16:57.488
    Aug 24 13:16:57.509: INFO: Waiting for pod pod-f43dc148-d23a-48f7-809a-19df98f2be85 to disappear
    Aug 24 13:16:57.515: INFO: Pod pod-f43dc148-d23a-48f7-809a-19df98f2be85 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:16:57.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2739" for this suite. 08/24/23 13:16:57.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:16:57.554
Aug 24 13:16:57.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 13:16:57.556
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:57.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:57.6
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1631 08/24/23 13:16:57.605
STEP: creating replication controller nodeport-test in namespace services-1631 08/24/23 13:16:57.633
I0824 13:16:57.648064      14 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1631, replica count: 2
I0824 13:17:00.700849      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 13:17:00.701: INFO: Creating new exec pod
Aug 24 13:17:00.717: INFO: Waiting up to 5m0s for pod "execpodfjzdq" in namespace "services-1631" to be "running"
Aug 24 13:17:00.724: INFO: Pod "execpodfjzdq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011757ms
Aug 24 13:17:02.731: INFO: Pod "execpodfjzdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.014203314s
Aug 24 13:17:02.731: INFO: Pod "execpodfjzdq" satisfied condition "running"
Aug 24 13:17:03.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 24 13:17:04.076: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 24 13:17:04.076: INFO: stdout: ""
Aug 24 13:17:04.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 10.233.63.220 80'
Aug 24 13:17:04.319: INFO: stderr: "+ nc -v -z -w 2 10.233.63.220 80\nConnection to 10.233.63.220 80 port [tcp/http] succeeded!\n"
Aug 24 13:17:04.319: INFO: stdout: ""
Aug 24 13:17:04.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 192.168.121.130 30281'
Aug 24 13:17:04.594: INFO: stderr: "+ nc -v -z -w 2 192.168.121.130 30281\nConnection to 192.168.121.130 30281 port [tcp/*] succeeded!\n"
Aug 24 13:17:04.594: INFO: stdout: ""
Aug 24 13:17:04.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 30281'
Aug 24 13:17:04.900: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 30281\nConnection to 192.168.121.111 30281 port [tcp/*] succeeded!\n"
Aug 24 13:17:04.900: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 13:17:04.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1631" for this suite. 08/24/23 13:17:04.909
------------------------------
â€¢ [SLOW TEST] [7.368 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:16:57.554
    Aug 24 13:16:57.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 13:16:57.556
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:16:57.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:16:57.6
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1631 08/24/23 13:16:57.605
    STEP: creating replication controller nodeport-test in namespace services-1631 08/24/23 13:16:57.633
    I0824 13:16:57.648064      14 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1631, replica count: 2
    I0824 13:17:00.700849      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 13:17:00.701: INFO: Creating new exec pod
    Aug 24 13:17:00.717: INFO: Waiting up to 5m0s for pod "execpodfjzdq" in namespace "services-1631" to be "running"
    Aug 24 13:17:00.724: INFO: Pod "execpodfjzdq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011757ms
    Aug 24 13:17:02.731: INFO: Pod "execpodfjzdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.014203314s
    Aug 24 13:17:02.731: INFO: Pod "execpodfjzdq" satisfied condition "running"
    Aug 24 13:17:03.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 24 13:17:04.076: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 24 13:17:04.076: INFO: stdout: ""
    Aug 24 13:17:04.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 10.233.63.220 80'
    Aug 24 13:17:04.319: INFO: stderr: "+ nc -v -z -w 2 10.233.63.220 80\nConnection to 10.233.63.220 80 port [tcp/http] succeeded!\n"
    Aug 24 13:17:04.319: INFO: stdout: ""
    Aug 24 13:17:04.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 192.168.121.130 30281'
    Aug 24 13:17:04.594: INFO: stderr: "+ nc -v -z -w 2 192.168.121.130 30281\nConnection to 192.168.121.130 30281 port [tcp/*] succeeded!\n"
    Aug 24 13:17:04.594: INFO: stdout: ""
    Aug 24 13:17:04.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2729572383 --namespace=services-1631 exec execpodfjzdq -- /bin/sh -x -c nc -v -z -w 2 192.168.121.111 30281'
    Aug 24 13:17:04.900: INFO: stderr: "+ nc -v -z -w 2 192.168.121.111 30281\nConnection to 192.168.121.111 30281 port [tcp/*] succeeded!\n"
    Aug 24 13:17:04.900: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:17:04.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1631" for this suite. 08/24/23 13:17:04.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:17:04.923
Aug 24 13:17:04.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename init-container 08/24/23 13:17:04.925
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:04.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:04.962
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/24/23 13:17:04.966
Aug 24 13:17:04.968: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:17:09.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8690" for this suite. 08/24/23 13:17:09.134
------------------------------
â€¢ [4.224 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:17:04.923
    Aug 24 13:17:04.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename init-container 08/24/23 13:17:04.925
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:04.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:04.962
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/24/23 13:17:04.966
    Aug 24 13:17:04.968: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:17:09.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8690" for this suite. 08/24/23 13:17:09.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:17:09.15
Aug 24 13:17:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename gc 08/24/23 13:17:09.152
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:09.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:09.194
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/24/23 13:17:09.21
STEP: create the rc2 08/24/23 13:17:09.219
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/24/23 13:17:14.401
STEP: delete the rc simpletest-rc-to-be-deleted 08/24/23 13:17:22.046
STEP: wait for the rc to be deleted 08/24/23 13:17:22.211
Aug 24 13:17:27.484: INFO: 95 pods remaining
Aug 24 13:17:27.484: INFO: 71 pods has nil DeletionTimestamp
Aug 24 13:17:27.485: INFO: 
Aug 24 13:17:32.316: INFO: 81 pods remaining
Aug 24 13:17:32.316: INFO: 50 pods has nil DeletionTimestamp
Aug 24 13:17:32.317: INFO: 
STEP: Gathering metrics 08/24/23 13:17:37.257
Aug 24 13:17:37.701: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
Aug 24 13:17:37.719: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.684136ms
Aug 24 13:17:37.719: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
Aug 24 13:17:37.720: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
Aug 24 13:17:38.200: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 24 13:17:38.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-296xc" in namespace "gc-2444"
Aug 24 13:17:38.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5jb" in namespace "gc-2444"
Aug 24 13:17:38.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mdvv" in namespace "gc-2444"
Aug 24 13:17:38.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wd4l" in namespace "gc-2444"
Aug 24 13:17:38.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-46fms" in namespace "gc-2444"
Aug 24 13:17:38.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dndj" in namespace "gc-2444"
Aug 24 13:17:38.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jxfj" in namespace "gc-2444"
Aug 24 13:17:39.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mpf2" in namespace "gc-2444"
Aug 24 13:17:39.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s9nn" in namespace "gc-2444"
Aug 24 13:17:39.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wfkr" in namespace "gc-2444"
Aug 24 13:17:39.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-4whcl" in namespace "gc-2444"
Aug 24 13:17:39.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-4z2gj" in namespace "gc-2444"
Aug 24 13:17:39.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-54vzx" in namespace "gc-2444"
Aug 24 13:17:39.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-58gl8" in namespace "gc-2444"
Aug 24 13:17:39.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nggh" in namespace "gc-2444"
Aug 24 13:17:39.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-6htgx" in namespace "gc-2444"
Aug 24 13:17:39.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j6kn" in namespace "gc-2444"
Aug 24 13:17:39.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kwvl" in namespace "gc-2444"
Aug 24 13:17:39.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mx42" in namespace "gc-2444"
Aug 24 13:17:40.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p4ct" in namespace "gc-2444"
Aug 24 13:17:40.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pr76" in namespace "gc-2444"
Aug 24 13:17:40.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tbzp" in namespace "gc-2444"
Aug 24 13:17:40.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bscb" in namespace "gc-2444"
Aug 24 13:17:40.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rf6n" in namespace "gc-2444"
Aug 24 13:17:40.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xl8t" in namespace "gc-2444"
Aug 24 13:17:40.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-98d5g" in namespace "gc-2444"
Aug 24 13:17:41.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-9crll" in namespace "gc-2444"
Aug 24 13:17:41.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fvrj" in namespace "gc-2444"
Aug 24 13:17:41.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ksqm" in namespace "gc-2444"
Aug 24 13:17:41.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t79h" in namespace "gc-2444"
Aug 24 13:17:41.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9z4c" in namespace "gc-2444"
Aug 24 13:17:41.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjgz4" in namespace "gc-2444"
Aug 24 13:17:41.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsrx7" in namespace "gc-2444"
Aug 24 13:17:41.553: INFO: Deleting pod "simpletest-rc-to-be-deleted-btkfw" in namespace "gc-2444"
Aug 24 13:17:41.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4hm7" in namespace "gc-2444"
Aug 24 13:17:41.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9m4f" in namespace "gc-2444"
Aug 24 13:17:41.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch6b6" in namespace "gc-2444"
Aug 24 13:17:41.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjqkl" in namespace "gc-2444"
Aug 24 13:17:41.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-d82qt" in namespace "gc-2444"
Aug 24 13:17:41.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhkjk" in namespace "gc-2444"
Aug 24 13:17:41.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-djhmp" in namespace "gc-2444"
Aug 24 13:17:42.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4xsv" in namespace "gc-2444"
Aug 24 13:17:42.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff6nx" in namespace "gc-2444"
Aug 24 13:17:42.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr55f" in namespace "gc-2444"
Aug 24 13:17:42.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxwgm" in namespace "gc-2444"
Aug 24 13:17:42.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz7qc" in namespace "gc-2444"
Aug 24 13:17:42.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6wwk" in namespace "gc-2444"
Aug 24 13:17:42.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8nxt" in namespace "gc-2444"
Aug 24 13:17:42.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc5mt" in namespace "gc-2444"
Aug 24 13:17:42.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-gggjm" in namespace "gc-2444"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 13:17:42.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2444" for this suite. 08/24/23 13:17:42.862
------------------------------
â€¢ [SLOW TEST] [33.794 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:17:09.15
    Aug 24 13:17:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename gc 08/24/23 13:17:09.152
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:09.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:09.194
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/24/23 13:17:09.21
    STEP: create the rc2 08/24/23 13:17:09.219
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/24/23 13:17:14.401
    STEP: delete the rc simpletest-rc-to-be-deleted 08/24/23 13:17:22.046
    STEP: wait for the rc to be deleted 08/24/23 13:17:22.211
    Aug 24 13:17:27.484: INFO: 95 pods remaining
    Aug 24 13:17:27.484: INFO: 71 pods has nil DeletionTimestamp
    Aug 24 13:17:27.485: INFO: 
    Aug 24 13:17:32.316: INFO: 81 pods remaining
    Aug 24 13:17:32.316: INFO: 50 pods has nil DeletionTimestamp
    Aug 24 13:17:32.317: INFO: 
    STEP: Gathering metrics 08/24/23 13:17:37.257
    Aug 24 13:17:37.701: INFO: Waiting up to 5m0s for pod "kube-controller-manager-pe9deep4seen-2" in namespace "kube-system" to be "running and ready"
    Aug 24 13:17:37.719: INFO: Pod "kube-controller-manager-pe9deep4seen-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.684136ms
    Aug 24 13:17:37.719: INFO: The phase of Pod kube-controller-manager-pe9deep4seen-2 is Running (Ready = true)
    Aug 24 13:17:37.720: INFO: Pod "kube-controller-manager-pe9deep4seen-2" satisfied condition "running and ready"
    Aug 24 13:17:38.200: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 24 13:17:38.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-296xc" in namespace "gc-2444"
    Aug 24 13:17:38.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5jb" in namespace "gc-2444"
    Aug 24 13:17:38.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mdvv" in namespace "gc-2444"
    Aug 24 13:17:38.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wd4l" in namespace "gc-2444"
    Aug 24 13:17:38.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-46fms" in namespace "gc-2444"
    Aug 24 13:17:38.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dndj" in namespace "gc-2444"
    Aug 24 13:17:38.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jxfj" in namespace "gc-2444"
    Aug 24 13:17:39.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mpf2" in namespace "gc-2444"
    Aug 24 13:17:39.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s9nn" in namespace "gc-2444"
    Aug 24 13:17:39.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wfkr" in namespace "gc-2444"
    Aug 24 13:17:39.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-4whcl" in namespace "gc-2444"
    Aug 24 13:17:39.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-4z2gj" in namespace "gc-2444"
    Aug 24 13:17:39.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-54vzx" in namespace "gc-2444"
    Aug 24 13:17:39.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-58gl8" in namespace "gc-2444"
    Aug 24 13:17:39.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nggh" in namespace "gc-2444"
    Aug 24 13:17:39.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-6htgx" in namespace "gc-2444"
    Aug 24 13:17:39.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j6kn" in namespace "gc-2444"
    Aug 24 13:17:39.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kwvl" in namespace "gc-2444"
    Aug 24 13:17:39.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mx42" in namespace "gc-2444"
    Aug 24 13:17:40.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p4ct" in namespace "gc-2444"
    Aug 24 13:17:40.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pr76" in namespace "gc-2444"
    Aug 24 13:17:40.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tbzp" in namespace "gc-2444"
    Aug 24 13:17:40.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bscb" in namespace "gc-2444"
    Aug 24 13:17:40.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rf6n" in namespace "gc-2444"
    Aug 24 13:17:40.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xl8t" in namespace "gc-2444"
    Aug 24 13:17:40.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-98d5g" in namespace "gc-2444"
    Aug 24 13:17:41.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-9crll" in namespace "gc-2444"
    Aug 24 13:17:41.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fvrj" in namespace "gc-2444"
    Aug 24 13:17:41.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ksqm" in namespace "gc-2444"
    Aug 24 13:17:41.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t79h" in namespace "gc-2444"
    Aug 24 13:17:41.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9z4c" in namespace "gc-2444"
    Aug 24 13:17:41.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjgz4" in namespace "gc-2444"
    Aug 24 13:17:41.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsrx7" in namespace "gc-2444"
    Aug 24 13:17:41.553: INFO: Deleting pod "simpletest-rc-to-be-deleted-btkfw" in namespace "gc-2444"
    Aug 24 13:17:41.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4hm7" in namespace "gc-2444"
    Aug 24 13:17:41.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9m4f" in namespace "gc-2444"
    Aug 24 13:17:41.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch6b6" in namespace "gc-2444"
    Aug 24 13:17:41.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjqkl" in namespace "gc-2444"
    Aug 24 13:17:41.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-d82qt" in namespace "gc-2444"
    Aug 24 13:17:41.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhkjk" in namespace "gc-2444"
    Aug 24 13:17:41.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-djhmp" in namespace "gc-2444"
    Aug 24 13:17:42.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4xsv" in namespace "gc-2444"
    Aug 24 13:17:42.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff6nx" in namespace "gc-2444"
    Aug 24 13:17:42.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr55f" in namespace "gc-2444"
    Aug 24 13:17:42.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxwgm" in namespace "gc-2444"
    Aug 24 13:17:42.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz7qc" in namespace "gc-2444"
    Aug 24 13:17:42.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6wwk" in namespace "gc-2444"
    Aug 24 13:17:42.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8nxt" in namespace "gc-2444"
    Aug 24 13:17:42.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc5mt" in namespace "gc-2444"
    Aug 24 13:17:42.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-gggjm" in namespace "gc-2444"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:17:42.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2444" for this suite. 08/24/23 13:17:42.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:17:42.948
Aug 24 13:17:42.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 13:17:42.957
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:43.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:43.084
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 13:17:43.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6666" for this suite. 08/24/23 13:17:43.143
------------------------------
â€¢ [0.246 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:17:42.948
    Aug 24 13:17:42.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 13:17:42.957
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:43.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:43.084
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:17:43.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6666" for this suite. 08/24/23 13:17:43.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:17:43.195
Aug 24 13:17:43.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename services 08/24/23 13:17:43.198
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:43.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:43.265
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/24/23 13:17:43.371
STEP: waiting for available Endpoint 08/24/23 13:17:43.39
STEP: listing all Endpoints 08/24/23 13:17:43.395
STEP: updating the Endpoint 08/24/23 13:17:43.574
STEP: fetching the Endpoint 08/24/23 13:17:43.597
STEP: patching the Endpoint 08/24/23 13:17:43.616
STEP: fetching the Endpoint 08/24/23 13:17:43.654
STEP: deleting the Endpoint by Collection 08/24/23 13:17:43.666
STEP: waiting for Endpoint deletion 08/24/23 13:17:43.709
STEP: fetching the Endpoint 08/24/23 13:17:43.717
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 13:17:43.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8150" for this suite. 08/24/23 13:17:43.75
------------------------------
â€¢ [0.573 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:17:43.195
    Aug 24 13:17:43.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename services 08/24/23 13:17:43.198
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:43.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:43.265
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/24/23 13:17:43.371
    STEP: waiting for available Endpoint 08/24/23 13:17:43.39
    STEP: listing all Endpoints 08/24/23 13:17:43.395
    STEP: updating the Endpoint 08/24/23 13:17:43.574
    STEP: fetching the Endpoint 08/24/23 13:17:43.597
    STEP: patching the Endpoint 08/24/23 13:17:43.616
    STEP: fetching the Endpoint 08/24/23 13:17:43.654
    STEP: deleting the Endpoint by Collection 08/24/23 13:17:43.666
    STEP: waiting for Endpoint deletion 08/24/23 13:17:43.709
    STEP: fetching the Endpoint 08/24/23 13:17:43.717
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:17:43.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8150" for this suite. 08/24/23 13:17:43.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:17:43.772
Aug 24 13:17:43.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename job 08/24/23 13:17:43.786
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:43.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:43.838
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/24/23 13:17:43.844
STEP: Ensuring active pods == parallelism 08/24/23 13:17:43.862
STEP: delete a job 08/24/23 13:17:47.877
STEP: deleting Job.batch foo in namespace job-6907, will wait for the garbage collector to delete the pods 08/24/23 13:17:47.877
Aug 24 13:17:47.995: INFO: Deleting Job.batch foo took: 57.808718ms
Aug 24 13:17:48.196: INFO: Terminating Job.batch foo pods took: 201.192085ms
STEP: Ensuring job was deleted 08/24/23 13:18:19.997
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 13:18:20.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6907" for this suite. 08/24/23 13:18:20.012
------------------------------
â€¢ [SLOW TEST] [36.251 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:17:43.772
    Aug 24 13:17:43.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename job 08/24/23 13:17:43.786
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:17:43.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:17:43.838
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/24/23 13:17:43.844
    STEP: Ensuring active pods == parallelism 08/24/23 13:17:43.862
    STEP: delete a job 08/24/23 13:17:47.877
    STEP: deleting Job.batch foo in namespace job-6907, will wait for the garbage collector to delete the pods 08/24/23 13:17:47.877
    Aug 24 13:17:47.995: INFO: Deleting Job.batch foo took: 57.808718ms
    Aug 24 13:17:48.196: INFO: Terminating Job.batch foo pods took: 201.192085ms
    STEP: Ensuring job was deleted 08/24/23 13:18:19.997
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:18:20.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6907" for this suite. 08/24/23 13:18:20.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:18:20.029
Aug 24 13:18:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename webhook 08/24/23 13:18:20.031
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:18:20.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:18:20.066
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 13:18:20.1
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:18:20.946
STEP: Deploying the webhook pod 08/24/23 13:18:20.957
STEP: Wait for the deployment to be ready 08/24/23 13:18:20.98
Aug 24 13:18:21.002: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 13:18:23.032
STEP: Verifying the service has paired with the endpoint 08/24/23 13:18:23.062
Aug 24 13:18:24.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/24/23 13:18:24.071
STEP: Creating a custom resource definition that should be denied by the webhook 08/24/23 13:18:24.111
Aug 24 13:18:24.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 13:18:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6724" for this suite. 08/24/23 13:18:24.274
STEP: Destroying namespace "webhook-6724-markers" for this suite. 08/24/23 13:18:24.287
------------------------------
â€¢ [4.279 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:18:20.029
    Aug 24 13:18:20.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename webhook 08/24/23 13:18:20.031
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:18:20.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:18:20.066
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 13:18:20.1
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 13:18:20.946
    STEP: Deploying the webhook pod 08/24/23 13:18:20.957
    STEP: Wait for the deployment to be ready 08/24/23 13:18:20.98
    Aug 24 13:18:21.002: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 13:18:23.032
    STEP: Verifying the service has paired with the endpoint 08/24/23 13:18:23.062
    Aug 24 13:18:24.063: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/24/23 13:18:24.071
    STEP: Creating a custom resource definition that should be denied by the webhook 08/24/23 13:18:24.111
    Aug 24 13:18:24.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:18:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6724" for this suite. 08/24/23 13:18:24.274
    STEP: Destroying namespace "webhook-6724-markers" for this suite. 08/24/23 13:18:24.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:18:24.314
Aug 24 13:18:24.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename cronjob 08/24/23 13:18:24.316
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:18:24.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:18:24.383
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/24/23 13:18:24.395
STEP: Ensuring a job is scheduled 08/24/23 13:18:24.409
STEP: Ensuring exactly one is scheduled 08/24/23 13:19:00.417
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 13:19:00.422
STEP: Ensuring the job is replaced with a new one 08/24/23 13:19:00.428
STEP: Removing cronjob 08/24/23 13:20:00.436
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:00.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1717" for this suite. 08/24/23 13:20:00.471
------------------------------
â€¢ [SLOW TEST] [96.176 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:18:24.314
    Aug 24 13:18:24.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename cronjob 08/24/23 13:18:24.316
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:18:24.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:18:24.383
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/24/23 13:18:24.395
    STEP: Ensuring a job is scheduled 08/24/23 13:18:24.409
    STEP: Ensuring exactly one is scheduled 08/24/23 13:19:00.417
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 13:19:00.422
    STEP: Ensuring the job is replaced with a new one 08/24/23 13:19:00.428
    STEP: Removing cronjob 08/24/23 13:20:00.436
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:00.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1717" for this suite. 08/24/23 13:20:00.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:00.503
Aug 24 13:20:00.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename secrets 08/24/23 13:20:00.506
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:00.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:00.55
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-1ace88eb-68ec-49b1-b267-a1e874d79e4f 08/24/23 13:20:00.557
STEP: Creating a pod to test consume secrets 08/24/23 13:20:00.569
Aug 24 13:20:00.588: INFO: Waiting up to 5m0s for pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3" in namespace "secrets-4604" to be "Succeeded or Failed"
Aug 24 13:20:00.598: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073448ms
Aug 24 13:20:02.610: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021799747s
Aug 24 13:20:04.607: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018796888s
STEP: Saw pod success 08/24/23 13:20:04.607
Aug 24 13:20:04.607: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3" satisfied condition "Succeeded or Failed"
Aug 24 13:20:04.616: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 13:20:04.651
Aug 24 13:20:04.675: INFO: Waiting for pod pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3 to disappear
Aug 24 13:20:04.681: INFO: Pod pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:04.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4604" for this suite. 08/24/23 13:20:04.693
------------------------------
â€¢ [4.206 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:00.503
    Aug 24 13:20:00.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename secrets 08/24/23 13:20:00.506
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:00.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:00.55
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-1ace88eb-68ec-49b1-b267-a1e874d79e4f 08/24/23 13:20:00.557
    STEP: Creating a pod to test consume secrets 08/24/23 13:20:00.569
    Aug 24 13:20:00.588: INFO: Waiting up to 5m0s for pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3" in namespace "secrets-4604" to be "Succeeded or Failed"
    Aug 24 13:20:00.598: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073448ms
    Aug 24 13:20:02.610: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021799747s
    Aug 24 13:20:04.607: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018796888s
    STEP: Saw pod success 08/24/23 13:20:04.607
    Aug 24 13:20:04.607: INFO: Pod "pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:04.616: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:20:04.651
    Aug 24 13:20:04.675: INFO: Waiting for pod pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3 to disappear
    Aug 24 13:20:04.681: INFO: Pod pod-secrets-a57a5d16-ae2c-417a-a676-e25f8a9d55c3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:04.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4604" for this suite. 08/24/23 13:20:04.693
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:04.712
Aug 24 13:20:04.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 13:20:04.715
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:04.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:04.748
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/24/23 13:20:04.754
STEP: getting 08/24/23 13:20:04.783
STEP: listing in namespace 08/24/23 13:20:04.797
STEP: patching 08/24/23 13:20:04.806
STEP: deleting 08/24/23 13:20:04.839
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:04.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6324" for this suite. 08/24/23 13:20:04.867
------------------------------
â€¢ [0.171 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:04.712
    Aug 24 13:20:04.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 13:20:04.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:04.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:04.748
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/24/23 13:20:04.754
    STEP: getting 08/24/23 13:20:04.783
    STEP: listing in namespace 08/24/23 13:20:04.797
    STEP: patching 08/24/23 13:20:04.806
    STEP: deleting 08/24/23 13:20:04.839
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:04.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6324" for this suite. 08/24/23 13:20:04.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:04.887
Aug 24 13:20:04.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 13:20:04.889
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:04.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:04.92
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/24/23 13:20:04.964
Aug 24 13:20:04.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd" in namespace "downward-api-7803" to be "Succeeded or Failed"
Aug 24 13:20:04.992: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.682ms
Aug 24 13:20:07.011: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029497935s
Aug 24 13:20:09.002: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020433363s
STEP: Saw pod success 08/24/23 13:20:09.002
Aug 24 13:20:09.002: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd" satisfied condition "Succeeded or Failed"
Aug 24 13:20:09.010: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd container client-container: <nil>
STEP: delete the pod 08/24/23 13:20:09.022
Aug 24 13:20:09.044: INFO: Waiting for pod downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd to disappear
Aug 24 13:20:09.048: INFO: Pod downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:09.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7803" for this suite. 08/24/23 13:20:09.057
------------------------------
â€¢ [4.198 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:04.887
    Aug 24 13:20:04.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 13:20:04.889
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:04.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:04.92
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/24/23 13:20:04.964
    Aug 24 13:20:04.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd" in namespace "downward-api-7803" to be "Succeeded or Failed"
    Aug 24 13:20:04.992: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.682ms
    Aug 24 13:20:07.011: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029497935s
    Aug 24 13:20:09.002: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020433363s
    STEP: Saw pod success 08/24/23 13:20:09.002
    Aug 24 13:20:09.002: INFO: Pod "downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:09.010: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd container client-container: <nil>
    STEP: delete the pod 08/24/23 13:20:09.022
    Aug 24 13:20:09.044: INFO: Waiting for pod downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd to disappear
    Aug 24 13:20:09.048: INFO: Pod downwardapi-volume-14839d71-e505-4ab6-ad1a-87bc8e06debd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:09.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7803" for this suite. 08/24/23 13:20:09.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:09.091
Aug 24 13:20:09.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 13:20:09.094
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:09.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:09.122
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/24/23 13:20:09.128
Aug 24 13:20:09.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d" in namespace "projected-2801" to be "Succeeded or Failed"
Aug 24 13:20:09.147: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73949ms
Aug 24 13:20:11.156: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Running", Reason="", readiness=true. Elapsed: 2.015155227s
Aug 24 13:20:13.156: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Running", Reason="", readiness=false. Elapsed: 4.014571663s
Aug 24 13:20:15.155: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013577579s
STEP: Saw pod success 08/24/23 13:20:15.155
Aug 24 13:20:15.155: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d" satisfied condition "Succeeded or Failed"
Aug 24 13:20:15.162: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d container client-container: <nil>
STEP: delete the pod 08/24/23 13:20:15.175
Aug 24 13:20:15.195: INFO: Waiting for pod downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d to disappear
Aug 24 13:20:15.200: INFO: Pod downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:15.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2801" for this suite. 08/24/23 13:20:15.213
------------------------------
â€¢ [SLOW TEST] [6.132 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:09.091
    Aug 24 13:20:09.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 13:20:09.094
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:09.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:09.122
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/24/23 13:20:09.128
    Aug 24 13:20:09.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d" in namespace "projected-2801" to be "Succeeded or Failed"
    Aug 24 13:20:09.147: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73949ms
    Aug 24 13:20:11.156: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Running", Reason="", readiness=true. Elapsed: 2.015155227s
    Aug 24 13:20:13.156: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Running", Reason="", readiness=false. Elapsed: 4.014571663s
    Aug 24 13:20:15.155: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013577579s
    STEP: Saw pod success 08/24/23 13:20:15.155
    Aug 24 13:20:15.155: INFO: Pod "downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:15.162: INFO: Trying to get logs from node pe9deep4seen-3 pod downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d container client-container: <nil>
    STEP: delete the pod 08/24/23 13:20:15.175
    Aug 24 13:20:15.195: INFO: Waiting for pod downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d to disappear
    Aug 24 13:20:15.200: INFO: Pod downwardapi-volume-297e6edf-252d-4d19-b9e3-044364e1412d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:15.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2801" for this suite. 08/24/23 13:20:15.213
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:15.225
Aug 24 13:20:15.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename job 08/24/23 13:20:15.228
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:15.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:15.261
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/24/23 13:20:15.266
STEP: Ensuring job reaches completions 08/24/23 13:20:15.277
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:29.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8399" for this suite. 08/24/23 13:20:29.299
------------------------------
â€¢ [SLOW TEST] [14.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:15.225
    Aug 24 13:20:15.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename job 08/24/23 13:20:15.228
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:15.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:15.261
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/24/23 13:20:15.266
    STEP: Ensuring job reaches completions 08/24/23 13:20:15.277
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:29.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8399" for this suite. 08/24/23 13:20:29.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:29.317
Aug 24 13:20:29.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename pods 08/24/23 13:20:29.32
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:29.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:29.355
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 24 13:20:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: creating the pod 08/24/23 13:20:29.363
STEP: submitting the pod to kubernetes 08/24/23 13:20:29.364
Aug 24 13:20:29.382: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832" in namespace "pods-6461" to be "running and ready"
Aug 24 13:20:29.389: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07719ms
Aug 24 13:20:29.389: INFO: The phase of Pod pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:20:31.447: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065690012s
Aug 24 13:20:31.448: INFO: The phase of Pod pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:20:33.399: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832": Phase="Running", Reason="", readiness=true. Elapsed: 4.017222199s
Aug 24 13:20:33.399: INFO: The phase of Pod pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832 is Running (Ready = true)
Aug 24 13:20:33.399: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:33.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6461" for this suite. 08/24/23 13:20:33.461
------------------------------
â€¢ [4.155 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:29.317
    Aug 24 13:20:29.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename pods 08/24/23 13:20:29.32
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:29.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:29.355
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 24 13:20:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: creating the pod 08/24/23 13:20:29.363
    STEP: submitting the pod to kubernetes 08/24/23 13:20:29.364
    Aug 24 13:20:29.382: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832" in namespace "pods-6461" to be "running and ready"
    Aug 24 13:20:29.389: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07719ms
    Aug 24 13:20:29.389: INFO: The phase of Pod pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:20:31.447: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065690012s
    Aug 24 13:20:31.448: INFO: The phase of Pod pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:20:33.399: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832": Phase="Running", Reason="", readiness=true. Elapsed: 4.017222199s
    Aug 24 13:20:33.399: INFO: The phase of Pod pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832 is Running (Ready = true)
    Aug 24 13:20:33.399: INFO: Pod "pod-logs-websocket-815ee418-c888-4e63-b09a-90dd61201832" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:33.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6461" for this suite. 08/24/23 13:20:33.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:33.479
Aug 24 13:20:33.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 13:20:33.481
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:33.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:33.512
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-8a0c1a9f-d5cc-45de-a31b-b80d16c7c253 08/24/23 13:20:33.518
STEP: Creating a pod to test consume configMaps 08/24/23 13:20:33.526
Aug 24 13:20:33.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c" in namespace "projected-8062" to be "Succeeded or Failed"
Aug 24 13:20:33.563: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.107368ms
Aug 24 13:20:35.574: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025197834s
Aug 24 13:20:37.573: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024250226s
STEP: Saw pod success 08/24/23 13:20:37.573
Aug 24 13:20:37.573: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c" satisfied condition "Succeeded or Failed"
Aug 24 13:20:37.578: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c container agnhost-container: <nil>
STEP: delete the pod 08/24/23 13:20:37.59
Aug 24 13:20:37.613: INFO: Waiting for pod pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c to disappear
Aug 24 13:20:37.618: INFO: Pod pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:37.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8062" for this suite. 08/24/23 13:20:37.627
------------------------------
â€¢ [4.160 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:33.479
    Aug 24 13:20:33.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 13:20:33.481
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:33.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:33.512
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-8a0c1a9f-d5cc-45de-a31b-b80d16c7c253 08/24/23 13:20:33.518
    STEP: Creating a pod to test consume configMaps 08/24/23 13:20:33.526
    Aug 24 13:20:33.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c" in namespace "projected-8062" to be "Succeeded or Failed"
    Aug 24 13:20:33.563: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.107368ms
    Aug 24 13:20:35.574: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025197834s
    Aug 24 13:20:37.573: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024250226s
    STEP: Saw pod success 08/24/23 13:20:37.573
    Aug 24 13:20:37.573: INFO: Pod "pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:37.578: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 13:20:37.59
    Aug 24 13:20:37.613: INFO: Waiting for pod pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c to disappear
    Aug 24 13:20:37.618: INFO: Pod pod-projected-configmaps-e346f715-c600-4b2e-b2ae-337f0448368c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:37.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8062" for this suite. 08/24/23 13:20:37.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:37.641
Aug 24 13:20:37.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename var-expansion 08/24/23 13:20:37.644
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:37.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:37.673
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/24/23 13:20:37.677
Aug 24 13:20:37.689: INFO: Waiting up to 5m0s for pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4" in namespace "var-expansion-772" to be "Succeeded or Failed"
Aug 24 13:20:37.695: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485546ms
Aug 24 13:20:39.703: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01409494s
Aug 24 13:20:41.704: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014875568s
STEP: Saw pod success 08/24/23 13:20:41.704
Aug 24 13:20:41.705: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4" satisfied condition "Succeeded or Failed"
Aug 24 13:20:41.714: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-c9180157-660c-4f40-847b-5984136de4b4 container dapi-container: <nil>
STEP: delete the pod 08/24/23 13:20:41.732
Aug 24 13:20:41.760: INFO: Waiting for pod var-expansion-c9180157-660c-4f40-847b-5984136de4b4 to disappear
Aug 24 13:20:41.765: INFO: Pod var-expansion-c9180157-660c-4f40-847b-5984136de4b4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:41.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-772" for this suite. 08/24/23 13:20:41.775
------------------------------
â€¢ [4.148 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:37.641
    Aug 24 13:20:37.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename var-expansion 08/24/23 13:20:37.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:37.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:37.673
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/24/23 13:20:37.677
    Aug 24 13:20:37.689: INFO: Waiting up to 5m0s for pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4" in namespace "var-expansion-772" to be "Succeeded or Failed"
    Aug 24 13:20:37.695: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485546ms
    Aug 24 13:20:39.703: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01409494s
    Aug 24 13:20:41.704: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014875568s
    STEP: Saw pod success 08/24/23 13:20:41.704
    Aug 24 13:20:41.705: INFO: Pod "var-expansion-c9180157-660c-4f40-847b-5984136de4b4" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:41.714: INFO: Trying to get logs from node pe9deep4seen-3 pod var-expansion-c9180157-660c-4f40-847b-5984136de4b4 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 13:20:41.732
    Aug 24 13:20:41.760: INFO: Waiting for pod var-expansion-c9180157-660c-4f40-847b-5984136de4b4 to disappear
    Aug 24 13:20:41.765: INFO: Pod var-expansion-c9180157-660c-4f40-847b-5984136de4b4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:41.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-772" for this suite. 08/24/23 13:20:41.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:41.794
Aug 24 13:20:41.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename kubelet-test 08/24/23 13:20:41.797
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:41.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:41.848
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 24 13:20:41.865: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db" in namespace "kubelet-test-4584" to be "running and ready"
Aug 24 13:20:41.874: INFO: Pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.500996ms
Aug 24 13:20:41.874: INFO: The phase of Pod busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db is Pending, waiting for it to be Running (with Ready = true)
Aug 24 13:20:43.884: INFO: Pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db": Phase="Running", Reason="", readiness=true. Elapsed: 2.019229791s
Aug 24 13:20:43.885: INFO: The phase of Pod busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db is Running (Ready = true)
Aug 24 13:20:43.885: INFO: Pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4584" for this suite. 08/24/23 13:20:43.914
------------------------------
â€¢ [2.135 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:41.794
    Aug 24 13:20:41.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 13:20:41.797
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:41.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:41.848
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 24 13:20:41.865: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db" in namespace "kubelet-test-4584" to be "running and ready"
    Aug 24 13:20:41.874: INFO: Pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.500996ms
    Aug 24 13:20:41.874: INFO: The phase of Pod busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 13:20:43.884: INFO: Pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db": Phase="Running", Reason="", readiness=true. Elapsed: 2.019229791s
    Aug 24 13:20:43.885: INFO: The phase of Pod busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db is Running (Ready = true)
    Aug 24 13:20:43.885: INFO: Pod "busybox-readonly-fs0d57b064-b1bf-4e40-b955-86c324e2b2db" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4584" for this suite. 08/24/23 13:20:43.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:43.931
Aug 24 13:20:43.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 13:20:43.933
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:43.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:43.964
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-c1e017cc-2fe3-4740-b03d-1b86315e8447 08/24/23 13:20:43.968
STEP: Creating a pod to test consume secrets 08/24/23 13:20:43.976
Aug 24 13:20:43.991: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766" in namespace "projected-5011" to be "Succeeded or Failed"
Aug 24 13:20:43.998: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163449ms
Aug 24 13:20:46.016: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024111761s
Aug 24 13:20:48.020: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028611996s
STEP: Saw pod success 08/24/23 13:20:48.022
Aug 24 13:20:48.024: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766" satisfied condition "Succeeded or Failed"
Aug 24 13:20:48.034: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 13:20:48.049
Aug 24 13:20:48.068: INFO: Waiting for pod pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766 to disappear
Aug 24 13:20:48.074: INFO: Pod pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5011" for this suite. 08/24/23 13:20:48.087
------------------------------
â€¢ [4.169 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:43.931
    Aug 24 13:20:43.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 13:20:43.933
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:43.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:43.964
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-c1e017cc-2fe3-4740-b03d-1b86315e8447 08/24/23 13:20:43.968
    STEP: Creating a pod to test consume secrets 08/24/23 13:20:43.976
    Aug 24 13:20:43.991: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766" in namespace "projected-5011" to be "Succeeded or Failed"
    Aug 24 13:20:43.998: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163449ms
    Aug 24 13:20:46.016: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024111761s
    Aug 24 13:20:48.020: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028611996s
    STEP: Saw pod success 08/24/23 13:20:48.022
    Aug 24 13:20:48.024: INFO: Pod "pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:48.034: INFO: Trying to get logs from node pe9deep4seen-3 pod pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:20:48.049
    Aug 24 13:20:48.068: INFO: Waiting for pod pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766 to disappear
    Aug 24 13:20:48.074: INFO: Pod pod-projected-secrets-70cf03af-f4cf-435a-a945-011e6633b766 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:48.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5011" for this suite. 08/24/23 13:20:48.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:48.102
Aug 24 13:20:48.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename security-context-test 08/24/23 13:20:48.105
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:48.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:48.134
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 24 13:20:48.154: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c" in namespace "security-context-test-2078" to be "Succeeded or Failed"
Aug 24 13:20:48.161: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.120946ms
Aug 24 13:20:50.171: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017270883s
Aug 24 13:20:52.171: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016444674s
Aug 24 13:20:54.169: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01472697s
Aug 24 13:20:54.169: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:54.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2078" for this suite. 08/24/23 13:20:54.192
------------------------------
â€¢ [SLOW TEST] [6.104 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:48.102
    Aug 24 13:20:48.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename security-context-test 08/24/23 13:20:48.105
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:48.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:48.134
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 24 13:20:48.154: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c" in namespace "security-context-test-2078" to be "Succeeded or Failed"
    Aug 24 13:20:48.161: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.120946ms
    Aug 24 13:20:50.171: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017270883s
    Aug 24 13:20:52.171: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016444674s
    Aug 24 13:20:54.169: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01472697s
    Aug 24 13:20:54.169: INFO: Pod "alpine-nnp-false-172bdf90-9149-4bb8-8da8-0b667366894c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:54.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2078" for this suite. 08/24/23 13:20:54.192
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:54.207
Aug 24 13:20:54.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename downward-api 08/24/23 13:20:54.21
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:54.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:54.252
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/24/23 13:20:54.264
Aug 24 13:20:54.281: INFO: Waiting up to 5m0s for pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df" in namespace "downward-api-5088" to be "Succeeded or Failed"
Aug 24 13:20:54.287: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983488ms
Aug 24 13:20:56.294: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013005182s
Aug 24 13:20:58.294: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012511719s
STEP: Saw pod success 08/24/23 13:20:58.294
Aug 24 13:20:58.294: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df" satisfied condition "Succeeded or Failed"
Aug 24 13:20:58.300: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-8d121c71-8b32-4422-b3ca-39df272c86df container dapi-container: <nil>
STEP: delete the pod 08/24/23 13:20:58.317
Aug 24 13:20:58.337: INFO: Waiting for pod downward-api-8d121c71-8b32-4422-b3ca-39df272c86df to disappear
Aug 24 13:20:58.342: INFO: Pod downward-api-8d121c71-8b32-4422-b3ca-39df272c86df no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 13:20:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5088" for this suite. 08/24/23 13:20:58.351
------------------------------
â€¢ [4.157 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:54.207
    Aug 24 13:20:54.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename downward-api 08/24/23 13:20:54.21
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:54.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:54.252
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/24/23 13:20:54.264
    Aug 24 13:20:54.281: INFO: Waiting up to 5m0s for pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df" in namespace "downward-api-5088" to be "Succeeded or Failed"
    Aug 24 13:20:54.287: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.983488ms
    Aug 24 13:20:56.294: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013005182s
    Aug 24 13:20:58.294: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012511719s
    STEP: Saw pod success 08/24/23 13:20:58.294
    Aug 24 13:20:58.294: INFO: Pod "downward-api-8d121c71-8b32-4422-b3ca-39df272c86df" satisfied condition "Succeeded or Failed"
    Aug 24 13:20:58.300: INFO: Trying to get logs from node pe9deep4seen-3 pod downward-api-8d121c71-8b32-4422-b3ca-39df272c86df container dapi-container: <nil>
    STEP: delete the pod 08/24/23 13:20:58.317
    Aug 24 13:20:58.337: INFO: Waiting for pod downward-api-8d121c71-8b32-4422-b3ca-39df272c86df to disappear
    Aug 24 13:20:58.342: INFO: Pod downward-api-8d121c71-8b32-4422-b3ca-39df272c86df no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:20:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5088" for this suite. 08/24/23 13:20:58.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:20:58.372
Aug 24 13:20:58.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 13:20:58.373
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:58.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:58.41
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-08d03ea5-0a9e-4203-9671-3c92224ff37d 08/24/23 13:20:58.416
STEP: Creating secret with name secret-projected-all-test-volume-99946c94-9d2f-4c90-b1e0-9584a4190f29 08/24/23 13:20:58.425
STEP: Creating a pod to test Check all projections for projected volume plugin 08/24/23 13:20:58.431
Aug 24 13:20:58.445: INFO: Waiting up to 5m0s for pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd" in namespace "projected-958" to be "Succeeded or Failed"
Aug 24 13:20:58.456: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.651599ms
Aug 24 13:21:00.463: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018404821s
Aug 24 13:21:02.465: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020335641s
STEP: Saw pod success 08/24/23 13:21:02.465
Aug 24 13:21:02.466: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd" satisfied condition "Succeeded or Failed"
Aug 24 13:21:02.473: INFO: Trying to get logs from node pe9deep4seen-3 pod projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd container projected-all-volume-test: <nil>
STEP: delete the pod 08/24/23 13:21:02.485
Aug 24 13:21:02.505: INFO: Waiting for pod projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd to disappear
Aug 24 13:21:02.512: INFO: Pod projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 24 13:21:02.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-958" for this suite. 08/24/23 13:21:02.52
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:20:58.372
    Aug 24 13:20:58.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 13:20:58.373
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:20:58.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:20:58.41
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-08d03ea5-0a9e-4203-9671-3c92224ff37d 08/24/23 13:20:58.416
    STEP: Creating secret with name secret-projected-all-test-volume-99946c94-9d2f-4c90-b1e0-9584a4190f29 08/24/23 13:20:58.425
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/24/23 13:20:58.431
    Aug 24 13:20:58.445: INFO: Waiting up to 5m0s for pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd" in namespace "projected-958" to be "Succeeded or Failed"
    Aug 24 13:20:58.456: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.651599ms
    Aug 24 13:21:00.463: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018404821s
    Aug 24 13:21:02.465: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020335641s
    STEP: Saw pod success 08/24/23 13:21:02.465
    Aug 24 13:21:02.466: INFO: Pod "projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd" satisfied condition "Succeeded or Failed"
    Aug 24 13:21:02.473: INFO: Trying to get logs from node pe9deep4seen-3 pod projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd container projected-all-volume-test: <nil>
    STEP: delete the pod 08/24/23 13:21:02.485
    Aug 24 13:21:02.505: INFO: Waiting for pod projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd to disappear
    Aug 24 13:21:02.512: INFO: Pod projected-volume-b27781ee-3226-45a0-9bb5-abd977a4eddd no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:21:02.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-958" for this suite. 08/24/23 13:21:02.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:21:02.537
Aug 24 13:21:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 13:21:02.539
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:02.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:02.571
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 24 13:21:02.600: INFO: created pod
Aug 24 13:21:02.600: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6889" to be "Succeeded or Failed"
Aug 24 13:21:02.607: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701185ms
Aug 24 13:21:04.620: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019793588s
Aug 24 13:21:06.617: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017045463s
Aug 24 13:21:08.617: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016739893s
STEP: Saw pod success 08/24/23 13:21:08.617
Aug 24 13:21:08.617: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 24 13:21:38.618: INFO: polling logs
Aug 24 13:21:38.634: INFO: Pod logs: 
I0824 13:21:03.720044       1 log.go:198] OK: Got token
I0824 13:21:03.720310       1 log.go:198] validating with in-cluster discovery
I0824 13:21:03.722207       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0824 13:21:03.722279       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6889:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692883862, NotBefore:1692883262, IssuedAt:1692883262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6889", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ef028ed8-2f14-4257-b226-5c83301a9727"}}}
I0824 13:21:03.755495       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0824 13:21:03.765996       1 log.go:198] OK: Validated signature on JWT
I0824 13:21:03.766193       1 log.go:198] OK: Got valid claims from token!
I0824 13:21:03.766240       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6889:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692883862, NotBefore:1692883262, IssuedAt:1692883262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6889", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ef028ed8-2f14-4257-b226-5c83301a9727"}}}

Aug 24 13:21:38.634: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 13:21:38.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6889" for this suite. 08/24/23 13:21:38.655
------------------------------
â€¢ [SLOW TEST] [36.130 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:21:02.537
    Aug 24 13:21:02.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 13:21:02.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:02.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:02.571
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 24 13:21:02.600: INFO: created pod
    Aug 24 13:21:02.600: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6889" to be "Succeeded or Failed"
    Aug 24 13:21:02.607: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701185ms
    Aug 24 13:21:04.620: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019793588s
    Aug 24 13:21:06.617: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017045463s
    Aug 24 13:21:08.617: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016739893s
    STEP: Saw pod success 08/24/23 13:21:08.617
    Aug 24 13:21:08.617: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 24 13:21:38.618: INFO: polling logs
    Aug 24 13:21:38.634: INFO: Pod logs: 
    I0824 13:21:03.720044       1 log.go:198] OK: Got token
    I0824 13:21:03.720310       1 log.go:198] validating with in-cluster discovery
    I0824 13:21:03.722207       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0824 13:21:03.722279       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6889:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692883862, NotBefore:1692883262, IssuedAt:1692883262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6889", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ef028ed8-2f14-4257-b226-5c83301a9727"}}}
    I0824 13:21:03.755495       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0824 13:21:03.765996       1 log.go:198] OK: Validated signature on JWT
    I0824 13:21:03.766193       1 log.go:198] OK: Got valid claims from token!
    I0824 13:21:03.766240       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6889:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692883862, NotBefore:1692883262, IssuedAt:1692883262, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6889", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ef028ed8-2f14-4257-b226-5c83301a9727"}}}

    Aug 24 13:21:38.634: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:21:38.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6889" for this suite. 08/24/23 13:21:38.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:21:38.671
Aug 24 13:21:38.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename svcaccounts 08/24/23 13:21:38.673
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:38.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:38.706
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 24 13:21:38.735: INFO: created pod pod-service-account-defaultsa
Aug 24 13:21:38.736: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 24 13:21:38.749: INFO: created pod pod-service-account-mountsa
Aug 24 13:21:38.749: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 24 13:21:38.766: INFO: created pod pod-service-account-nomountsa
Aug 24 13:21:38.766: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 24 13:21:38.779: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 24 13:21:38.780: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 24 13:21:38.791: INFO: created pod pod-service-account-mountsa-mountspec
Aug 24 13:21:38.792: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 24 13:21:38.802: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 24 13:21:38.802: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 24 13:21:38.832: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 24 13:21:38.832: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 24 13:21:38.850: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 24 13:21:38.850: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 24 13:21:38.914: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 24 13:21:38.914: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 13:21:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8786" for this suite. 08/24/23 13:21:38.992
------------------------------
â€¢ [0.369 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:21:38.671
    Aug 24 13:21:38.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 13:21:38.673
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:38.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:38.706
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 24 13:21:38.735: INFO: created pod pod-service-account-defaultsa
    Aug 24 13:21:38.736: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 24 13:21:38.749: INFO: created pod pod-service-account-mountsa
    Aug 24 13:21:38.749: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 24 13:21:38.766: INFO: created pod pod-service-account-nomountsa
    Aug 24 13:21:38.766: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 24 13:21:38.779: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 24 13:21:38.780: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 24 13:21:38.791: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 24 13:21:38.792: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 24 13:21:38.802: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 24 13:21:38.802: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 24 13:21:38.832: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 24 13:21:38.832: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 24 13:21:38.850: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 24 13:21:38.850: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 24 13:21:38.914: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 24 13:21:38.914: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:21:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8786" for this suite. 08/24/23 13:21:38.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:21:39.042
Aug 24 13:21:39.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename events 08/24/23 13:21:39.051
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:39.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:39.182
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/24/23 13:21:39.223
STEP: get a list of Events with a label in the current namespace 08/24/23 13:21:39.606
STEP: delete a list of events 08/24/23 13:21:39.814
Aug 24 13:21:39.815: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/24/23 13:21:39.957
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 24 13:21:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4554" for this suite. 08/24/23 13:21:40.115
------------------------------
â€¢ [1.091 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:21:39.042
    Aug 24 13:21:39.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename events 08/24/23 13:21:39.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:39.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:39.182
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/24/23 13:21:39.223
    STEP: get a list of Events with a label in the current namespace 08/24/23 13:21:39.606
    STEP: delete a list of events 08/24/23 13:21:39.814
    Aug 24 13:21:39.815: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/24/23 13:21:39.957
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:21:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4554" for this suite. 08/24/23 13:21:40.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:21:40.251
Aug 24 13:21:40.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename projected 08/24/23 13:21:40.255
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:40.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:40.3
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/24/23 13:21:40.314
Aug 24 13:21:40.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6" in namespace "projected-2124" to be "Succeeded or Failed"
Aug 24 13:21:40.374: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6": Phase="Pending", Reason="", readiness=false. Elapsed: 27.827302ms
Aug 24 13:21:42.388: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042000862s
Aug 24 13:21:44.385: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039157323s
STEP: Saw pod success 08/24/23 13:21:44.385
Aug 24 13:21:44.385: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6" satisfied condition "Succeeded or Failed"
Aug 24 13:21:44.391: INFO: Trying to get logs from node pe9deep4seen-2 pod downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6 container client-container: <nil>
STEP: delete the pod 08/24/23 13:21:44.43
Aug 24 13:21:44.455: INFO: Waiting for pod downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6 to disappear
Aug 24 13:21:44.462: INFO: Pod downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 13:21:44.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2124" for this suite. 08/24/23 13:21:44.47
------------------------------
â€¢ [4.237 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:21:40.251
    Aug 24 13:21:40.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename projected 08/24/23 13:21:40.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:40.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:40.3
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/24/23 13:21:40.314
    Aug 24 13:21:40.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6" in namespace "projected-2124" to be "Succeeded or Failed"
    Aug 24 13:21:40.374: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6": Phase="Pending", Reason="", readiness=false. Elapsed: 27.827302ms
    Aug 24 13:21:42.388: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042000862s
    Aug 24 13:21:44.385: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039157323s
    STEP: Saw pod success 08/24/23 13:21:44.385
    Aug 24 13:21:44.385: INFO: Pod "downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6" satisfied condition "Succeeded or Failed"
    Aug 24 13:21:44.391: INFO: Trying to get logs from node pe9deep4seen-2 pod downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6 container client-container: <nil>
    STEP: delete the pod 08/24/23 13:21:44.43
    Aug 24 13:21:44.455: INFO: Waiting for pod downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6 to disappear
    Aug 24 13:21:44.462: INFO: Pod downwardapi-volume-7ddd0f9e-b645-469d-a906-b90138fa08e6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:21:44.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2124" for this suite. 08/24/23 13:21:44.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 13:21:44.492
Aug 24 13:21:44.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
STEP: Building a namespace api object, basename disruption 08/24/23 13:21:44.495
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:44.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:44.565
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/24/23 13:21:44.585
STEP: Waiting for all pods to be running 08/24/23 13:21:46.664
Aug 24 13:21:46.724: INFO: running pods: 0 < 3
Aug 24 13:21:48.783: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 13:21:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1681" for this suite. 08/24/23 13:21:50.752
------------------------------
â€¢ [SLOW TEST] [6.272 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 13:21:44.492
    Aug 24 13:21:44.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2729572383
    STEP: Building a namespace api object, basename disruption 08/24/23 13:21:44.495
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 13:21:44.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 13:21:44.565
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/24/23 13:21:44.585
    STEP: Waiting for all pods to be running 08/24/23 13:21:46.664
    Aug 24 13:21:46.724: INFO: running pods: 0 < 3
    Aug 24 13:21:48.783: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 13:21:50.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1681" for this suite. 08/24/23 13:21:50.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 24 13:21:50.777: INFO: Running AfterSuite actions on node 1
Aug 24 13:21:50.777: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 24 13:21:50.777: INFO: Running AfterSuite actions on node 1
    Aug 24 13:21:50.777: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.188 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6157.505 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h42m38.41461006s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

